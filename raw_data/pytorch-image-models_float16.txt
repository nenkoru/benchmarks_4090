Running bulk validation on these pretrained models: bat_resnext26ts.ch_in1k, beit_base_patch16_224.in22k_ft_in22k, beit_base_patch16_224.in22k_ft_in22k_in1k, beit_base_patch16_384.in22k_ft_in22k_in1k, beit_large_patch16_224.in22k_ft_in22k, beit_large_patch16_224.in22k_ft_in22k_in1k, beit_large_patch16_384.in22k_ft_in22k_in1k, beit_large_patch16_512.in22k_ft_in22k_in1k, beitv2_base_patch16_224.in1k_ft_in1k, beitv2_base_patch16_224.in1k_ft_in22k, beitv2_base_patch16_224.in1k_ft_in22k_in1k, beitv2_large_patch16_224.in1k_ft_in1k, beitv2_large_patch16_224.in1k_ft_in22k, beitv2_large_patch16_224.in1k_ft_in22k_in1k, botnet26t_256.c1_in1k, caformer_b36.sail_in1k, caformer_b36.sail_in1k_384, caformer_b36.sail_in22k, caformer_b36.sail_in22k_ft_in1k, caformer_b36.sail_in22k_ft_in1k_384, caformer_m36.sail_in1k, caformer_m36.sail_in1k_384, caformer_m36.sail_in22k, caformer_m36.sail_in22k_ft_in1k, caformer_m36.sail_in22k_ft_in1k_384, caformer_s18.sail_in1k, caformer_s18.sail_in1k_384, caformer_s18.sail_in22k, caformer_s18.sail_in22k_ft_in1k, caformer_s18.sail_in22k_ft_in1k_384, caformer_s36.sail_in1k, caformer_s36.sail_in1k_384, caformer_s36.sail_in22k, caformer_s36.sail_in22k_ft_in1k, caformer_s36.sail_in22k_ft_in1k_384, cait_m36_384.fb_dist_in1k, cait_m48_448.fb_dist_in1k, cait_s24_224.fb_dist_in1k, cait_s24_384.fb_dist_in1k, cait_s36_384.fb_dist_in1k, cait_xs24_384.fb_dist_in1k, cait_xxs24_224.fb_dist_in1k, cait_xxs24_384.fb_dist_in1k, cait_xxs36_224.fb_dist_in1k, cait_xxs36_384.fb_dist_in1k, coat_lite_medium.in1k, coat_lite_medium_384.in1k, coat_lite_mini.in1k, coat_lite_small.in1k, coat_lite_tiny.in1k, coat_mini.in1k, coat_small.in1k, coat_tiny.in1k, coatnet_0_rw_224.sw_in1k, coatnet_1_rw_224.sw_in1k, coatnet_2_rw_224.sw_in12k, coatnet_2_rw_224.sw_in12k_ft_in1k, coatnet_3_rw_224.sw_in12k, coatnet_bn_0_rw_224.sw_in1k, coatnet_nano_rw_224.sw_in1k, coatnet_rmlp_1_rw2_224.sw_in12k, coatnet_rmlp_1_rw2_224.sw_in12k_ft_in1k, coatnet_rmlp_1_rw_224.sw_in1k, coatnet_rmlp_2_rw_224.sw_in1k, coatnet_rmlp_2_rw_224.sw_in12k, coatnet_rmlp_2_rw_224.sw_in12k_ft_in1k, coatnet_rmlp_2_rw_384.sw_in12k_ft_in1k, coatnet_rmlp_nano_rw_224.sw_in1k, coatnext_nano_rw_224.sw_in1k, convformer_b36.sail_in1k, convformer_b36.sail_in1k_384, convformer_b36.sail_in22k, convformer_b36.sail_in22k_ft_in1k, convformer_b36.sail_in22k_ft_in1k_384, convformer_m36.sail_in1k, convformer_m36.sail_in1k_384, convformer_m36.sail_in22k, convformer_m36.sail_in22k_ft_in1k, convformer_m36.sail_in22k_ft_in1k_384, convformer_s18.sail_in1k, convformer_s18.sail_in1k_384, convformer_s18.sail_in22k, convformer_s18.sail_in22k_ft_in1k, convformer_s18.sail_in22k_ft_in1k_384, convformer_s36.sail_in1k, convformer_s36.sail_in1k_384, convformer_s36.sail_in22k, convformer_s36.sail_in22k_ft_in1k, convformer_s36.sail_in22k_ft_in1k_384, convit_base.fb_in1k, convit_small.fb_in1k, convit_tiny.fb_in1k, convmixer_768_32.in1k, convmixer_1024_20_ks9_p14.in1k, convmixer_1536_20.in1k, convnext_atto.d2_in1k, convnext_atto_ols.a2_in1k, convnext_base.clip_laion2b, convnext_base.clip_laion2b_augreg, convnext_base.clip_laion2b_augreg_ft_in1k, convnext_base.clip_laion2b_augreg_ft_in12k, convnext_base.clip_laion2b_augreg_ft_in12k_in1k, convnext_base.clip_laion2b_augreg_ft_in12k_in1k_384, convnext_base.clip_laiona, convnext_base.clip_laiona_320, convnext_base.clip_laiona_augreg_320, convnext_base.clip_laiona_augreg_ft_in1k_384, convnext_base.fb_in1k, convnext_base.fb_in22k, convnext_base.fb_in22k_ft_in1k, convnext_base.fb_in22k_ft_in1k_384, convnext_femto.d1_in1k, convnext_femto_ols.d1_in1k, convnext_large.fb_in1k, convnext_large.fb_in22k, convnext_large.fb_in22k_ft_in1k, convnext_large.fb_in22k_ft_in1k_384, convnext_large_mlp.clip_laion2b_augreg, convnext_large_mlp.clip_laion2b_augreg_ft_in1k, convnext_large_mlp.clip_laion2b_augreg_ft_in1k_384, convnext_large_mlp.clip_laion2b_augreg_ft_in12k_384, convnext_large_mlp.clip_laion2b_ft_320, convnext_large_mlp.clip_laion2b_ft_soup_320, convnext_large_mlp.clip_laion2b_soup_ft_in12k_320, convnext_large_mlp.clip_laion2b_soup_ft_in12k_384, convnext_large_mlp.clip_laion2b_soup_ft_in12k_in1k_320, convnext_large_mlp.clip_laion2b_soup_ft_in12k_in1k_384, convnext_nano.d1h_in1k, convnext_nano.in12k, convnext_nano.in12k_ft_in1k, convnext_nano_ols.d1h_in1k, convnext_pico.d1_in1k, convnext_pico_ols.d1_in1k, convnext_small.fb_in1k, convnext_small.fb_in22k, convnext_small.fb_in22k_ft_in1k, convnext_small.fb_in22k_ft_in1k_384, convnext_small.in12k, convnext_small.in12k_ft_in1k, convnext_small.in12k_ft_in1k_384, convnext_tiny.fb_in1k, convnext_tiny.fb_in22k, convnext_tiny.fb_in22k_ft_in1k, convnext_tiny.fb_in22k_ft_in1k_384, convnext_tiny.in12k, convnext_tiny.in12k_ft_in1k, convnext_tiny.in12k_ft_in1k_384, convnext_tiny_hnf.a2h_in1k, convnext_xlarge.fb_in22k, convnext_xlarge.fb_in22k_ft_in1k, convnext_xlarge.fb_in22k_ft_in1k_384, convnext_xxlarge.clip_laion2b_rewind, convnext_xxlarge.clip_laion2b_soup, convnext_xxlarge.clip_laion2b_soup_ft_in1k, convnextv2_atto.fcmae, convnextv2_atto.fcmae_ft_in1k, convnextv2_base.fcmae, convnextv2_base.fcmae_ft_in1k, convnextv2_base.fcmae_ft_in22k_in1k, convnextv2_base.fcmae_ft_in22k_in1k_384, convnextv2_femto.fcmae, convnextv2_femto.fcmae_ft_in1k, convnextv2_huge.fcmae, convnextv2_huge.fcmae_ft_in1k, convnextv2_huge.fcmae_ft_in22k_in1k_384, convnextv2_huge.fcmae_ft_in22k_in1k_512, convnextv2_large.fcmae, convnextv2_large.fcmae_ft_in1k, convnextv2_large.fcmae_ft_in22k_in1k, convnextv2_large.fcmae_ft_in22k_in1k_384, convnextv2_nano.fcmae, convnextv2_nano.fcmae_ft_in1k, convnextv2_nano.fcmae_ft_in22k_in1k, convnextv2_nano.fcmae_ft_in22k_in1k_384, convnextv2_pico.fcmae, convnextv2_pico.fcmae_ft_in1k, convnextv2_tiny.fcmae, convnextv2_tiny.fcmae_ft_in1k, convnextv2_tiny.fcmae_ft_in22k_in1k, convnextv2_tiny.fcmae_ft_in22k_in1k_384, crossvit_9_240.in1k, crossvit_9_dagger_240.in1k, crossvit_15_240.in1k, crossvit_15_dagger_240.in1k, crossvit_15_dagger_408.in1k, crossvit_18_240.in1k, crossvit_18_dagger_240.in1k, crossvit_18_dagger_408.in1k, crossvit_base_240.in1k, crossvit_small_240.in1k, crossvit_tiny_240.in1k, cs3darknet_focus_l.c2ns_in1k, cs3darknet_focus_m.c2ns_in1k, cs3darknet_l.c2ns_in1k, cs3darknet_m.c2ns_in1k, cs3darknet_x.c2ns_in1k, cs3edgenet_x.c2_in1k, cs3se_edgenet_x.c2ns_in1k, cs3sedarknet_l.c2ns_in1k, cs3sedarknet_x.c2ns_in1k, cspdarknet53.ra_in1k, cspresnet50.ra_in1k, cspresnext50.ra_in1k, darknet53.c2ns_in1k, darknetaa53.c2ns_in1k, davit_base.msft_in1k, davit_small.msft_in1k, davit_tiny.msft_in1k, deit3_base_patch16_224.fb_in1k, deit3_base_patch16_224.fb_in22k_ft_in1k, deit3_base_patch16_384.fb_in1k, deit3_base_patch16_384.fb_in22k_ft_in1k, deit3_huge_patch14_224.fb_in1k, deit3_huge_patch14_224.fb_in22k_ft_in1k, deit3_large_patch16_224.fb_in1k, deit3_large_patch16_224.fb_in22k_ft_in1k, deit3_large_patch16_384.fb_in1k, deit3_large_patch16_384.fb_in22k_ft_in1k, deit3_medium_patch16_224.fb_in1k, deit3_medium_patch16_224.fb_in22k_ft_in1k, deit3_small_patch16_224.fb_in1k, deit3_small_patch16_224.fb_in22k_ft_in1k, deit3_small_patch16_384.fb_in1k, deit3_small_patch16_384.fb_in22k_ft_in1k, deit_base_distilled_patch16_224.fb_in1k, deit_base_distilled_patch16_384.fb_in1k, deit_base_patch16_224.fb_in1k, deit_base_patch16_384.fb_in1k, deit_small_distilled_patch16_224.fb_in1k, deit_small_patch16_224.fb_in1k, deit_tiny_distilled_patch16_224.fb_in1k, deit_tiny_patch16_224.fb_in1k, densenet121.ra_in1k, densenet121.tv_in1k, densenet161.tv_in1k, densenet169.tv_in1k, densenet201.tv_in1k, densenetblur121d.ra_in1k, dla34.in1k, dla46_c.in1k, dla46x_c.in1k, dla60.in1k, dla60_res2net.in1k, dla60_res2next.in1k, dla60x.in1k, dla60x_c.in1k, dla102.in1k, dla102x2.in1k, dla102x.in1k, dla169.in1k, dm_nfnet_f0.dm_in1k, dm_nfnet_f1.dm_in1k, dm_nfnet_f2.dm_in1k, dm_nfnet_f3.dm_in1k, dm_nfnet_f4.dm_in1k, dm_nfnet_f5.dm_in1k, dm_nfnet_f6.dm_in1k, dpn68.mx_in1k, dpn68b.mx_in1k, dpn68b.ra_in1k, dpn92.mx_in1k, dpn98.mx_in1k, dpn107.mx_in1k, dpn131.mx_in1k, eca_botnext26ts_256.c1_in1k, eca_halonext26ts.c1_in1k, eca_nfnet_l0.ra2_in1k, eca_nfnet_l1.ra2_in1k, eca_nfnet_l2.ra3_in1k, eca_resnet33ts.ra2_in1k, eca_resnext26ts.ch_in1k, ecaresnet26t.ra2_in1k, ecaresnet50d.miil_in1k, ecaresnet50d_pruned.miil_in1k, ecaresnet50t.a1_in1k, ecaresnet50t.a2_in1k, ecaresnet50t.a3_in1k, ecaresnet50t.ra2_in1k, ecaresnet101d.miil_in1k, ecaresnet101d_pruned.miil_in1k, ecaresnet269d.ra2_in1k, ecaresnetlight.miil_in1k, edgenext_base.in21k_ft_in1k, edgenext_base.usi_in1k, edgenext_small.usi_in1k, edgenext_small_rw.sw_in1k, edgenext_x_small.in1k, edgenext_xx_small.in1k, efficientformer_l1.snap_dist_in1k, efficientformer_l3.snap_dist_in1k, efficientformer_l7.snap_dist_in1k, efficientformerv2_l.snap_dist_in1k, efficientformerv2_s0.snap_dist_in1k, efficientformerv2_s1.snap_dist_in1k, efficientformerv2_s2.snap_dist_in1k, efficientnet_b0.ra_in1k, efficientnet_b1.ft_in1k, efficientnet_b1_pruned.in1k, efficientnet_b2.ra_in1k, efficientnet_b2_pruned.in1k, efficientnet_b3.ra2_in1k, efficientnet_b3_pruned.in1k, efficientnet_b4.ra2_in1k, efficientnet_b5.sw_in12k, efficientnet_b5.sw_in12k_ft_in1k, efficientnet_el.ra_in1k, efficientnet_el_pruned.in1k, efficientnet_em.ra2_in1k, efficientnet_es.ra_in1k, efficientnet_es_pruned.in1k, efficientnet_lite0.ra_in1k, efficientnetv2_rw_m.agc_in1k, efficientnetv2_rw_s.ra2_in1k, efficientnetv2_rw_t.ra2_in1k, ese_vovnet19b_dw.ra_in1k, ese_vovnet39b.ra_in1k, eva02_base_patch14_224.mim_in22k, eva02_base_patch14_448.mim_in22k_ft_in1k, eva02_base_patch14_448.mim_in22k_ft_in22k, eva02_base_patch14_448.mim_in22k_ft_in22k_in1k, eva02_base_patch16_clip_224.merged2b, eva02_enormous_patch14_clip_224.laion2b, eva02_enormous_patch14_clip_224.laion2b_plus, eva02_large_patch14_224.mim_in22k, eva02_large_patch14_224.mim_m38m, eva02_large_patch14_448.mim_in22k_ft_in1k, eva02_large_patch14_448.mim_in22k_ft_in22k, eva02_large_patch14_448.mim_in22k_ft_in22k_in1k, eva02_large_patch14_448.mim_m38m_ft_in1k, eva02_large_patch14_448.mim_m38m_ft_in22k, eva02_large_patch14_448.mim_m38m_ft_in22k_in1k, eva02_large_patch14_clip_224.merged2b, eva02_large_patch14_clip_336.merged2b, eva02_small_patch14_224.mim_in22k, eva02_small_patch14_336.mim_in22k_ft_in1k, eva02_tiny_patch14_224.mim_in22k, eva02_tiny_patch14_336.mim_in22k_ft_in1k, eva_giant_patch14_224.clip_ft_in1k, eva_giant_patch14_336.clip_ft_in1k, eva_giant_patch14_336.m30m_ft_in22k_in1k, eva_giant_patch14_560.m30m_ft_in22k_in1k, eva_giant_patch14_clip_224.laion400m, eva_giant_patch14_clip_224.merged2b, eva_large_patch14_196.in22k_ft_in1k, eva_large_patch14_196.in22k_ft_in22k_in1k, eva_large_patch14_336.in22k_ft_in1k, eva_large_patch14_336.in22k_ft_in22k_in1k, fbnetc_100.rmsp_in1k, fbnetv3_b.ra2_in1k, fbnetv3_d.ra2_in1k, fbnetv3_g.ra2_in1k, flexivit_base.300ep_in1k, flexivit_base.600ep_in1k, flexivit_base.1200ep_in1k, flexivit_large.300ep_in1k, flexivit_large.600ep_in1k, flexivit_large.1200ep_in1k, flexivit_small.300ep_in1k, flexivit_small.600ep_in1k, flexivit_small.1200ep_in1k, focalnet_base_lrf.ms_in1k, focalnet_base_srf.ms_in1k, focalnet_huge_fl3.ms_in22k, focalnet_huge_fl4.ms_in22k, focalnet_large_fl3.ms_in22k, focalnet_large_fl4.ms_in22k, focalnet_small_lrf.ms_in1k, focalnet_small_srf.ms_in1k, focalnet_tiny_lrf.ms_in1k, focalnet_tiny_srf.ms_in1k, focalnet_xlarge_fl3.ms_in22k, focalnet_xlarge_fl4.ms_in22k, gc_efficientnetv2_rw_t.agc_in1k, gcresnet33ts.ra2_in1k, gcresnet50t.ra2_in1k, gcresnext26ts.ch_in1k, gcresnext50ts.ch_in1k, gcvit_base.in1k, gcvit_small.in1k, gcvit_tiny.in1k, gcvit_xtiny.in1k, gcvit_xxtiny.in1k, gernet_l.idstcv_in1k, gernet_m.idstcv_in1k, gernet_s.idstcv_in1k, ghostnet_100.in1k, gmixer_24_224.ra3_in1k, gmlp_s16_224.ra3_in1k, halo2botnet50ts_256.a1h_in1k, halonet26t.a1h_in1k, halonet50ts.a1h_in1k, haloregnetz_b.ra3_in1k, hardcorenas_a.miil_green_in1k, hardcorenas_b.miil_green_in1k, hardcorenas_c.miil_green_in1k, hardcorenas_d.miil_green_in1k, hardcorenas_e.miil_green_in1k, hardcorenas_f.miil_green_in1k, hrnet_w18.ms_aug_in1k, hrnet_w18.ms_in1k, hrnet_w18_small.ms_in1k, hrnet_w18_small_v2.ms_in1k, hrnet_w18_ssld.paddle_in1k, hrnet_w30.ms_in1k, hrnet_w32.ms_in1k, hrnet_w40.ms_in1k, hrnet_w44.ms_in1k, hrnet_w48.ms_in1k, hrnet_w48_ssld.paddle_in1k, hrnet_w64.ms_in1k, inception_resnet_v2.tf_ens_adv_in1k, inception_resnet_v2.tf_in1k, inception_v3.gluon_in1k, inception_v3.tf_adv_in1k, inception_v3.tf_in1k, inception_v3.tv_in1k, inception_v4.tf_in1k, lambda_resnet26rpt_256.c1_in1k, lambda_resnet26t.c1_in1k, lambda_resnet50ts.a1h_in1k, lamhalobotnet50ts_256.a1h_in1k, lcnet_050.ra2_in1k, lcnet_075.ra2_in1k, lcnet_100.ra2_in1k, legacy_senet154.in1k, legacy_seresnet18.in1k, legacy_seresnet34.in1k, legacy_seresnet50.in1k, legacy_seresnet101.in1k, legacy_seresnet152.in1k, legacy_seresnext26_32x4d.in1k, legacy_seresnext50_32x4d.in1k, legacy_seresnext101_32x4d.in1k, legacy_xception.tf_in1k, levit_128.fb_dist_in1k, levit_128s.fb_dist_in1k, levit_192.fb_dist_in1k, levit_256.fb_dist_in1k, levit_384.fb_dist_in1k, levit_conv_128.fb_dist_in1k, levit_conv_128s.fb_dist_in1k, levit_conv_192.fb_dist_in1k, levit_conv_256.fb_dist_in1k, levit_conv_384.fb_dist_in1k, maxvit_base_tf_224.in1k, maxvit_base_tf_384.in1k, maxvit_base_tf_384.in21k_ft_in1k, maxvit_base_tf_512.in1k, maxvit_base_tf_512.in21k_ft_in1k, maxvit_large_tf_224.in1k, maxvit_large_tf_384.in1k, maxvit_large_tf_384.in21k_ft_in1k, maxvit_large_tf_512.in1k, maxvit_large_tf_512.in21k_ft_in1k, maxvit_nano_rw_256.sw_in1k, maxvit_rmlp_base_rw_224.sw_in12k, maxvit_rmlp_base_rw_224.sw_in12k_ft_in1k, maxvit_rmlp_base_rw_384.sw_in12k_ft_in1k, maxvit_rmlp_nano_rw_256.sw_in1k, maxvit_rmlp_pico_rw_256.sw_in1k, maxvit_rmlp_small_rw_224.sw_in1k, maxvit_rmlp_tiny_rw_256.sw_in1k, maxvit_small_tf_224.in1k, maxvit_small_tf_384.in1k, maxvit_small_tf_512.in1k, maxvit_tiny_rw_224.sw_in1k, maxvit_tiny_tf_224.in1k, maxvit_tiny_tf_384.in1k, maxvit_tiny_tf_512.in1k, maxvit_xlarge_tf_384.in21k_ft_in1k, maxvit_xlarge_tf_512.in21k_ft_in1k, maxxvit_rmlp_nano_rw_256.sw_in1k, maxxvit_rmlp_small_rw_256.sw_in1k, maxxvitv2_nano_rw_256.sw_in1k, maxxvitv2_rmlp_base_rw_224.sw_in12k, maxxvitv2_rmlp_base_rw_224.sw_in12k_ft_in1k, maxxvitv2_rmlp_base_rw_384.sw_in12k_ft_in1k, mixer_b16_224.goog_in21k_ft_in1k, mixer_b16_224.miil_in21k_ft_in1k, mixer_l16_224.goog_in21k_ft_in1k, mixnet_l.ft_in1k, mixnet_m.ft_in1k, mixnet_s.ft_in1k, mixnet_xl.ra_in1k, mnasnet_100.rmsp_in1k, mnasnet_small.lamb_in1k, mobilenetv2_050.lamb_in1k, mobilenetv2_100.ra_in1k, mobilenetv2_110d.ra_in1k, mobilenetv2_120d.ra_in1k, mobilenetv2_140.ra_in1k, mobilenetv3_large_100.miil_in21k_ft_in1k, mobilenetv3_large_100.ra_in1k, mobilenetv3_rw.rmsp_in1k, mobilenetv3_small_050.lamb_in1k, mobilenetv3_small_075.lamb_in1k, mobilenetv3_small_100.lamb_in1k, mobilevit_s.cvnets_in1k, mobilevit_xs.cvnets_in1k, mobilevit_xxs.cvnets_in1k, mobilevitv2_050.cvnets_in1k, mobilevitv2_075.cvnets_in1k, mobilevitv2_100.cvnets_in1k, mobilevitv2_125.cvnets_in1k, mobilevitv2_150.cvnets_in1k, mobilevitv2_150.cvnets_in22k_ft_in1k, mobilevitv2_150.cvnets_in22k_ft_in1k_384, mobilevitv2_175.cvnets_in1k, mobilevitv2_175.cvnets_in22k_ft_in1k, mobilevitv2_175.cvnets_in22k_ft_in1k_384, mobilevitv2_200.cvnets_in1k, mobilevitv2_200.cvnets_in22k_ft_in1k, mobilevitv2_200.cvnets_in22k_ft_in1k_384, mvitv2_base.fb_in1k, mvitv2_base_cls.fb_inw21k, mvitv2_huge_cls.fb_inw21k, mvitv2_large.fb_in1k, mvitv2_large_cls.fb_inw21k, mvitv2_small.fb_in1k, mvitv2_tiny.fb_in1k, nasnetalarge.tf_in1k, nest_base_jx.goog_in1k, nest_small_jx.goog_in1k, nest_tiny_jx.goog_in1k, nf_regnet_b1.ra2_in1k, nf_resnet50.ra2_in1k, nfnet_l0.ra2_in1k, pit_b_224.in1k, pit_b_distilled_224.in1k, pit_s_224.in1k, pit_s_distilled_224.in1k, pit_ti_224.in1k, pit_ti_distilled_224.in1k, pit_xs_224.in1k, pit_xs_distilled_224.in1k, pnasnet5large.tf_in1k, poolformer_m36.sail_in1k, poolformer_m48.sail_in1k, poolformer_s12.sail_in1k, poolformer_s24.sail_in1k, poolformer_s36.sail_in1k, poolformerv2_m36.sail_in1k, poolformerv2_m48.sail_in1k, poolformerv2_s12.sail_in1k, poolformerv2_s24.sail_in1k, poolformerv2_s36.sail_in1k, pvt_v2_b0.in1k, pvt_v2_b1.in1k, pvt_v2_b2.in1k, pvt_v2_b2_li.in1k, pvt_v2_b3.in1k, pvt_v2_b4.in1k, pvt_v2_b5.in1k, regnetv_040.ra3_in1k, regnetv_064.ra3_in1k, regnetx_002.pycls_in1k, regnetx_004.pycls_in1k, regnetx_004_tv.tv2_in1k, regnetx_006.pycls_in1k, regnetx_008.pycls_in1k, regnetx_008.tv2_in1k, regnetx_016.pycls_in1k, regnetx_016.tv2_in1k, regnetx_032.pycls_in1k, regnetx_032.tv2_in1k, regnetx_040.pycls_in1k, regnetx_064.pycls_in1k, regnetx_080.pycls_in1k, regnetx_080.tv2_in1k, regnetx_120.pycls_in1k, regnetx_160.pycls_in1k, regnetx_160.tv2_in1k, regnetx_320.pycls_in1k, regnetx_320.tv2_in1k, regnety_002.pycls_in1k, regnety_004.pycls_in1k, regnety_004.tv2_in1k, regnety_006.pycls_in1k, regnety_008.pycls_in1k, regnety_008_tv.tv2_in1k, regnety_016.pycls_in1k, regnety_016.tv2_in1k, regnety_032.pycls_in1k, regnety_032.ra_in1k, regnety_032.tv2_in1k, regnety_040.pycls_in1k, regnety_040.ra3_in1k, regnety_064.pycls_in1k, regnety_064.ra3_in1k, regnety_080.pycls_in1k, regnety_080.ra3_in1k, regnety_080_tv.tv2_in1k, regnety_120.pycls_in1k, regnety_120.sw_in12k, regnety_120.sw_in12k_ft_in1k, regnety_160.deit_in1k, regnety_160.lion_in12k_ft_in1k, regnety_160.pycls_in1k, regnety_160.sw_in12k, regnety_160.sw_in12k_ft_in1k, regnety_160.swag_ft_in1k, regnety_160.swag_lc_in1k, regnety_160.tv2_in1k, regnety_320.pycls_in1k, regnety_320.seer, regnety_320.seer_ft_in1k, regnety_320.swag_ft_in1k, regnety_320.swag_lc_in1k, regnety_320.tv2_in1k, regnety_640.seer, regnety_640.seer_ft_in1k, regnety_1280.seer, regnety_1280.seer_ft_in1k, regnety_1280.swag_ft_in1k, regnety_1280.swag_lc_in1k, regnety_2560.seer_ft_in1k, regnetz_040.ra3_in1k, regnetz_040_h.ra3_in1k, regnetz_b16.ra3_in1k, regnetz_c16.ra3_in1k, regnetz_c16_evos.ch_in1k, regnetz_d8.ra3_in1k, regnetz_d8_evos.ch_in1k, regnetz_d32.ra3_in1k, regnetz_e8.ra3_in1k, repvgg_a2.rvgg_in1k, repvgg_b0.rvgg_in1k, repvgg_b1.rvgg_in1k, repvgg_b1g4.rvgg_in1k, repvgg_b2.rvgg_in1k, repvgg_b2g4.rvgg_in1k, repvgg_b3.rvgg_in1k, repvgg_b3g4.rvgg_in1k, res2net50_14w_8s.in1k, res2net50_26w_4s.in1k, res2net50_26w_6s.in1k, res2net50_26w_8s.in1k, res2net50_48w_2s.in1k, res2net50d.in1k, res2net101_26w_4s.in1k, res2net101d.in1k, res2next50.in1k, resmlp_12_224.fb_dino, resmlp_12_224.fb_distilled_in1k, resmlp_12_224.fb_in1k, resmlp_24_224.fb_dino, resmlp_24_224.fb_distilled_in1k, resmlp_24_224.fb_in1k, resmlp_36_224.fb_distilled_in1k, resmlp_36_224.fb_in1k, resmlp_big_24_224.fb_distilled_in1k, resmlp_big_24_224.fb_in1k, resmlp_big_24_224.fb_in22k_ft_in1k, resnest14d.gluon_in1k, resnest26d.gluon_in1k, resnest50d.in1k, resnest50d_1s4x24d.in1k, resnest50d_4s2x40d.in1k, resnest101e.in1k, resnest200e.in1k, resnest269e.in1k, resnet10t.c3_in1k, resnet14t.c3_in1k, resnet18.a1_in1k, resnet18.a2_in1k, resnet18.a3_in1k, resnet18.fb_ssl_yfcc100m_ft_in1k, resnet18.fb_swsl_ig1b_ft_in1k, resnet18.gluon_in1k, resnet18.tv_in1k, resnet18d.ra2_in1k, resnet26.bt_in1k, resnet26d.bt_in1k, resnet26t.ra2_in1k, resnet32ts.ra2_in1k, resnet33ts.ra2_in1k, resnet34.a1_in1k, resnet34.a2_in1k, resnet34.a3_in1k, resnet34.bt_in1k, resnet34.gluon_in1k, resnet34.tv_in1k, resnet34d.ra2_in1k, resnet50.a1_in1k, resnet50.a1h_in1k, resnet50.a2_in1k, resnet50.a3_in1k, resnet50.am_in1k, resnet50.b1k_in1k, resnet50.b2k_in1k, resnet50.bt_in1k, resnet50.c1_in1k, resnet50.c2_in1k, resnet50.d_in1k, resnet50.fb_ssl_yfcc100m_ft_in1k, resnet50.fb_swsl_ig1b_ft_in1k, resnet50.gluon_in1k, resnet50.ra_in1k, resnet50.ram_in1k, resnet50.tv2_in1k, resnet50.tv_in1k, resnet50_gn.a1h_in1k, resnet50c.gluon_in1k, resnet50d.a1_in1k, resnet50d.a2_in1k, resnet50d.a3_in1k, resnet50d.gluon_in1k, resnet50d.ra2_in1k, resnet50s.gluon_in1k, resnet51q.ra2_in1k, resnet61q.ra2_in1k, resnet101.a1_in1k, resnet101.a1h_in1k, resnet101.a2_in1k, resnet101.a3_in1k, resnet101.gluon_in1k, resnet101.tv2_in1k, resnet101.tv_in1k, resnet101c.gluon_in1k, resnet101d.gluon_in1k, resnet101d.ra2_in1k, resnet101s.gluon_in1k, resnet152.a1_in1k, resnet152.a1h_in1k, resnet152.a2_in1k, resnet152.a3_in1k, resnet152.gluon_in1k, resnet152.tv2_in1k, resnet152.tv_in1k, resnet152c.gluon_in1k, resnet152d.gluon_in1k, resnet152d.ra2_in1k, resnet152s.gluon_in1k, resnet200d.ra2_in1k, resnetaa50.a1h_in1k, resnetaa50d.d_in12k, resnetaa50d.sw_in12k, resnetaa50d.sw_in12k_ft_in1k, resnetaa101d.sw_in12k, resnetaa101d.sw_in12k_ft_in1k, resnetblur50.bt_in1k, resnetrs50.tf_in1k, resnetrs101.tf_in1k, resnetrs152.tf_in1k, resnetrs200.tf_in1k, resnetrs270.tf_in1k, resnetrs350.tf_in1k, resnetrs420.tf_in1k, resnetv2_50.a1h_in1k, resnetv2_50d_evos.ah_in1k, resnetv2_50d_gn.ah_in1k, resnetv2_50x1_bit.goog_distilled_in1k, resnetv2_50x1_bit.goog_in21k_ft_in1k, resnetv2_50x3_bit.goog_in21k_ft_in1k, resnetv2_101.a1h_in1k, resnetv2_101x1_bit.goog_in21k_ft_in1k, resnetv2_101x3_bit.goog_in21k_ft_in1k, resnetv2_152x2_bit.goog_in21k_ft_in1k, resnetv2_152x2_bit.goog_teacher_in21k_ft_in1k, resnetv2_152x2_bit.goog_teacher_in21k_ft_in1k_384, resnetv2_152x4_bit.goog_in21k_ft_in1k, resnext26ts.ra2_in1k, resnext50_32x4d.a1_in1k, resnext50_32x4d.a1h_in1k, resnext50_32x4d.a2_in1k, resnext50_32x4d.a3_in1k, resnext50_32x4d.fb_ssl_yfcc100m_ft_in1k, resnext50_32x4d.fb_swsl_ig1b_ft_in1k, resnext50_32x4d.gluon_in1k, resnext50_32x4d.ra_in1k, resnext50_32x4d.tv2_in1k, resnext50_32x4d.tv_in1k, resnext50d_32x4d.bt_in1k, resnext101_32x4d.fb_ssl_yfcc100m_ft_in1k, resnext101_32x4d.fb_swsl_ig1b_ft_in1k, resnext101_32x4d.gluon_in1k, resnext101_32x8d.fb_ssl_yfcc100m_ft_in1k, resnext101_32x8d.fb_swsl_ig1b_ft_in1k, resnext101_32x8d.fb_wsl_ig1b_ft_in1k, resnext101_32x8d.tv2_in1k, resnext101_32x8d.tv_in1k, resnext101_32x16d.fb_ssl_yfcc100m_ft_in1k, resnext101_32x16d.fb_swsl_ig1b_ft_in1k, resnext101_32x16d.fb_wsl_ig1b_ft_in1k, resnext101_32x32d.fb_wsl_ig1b_ft_in1k, resnext101_64x4d.c1_in1k, resnext101_64x4d.gluon_in1k, resnext101_64x4d.tv_in1k, rexnet_100.nav_in1k, rexnet_130.nav_in1k, rexnet_150.nav_in1k, rexnet_200.nav_in1k, rexnet_300.nav_in1k, rexnetr_200.sw_in12k, rexnetr_200.sw_in12k_ft_in1k, rexnetr_300.sw_in12k, rexnetr_300.sw_in12k_ft_in1k, samvit_base_patch16.sa1b, samvit_huge_patch16.sa1b, samvit_large_patch16.sa1b, sebotnet33ts_256.a1h_in1k, sehalonet33ts.ra2_in1k, SelecSls42b.in1k, SelecSls60.in1k, SelecSls60b.in1k, semnasnet_075.rmsp_in1k, semnasnet_100.rmsp_in1k, senet154.gluon_in1k, sequencer2d_l.in1k, sequencer2d_m.in1k, sequencer2d_s.in1k, seresnet33ts.ra2_in1k, seresnet50.a1_in1k, seresnet50.a2_in1k, seresnet50.a3_in1k, seresnet50.ra2_in1k, seresnet152d.ra2_in1k, seresnext26d_32x4d.bt_in1k, seresnext26t_32x4d.bt_in1k, seresnext26ts.ch_in1k, seresnext50_32x4d.gluon_in1k, seresnext50_32x4d.racm_in1k, seresnext101_32x4d.gluon_in1k, seresnext101_32x8d.ah_in1k, seresnext101_64x4d.gluon_in1k, seresnext101d_32x8d.ah_in1k, seresnextaa101d_32x8d.ah_in1k, seresnextaa101d_32x8d.sw_in12k, seresnextaa101d_32x8d.sw_in12k_ft_in1k, seresnextaa101d_32x8d.sw_in12k_ft_in1k_288, skresnet18.ra_in1k, skresnet34.ra_in1k, skresnext50_32x4d.ra_in1k, spnasnet_100.rmsp_in1k, swin_base_patch4_window7_224.ms_in1k, swin_base_patch4_window7_224.ms_in22k, swin_base_patch4_window7_224.ms_in22k_ft_in1k, swin_base_patch4_window12_384.ms_in1k, swin_base_patch4_window12_384.ms_in22k, swin_base_patch4_window12_384.ms_in22k_ft_in1k, swin_large_patch4_window7_224.ms_in22k, swin_large_patch4_window7_224.ms_in22k_ft_in1k, swin_large_patch4_window12_384.ms_in22k, swin_large_patch4_window12_384.ms_in22k_ft_in1k, swin_s3_base_224.ms_in1k, swin_s3_small_224.ms_in1k, swin_s3_tiny_224.ms_in1k, swin_small_patch4_window7_224.ms_in1k, swin_small_patch4_window7_224.ms_in22k, swin_small_patch4_window7_224.ms_in22k_ft_in1k, swin_tiny_patch4_window7_224.ms_in1k, swin_tiny_patch4_window7_224.ms_in22k, swin_tiny_patch4_window7_224.ms_in22k_ft_in1k, swinv2_base_window8_256.ms_in1k, swinv2_base_window12_192.ms_in22k, swinv2_base_window12to16_192to256.ms_in22k_ft_in1k, swinv2_base_window12to24_192to384.ms_in22k_ft_in1k, swinv2_base_window16_256.ms_in1k, swinv2_cr_small_224.sw_in1k, swinv2_cr_small_ns_224.sw_in1k, swinv2_cr_tiny_ns_224.sw_in1k, swinv2_large_window12_192.ms_in22k, swinv2_large_window12to16_192to256.ms_in22k_ft_in1k, swinv2_large_window12to24_192to384.ms_in22k_ft_in1k, swinv2_small_window8_256.ms_in1k, swinv2_small_window16_256.ms_in1k, swinv2_tiny_window8_256.ms_in1k, swinv2_tiny_window16_256.ms_in1k, tf_efficientnet_b0.aa_in1k, tf_efficientnet_b0.ap_in1k, tf_efficientnet_b0.in1k, tf_efficientnet_b0.ns_jft_in1k, tf_efficientnet_b1.aa_in1k, tf_efficientnet_b1.ap_in1k, tf_efficientnet_b1.in1k, tf_efficientnet_b1.ns_jft_in1k, tf_efficientnet_b2.aa_in1k, tf_efficientnet_b2.ap_in1k, tf_efficientnet_b2.in1k, tf_efficientnet_b2.ns_jft_in1k, tf_efficientnet_b3.aa_in1k, tf_efficientnet_b3.ap_in1k, tf_efficientnet_b3.in1k, tf_efficientnet_b3.ns_jft_in1k, tf_efficientnet_b4.aa_in1k, tf_efficientnet_b4.ap_in1k, tf_efficientnet_b4.in1k, tf_efficientnet_b4.ns_jft_in1k, tf_efficientnet_b5.aa_in1k, tf_efficientnet_b5.ap_in1k, tf_efficientnet_b5.in1k, tf_efficientnet_b5.ns_jft_in1k, tf_efficientnet_b5.ra_in1k, tf_efficientnet_b6.aa_in1k, tf_efficientnet_b6.ap_in1k, tf_efficientnet_b6.ns_jft_in1k, tf_efficientnet_b7.aa_in1k, tf_efficientnet_b7.ap_in1k, tf_efficientnet_b7.ns_jft_in1k, tf_efficientnet_b7.ra_in1k, tf_efficientnet_b8.ap_in1k, tf_efficientnet_b8.ra_in1k, tf_efficientnet_cc_b0_4e.in1k, tf_efficientnet_cc_b0_8e.in1k, tf_efficientnet_cc_b1_8e.in1k, tf_efficientnet_el.in1k, tf_efficientnet_em.in1k, tf_efficientnet_es.in1k, tf_efficientnet_l2.ns_jft_in1k, tf_efficientnet_l2.ns_jft_in1k_475, tf_efficientnet_lite0.in1k, tf_efficientnet_lite1.in1k, tf_efficientnet_lite2.in1k, tf_efficientnet_lite3.in1k, tf_efficientnet_lite4.in1k, tf_efficientnetv2_b0.in1k, tf_efficientnetv2_b1.in1k, tf_efficientnetv2_b2.in1k, tf_efficientnetv2_b3.in1k, tf_efficientnetv2_b3.in21k_ft_in1k, tf_efficientnetv2_l.in1k, tf_efficientnetv2_l.in21k_ft_in1k, tf_efficientnetv2_m.in1k, tf_efficientnetv2_m.in21k_ft_in1k, tf_efficientnetv2_s.in1k, tf_efficientnetv2_s.in21k_ft_in1k, tf_efficientnetv2_xl.in21k_ft_in1k, tf_mixnet_l.in1k, tf_mixnet_m.in1k, tf_mixnet_s.in1k, tf_mobilenetv3_large_075.in1k, tf_mobilenetv3_large_100.in1k, tf_mobilenetv3_large_minimal_100.in1k, tf_mobilenetv3_small_075.in1k, tf_mobilenetv3_small_100.in1k, tf_mobilenetv3_small_minimal_100.in1k, tinynet_a.in1k, tinynet_b.in1k, tinynet_c.in1k, tinynet_d.in1k, tinynet_e.in1k, tnt_s_patch16_224, tresnet_l.miil_in1k, tresnet_l.miil_in1k_448, tresnet_m.miil_in1k, tresnet_m.miil_in1k_448, tresnet_m.miil_in21k_ft_in1k, tresnet_v2_l.miil_in21k_ft_in1k, tresnet_xl.miil_in1k, tresnet_xl.miil_in1k_448, twins_pcpvt_base.in1k, twins_pcpvt_large.in1k, twins_pcpvt_small.in1k, twins_svt_base.in1k, twins_svt_large.in1k, twins_svt_small.in1k, vgg11.tv_in1k, vgg11_bn.tv_in1k, vgg13.tv_in1k, vgg13_bn.tv_in1k, vgg16.tv_in1k, vgg16_bn.tv_in1k, vgg19.tv_in1k, vgg19_bn.tv_in1k, visformer_small.in1k, visformer_tiny.in1k, vit_base_patch8_224.augreg2_in21k_ft_in1k, vit_base_patch8_224.augreg_in21k_ft_in1k, vit_base_patch8_224.dino, vit_base_patch14_dinov2.lvd142m, vit_base_patch16_224.augreg2_in21k_ft_in1k, vit_base_patch16_224.augreg_in1k, vit_base_patch16_224.augreg_in21k_ft_in1k, vit_base_patch16_224.dino, vit_base_patch16_224.mae, vit_base_patch16_224.orig_in21k_ft_in1k, vit_base_patch16_224.sam_in1k, vit_base_patch16_224_miil.in21k_ft_in1k, vit_base_patch16_384.augreg_in1k, vit_base_patch16_384.augreg_in21k_ft_in1k, vit_base_patch16_384.orig_in21k_ft_in1k, vit_base_patch16_clip_224.datacompxl, vit_base_patch16_clip_224.laion2b, vit_base_patch16_clip_224.laion2b_ft_in1k, vit_base_patch16_clip_224.laion2b_ft_in12k, vit_base_patch16_clip_224.laion2b_ft_in12k_in1k, vit_base_patch16_clip_224.openai, vit_base_patch16_clip_224.openai_ft_in1k, vit_base_patch16_clip_224.openai_ft_in12k, vit_base_patch16_clip_224.openai_ft_in12k_in1k, vit_base_patch16_clip_384.laion2b_ft_in1k, vit_base_patch16_clip_384.laion2b_ft_in12k_in1k, vit_base_patch16_clip_384.openai_ft_in1k, vit_base_patch16_clip_384.openai_ft_in12k_in1k, vit_base_patch16_rpn_224.sw_in1k, vit_base_patch32_224.augreg_in1k, vit_base_patch32_224.augreg_in21k_ft_in1k, vit_base_patch32_224.sam_in1k, vit_base_patch32_384.augreg_in1k, vit_base_patch32_384.augreg_in21k_ft_in1k, vit_base_patch32_clip_224.laion2b, vit_base_patch32_clip_224.laion2b_ft_in1k, vit_base_patch32_clip_224.laion2b_ft_in12k_in1k, vit_base_patch32_clip_224.openai, vit_base_patch32_clip_224.openai_ft_in1k, vit_base_patch32_clip_384.laion2b_ft_in12k_in1k, vit_base_patch32_clip_384.openai_ft_in12k_in1k, vit_base_patch32_clip_448.laion2b_ft_in12k_in1k, vit_base_r50_s16_384.orig_in21k_ft_in1k, vit_giant_patch14_clip_224.laion2b, vit_giant_patch14_dinov2.lvd142m, vit_gigantic_patch14_clip_224.laion2b, vit_gigantic_patch16_224_ijepa.in22k, vit_huge_patch14_224.mae, vit_huge_patch14_224_ijepa.in1k, vit_huge_patch14_224_ijepa.in22k, vit_huge_patch14_clip_224.laion2b, vit_huge_patch14_clip_224.laion2b_ft_in1k, vit_huge_patch14_clip_224.laion2b_ft_in12k, vit_huge_patch14_clip_224.laion2b_ft_in12k_in1k, vit_huge_patch14_clip_336.laion2b_ft_in12k_in1k, vit_huge_patch16_448_ijepa.in1k, vit_large_patch14_clip_224.datacompxl, vit_large_patch14_clip_224.laion2b, vit_large_patch14_clip_224.laion2b_ft_in1k, vit_large_patch14_clip_224.laion2b_ft_in12k, vit_large_patch14_clip_224.laion2b_ft_in12k_in1k, vit_large_patch14_clip_224.openai, vit_large_patch14_clip_224.openai_ft_in1k, vit_large_patch14_clip_224.openai_ft_in12k, vit_large_patch14_clip_224.openai_ft_in12k_in1k, vit_large_patch14_clip_336.laion2b_ft_in1k, vit_large_patch14_clip_336.laion2b_ft_in12k_in1k, vit_large_patch14_clip_336.openai, vit_large_patch14_clip_336.openai_ft_in12k_in1k, vit_large_patch14_dinov2.lvd142m, vit_large_patch16_224.augreg_in21k_ft_in1k, vit_large_patch16_224.mae, vit_large_patch16_384.augreg_in21k_ft_in1k, vit_large_patch32_384.orig_in21k_ft_in1k, vit_large_r50_s32_224.augreg_in21k_ft_in1k, vit_large_r50_s32_384.augreg_in21k_ft_in1k, vit_medium_patch16_gap_240.sw_in12k, vit_medium_patch16_gap_256.sw_in12k_ft_in1k, vit_medium_patch16_gap_384.sw_in12k_ft_in1k, vit_relpos_base_patch16_224.sw_in1k, vit_relpos_base_patch16_clsgap_224.sw_in1k, vit_relpos_base_patch32_plus_rpn_256.sw_in1k, vit_relpos_medium_patch16_224.sw_in1k, vit_relpos_medium_patch16_cls_224.sw_in1k, vit_relpos_medium_patch16_rpn_224.sw_in1k, vit_relpos_small_patch16_224.sw_in1k, vit_small_patch8_224.dino, vit_small_patch14_dinov2.lvd142m, vit_small_patch16_224.augreg_in1k, vit_small_patch16_224.augreg_in21k_ft_in1k, vit_small_patch16_224.dino, vit_small_patch16_384.augreg_in1k, vit_small_patch16_384.augreg_in21k_ft_in1k, vit_small_patch32_224.augreg_in21k_ft_in1k, vit_small_patch32_384.augreg_in21k_ft_in1k, vit_small_r26_s32_224.augreg_in21k_ft_in1k, vit_small_r26_s32_384.augreg_in21k_ft_in1k, vit_srelpos_medium_patch16_224.sw_in1k, vit_srelpos_small_patch16_224.sw_in1k, vit_tiny_patch16_224.augreg_in21k_ft_in1k, vit_tiny_patch16_384.augreg_in21k_ft_in1k, vit_tiny_r_s16_p8_224.augreg_in21k_ft_in1k, vit_tiny_r_s16_p8_384.augreg_in21k_ft_in1k, volo_d1_224.sail_in1k, volo_d1_384.sail_in1k, volo_d2_224.sail_in1k, volo_d2_384.sail_in1k, volo_d3_224.sail_in1k, volo_d3_448.sail_in1k, volo_d4_224.sail_in1k, volo_d4_448.sail_in1k, volo_d5_224.sail_in1k, volo_d5_448.sail_in1k, volo_d5_512.sail_in1k, wide_resnet50_2.racm_in1k, wide_resnet50_2.tv2_in1k, wide_resnet50_2.tv_in1k, wide_resnet101_2.tv2_in1k, wide_resnet101_2.tv_in1k, xception41.tf_in1k, xception41p.ra3_in1k, xception65.ra3_in1k, xception65.tf_in1k, xception65p.ra3_in1k, xception71.tf_in1k, xcit_large_24_p8_224.fb_dist_in1k, xcit_large_24_p8_224.fb_in1k, xcit_large_24_p8_384.fb_dist_in1k, xcit_large_24_p16_224.fb_dist_in1k, xcit_large_24_p16_224.fb_in1k, xcit_large_24_p16_384.fb_dist_in1k, xcit_medium_24_p8_224.fb_dist_in1k, xcit_medium_24_p8_224.fb_in1k, xcit_medium_24_p8_384.fb_dist_in1k, xcit_medium_24_p16_224.fb_dist_in1k, xcit_medium_24_p16_224.fb_in1k, xcit_medium_24_p16_384.fb_dist_in1k, xcit_nano_12_p8_224.fb_dist_in1k, xcit_nano_12_p8_224.fb_in1k, xcit_nano_12_p8_384.fb_dist_in1k, xcit_nano_12_p16_224.fb_dist_in1k, xcit_nano_12_p16_224.fb_in1k, xcit_nano_12_p16_384.fb_dist_in1k, xcit_small_12_p8_224.fb_dist_in1k, xcit_small_12_p8_224.fb_in1k, xcit_small_12_p8_384.fb_dist_in1k, xcit_small_12_p16_224.fb_dist_in1k, xcit_small_12_p16_224.fb_in1k, xcit_small_12_p16_384.fb_dist_in1k, xcit_small_24_p8_224.fb_dist_in1k, xcit_small_24_p8_224.fb_in1k, xcit_small_24_p8_384.fb_dist_in1k, xcit_small_24_p16_224.fb_dist_in1k, xcit_small_24_p16_224.fb_in1k, xcit_small_24_p16_384.fb_dist_in1k, xcit_tiny_12_p8_224.fb_dist_in1k, xcit_tiny_12_p8_224.fb_in1k, xcit_tiny_12_p8_384.fb_dist_in1k, xcit_tiny_12_p16_224.fb_dist_in1k, xcit_tiny_12_p16_224.fb_in1k, xcit_tiny_12_p16_384.fb_dist_in1k, xcit_tiny_24_p8_224.fb_dist_in1k, xcit_tiny_24_p8_224.fb_in1k, xcit_tiny_24_p8_384.fb_dist_in1k, xcit_tiny_24_p16_224.fb_dist_in1k, xcit_tiny_24_p16_224.fb_in1k, xcit_tiny_24_p16_384.fb_dist_in1k
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model bat_resnext26ts.ch_in1k created, param count: 10731200
Running inference benchmark on bat_resnext26ts.ch_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
Infer [8/40]. 4394.88 samples/sec. 58.250 ms/step.
Infer [16/40]. 4394.71 samples/sec. 58.252 ms/step.
Infer [24/40]. 4394.78 samples/sec. 58.251 ms/step.
Infer [32/40]. 4394.62 samples/sec. 58.253 ms/step.
Infer [40/40]. 4394.25 samples/sec. 58.258 ms/step.
Inference benchmark of bat_resnext26ts.ch_in1k done. 4392.32 samples/sec, 58.26 ms/step
Model bat_resnext26ts.ch_in1k created, param count: 10731200
Running train benchmark on bat_resnext26ts.ch_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
Train [8/40]. 1114.75 samples/sec. 229.647 ms/step.
Train [16/40]. 1114.90 samples/sec. 229.618 ms/step.
Train [24/40]. 1115.03 samples/sec. 229.589 ms/step.
Train [32/40]. 1115.10 samples/sec. 229.575 ms/step.
Train [40/40]. 1115.12 samples/sec. 229.572 ms/step.
Train benchmark of bat_resnext26ts.ch_in1k done. 1110.31 samples/sec, 229.57 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
/home/nenkoru/miniconda3/envs/starcoder/lib/python3.10/site-packages/torch/functional.py:505: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3491.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
Model beit_base_patch16_224.in22k_ft_in22k created, param count: 102557713
Running inference benchmark on beit_base_patch16_224.in22k_ft_in22k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 2196.37 samples/sec. 116.556 ms/step.
Infer [16/40]. 2196.41 samples/sec. 116.554 ms/step.
Infer [24/40]. 2196.38 samples/sec. 116.555 ms/step.
Infer [32/40]. 2196.33 samples/sec. 116.558 ms/step.
Infer [40/40]. 2196.32 samples/sec. 116.558 ms/step.
Inference benchmark of beit_base_patch16_224.in22k_ft_in22k done. 2195.70 samples/sec, 116.56 ms/step
Model beit_base_patch16_224.in22k_ft_in22k created, param count: 102557713
Running train benchmark on beit_base_patch16_224.in22k_ft_in22k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 755.95 samples/sec. 338.648 ms/step.
Train [16/40]. 755.91 samples/sec. 338.663 ms/step.
Train [24/40]. 755.92 samples/sec. 338.658 ms/step.
Train [32/40]. 755.93 samples/sec. 338.654 ms/step.
Train [40/40]. 755.93 samples/sec. 338.657 ms/step.
Train benchmark of beit_base_patch16_224.in22k_ft_in22k done. 753.26 samples/sec, 338.66 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model beit_base_patch16_224.in22k_ft_in22k_in1k created, param count: 86530984
Running inference benchmark on beit_base_patch16_224.in22k_ft_in22k_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 2197.67 samples/sec. 116.487 ms/step.
Infer [16/40]. 2197.79 samples/sec. 116.481 ms/step.
Infer [24/40]. 2197.70 samples/sec. 116.486 ms/step.
Infer [32/40]. 2197.64 samples/sec. 116.488 ms/step.
Infer [40/40]. 2197.65 samples/sec. 116.488 ms/step.
Inference benchmark of beit_base_patch16_224.in22k_ft_in22k_in1k done. 2197.04 samples/sec, 116.49 ms/step
Model beit_base_patch16_224.in22k_ft_in22k_in1k created, param count: 86530984
Running train benchmark on beit_base_patch16_224.in22k_ft_in22k_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 757.41 samples/sec. 337.994 ms/step.
Train [16/40]. 757.42 samples/sec. 337.991 ms/step.
Train [24/40]. 757.42 samples/sec. 337.991 ms/step.
Train [32/40]. 757.41 samples/sec. 337.992 ms/step.
Train [40/40]. 757.42 samples/sec. 337.991 ms/step.
Train benchmark of beit_base_patch16_224.in22k_ft_in22k_in1k done. 754.67 samples/sec, 337.99 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model beit_base_patch16_384.in22k_ft_in22k_in1k created, param count: 86744104
Running inference benchmark on beit_base_patch16_384.in22k_ft_in22k_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 560.02 samples/sec. 457.129 ms/step.
Infer [16/40]. 559.96 samples/sec. 457.173 ms/step.
Infer [24/40]. 559.95 samples/sec. 457.185 ms/step.
Infer [32/40]. 559.95 samples/sec. 457.187 ms/step.
Infer [40/40]. 559.94 samples/sec. 457.193 ms/step.
Inference benchmark of beit_base_patch16_384.in22k_ft_in22k_in1k done. 559.87 samples/sec, 457.19 ms/step
Model beit_base_patch16_384.in22k_ft_in22k_in1k created, param count: 86744104
Running train benchmark on beit_base_patch16_384.in22k_ft_in22k_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 1.91 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.76 GiB is free. Including non-PyTorch memory, this process has 21.88 GiB memory in use. Of the allocated memory 21.16 GiB is allocated by PyTorch, and 239.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model beit_base_patch16_384.in22k_ft_in22k_in1k created, param count: 86744104
Running train benchmark on beit_base_patch16_384.in22k_ft_in22k_in1k for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 164.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 44.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 22.74 GiB is allocated by PyTorch, and 371.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model beit_base_patch16_384.in22k_ft_in22k_in1k created, param count: 86744104
Running train benchmark on beit_base_patch16_384.in22k_ft_in22k_in1k for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 976.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 774.06 MiB is free. Including non-PyTorch memory, this process has 22.88 GiB memory in use. Of the allocated memory 22.11 GiB is allocated by PyTorch, and 289.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model beit_base_patch16_384.in22k_ft_in22k_in1k created, param count: 86744104
Running train benchmark on beit_base_patch16_384.in22k_ft_in22k_in1k for 40 steps w/ input size (3, 384, 384) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 732.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 532.06 MiB is free. Including non-PyTorch memory, this process has 23.12 GiB memory in use. Of the allocated memory 22.49 GiB is allocated by PyTorch, and 147.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model beit_base_patch16_384.in22k_ft_in22k_in1k created, param count: 86744104
Running train benchmark on beit_base_patch16_384.in22k_ft_in22k_in1k for 40 steps w/ input size (3, 384, 384) and batch size 64.
Train [8/40]. 194.76 samples/sec. 328.611 ms/step.
Train [16/40]. 194.76 samples/sec. 328.608 ms/step.
Train [24/40]. 194.76 samples/sec. 328.609 ms/step.
Train [32/40]. 194.76 samples/sec. 328.611 ms/step.
Train [40/40]. 194.76 samples/sec. 328.613 ms/step.
Train benchmark of beit_base_patch16_384.in22k_ft_in22k_in1k done. 194.01 samples/sec, 328.61 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model beit_large_patch16_224.in22k_ft_in22k created, param count: 325792593
Running inference benchmark on beit_large_patch16_224.in22k_ft_in22k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 731.26 samples/sec. 350.081 ms/step.
Infer [16/40]. 731.25 samples/sec. 350.084 ms/step.
Infer [24/40]. 731.25 samples/sec. 350.085 ms/step.
Infer [32/40]. 730.98 samples/sec. 350.213 ms/step.
Infer [40/40]. 730.74 samples/sec. 350.331 ms/step.
Inference benchmark of beit_large_patch16_224.in22k_ft_in22k done. 730.64 samples/sec, 350.33 ms/step
Model beit_large_patch16_224.in22k_ft_in22k created, param count: 325792593
Running train benchmark on beit_large_patch16_224.in22k_ft_in22k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 38.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 22.93 GiB is allocated by PyTorch, and 187.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model beit_large_patch16_224.in22k_ft_in22k created, param count: 325792593
Running train benchmark on beit_large_patch16_224.in22k_ft_in22k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 228.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 184.06 MiB is free. Including non-PyTorch memory, this process has 23.46 GiB memory in use. Of the allocated memory 22.67 GiB is allocated by PyTorch, and 312.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model beit_large_patch16_224.in22k_ft_in22k created, param count: 325792593
Running train benchmark on beit_large_patch16_224.in22k_ft_in22k for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 198.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 78.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 22.86 GiB is allocated by PyTorch, and 222.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model beit_large_patch16_224.in22k_ft_in22k created, param count: 325792593
Running train benchmark on beit_large_patch16_224.in22k_ft_in22k for 40 steps w/ input size (3, 224, 224) and batch size 96.
Train [8/40]. 245.68 samples/sec. 390.751 ms/step.
Train [16/40]. 245.66 samples/sec. 390.783 ms/step.
Train [24/40]. 245.65 samples/sec. 390.798 ms/step.
Train [32/40]. 245.65 samples/sec. 390.802 ms/step.
Train [40/40]. 245.65 samples/sec. 390.803 ms/step.
Train benchmark of beit_large_patch16_224.in22k_ft_in22k done. 244.48 samples/sec, 390.80 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model beit_large_patch16_224.in22k_ft_in22k_in1k created, param count: 304430568
Running inference benchmark on beit_large_patch16_224.in22k_ft_in22k_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 731.24 samples/sec. 350.089 ms/step.
Infer [16/40]. 731.25 samples/sec. 350.087 ms/step.
Infer [24/40]. 731.24 samples/sec. 350.091 ms/step.
Infer [32/40]. 731.24 samples/sec. 350.089 ms/step.
Infer [40/40]. 731.25 samples/sec. 350.086 ms/step.
Inference benchmark of beit_large_patch16_224.in22k_ft_in22k_in1k done. 731.14 samples/sec, 350.09 ms/step
Model beit_large_patch16_224.in22k_ft_in22k_in1k created, param count: 304430568
Running train benchmark on beit_large_patch16_224.in22k_ft_in22k_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 36.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 22.89 GiB is allocated by PyTorch, and 228.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model beit_large_patch16_224.in22k_ft_in22k_in1k created, param count: 304430568
Running train benchmark on beit_large_patch16_224.in22k_ft_in22k_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 228.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 112.06 MiB is free. Including non-PyTorch memory, this process has 23.53 GiB memory in use. Of the allocated memory 22.78 GiB is allocated by PyTorch, and 268.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model beit_large_patch16_224.in22k_ft_in22k_in1k created, param count: 304430568
Running train benchmark on beit_large_patch16_224.in22k_ft_in22k_in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 198.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 72.06 MiB is free. Including non-PyTorch memory, this process has 23.57 GiB memory in use. Of the allocated memory 22.82 GiB is allocated by PyTorch, and 267.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model beit_large_patch16_224.in22k_ft_in22k_in1k created, param count: 304430568
Running train benchmark on beit_large_patch16_224.in22k_ft_in22k_in1k for 40 steps w/ input size (3, 224, 224) and batch size 96.
Train [8/40]. 246.34 samples/sec. 389.699 ms/step.
Train [16/40]. 246.33 samples/sec. 389.713 ms/step.
Train [24/40]. 246.34 samples/sec. 389.712 ms/step.
Train [32/40]. 246.34 samples/sec. 389.712 ms/step.
Train [40/40]. 246.33 samples/sec. 389.721 ms/step.
Train benchmark of beit_large_patch16_224.in22k_ft_in22k_in1k done. 245.14 samples/sec, 389.72 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model beit_large_patch16_384.in22k_ft_in22k_in1k created, param count: 304998888
Running inference benchmark on beit_large_patch16_384.in22k_ft_in22k_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 191.44 samples/sec. 1337.231 ms/step.
Infer [16/40]. 191.45 samples/sec. 1337.138 ms/step.
Infer [24/40]. 191.45 samples/sec. 1337.168 ms/step.
Infer [32/40]. 191.45 samples/sec. 1337.164 ms/step.
Infer [40/40]. 191.41 samples/sec. 1337.418 ms/step.
Inference benchmark of beit_large_patch16_384.in22k_ft_in22k_in1k done. 191.41 samples/sec, 1337.42 ms/step
Model beit_large_patch16_384.in22k_ft_in22k_in1k created, param count: 304998888
Running train benchmark on beit_large_patch16_384.in22k_ft_in22k_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 2.54 GiB. GPU 0 has a total capacty of 23.65 GiB of which 2.06 GiB is free. Including non-PyTorch memory, this process has 21.58 GiB memory in use. Of the allocated memory 20.89 GiB is allocated by PyTorch, and 205.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model beit_large_patch16_384.in22k_ft_in22k_in1k created, param count: 304998888
Running train benchmark on beit_large_patch16_384.in22k_ft_in22k_in1k for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 1.91 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.06 GiB is free. Including non-PyTorch memory, this process has 22.58 GiB memory in use. Of the allocated memory 21.54 GiB is allocated by PyTorch, and 560.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model beit_large_patch16_384.in22k_ft_in22k_in1k created, param count: 304998888
Running train benchmark on beit_large_patch16_384.in22k_ft_in22k_in1k for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 1.27 GiB. GPU 0 has a total capacty of 23.65 GiB of which 714.06 MiB is free. Including non-PyTorch memory, this process has 22.94 GiB memory in use. Of the allocated memory 22.19 GiB is allocated by PyTorch, and 265.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model beit_large_patch16_384.in22k_ft_in22k_in1k created, param count: 304998888
Running train benchmark on beit_large_patch16_384.in22k_ft_in22k_in1k for 40 steps w/ input size (3, 384, 384) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 976.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 176.06 MiB is free. Including non-PyTorch memory, this process has 23.47 GiB memory in use. Of the allocated memory 22.52 GiB is allocated by PyTorch, and 463.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model beit_large_patch16_384.in22k_ft_in22k_in1k created, param count: 304998888
Running train benchmark on beit_large_patch16_384.in22k_ft_in22k_in1k for 40 steps w/ input size (3, 384, 384) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 652.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 592.06 MiB is free. Including non-PyTorch memory, this process has 23.06 GiB memory in use. Of the allocated memory 22.28 GiB is allocated by PyTorch, and 296.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model beit_large_patch16_384.in22k_ft_in22k_in1k created, param count: 304998888
Running train benchmark on beit_large_patch16_384.in22k_ft_in22k_in1k for 40 steps w/ input size (3, 384, 384) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 488.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 220.06 MiB is free. Including non-PyTorch memory, this process has 23.43 GiB memory in use. Of the allocated memory 22.59 GiB is allocated by PyTorch, and 354.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model beit_large_patch16_384.in22k_ft_in22k_in1k created, param count: 304998888
Running train benchmark on beit_large_patch16_384.in22k_ft_in22k_in1k for 40 steps w/ input size (3, 384, 384) and batch size 32.
ERROR: "CUDA out of memory. Tried to allocate 110.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 36.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 22.65 GiB is allocated by PyTorch, and 473.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model beit_large_patch16_384.in22k_ft_in22k_in1k created, param count: 304998888
Running train benchmark on beit_large_patch16_384.in22k_ft_in22k_in1k for 40 steps w/ input size (3, 384, 384) and batch size 24.
Train [8/40]. 65.84 samples/sec. 364.536 ms/step.
Train [16/40]. 65.84 samples/sec. 364.539 ms/step.
Train [24/40]. 65.84 samples/sec. 364.545 ms/step.
Train [32/40]. 65.83 samples/sec. 364.554 ms/step.
Train [40/40]. 65.83 samples/sec. 364.557 ms/step.
Train benchmark of beit_large_patch16_384.in22k_ft_in22k_in1k done. 65.51 samples/sec, 364.56 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model beit_large_patch16_512.in22k_ft_in22k_in1k created, param count: 305674728
Running inference benchmark on beit_large_patch16_512.in22k_ft_in22k_in1k for 40 steps w/ input size (3, 512, 512) and batch size 256.
Infer [8/40]. 84.98 samples/sec. 3012.410 ms/step.
Infer [16/40]. 84.97 samples/sec. 3012.770 ms/step.
Infer [24/40]. 84.95 samples/sec. 3013.612 ms/step.
Infer [32/40]. 84.93 samples/sec. 3014.315 ms/step.
Infer [40/40]. 84.92 samples/sec. 3014.662 ms/step.
Inference benchmark of beit_large_patch16_512.in22k_ft_in22k_in1k done. 84.92 samples/sec, 3014.66 ms/step
Model beit_large_patch16_512.in22k_ft_in22k_in1k created, param count: 305674728
Running train benchmark on beit_large_patch16_512.in22k_ft_in22k_in1k for 40 steps w/ input size (3, 512, 512) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 514.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 414.06 MiB is free. Including non-PyTorch memory, this process has 23.24 GiB memory in use. Of the allocated memory 22.22 GiB is allocated by PyTorch, and 539.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model beit_large_patch16_512.in22k_ft_in22k_in1k created, param count: 305674728
Running train benchmark on beit_large_patch16_512.in22k_ft_in22k_in1k for 40 steps w/ input size (3, 512, 512) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 6.01 GiB. GPU 0 has a total capacty of 23.65 GiB of which 5.28 GiB is free. Including non-PyTorch memory, this process has 18.36 GiB memory in use. Of the allocated memory 17.24 GiB is allocated by PyTorch, and 648.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model beit_large_patch16_512.in22k_ft_in22k_in1k created, param count: 305674728
Running train benchmark on beit_large_patch16_512.in22k_ft_in22k_in1k for 40 steps w/ input size (3, 512, 512) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 4.01 GiB. GPU 0 has a total capacty of 23.65 GiB of which 2.40 GiB is free. Including non-PyTorch memory, this process has 21.24 GiB memory in use. Of the allocated memory 20.27 GiB is allocated by PyTorch, and 483.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model beit_large_patch16_512.in22k_ft_in22k_in1k created, param count: 305674728
Running train benchmark on beit_large_patch16_512.in22k_ft_in22k_in1k for 40 steps w/ input size (3, 512, 512) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 3.01 GiB. GPU 0 has a total capacty of 23.65 GiB of which 858.06 MiB is free. Including non-PyTorch memory, this process has 22.80 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 530.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model beit_large_patch16_512.in22k_ft_in22k_in1k created, param count: 305674728
Running train benchmark on beit_large_patch16_512.in22k_ft_in22k_in1k for 40 steps w/ input size (3, 512, 512) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 2.01 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.63 GiB is free. Including non-PyTorch memory, this process has 22.01 GiB memory in use. Of the allocated memory 20.93 GiB is allocated by PyTorch, and 595.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model beit_large_patch16_512.in22k_ft_in22k_in1k created, param count: 305674728
Running train benchmark on beit_large_patch16_512.in22k_ft_in22k_in1k for 40 steps w/ input size (3, 512, 512) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 1.50 GiB. GPU 0 has a total capacty of 23.65 GiB of which 486.06 MiB is free. Including non-PyTorch memory, this process has 23.17 GiB memory in use. Of the allocated memory 22.29 GiB is allocated by PyTorch, and 390.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model beit_large_patch16_512.in22k_ft_in22k_in1k created, param count: 305674728
Running train benchmark on beit_large_patch16_512.in22k_ft_in22k_in1k for 40 steps w/ input size (3, 512, 512) and batch size 32.
ERROR: "CUDA out of memory. Tried to allocate 1.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 72.06 MiB is free. Including non-PyTorch memory, this process has 23.57 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 386.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model beit_large_patch16_512.in22k_ft_in22k_in1k created, param count: 305674728
Running train benchmark on beit_large_patch16_512.in22k_ft_in22k_in1k for 40 steps w/ input size (3, 512, 512) and batch size 24.
ERROR: "CUDA out of memory. Tried to allocate 770.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 96.06 MiB is free. Including non-PyTorch memory, this process has 23.55 GiB memory in use. Of the allocated memory 22.73 GiB is allocated by PyTorch, and 335.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model beit_large_patch16_512.in22k_ft_in22k_in1k created, param count: 305674728
Running train benchmark on beit_large_patch16_512.in22k_ft_in22k_in1k for 40 steps w/ input size (3, 512, 512) and batch size 16.
ERROR: "CUDA out of memory. Tried to allocate 514.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 296.06 MiB is free. Including non-PyTorch memory, this process has 23.35 GiB memory in use. Of the allocated memory 22.42 GiB is allocated by PyTorch, and 453.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 12 for retry.
Model beit_large_patch16_512.in22k_ft_in22k_in1k created, param count: 305674728
Running train benchmark on beit_large_patch16_512.in22k_ft_in22k_in1k for 40 steps w/ input size (3, 512, 512) and batch size 12.
Train [8/40]. 28.26 samples/sec. 424.599 ms/step.
Train [16/40]. 28.26 samples/sec. 424.622 ms/step.
Train [24/40]. 28.26 samples/sec. 424.625 ms/step.
Train [32/40]. 28.26 samples/sec. 424.640 ms/step.
Train [40/40]. 28.26 samples/sec. 424.647 ms/step.
Train benchmark of beit_large_patch16_512.in22k_ft_in22k_in1k done. 28.13 samples/sec, 424.65 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model beitv2_base_patch16_224.in1k_ft_in1k created, param count: 86530984
Running inference benchmark on beitv2_base_patch16_224.in1k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 2198.23 samples/sec. 116.457 ms/step.
Infer [16/40]. 2198.16 samples/sec. 116.461 ms/step.
Infer [24/40]. 2197.85 samples/sec. 116.478 ms/step.
Infer [32/40]. 2197.72 samples/sec. 116.484 ms/step.
Infer [40/40]. 2197.63 samples/sec. 116.489 ms/step.
Inference benchmark of beitv2_base_patch16_224.in1k_ft_in1k done. 2197.10 samples/sec, 116.49 ms/step
Model beitv2_base_patch16_224.in1k_ft_in1k created, param count: 86530984
Running train benchmark on beitv2_base_patch16_224.in1k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 757.26 samples/sec. 338.062 ms/step.
Train [16/40]. 757.25 samples/sec. 338.067 ms/step.
Train [24/40]. 757.25 samples/sec. 338.065 ms/step.
Train [32/40]. 757.26 samples/sec. 338.061 ms/step.
Train [40/40]. 757.26 samples/sec. 338.061 ms/step.
Train benchmark of beitv2_base_patch16_224.in1k_ft_in1k done. 754.51 samples/sec, 338.06 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model beitv2_base_patch16_224.in1k_ft_in22k created, param count: 102557713
Running inference benchmark on beitv2_base_patch16_224.in1k_ft_in22k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 2196.49 samples/sec. 116.549 ms/step.
Infer [16/40]. 2195.98 samples/sec. 116.577 ms/step.
Infer [24/40]. 2195.81 samples/sec. 116.586 ms/step.
Infer [32/40]. 2195.74 samples/sec. 116.589 ms/step.
Infer [40/40]. 2195.59 samples/sec. 116.598 ms/step.
Inference benchmark of beitv2_base_patch16_224.in1k_ft_in22k done. 2195.03 samples/sec, 116.60 ms/step
Model beitv2_base_patch16_224.in1k_ft_in22k created, param count: 102557713
Running train benchmark on beitv2_base_patch16_224.in1k_ft_in22k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 755.92 samples/sec. 338.661 ms/step.
Train [16/40]. 755.92 samples/sec. 338.658 ms/step.
Train [24/40]. 755.94 samples/sec. 338.653 ms/step.
Train [32/40]. 755.92 samples/sec. 338.662 ms/step.
Train [40/40]. 755.88 samples/sec. 338.678 ms/step.
Train benchmark of beitv2_base_patch16_224.in1k_ft_in22k done. 753.08 samples/sec, 338.68 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model beitv2_base_patch16_224.in1k_ft_in22k_in1k created, param count: 86530984
Running inference benchmark on beitv2_base_patch16_224.in1k_ft_in22k_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 2198.38 samples/sec. 116.449 ms/step.
Infer [16/40]. 2197.99 samples/sec. 116.470 ms/step.
Infer [24/40]. 2197.77 samples/sec. 116.482 ms/step.
Infer [32/40]. 2197.61 samples/sec. 116.490 ms/step.
Infer [40/40]. 2197.50 samples/sec. 116.496 ms/step.
Inference benchmark of beitv2_base_patch16_224.in1k_ft_in22k_in1k done. 2196.92 samples/sec, 116.50 ms/step
Model beitv2_base_patch16_224.in1k_ft_in22k_in1k created, param count: 86530984
Running train benchmark on beitv2_base_patch16_224.in1k_ft_in22k_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 757.47 samples/sec. 337.969 ms/step.
Train [16/40]. 757.47 samples/sec. 337.969 ms/step.
Train [24/40]. 757.47 samples/sec. 337.966 ms/step.
Train [32/40]. 757.48 samples/sec. 337.964 ms/step.
Train [40/40]. 757.48 samples/sec. 337.962 ms/step.
Train benchmark of beitv2_base_patch16_224.in1k_ft_in22k_in1k done. 754.77 samples/sec, 337.96 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model beitv2_large_patch16_224.in1k_ft_in1k created, param count: 304430568
Running inference benchmark on beitv2_large_patch16_224.in1k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 731.30 samples/sec. 350.063 ms/step.
Infer [16/40]. 731.32 samples/sec. 350.053 ms/step.
Infer [24/40]. 731.33 samples/sec. 350.047 ms/step.
Infer [32/40]. 731.35 samples/sec. 350.037 ms/step.
Infer [40/40]. 731.35 samples/sec. 350.036 ms/step.
Inference benchmark of beitv2_large_patch16_224.in1k_ft_in1k done. 731.26 samples/sec, 350.04 ms/step
Model beitv2_large_patch16_224.in1k_ft_in1k created, param count: 304430568
Running train benchmark on beitv2_large_patch16_224.in1k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 36.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 22.89 GiB is allocated by PyTorch, and 228.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model beitv2_large_patch16_224.in1k_ft_in1k created, param count: 304430568
Running train benchmark on beitv2_large_patch16_224.in1k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 228.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 112.06 MiB is free. Including non-PyTorch memory, this process has 23.53 GiB memory in use. Of the allocated memory 22.78 GiB is allocated by PyTorch, and 268.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model beitv2_large_patch16_224.in1k_ft_in1k created, param count: 304430568
Running train benchmark on beitv2_large_patch16_224.in1k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 198.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 72.06 MiB is free. Including non-PyTorch memory, this process has 23.57 GiB memory in use. Of the allocated memory 22.82 GiB is allocated by PyTorch, and 267.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model beitv2_large_patch16_224.in1k_ft_in1k created, param count: 304430568
Running train benchmark on beitv2_large_patch16_224.in1k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 96.
Train [8/40]. 246.35 samples/sec. 389.693 ms/step.
Train [16/40]. 246.35 samples/sec. 389.696 ms/step.
Train [24/40]. 246.35 samples/sec. 389.696 ms/step.
Train [32/40]. 246.35 samples/sec. 389.695 ms/step.
Train [40/40]. 246.35 samples/sec. 389.697 ms/step.
Train benchmark of beitv2_large_patch16_224.in1k_ft_in1k done. 245.20 samples/sec, 389.70 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model beitv2_large_patch16_224.in1k_ft_in22k created, param count: 325792593
Running inference benchmark on beitv2_large_patch16_224.in1k_ft_in22k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 731.35 samples/sec. 350.039 ms/step.
Infer [16/40]. 731.37 samples/sec. 350.030 ms/step.
Infer [24/40]. 731.35 samples/sec. 350.037 ms/step.
Infer [32/40]. 731.35 samples/sec. 350.038 ms/step.
Infer [40/40]. 731.13 samples/sec. 350.144 ms/step.
Inference benchmark of beitv2_large_patch16_224.in1k_ft_in22k done. 731.04 samples/sec, 350.14 ms/step
Model beitv2_large_patch16_224.in1k_ft_in22k created, param count: 325792593
Running train benchmark on beitv2_large_patch16_224.in1k_ft_in22k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 36.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 22.93 GiB is allocated by PyTorch, and 187.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model beitv2_large_patch16_224.in1k_ft_in22k created, param count: 325792593
Running train benchmark on beitv2_large_patch16_224.in1k_ft_in22k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 228.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 182.06 MiB is free. Including non-PyTorch memory, this process has 23.46 GiB memory in use. Of the allocated memory 22.67 GiB is allocated by PyTorch, and 312.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model beitv2_large_patch16_224.in1k_ft_in22k created, param count: 325792593
Running train benchmark on beitv2_large_patch16_224.in1k_ft_in22k for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 198.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 78.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 22.86 GiB is allocated by PyTorch, and 222.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model beitv2_large_patch16_224.in1k_ft_in22k created, param count: 325792593
Running train benchmark on beitv2_large_patch16_224.in1k_ft_in22k for 40 steps w/ input size (3, 224, 224) and batch size 96.
Train [8/40]. 245.85 samples/sec. 390.482 ms/step.
Train [16/40]. 245.83 samples/sec. 390.515 ms/step.
Train [24/40]. 245.80 samples/sec. 390.568 ms/step.
Train [32/40]. 245.78 samples/sec. 390.590 ms/step.
Train [40/40]. 245.77 samples/sec. 390.606 ms/step.
Train benchmark of beitv2_large_patch16_224.in1k_ft_in22k done. 244.64 samples/sec, 390.61 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model beitv2_large_patch16_224.in1k_ft_in22k_in1k created, param count: 304430568
Running inference benchmark on beitv2_large_patch16_224.in1k_ft_in22k_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 731.34 samples/sec. 350.044 ms/step.
Infer [16/40]. 731.31 samples/sec. 350.059 ms/step.
Infer [24/40]. 731.31 samples/sec. 350.056 ms/step.
Infer [32/40]. 731.32 samples/sec. 350.053 ms/step.
Infer [40/40]. 731.32 samples/sec. 350.051 ms/step.
Inference benchmark of beitv2_large_patch16_224.in1k_ft_in22k_in1k done. 731.23 samples/sec, 350.05 ms/step
Model beitv2_large_patch16_224.in1k_ft_in22k_in1k created, param count: 304430568
Running train benchmark on beitv2_large_patch16_224.in1k_ft_in22k_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 36.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 22.89 GiB is allocated by PyTorch, and 228.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model beitv2_large_patch16_224.in1k_ft_in22k_in1k created, param count: 304430568
Running train benchmark on beitv2_large_patch16_224.in1k_ft_in22k_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 228.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 112.06 MiB is free. Including non-PyTorch memory, this process has 23.53 GiB memory in use. Of the allocated memory 22.78 GiB is allocated by PyTorch, and 268.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model beitv2_large_patch16_224.in1k_ft_in22k_in1k created, param count: 304430568
Running train benchmark on beitv2_large_patch16_224.in1k_ft_in22k_in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 198.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 72.06 MiB is free. Including non-PyTorch memory, this process has 23.57 GiB memory in use. Of the allocated memory 22.82 GiB is allocated by PyTorch, and 267.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model beitv2_large_patch16_224.in1k_ft_in22k_in1k created, param count: 304430568
Running train benchmark on beitv2_large_patch16_224.in1k_ft_in22k_in1k for 40 steps w/ input size (3, 224, 224) and batch size 96.
Train [8/40]. 246.34 samples/sec. 389.700 ms/step.
Train [16/40]. 246.35 samples/sec. 389.697 ms/step.
Train [24/40]. 246.35 samples/sec. 389.693 ms/step.
Train [32/40]. 246.35 samples/sec. 389.696 ms/step.
Train [40/40]. 246.35 samples/sec. 389.695 ms/step.
Train benchmark of beitv2_large_patch16_224.in1k_ft_in22k_in1k done. 245.19 samples/sec, 389.69 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model botnet26t_256.c1_in1k created, param count: 12488672
Running inference benchmark on botnet26t_256.c1_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
Infer [8/40]. 4377.50 samples/sec. 58.481 ms/step.
Infer [16/40]. 4377.40 samples/sec. 58.482 ms/step.
Infer [24/40]. 4377.38 samples/sec. 58.482 ms/step.
Infer [32/40]. 4377.28 samples/sec. 58.484 ms/step.
Infer [40/40]. 4377.19 samples/sec. 58.485 ms/step.
Inference benchmark of botnet26t_256.c1_in1k done. 4375.17 samples/sec, 58.48 ms/step
Model botnet26t_256.c1_in1k created, param count: 12488672
Running train benchmark on botnet26t_256.c1_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
Train [8/40]. 1256.17 samples/sec. 203.794 ms/step.
Train [16/40]. 1256.25 samples/sec. 203.781 ms/step.
Train [24/40]. 1256.28 samples/sec. 203.776 ms/step.
Train [32/40]. 1256.31 samples/sec. 203.771 ms/step.
Train [40/40]. 1256.34 samples/sec. 203.767 ms/step.
Train benchmark of botnet26t_256.c1_in1k done. 1252.66 samples/sec, 203.77 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model caformer_b36.sail_in1k created, param count: 98753614
Running inference benchmark on caformer_b36.sail_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 968.20 samples/sec. 264.408 ms/step.
Infer [16/40]. 968.18 samples/sec. 264.413 ms/step.
Infer [24/40]. 968.16 samples/sec. 264.419 ms/step.
Infer [32/40]. 968.14 samples/sec. 264.425 ms/step.
Infer [40/40]. 968.13 samples/sec. 264.429 ms/step.
Inference benchmark of caformer_b36.sail_in1k done. 967.98 samples/sec, 264.43 ms/step
Model caformer_b36.sail_in1k created, param count: 98753614
Running train benchmark on caformer_b36.sail_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 392.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 284.06 MiB is free. Including non-PyTorch memory, this process has 23.36 GiB memory in use. Of the allocated memory 22.73 GiB is allocated by PyTorch, and 147.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model caformer_b36.sail_in1k created, param count: 98753614
Running train benchmark on caformer_b36.sail_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 294.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 162.06 MiB is free. Including non-PyTorch memory, this process has 23.48 GiB memory in use. Of the allocated memory 22.86 GiB is allocated by PyTorch, and 130.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model caformer_b36.sail_in1k created, param count: 98753614
Running train benchmark on caformer_b36.sail_in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 50.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 23.09 GiB is allocated by PyTorch, and 6.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model caformer_b36.sail_in1k created, param count: 98753614
Running train benchmark on caformer_b36.sail_in1k for 40 steps w/ input size (3, 224, 224) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 12.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 23.12 GiB is allocated by PyTorch, and 19.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model caformer_b36.sail_in1k created, param count: 98753614
Running train benchmark on caformer_b36.sail_in1k for 40 steps w/ input size (3, 224, 224) and batch size 64.
Train [8/40]. 296.23 samples/sec. 216.052 ms/step.
Train [16/40]. 296.22 samples/sec. 216.053 ms/step.
Train [24/40]. 296.18 samples/sec. 216.087 ms/step.
Train [32/40]. 296.15 samples/sec. 216.105 ms/step.
Train [40/40]. 296.14 samples/sec. 216.116 ms/step.
Train benchmark of caformer_b36.sail_in1k done. 294.17 samples/sec, 216.12 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model caformer_b36.sail_in1k_384 created, param count: 98753614
Running inference benchmark on caformer_b36.sail_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 321.99 samples/sec. 795.049 ms/step.
Infer [16/40]. 321.98 samples/sec. 795.082 ms/step.
Infer [24/40]. 321.97 samples/sec. 795.095 ms/step.
Infer [32/40]. 321.97 samples/sec. 795.107 ms/step.
Infer [40/40]. 321.97 samples/sec. 795.112 ms/step.
Inference benchmark of caformer_b36.sail_in1k_384 done. 321.95 samples/sec, 795.11 ms/step
Model caformer_b36.sail_in1k_384 created, param count: 98753614
Running train benchmark on caformer_b36.sail_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 2.25 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.49 GiB is free. Including non-PyTorch memory, this process has 22.15 GiB memory in use. Of the allocated memory 21.31 GiB is allocated by PyTorch, and 352.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model caformer_b36.sail_in1k_384 created, param count: 98753614
Running train benchmark on caformer_b36.sail_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 1.69 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.63 GiB is free. Including non-PyTorch memory, this process has 22.01 GiB memory in use. Of the allocated memory 21.10 GiB is allocated by PyTorch, and 431.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model caformer_b36.sail_in1k_384 created, param count: 98753614
Running train benchmark on caformer_b36.sail_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 1.12 GiB. GPU 0 has a total capacty of 23.65 GiB of which 944.06 MiB is free. Including non-PyTorch memory, this process has 22.72 GiB memory in use. Of the allocated memory 22.02 GiB is allocated by PyTorch, and 203.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model caformer_b36.sail_in1k_384 created, param count: 98753614
Running train benchmark on caformer_b36.sail_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 432.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 140.06 MiB is free. Including non-PyTorch memory, this process has 23.50 GiB memory in use. Of the allocated memory 22.91 GiB is allocated by PyTorch, and 96.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model caformer_b36.sail_in1k_384 created, param count: 98753614
Running train benchmark on caformer_b36.sail_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 288.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 134.06 MiB is free. Including non-PyTorch memory, this process has 23.51 GiB memory in use. Of the allocated memory 22.94 GiB is allocated by PyTorch, and 70.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model caformer_b36.sail_in1k_384 created, param count: 98753614
Running train benchmark on caformer_b36.sail_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 216.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 130.06 MiB is free. Including non-PyTorch memory, this process has 23.51 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 56.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model caformer_b36.sail_in1k_384 created, param count: 98753614
Running train benchmark on caformer_b36.sail_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 32.
Train [8/40]. 96.93 samples/sec. 330.138 ms/step.
Train [16/40]. 96.92 samples/sec. 330.153 ms/step.
Train [24/40]. 96.92 samples/sec. 330.153 ms/step.
Train [32/40]. 96.92 samples/sec. 330.156 ms/step.
Train [40/40]. 96.92 samples/sec. 330.159 ms/step.
Train benchmark of caformer_b36.sail_in1k_384 done. 96.44 samples/sec, 330.16 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model caformer_b36.sail_in22k created, param count: 162798007
Running inference benchmark on caformer_b36.sail_in22k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 966.37 samples/sec. 264.910 ms/step.
Infer [16/40]. 966.33 samples/sec. 264.921 ms/step.
Infer [24/40]. 966.25 samples/sec. 264.941 ms/step.
Infer [32/40]. 966.21 samples/sec. 264.952 ms/step.
Infer [40/40]. 966.19 samples/sec. 264.959 ms/step.
Inference benchmark of caformer_b36.sail_in22k done. 966.03 samples/sec, 264.96 ms/step
Model caformer_b36.sail_in22k created, param count: 162798007
Running train benchmark on caformer_b36.sail_in22k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 392.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 280.06 MiB is free. Including non-PyTorch memory, this process has 23.37 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 25.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model caformer_b36.sail_in22k created, param count: 162798007
Running train benchmark on caformer_b36.sail_in22k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 294.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 18.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.98 GiB is allocated by PyTorch, and 155.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model caformer_b36.sail_in22k created, param count: 162798007
Running train benchmark on caformer_b36.sail_in22k for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 78.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 23.01 GiB is allocated by PyTorch, and 54.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model caformer_b36.sail_in22k created, param count: 162798007
Running train benchmark on caformer_b36.sail_in22k for 40 steps w/ input size (3, 224, 224) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 4.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 23.12 GiB is allocated by PyTorch, and 18.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model caformer_b36.sail_in22k created, param count: 162798007
Running train benchmark on caformer_b36.sail_in22k for 40 steps w/ input size (3, 224, 224) and batch size 64.
Train [8/40]. 293.47 samples/sec. 218.079 ms/step.
Train [16/40]. 293.45 samples/sec. 218.095 ms/step.
Train [24/40]. 293.45 samples/sec. 218.094 ms/step.
Train [32/40]. 293.46 samples/sec. 218.091 ms/step.
Train [40/40]. 293.45 samples/sec. 218.095 ms/step.
Train benchmark of caformer_b36.sail_in22k done. 291.44 samples/sec, 218.09 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model caformer_b36.sail_in22k_ft_in1k created, param count: 98753614
Running inference benchmark on caformer_b36.sail_in22k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 967.93 samples/sec. 264.483 ms/step.
Infer [16/40]. 967.91 samples/sec. 264.486 ms/step.
Infer [24/40]. 967.88 samples/sec. 264.496 ms/step.
Infer [32/40]. 967.85 samples/sec. 264.503 ms/step.
Infer [40/40]. 967.82 samples/sec. 264.512 ms/step.
Inference benchmark of caformer_b36.sail_in22k_ft_in1k done. 967.67 samples/sec, 264.51 ms/step
Model caformer_b36.sail_in22k_ft_in1k created, param count: 98753614
Running train benchmark on caformer_b36.sail_in22k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 392.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 280.06 MiB is free. Including non-PyTorch memory, this process has 23.37 GiB memory in use. Of the allocated memory 22.73 GiB is allocated by PyTorch, and 147.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model caformer_b36.sail_in22k_ft_in1k created, param count: 98753614
Running train benchmark on caformer_b36.sail_in22k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 294.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 166.06 MiB is free. Including non-PyTorch memory, this process has 23.48 GiB memory in use. Of the allocated memory 22.86 GiB is allocated by PyTorch, and 130.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model caformer_b36.sail_in22k_ft_in1k created, param count: 98753614
Running train benchmark on caformer_b36.sail_in22k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 78.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 22.99 GiB is allocated by PyTorch, and 76.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model caformer_b36.sail_in22k_ft_in1k created, param count: 98753614
Running train benchmark on caformer_b36.sail_in22k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 22.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 23.08 GiB is allocated by PyTorch, and 40.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model caformer_b36.sail_in22k_ft_in1k created, param count: 98753614
Running train benchmark on caformer_b36.sail_in22k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 64.
Train [8/40]. 296.05 samples/sec. 216.179 ms/step.
Train [16/40]. 296.06 samples/sec. 216.176 ms/step.
Train [24/40]. 296.05 samples/sec. 216.181 ms/step.
Train [32/40]. 296.05 samples/sec. 216.181 ms/step.
Train [40/40]. 296.05 samples/sec. 216.182 ms/step.
Train benchmark of caformer_b36.sail_in22k_ft_in1k done. 294.09 samples/sec, 216.18 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model caformer_b36.sail_in22k_ft_in1k_384 created, param count: 98753614
Running inference benchmark on caformer_b36.sail_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 321.92 samples/sec. 795.222 ms/step.
Infer [16/40]. 321.92 samples/sec. 795.229 ms/step.
Infer [24/40]. 321.92 samples/sec. 795.237 ms/step.
Infer [32/40]. 321.91 samples/sec. 795.252 ms/step.
Infer [40/40]. 321.90 samples/sec. 795.277 ms/step.
Inference benchmark of caformer_b36.sail_in22k_ft_in1k_384 done. 321.88 samples/sec, 795.28 ms/step
Model caformer_b36.sail_in22k_ft_in1k_384 created, param count: 98753614
Running train benchmark on caformer_b36.sail_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 2.25 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.49 GiB is free. Including non-PyTorch memory, this process has 22.15 GiB memory in use. Of the allocated memory 21.31 GiB is allocated by PyTorch, and 352.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model caformer_b36.sail_in22k_ft_in1k_384 created, param count: 98753614
Running train benchmark on caformer_b36.sail_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 1.69 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.63 GiB is free. Including non-PyTorch memory, this process has 22.01 GiB memory in use. Of the allocated memory 21.10 GiB is allocated by PyTorch, and 431.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model caformer_b36.sail_in22k_ft_in1k_384 created, param count: 98753614
Running train benchmark on caformer_b36.sail_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 1.12 GiB. GPU 0 has a total capacty of 23.65 GiB of which 944.06 MiB is free. Including non-PyTorch memory, this process has 22.72 GiB memory in use. Of the allocated memory 22.02 GiB is allocated by PyTorch, and 203.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model caformer_b36.sail_in22k_ft_in1k_384 created, param count: 98753614
Running train benchmark on caformer_b36.sail_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 432.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 140.06 MiB is free. Including non-PyTorch memory, this process has 23.50 GiB memory in use. Of the allocated memory 22.91 GiB is allocated by PyTorch, and 96.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model caformer_b36.sail_in22k_ft_in1k_384 created, param count: 98753614
Running train benchmark on caformer_b36.sail_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 288.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 62.06 MiB is free. Including non-PyTorch memory, this process has 23.58 GiB memory in use. Of the allocated memory 22.94 GiB is allocated by PyTorch, and 142.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model caformer_b36.sail_in22k_ft_in1k_384 created, param count: 98753614
Running train benchmark on caformer_b36.sail_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 216.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 130.06 MiB is free. Including non-PyTorch memory, this process has 23.51 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 56.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model caformer_b36.sail_in22k_ft_in1k_384 created, param count: 98753614
Running train benchmark on caformer_b36.sail_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 32.
Train [8/40]. 97.00 samples/sec. 329.902 ms/step.
Train [16/40]. 96.99 samples/sec. 329.921 ms/step.
Train [24/40]. 96.98 samples/sec. 329.952 ms/step.
Train [32/40]. 96.98 samples/sec. 329.968 ms/step.
Train [40/40]. 96.98 samples/sec. 329.981 ms/step.
Train benchmark of caformer_b36.sail_in22k_ft_in1k_384 done. 96.49 samples/sec, 329.98 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model caformer_m36.sail_in1k created, param count: 56204878
Running inference benchmark on caformer_m36.sail_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1316.08 samples/sec. 194.517 ms/step.
Infer [16/40]. 1316.10 samples/sec. 194.514 ms/step.
Infer [24/40]. 1316.08 samples/sec. 194.517 ms/step.
Infer [32/40]. 1316.07 samples/sec. 194.518 ms/step.
Infer [40/40]. 1316.08 samples/sec. 194.518 ms/step.
Inference benchmark of caformer_m36.sail_in1k done. 1315.86 samples/sec, 194.52 ms/step
Model caformer_m36.sail_in1k created, param count: 56204878
Running train benchmark on caformer_m36.sail_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 294.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 6.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 41.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model caformer_m36.sail_in1k created, param count: 56204878
Running train benchmark on caformer_m36.sail_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 222.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 156.06 MiB is free. Including non-PyTorch memory, this process has 23.49 GiB memory in use. Of the allocated memory 22.75 GiB is allocated by PyTorch, and 245.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model caformer_m36.sail_in1k created, param count: 56204878
Running train benchmark on caformer_m36.sail_in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 20.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 23.09 GiB is allocated by PyTorch, and 31.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model caformer_m36.sail_in1k created, param count: 56204878
Running train benchmark on caformer_m36.sail_in1k for 40 steps w/ input size (3, 224, 224) and batch size 96.
Train [8/40]. 395.44 samples/sec. 242.770 ms/step.
Train [16/40]. 395.44 samples/sec. 242.767 ms/step.
Train [24/40]. 395.42 samples/sec. 242.779 ms/step.
Train [32/40]. 395.43 samples/sec. 242.776 ms/step.
Train [40/40]. 395.43 samples/sec. 242.775 ms/step.
Train benchmark of caformer_m36.sail_in1k done. 393.00 samples/sec, 242.78 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model caformer_m36.sail_in1k_384 created, param count: 56204878
Running inference benchmark on caformer_m36.sail_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 440.36 samples/sec. 581.344 ms/step.
Infer [16/40]. 440.37 samples/sec. 581.335 ms/step.
Infer [24/40]. 440.36 samples/sec. 581.342 ms/step.
Infer [32/40]. 440.36 samples/sec. 581.344 ms/step.
Infer [40/40]. 440.36 samples/sec. 581.343 ms/step.
Inference benchmark of caformer_m36.sail_in1k_384 done. 440.32 samples/sec, 581.34 ms/step
Model caformer_m36.sail_in1k_384 created, param count: 56204878
Running train benchmark on caformer_m36.sail_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 1.69 GiB. GPU 0 has a total capacty of 23.65 GiB of which 364.06 MiB is free. Including non-PyTorch memory, this process has 23.29 GiB memory in use. Of the allocated memory 22.78 GiB is allocated by PyTorch, and 7.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model caformer_m36.sail_in1k_384 created, param count: 56204878
Running train benchmark on caformer_m36.sail_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 1.27 GiB. GPU 0 has a total capacty of 23.65 GiB of which 780.06 MiB is free. Including non-PyTorch memory, this process has 22.88 GiB memory in use. Of the allocated memory 22.20 GiB is allocated by PyTorch, and 181.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model caformer_m36.sail_in1k_384 created, param count: 56204878
Running train benchmark on caformer_m36.sail_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 432.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 246.06 MiB is free. Including non-PyTorch memory, this process has 23.40 GiB memory in use. Of the allocated memory 22.88 GiB is allocated by PyTorch, and 21.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model caformer_m36.sail_in1k_384 created, param count: 56204878
Running train benchmark on caformer_m36.sail_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 324.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 118.06 MiB is free. Including non-PyTorch memory, this process has 23.53 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 132.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model caformer_m36.sail_in1k_384 created, param count: 56204878
Running train benchmark on caformer_m36.sail_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 216.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 114.06 MiB is free. Including non-PyTorch memory, this process has 23.53 GiB memory in use. Of the allocated memory 22.91 GiB is allocated by PyTorch, and 121.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model caformer_m36.sail_in1k_384 created, param count: 56204878
Running train benchmark on caformer_m36.sail_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 82.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 80.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 23.05 GiB is allocated by PyTorch, and 16.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model caformer_m36.sail_in1k_384 created, param count: 56204878
Running train benchmark on caformer_m36.sail_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 32.
Train [8/40]. 130.97 samples/sec. 244.328 ms/step.
Train [16/40]. 130.97 samples/sec. 244.329 ms/step.
Train [24/40]. 130.97 samples/sec. 244.322 ms/step.
Train [32/40]. 130.98 samples/sec. 244.321 ms/step.
Train [40/40]. 130.97 samples/sec. 244.323 ms/step.
Train benchmark of caformer_m36.sail_in1k_384 done. 130.15 samples/sec, 244.32 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model caformer_m36.sail_in22k created, param count: 104243383
Running inference benchmark on caformer_m36.sail_in22k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1315.50 samples/sec. 194.603 ms/step.
Infer [16/40]. 1315.51 samples/sec. 194.602 ms/step.
Infer [24/40]. 1315.47 samples/sec. 194.607 ms/step.
Infer [32/40]. 1315.49 samples/sec. 194.604 ms/step.
Infer [40/40]. 1315.51 samples/sec. 194.602 ms/step.
Inference benchmark of caformer_m36.sail_in22k done. 1315.28 samples/sec, 194.60 ms/step
Model caformer_m36.sail_in22k created, param count: 104243383
Running train benchmark on caformer_m36.sail_in22k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 294.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 150.06 MiB is free. Including non-PyTorch memory, this process has 23.49 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 96.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model caformer_m36.sail_in22k created, param count: 104243383
Running train benchmark on caformer_m36.sail_in22k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 222.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 54.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 22.84 GiB is allocated by PyTorch, and 254.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model caformer_m36.sail_in22k created, param count: 104243383
Running train benchmark on caformer_m36.sail_in22k for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 8.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 20.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model caformer_m36.sail_in22k created, param count: 104243383
Running train benchmark on caformer_m36.sail_in22k for 40 steps w/ input size (3, 224, 224) and batch size 96.
Train [8/40]. 392.68 samples/sec. 244.476 ms/step.
Train [16/40]. 392.69 samples/sec. 244.465 ms/step.
Train [24/40]. 392.69 samples/sec. 244.466 ms/step.
Train [32/40]. 392.69 samples/sec. 244.465 ms/step.
Train [40/40]. 392.70 samples/sec. 244.464 ms/step.
Train benchmark of caformer_m36.sail_in22k done. 390.21 samples/sec, 244.46 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model caformer_m36.sail_in22k_ft_in1k created, param count: 56204878
Running inference benchmark on caformer_m36.sail_in22k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1315.80 samples/sec. 194.558 ms/step.
Infer [16/40]. 1315.82 samples/sec. 194.555 ms/step.
Infer [24/40]. 1315.78 samples/sec. 194.561 ms/step.
Infer [32/40]. 1315.77 samples/sec. 194.562 ms/step.
Infer [40/40]. 1315.76 samples/sec. 194.564 ms/step.
Inference benchmark of caformer_m36.sail_in22k_ft_in1k done. 1315.54 samples/sec, 194.56 ms/step
Model caformer_m36.sail_in22k_ft_in1k created, param count: 56204878
Running train benchmark on caformer_m36.sail_in22k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 294.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 4.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 41.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model caformer_m36.sail_in22k_ft_in1k created, param count: 56204878
Running train benchmark on caformer_m36.sail_in22k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 222.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 164.06 MiB is free. Including non-PyTorch memory, this process has 23.48 GiB memory in use. Of the allocated memory 22.75 GiB is allocated by PyTorch, and 235.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model caformer_m36.sail_in22k_ft_in1k created, param count: 56204878
Running train benchmark on caformer_m36.sail_in22k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 26.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 23.08 GiB is allocated by PyTorch, and 38.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model caformer_m36.sail_in22k_ft_in1k created, param count: 56204878
Running train benchmark on caformer_m36.sail_in22k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 96.
Train [8/40]. 395.27 samples/sec. 242.872 ms/step.
Train [16/40]. 395.28 samples/sec. 242.864 ms/step.
Train [24/40]. 395.28 samples/sec. 242.867 ms/step.
Train [32/40]. 395.28 samples/sec. 242.863 ms/step.
Train [40/40]. 395.29 samples/sec. 242.858 ms/step.
Train benchmark of caformer_m36.sail_in22k_ft_in1k done. 392.84 samples/sec, 242.86 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model caformer_m36.sail_in22k_ft_in1k_384 created, param count: 56204878
Running inference benchmark on caformer_m36.sail_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 440.43 samples/sec. 581.249 ms/step.
Infer [16/40]. 440.43 samples/sec. 581.250 ms/step.
Infer [24/40]. 440.43 samples/sec. 581.252 ms/step.
Infer [32/40]. 440.43 samples/sec. 581.254 ms/step.
Infer [40/40]. 440.43 samples/sec. 581.256 ms/step.
Inference benchmark of caformer_m36.sail_in22k_ft_in1k_384 done. 440.39 samples/sec, 581.26 ms/step
Model caformer_m36.sail_in22k_ft_in1k_384 created, param count: 56204878
Running train benchmark on caformer_m36.sail_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 1.69 GiB. GPU 0 has a total capacty of 23.65 GiB of which 364.06 MiB is free. Including non-PyTorch memory, this process has 23.29 GiB memory in use. Of the allocated memory 22.78 GiB is allocated by PyTorch, and 7.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model caformer_m36.sail_in22k_ft_in1k_384 created, param count: 56204878
Running train benchmark on caformer_m36.sail_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 1.27 GiB. GPU 0 has a total capacty of 23.65 GiB of which 780.06 MiB is free. Including non-PyTorch memory, this process has 22.88 GiB memory in use. Of the allocated memory 22.20 GiB is allocated by PyTorch, and 181.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model caformer_m36.sail_in22k_ft_in1k_384 created, param count: 56204878
Running train benchmark on caformer_m36.sail_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 432.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 138.06 MiB is free. Including non-PyTorch memory, this process has 23.51 GiB memory in use. Of the allocated memory 22.88 GiB is allocated by PyTorch, and 129.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model caformer_m36.sail_in22k_ft_in1k_384 created, param count: 56204878
Running train benchmark on caformer_m36.sail_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 324.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 120.06 MiB is free. Including non-PyTorch memory, this process has 23.52 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 132.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model caformer_m36.sail_in22k_ft_in1k_384 created, param count: 56204878
Running train benchmark on caformer_m36.sail_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 216.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 114.06 MiB is free. Including non-PyTorch memory, this process has 23.53 GiB memory in use. Of the allocated memory 22.91 GiB is allocated by PyTorch, and 121.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model caformer_m36.sail_in22k_ft_in1k_384 created, param count: 56204878
Running train benchmark on caformer_m36.sail_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 82.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 34.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 23.04 GiB is allocated by PyTorch, and 73.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model caformer_m36.sail_in22k_ft_in1k_384 created, param count: 56204878
Running train benchmark on caformer_m36.sail_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 32.
Train [8/40]. 131.05 samples/sec. 244.190 ms/step.
Train [16/40]. 131.04 samples/sec. 244.196 ms/step.
Train [24/40]. 131.04 samples/sec. 244.194 ms/step.
Train [32/40]. 131.04 samples/sec. 244.194 ms/step.
Train [40/40]. 131.04 samples/sec. 244.194 ms/step.
Train benchmark of caformer_m36.sail_in22k_ft_in1k_384 done. 130.20 samples/sec, 244.19 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model caformer_s18.sail_in1k created, param count: 26341656
Running inference benchmark on caformer_s18.sail_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3449.37 samples/sec. 74.216 ms/step.
Infer [16/40]. 3448.76 samples/sec. 74.230 ms/step.
Infer [24/40]. 3448.91 samples/sec. 74.226 ms/step.
Infer [32/40]. 3448.97 samples/sec. 74.225 ms/step.
Infer [40/40]. 3449.07 samples/sec. 74.223 ms/step.
Inference benchmark of caformer_s18.sail_in1k done. 3447.73 samples/sec, 74.22 ms/step
Model caformer_s18.sail_in1k created, param count: 26341656
Running train benchmark on caformer_s18.sail_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 973.74 samples/sec. 262.903 ms/step.
Train [16/40]. 973.75 samples/sec. 262.902 ms/step.
Train [24/40]. 973.74 samples/sec. 262.904 ms/step.
Train [32/40]. 973.75 samples/sec. 262.900 ms/step.
Train [40/40]. 973.75 samples/sec. 262.902 ms/step.
Train benchmark of caformer_s18.sail_in1k done. 969.68 samples/sec, 262.90 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model caformer_s18.sail_in1k_384 created, param count: 26341656
Running inference benchmark on caformer_s18.sail_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 1127.68 samples/sec. 227.015 ms/step.
Infer [16/40]. 1127.71 samples/sec. 227.009 ms/step.
Infer [24/40]. 1127.71 samples/sec. 227.009 ms/step.
Infer [32/40]. 1127.69 samples/sec. 227.012 ms/step.
Infer [40/40]. 1127.70 samples/sec. 227.011 ms/step.
Inference benchmark of caformer_s18.sail_in1k_384 done. 1127.51 samples/sec, 227.01 ms/step
Model caformer_s18.sail_in1k_384 created, param count: 26341656
Running train benchmark on caformer_s18.sail_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 1.12 GiB. GPU 0 has a total capacty of 23.65 GiB of which 948.06 MiB is free. Including non-PyTorch memory, this process has 22.71 GiB memory in use. Of the allocated memory 22.06 GiB is allocated by PyTorch, and 164.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model caformer_s18.sail_in1k_384 created, param count: 26341656
Running train benchmark on caformer_s18.sail_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 432.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 134.06 MiB is free. Including non-PyTorch memory, this process has 23.51 GiB memory in use. Of the allocated memory 22.92 GiB is allocated by PyTorch, and 93.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model caformer_s18.sail_in1k_384 created, param count: 26341656
Running train benchmark on caformer_s18.sail_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 2.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 23.12 GiB is allocated by PyTorch, and 23.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model caformer_s18.sail_in1k_384 created, param count: 26341656
Running train benchmark on caformer_s18.sail_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 96.
Train [8/40]. 319.79 samples/sec. 300.199 ms/step.
Train [16/40]. 319.77 samples/sec. 300.218 ms/step.
Train [24/40]. 319.77 samples/sec. 300.219 ms/step.
Train [32/40]. 319.77 samples/sec. 300.220 ms/step.
Train [40/40]. 319.76 samples/sec. 300.221 ms/step.
Train benchmark of caformer_s18.sail_in1k_384 done. 318.62 samples/sec, 300.22 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model caformer_s18.sail_in22k created, param count: 69044865
Running inference benchmark on caformer_s18.sail_in22k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3438.86 samples/sec. 74.443 ms/step.
Infer [16/40]. 3438.67 samples/sec. 74.447 ms/step.
Infer [24/40]. 3438.62 samples/sec. 74.448 ms/step.
Infer [32/40]. 3438.59 samples/sec. 74.449 ms/step.
Infer [40/40]. 3438.59 samples/sec. 74.449 ms/step.
Inference benchmark of caformer_s18.sail_in22k done. 3437.36 samples/sec, 74.45 ms/step
Model caformer_s18.sail_in22k created, param count: 69044865
Running train benchmark on caformer_s18.sail_in22k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 967.12 samples/sec. 264.704 ms/step.
Train [16/40]. 967.15 samples/sec. 264.696 ms/step.
Train [24/40]. 967.15 samples/sec. 264.696 ms/step.
Train [32/40]. 967.15 samples/sec. 264.695 ms/step.
Train [40/40]. 967.14 samples/sec. 264.697 ms/step.
Train benchmark of caformer_s18.sail_in22k done. 963.48 samples/sec, 264.70 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model caformer_s18.sail_in22k_ft_in1k created, param count: 26341656
Running inference benchmark on caformer_s18.sail_in22k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3451.19 samples/sec. 74.177 ms/step.
Infer [16/40]. 3451.06 samples/sec. 74.180 ms/step.
Infer [24/40]. 3450.70 samples/sec. 74.188 ms/step.
Infer [32/40]. 3450.20 samples/sec. 74.199 ms/step.
Infer [40/40]. 3449.83 samples/sec. 74.207 ms/step.
Inference benchmark of caformer_s18.sail_in22k_ft_in1k done. 3448.63 samples/sec, 74.21 ms/step
Model caformer_s18.sail_in22k_ft_in1k created, param count: 26341656
Running train benchmark on caformer_s18.sail_in22k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 973.56 samples/sec. 262.951 ms/step.
Train [16/40]. 973.56 samples/sec. 262.952 ms/step.
Train [24/40]. 973.54 samples/sec. 262.959 ms/step.
Train [32/40]. 973.51 samples/sec. 262.967 ms/step.
Train [40/40]. 973.48 samples/sec. 262.974 ms/step.
Train benchmark of caformer_s18.sail_in22k_ft_in1k done. 969.73 samples/sec, 262.97 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model caformer_s18.sail_in22k_ft_in1k_384 created, param count: 26341656
Running inference benchmark on caformer_s18.sail_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 1128.02 samples/sec. 226.945 ms/step.
Infer [16/40]. 1127.97 samples/sec. 226.956 ms/step.
Infer [24/40]. 1127.98 samples/sec. 226.954 ms/step.
Infer [32/40]. 1127.99 samples/sec. 226.953 ms/step.
Infer [40/40]. 1127.99 samples/sec. 226.952 ms/step.
Inference benchmark of caformer_s18.sail_in22k_ft_in1k_384 done. 1127.81 samples/sec, 226.95 ms/step
Model caformer_s18.sail_in22k_ft_in1k_384 created, param count: 26341656
Running train benchmark on caformer_s18.sail_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 1.12 GiB. GPU 0 has a total capacty of 23.65 GiB of which 948.06 MiB is free. Including non-PyTorch memory, this process has 22.71 GiB memory in use. Of the allocated memory 22.06 GiB is allocated by PyTorch, and 164.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model caformer_s18.sail_in22k_ft_in1k_384 created, param count: 26341656
Running train benchmark on caformer_s18.sail_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 432.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 134.06 MiB is free. Including non-PyTorch memory, this process has 23.51 GiB memory in use. Of the allocated memory 22.92 GiB is allocated by PyTorch, and 93.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model caformer_s18.sail_in22k_ft_in1k_384 created, param count: 26341656
Running train benchmark on caformer_s18.sail_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 168.06 MiB is free. Including non-PyTorch memory, this process has 23.48 GiB memory in use. Of the allocated memory 22.94 GiB is allocated by PyTorch, and 41.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model caformer_s18.sail_in22k_ft_in1k_384 created, param count: 26341656
Running train benchmark on caformer_s18.sail_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 96.
Train [8/40]. 319.75 samples/sec. 300.233 ms/step.
Train [16/40]. 319.74 samples/sec. 300.243 ms/step.
Train [24/40]. 319.74 samples/sec. 300.242 ms/step.
Train [32/40]. 319.74 samples/sec. 300.247 ms/step.
Train [40/40]. 319.74 samples/sec. 300.247 ms/step.
Train benchmark of caformer_s18.sail_in22k_ft_in1k_384 done. 318.60 samples/sec, 300.25 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model caformer_s36.sail_in1k created, param count: 39297102
Running inference benchmark on caformer_s36.sail_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1837.64 samples/sec. 139.309 ms/step.
Infer [16/40]. 1837.58 samples/sec. 139.314 ms/step.
Infer [24/40]. 1837.59 samples/sec. 139.313 ms/step.
Infer [32/40]. 1837.57 samples/sec. 139.314 ms/step.
Infer [40/40]. 1837.57 samples/sec. 139.315 ms/step.
Inference benchmark of caformer_s36.sail_in1k done. 1837.19 samples/sec, 139.31 ms/step
Model caformer_s36.sail_in1k created, param count: 39297102
Running train benchmark on caformer_s36.sail_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 124.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 6.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 23.01 GiB is allocated by PyTorch, and 124.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model caformer_s36.sail_in1k created, param count: 39297102
Running train benchmark on caformer_s36.sail_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 92.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 62.06 MiB is free. Including non-PyTorch memory, this process has 23.58 GiB memory in use. Of the allocated memory 23.05 GiB is allocated by PyTorch, and 31.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model caformer_s36.sail_in1k created, param count: 39297102
Running train benchmark on caformer_s36.sail_in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
Train [8/40]. 542.18 samples/sec. 236.084 ms/step.
Train [16/40]. 542.20 samples/sec. 236.074 ms/step.
Train [24/40]. 542.20 samples/sec. 236.075 ms/step.
Train [32/40]. 542.20 samples/sec. 236.076 ms/step.
Train [40/40]. 542.20 samples/sec. 236.076 ms/step.
Train benchmark of caformer_s36.sail_in1k done. 538.78 samples/sec, 236.08 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model caformer_s36.sail_in1k_384 created, param count: 39297102
Running inference benchmark on caformer_s36.sail_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 600.72 samples/sec. 426.152 ms/step.
Infer [16/40]. 600.70 samples/sec. 426.167 ms/step.
Infer [24/40]. 600.70 samples/sec. 426.172 ms/step.
Infer [32/40]. 600.70 samples/sec. 426.170 ms/step.
Infer [40/40]. 600.69 samples/sec. 426.175 ms/step.
Inference benchmark of caformer_s36.sail_in1k_384 done. 600.63 samples/sec, 426.18 ms/step
Model caformer_s36.sail_in1k_384 created, param count: 39297102
Running train benchmark on caformer_s36.sail_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 1.12 GiB. GPU 0 has a total capacty of 23.65 GiB of which 922.06 MiB is free. Including non-PyTorch memory, this process has 22.74 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 166.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model caformer_s36.sail_in1k_384 created, param count: 39297102
Running train benchmark on caformer_s36.sail_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 432.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 88.06 MiB is free. Including non-PyTorch memory, this process has 23.55 GiB memory in use. Of the allocated memory 22.94 GiB is allocated by PyTorch, and 114.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model caformer_s36.sail_in1k_384 created, param count: 39297102
Running train benchmark on caformer_s36.sail_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 288.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 154.06 MiB is free. Including non-PyTorch memory, this process has 23.49 GiB memory in use. Of the allocated memory 22.94 GiB is allocated by PyTorch, and 56.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model caformer_s36.sail_in1k_384 created, param count: 39297102
Running train benchmark on caformer_s36.sail_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 216.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 186.06 MiB is free. Including non-PyTorch memory, this process has 23.46 GiB memory in use. Of the allocated memory 22.94 GiB is allocated by PyTorch, and 24.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model caformer_s36.sail_in1k_384 created, param count: 39297102
Running train benchmark on caformer_s36.sail_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 76.06 MiB is free. Including non-PyTorch memory, this process has 23.57 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 225.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model caformer_s36.sail_in1k_384 created, param count: 39297102
Running train benchmark on caformer_s36.sail_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 48.
Train [8/40]. 176.40 samples/sec. 272.111 ms/step.
Train [16/40]. 176.40 samples/sec. 272.110 ms/step.
Train [24/40]. 176.40 samples/sec. 272.109 ms/step.
Train [32/40]. 176.39 samples/sec. 272.121 ms/step.
Train [40/40]. 176.38 samples/sec. 272.145 ms/step.
Train benchmark of caformer_s36.sail_in1k_384 done. 175.34 samples/sec, 272.14 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model caformer_s36.sail_in22k created, param count: 82000311
Running inference benchmark on caformer_s36.sail_in22k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1835.82 samples/sec. 139.447 ms/step.
Infer [16/40]. 1835.76 samples/sec. 139.452 ms/step.
Infer [24/40]. 1835.74 samples/sec. 139.454 ms/step.
Infer [32/40]. 1835.61 samples/sec. 139.463 ms/step.
Infer [40/40]. 1835.49 samples/sec. 139.472 ms/step.
Inference benchmark of caformer_s36.sail_in22k done. 1835.10 samples/sec, 139.47 ms/step
Model caformer_s36.sail_in22k created, param count: 82000311
Running train benchmark on caformer_s36.sail_in22k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 124.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 32.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 22.97 GiB is allocated by PyTorch, and 139.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model caformer_s36.sail_in22k created, param count: 82000311
Running train benchmark on caformer_s36.sail_in22k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 92.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 6.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 23.04 GiB is allocated by PyTorch, and 103.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model caformer_s36.sail_in22k created, param count: 82000311
Running train benchmark on caformer_s36.sail_in22k for 40 steps w/ input size (3, 224, 224) and batch size 128.
Train [8/40]. 538.99 samples/sec. 237.480 ms/step.
Train [16/40]. 538.89 samples/sec. 237.527 ms/step.
Train [24/40]. 538.82 samples/sec. 237.557 ms/step.
Train [32/40]. 538.78 samples/sec. 237.574 ms/step.
Train [40/40]. 538.76 samples/sec. 237.584 ms/step.
Train benchmark of caformer_s36.sail_in22k done. 535.33 samples/sec, 237.58 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model caformer_s36.sail_in22k_ft_in1k created, param count: 39297102
Running inference benchmark on caformer_s36.sail_in22k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1837.62 samples/sec. 139.311 ms/step.
Infer [16/40]. 1837.60 samples/sec. 139.312 ms/step.
Infer [24/40]. 1837.54 samples/sec. 139.317 ms/step.
Infer [32/40]. 1837.55 samples/sec. 139.316 ms/step.
Infer [40/40]. 1837.53 samples/sec. 139.317 ms/step.
Inference benchmark of caformer_s36.sail_in22k_ft_in1k done. 1837.16 samples/sec, 139.32 ms/step
Model caformer_s36.sail_in22k_ft_in1k created, param count: 39297102
Running train benchmark on caformer_s36.sail_in22k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 124.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 6.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 23.01 GiB is allocated by PyTorch, and 124.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model caformer_s36.sail_in22k_ft_in1k created, param count: 39297102
Running train benchmark on caformer_s36.sail_in22k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 92.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 62.06 MiB is free. Including non-PyTorch memory, this process has 23.58 GiB memory in use. Of the allocated memory 23.05 GiB is allocated by PyTorch, and 34.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model caformer_s36.sail_in22k_ft_in1k created, param count: 39297102
Running train benchmark on caformer_s36.sail_in22k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
Train [8/40]. 541.98 samples/sec. 236.171 ms/step.
Train [16/40]. 541.99 samples/sec. 236.168 ms/step.
Train [24/40]. 541.98 samples/sec. 236.170 ms/step.
Train [32/40]. 541.97 samples/sec. 236.175 ms/step.
Train [40/40]. 541.97 samples/sec. 236.174 ms/step.
Train benchmark of caformer_s36.sail_in22k_ft_in1k done. 538.50 samples/sec, 236.17 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model caformer_s36.sail_in22k_ft_in1k_384 created, param count: 39297102
Running inference benchmark on caformer_s36.sail_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 600.75 samples/sec. 426.133 ms/step.
Infer [16/40]. 600.75 samples/sec. 426.133 ms/step.
Infer [24/40]. 600.74 samples/sec. 426.140 ms/step.
Infer [32/40]. 600.74 samples/sec. 426.143 ms/step.
Infer [40/40]. 600.74 samples/sec. 426.145 ms/step.
Inference benchmark of caformer_s36.sail_in22k_ft_in1k_384 done. 600.67 samples/sec, 426.14 ms/step
Model caformer_s36.sail_in22k_ft_in1k_384 created, param count: 39297102
Running train benchmark on caformer_s36.sail_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 1.12 GiB. GPU 0 has a total capacty of 23.65 GiB of which 922.06 MiB is free. Including non-PyTorch memory, this process has 22.74 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 166.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model caformer_s36.sail_in22k_ft_in1k_384 created, param count: 39297102
Running train benchmark on caformer_s36.sail_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 432.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 88.06 MiB is free. Including non-PyTorch memory, this process has 23.55 GiB memory in use. Of the allocated memory 22.94 GiB is allocated by PyTorch, and 114.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model caformer_s36.sail_in22k_ft_in1k_384 created, param count: 39297102
Running train benchmark on caformer_s36.sail_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 288.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 154.06 MiB is free. Including non-PyTorch memory, this process has 23.49 GiB memory in use. Of the allocated memory 22.94 GiB is allocated by PyTorch, and 56.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model caformer_s36.sail_in22k_ft_in1k_384 created, param count: 39297102
Running train benchmark on caformer_s36.sail_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 216.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 186.06 MiB is free. Including non-PyTorch memory, this process has 23.46 GiB memory in use. Of the allocated memory 22.94 GiB is allocated by PyTorch, and 24.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model caformer_s36.sail_in22k_ft_in1k_384 created, param count: 39297102
Running train benchmark on caformer_s36.sail_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 22.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 279.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model caformer_s36.sail_in22k_ft_in1k_384 created, param count: 39297102
Running train benchmark on caformer_s36.sail_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 48.
Train [8/40]. 176.32 samples/sec. 272.229 ms/step.
Train [16/40]. 176.33 samples/sec. 272.213 ms/step.
Train [24/40]. 176.33 samples/sec. 272.219 ms/step.
Train [32/40]. 176.33 samples/sec. 272.221 ms/step.
Train [40/40]. 176.33 samples/sec. 272.216 ms/step.
Train benchmark of caformer_s36.sail_in22k_ft_in1k_384 done. 175.32 samples/sec, 272.22 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model cait_m36_384.fb_dist_in1k created, param count: 271221352
Running inference benchmark on cait_m36_384.fb_dist_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 73.97 samples/sec. 3461.072 ms/step.
Infer [16/40]. 73.97 samples/sec. 3461.002 ms/step.
Infer [24/40]. 73.97 samples/sec. 3460.961 ms/step.
Infer [32/40]. 73.97 samples/sec. 3460.950 ms/step.
Infer [40/40]. 73.97 samples/sec. 3460.950 ms/step.
Inference benchmark of cait_m36_384.fb_dist_in1k done. 73.97 samples/sec, 3460.95 ms/step
Model cait_m36_384.fb_dist_in1k created, param count: 271221352
Running train benchmark on cait_m36_384.fb_dist_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 2.53 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.65 GiB is free. Including non-PyTorch memory, this process has 21.99 GiB memory in use. Of the allocated memory 21.41 GiB is allocated by PyTorch, and 85.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model cait_m36_384.fb_dist_in1k created, param count: 271221352
Running train benchmark on cait_m36_384.fb_dist_in1k for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 1.90 GiB. GPU 0 has a total capacty of 23.65 GiB of which 202.06 MiB is free. Including non-PyTorch memory, this process has 23.44 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 96.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model cait_m36_384.fb_dist_in1k created, param count: 271221352
Running train benchmark on cait_m36_384.fb_dist_in1k for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 1.27 GiB. GPU 0 has a total capacty of 23.65 GiB of which 698.06 MiB is free. Including non-PyTorch memory, this process has 22.96 GiB memory in use. Of the allocated memory 22.36 GiB is allocated by PyTorch, and 104.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model cait_m36_384.fb_dist_in1k created, param count: 271221352
Running train benchmark on cait_m36_384.fb_dist_in1k for 40 steps w/ input size (3, 384, 384) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 972.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 874.06 MiB is free. Including non-PyTorch memory, this process has 22.79 GiB memory in use. Of the allocated memory 22.15 GiB is allocated by PyTorch, and 142.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model cait_m36_384.fb_dist_in1k created, param count: 271221352
Running train benchmark on cait_m36_384.fb_dist_in1k for 40 steps w/ input size (3, 384, 384) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 648.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 570.06 MiB is free. Including non-PyTorch memory, this process has 23.08 GiB memory in use. Of the allocated memory 22.52 GiB is allocated by PyTorch, and 70.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model cait_m36_384.fb_dist_in1k created, param count: 271221352
Running train benchmark on cait_m36_384.fb_dist_in1k for 40 steps w/ input size (3, 384, 384) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 486.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 326.06 MiB is free. Including non-PyTorch memory, this process has 23.32 GiB memory in use. Of the allocated memory 22.73 GiB is allocated by PyTorch, and 99.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model cait_m36_384.fb_dist_in1k created, param count: 271221352
Running train benchmark on cait_m36_384.fb_dist_in1k for 40 steps w/ input size (3, 384, 384) and batch size 32.
ERROR: "CUDA out of memory. Tried to allocate 324.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 126.06 MiB is free. Including non-PyTorch memory, this process has 23.52 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 62.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model cait_m36_384.fb_dist_in1k created, param count: 271221352
Running train benchmark on cait_m36_384.fb_dist_in1k for 40 steps w/ input size (3, 384, 384) and batch size 24.
ERROR: "CUDA out of memory. Tried to allocate 62.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 54.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 22.86 GiB is allocated by PyTorch, and 233.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model cait_m36_384.fb_dist_in1k created, param count: 271221352
Running train benchmark on cait_m36_384.fb_dist_in1k for 40 steps w/ input size (3, 384, 384) and batch size 16.
ERROR: "CUDA out of memory. Tried to allocate 162.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 160.06 MiB is free. Including non-PyTorch memory, this process has 23.48 GiB memory in use. Of the allocated memory 22.91 GiB is allocated by PyTorch, and 77.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 12 for retry.
Model cait_m36_384.fb_dist_in1k created, param count: 271221352
Running train benchmark on cait_m36_384.fb_dist_in1k for 40 steps w/ input size (3, 384, 384) and batch size 12.
ERROR: "CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 12.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 286.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 8 for retry.
Model cait_m36_384.fb_dist_in1k created, param count: 271221352
Running train benchmark on cait_m36_384.fb_dist_in1k for 40 steps w/ input size (3, 384, 384) and batch size 8.
Train [8/40]. 29.13 samples/sec. 274.658 ms/step.
Train [16/40]. 29.13 samples/sec. 274.606 ms/step.
Train [24/40]. 29.13 samples/sec. 274.628 ms/step.
Train [32/40]. 29.13 samples/sec. 274.634 ms/step.
Train [40/40]. 29.13 samples/sec. 274.646 ms/step.
Train benchmark of cait_m36_384.fb_dist_in1k done. 28.84 samples/sec, 274.65 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model cait_m48_448.fb_dist_in1k created, param count: 356460520
Running inference benchmark on cait_m48_448.fb_dist_in1k for 40 steps w/ input size (3, 448, 448) and batch size 256.
Infer [8/40]. 31.76 samples/sec. 8061.281 ms/step.
Infer [16/40]. 31.76 samples/sec. 8061.226 ms/step.
Infer [24/40]. 31.76 samples/sec. 8061.111 ms/step.
Infer [32/40]. 31.76 samples/sec. 8061.109 ms/step.
Infer [40/40]. 31.76 samples/sec. 8061.179 ms/step.
Inference benchmark of cait_m48_448.fb_dist_in1k done. 31.76 samples/sec, 8061.18 ms/step
Model cait_m48_448.fb_dist_in1k created, param count: 356460520
Running train benchmark on cait_m48_448.fb_dist_in1k for 40 steps w/ input size (3, 448, 448) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 4.69 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1006.06 MiB is free. Including non-PyTorch memory, this process has 22.66 GiB memory in use. Of the allocated memory 22.02 GiB is allocated by PyTorch, and 138.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model cait_m48_448.fb_dist_in1k created, param count: 356460520
Running train benchmark on cait_m48_448.fb_dist_in1k for 40 steps w/ input size (3, 448, 448) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 3.52 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.93 GiB is free. Including non-PyTorch memory, this process has 21.71 GiB memory in use. Of the allocated memory 20.59 GiB is allocated by PyTorch, and 631.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model cait_m48_448.fb_dist_in1k created, param count: 356460520
Running train benchmark on cait_m48_448.fb_dist_in1k for 40 steps w/ input size (3, 448, 448) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 2.35 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.81 GiB is free. Including non-PyTorch memory, this process has 21.83 GiB memory in use. Of the allocated memory 20.99 GiB is allocated by PyTorch, and 353.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model cait_m48_448.fb_dist_in1k created, param count: 356460520
Running train benchmark on cait_m48_448.fb_dist_in1k for 40 steps w/ input size (3, 448, 448) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 1.76 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.29 GiB is free. Including non-PyTorch memory, this process has 22.35 GiB memory in use. Of the allocated memory 21.37 GiB is allocated by PyTorch, and 495.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model cait_m48_448.fb_dist_in1k created, param count: 356460520
Running train benchmark on cait_m48_448.fb_dist_in1k for 40 steps w/ input size (3, 448, 448) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 1.17 GiB. GPU 0 has a total capacty of 23.65 GiB of which 76.06 MiB is free. Including non-PyTorch memory, this process has 23.57 GiB memory in use. Of the allocated memory 22.80 GiB is allocated by PyTorch, and 277.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model cait_m48_448.fb_dist_in1k created, param count: 356460520
Running train benchmark on cait_m48_448.fb_dist_in1k for 40 steps w/ input size (3, 448, 448) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 902.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 186.06 MiB is free. Including non-PyTorch memory, this process has 23.46 GiB memory in use. Of the allocated memory 22.65 GiB is allocated by PyTorch, and 321.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model cait_m48_448.fb_dist_in1k created, param count: 356460520
Running train benchmark on cait_m48_448.fb_dist_in1k for 40 steps w/ input size (3, 448, 448) and batch size 32.
ERROR: "CUDA out of memory. Tried to allocate 602.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 338.06 MiB is free. Including non-PyTorch memory, this process has 23.31 GiB memory in use. Of the allocated memory 22.53 GiB is allocated by PyTorch, and 288.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model cait_m48_448.fb_dist_in1k created, param count: 356460520
Running train benchmark on cait_m48_448.fb_dist_in1k for 40 steps w/ input size (3, 448, 448) and batch size 24.
ERROR: "CUDA out of memory. Tried to allocate 452.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 402.06 MiB is free. Including non-PyTorch memory, this process has 23.25 GiB memory in use. Of the allocated memory 22.44 GiB is allocated by PyTorch, and 312.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model cait_m48_448.fb_dist_in1k created, param count: 356460520
Running train benchmark on cait_m48_448.fb_dist_in1k for 40 steps w/ input size (3, 448, 448) and batch size 16.
ERROR: "CUDA out of memory. Tried to allocate 302.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 140.06 MiB is free. Including non-PyTorch memory, this process has 23.50 GiB memory in use. Of the allocated memory 22.66 GiB is allocated by PyTorch, and 350.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 12 for retry.
Model cait_m48_448.fb_dist_in1k created, param count: 356460520
Running train benchmark on cait_m48_448.fb_dist_in1k for 40 steps w/ input size (3, 448, 448) and batch size 12.
ERROR: "CUDA out of memory. Tried to allocate 226.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 112.06 MiB is free. Including non-PyTorch memory, this process has 23.53 GiB memory in use. Of the allocated memory 22.86 GiB is allocated by PyTorch, and 179.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 8 for retry.
Model cait_m48_448.fb_dist_in1k created, param count: 356460520
Running train benchmark on cait_m48_448.fb_dist_in1k for 40 steps w/ input size (3, 448, 448) and batch size 8.
ERROR: "CUDA out of memory. Tried to allocate 152.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 96.06 MiB is free. Including non-PyTorch memory, this process has 23.55 GiB memory in use. Of the allocated memory 22.60 GiB is allocated by PyTorch, and 462.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 6 for retry.
Model cait_m48_448.fb_dist_in1k created, param count: 356460520
Running train benchmark on cait_m48_448.fb_dist_in1k for 40 steps w/ input size (3, 448, 448) and batch size 6.
ERROR: "CUDA out of memory. Tried to allocate 114.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 20.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.50 GiB is allocated by PyTorch, and 636.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 4 for retry.
Model cait_m48_448.fb_dist_in1k created, param count: 356460520
Running train benchmark on cait_m48_448.fb_dist_in1k for 40 steps w/ input size (3, 448, 448) and batch size 4.
Train [8/40]. 12.42 samples/sec. 322.084 ms/step.
Train [16/40]. 12.44 samples/sec. 321.618 ms/step.
Train [24/40]. 12.44 samples/sec. 321.459 ms/step.
Train [32/40]. 12.45 samples/sec. 321.379 ms/step.
Train [40/40]. 12.45 samples/sec. 321.319 ms/step.
Train benchmark of cait_m48_448.fb_dist_in1k done. 12.30 samples/sec, 321.32 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model cait_s24_224.fb_dist_in1k created, param count: 46916200
Running inference benchmark on cait_s24_224.fb_dist_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1506.01 samples/sec. 169.985 ms/step.
Infer [16/40]. 1506.01 samples/sec. 169.986 ms/step.
Infer [24/40]. 1506.05 samples/sec. 169.981 ms/step.
Infer [32/40]. 1506.08 samples/sec. 169.978 ms/step.
Infer [40/40]. 1506.10 samples/sec. 169.976 ms/step.
Inference benchmark of cait_s24_224.fb_dist_in1k done. 1505.84 samples/sec, 169.98 ms/step
Model cait_s24_224.fb_dist_in1k created, param count: 46916200
Running train benchmark on cait_s24_224.fb_dist_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 152.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 50.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 22.67 GiB is allocated by PyTorch, and 429.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model cait_s24_224.fb_dist_in1k created, param count: 46916200
Running train benchmark on cait_s24_224.fb_dist_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 94.06 MiB is free. Including non-PyTorch memory, this process has 23.55 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 360.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model cait_s24_224.fb_dist_in1k created, param count: 46916200
Running train benchmark on cait_s24_224.fb_dist_in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
Train [8/40]. 565.36 samples/sec. 226.404 ms/step.
Train [16/40]. 565.35 samples/sec. 226.409 ms/step.
Train [24/40]. 565.36 samples/sec. 226.403 ms/step.
Train [32/40]. 565.38 samples/sec. 226.395 ms/step.
Train [40/40]. 565.40 samples/sec. 226.387 ms/step.
Train benchmark of cait_s24_224.fb_dist_in1k done. 560.53 samples/sec, 226.39 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model cait_s24_384.fb_dist_in1k created, param count: 47062120
Running inference benchmark on cait_s24_384.fb_dist_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 258.27 samples/sec. 991.201 ms/step.
Infer [16/40]. 258.27 samples/sec. 991.201 ms/step.
Infer [24/40]. 258.27 samples/sec. 991.215 ms/step.
Infer [32/40]. 258.27 samples/sec. 991.211 ms/step.
Infer [40/40]. 258.27 samples/sec. 991.208 ms/step.
Inference benchmark of cait_s24_384.fb_dist_in1k done. 258.26 samples/sec, 991.21 ms/step
Model cait_s24_384.fb_dist_in1k created, param count: 47062120
Running train benchmark on cait_s24_384.fb_dist_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 1.27 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.09 GiB is free. Including non-PyTorch memory, this process has 22.55 GiB memory in use. Of the allocated memory 22.05 GiB is allocated by PyTorch, and 4.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model cait_s24_384.fb_dist_in1k created, param count: 47062120
Running train benchmark on cait_s24_384.fb_dist_in1k for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 972.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 206.06 MiB is free. Including non-PyTorch memory, this process has 23.44 GiB memory in use. Of the allocated memory 22.75 GiB is allocated by PyTorch, and 195.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model cait_s24_384.fb_dist_in1k created, param count: 47062120
Running train benchmark on cait_s24_384.fb_dist_in1k for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 648.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 356.06 MiB is free. Including non-PyTorch memory, this process has 23.29 GiB memory in use. Of the allocated memory 22.79 GiB is allocated by PyTorch, and 7.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model cait_s24_384.fb_dist_in1k created, param count: 47062120
Running train benchmark on cait_s24_384.fb_dist_in1k for 40 steps w/ input size (3, 384, 384) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 486.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 224.06 MiB is free. Including non-PyTorch memory, this process has 23.42 GiB memory in use. Of the allocated memory 22.82 GiB is allocated by PyTorch, and 110.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model cait_s24_384.fb_dist_in1k created, param count: 47062120
Running train benchmark on cait_s24_384.fb_dist_in1k for 40 steps w/ input size (3, 384, 384) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 324.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 84.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 23.05 GiB is allocated by PyTorch, and 14.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model cait_s24_384.fb_dist_in1k created, param count: 47062120
Running train benchmark on cait_s24_384.fb_dist_in1k for 40 steps w/ input size (3, 384, 384) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 244.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 160.06 MiB is free. Including non-PyTorch memory, this process has 23.48 GiB memory in use. Of the allocated memory 22.82 GiB is allocated by PyTorch, and 174.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model cait_s24_384.fb_dist_in1k created, param count: 47062120
Running train benchmark on cait_s24_384.fb_dist_in1k for 40 steps w/ input size (3, 384, 384) and batch size 32.
Train [8/40]. 102.33 samples/sec. 312.724 ms/step.
Train [16/40]. 102.33 samples/sec. 312.720 ms/step.
Train [24/40]. 102.33 samples/sec. 312.717 ms/step.
Train [32/40]. 102.33 samples/sec. 312.714 ms/step.
Train [40/40]. 102.33 samples/sec. 312.714 ms/step.
Train benchmark of cait_s24_384.fb_dist_in1k done. 101.66 samples/sec, 312.71 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model cait_s36_384.fb_dist_in1k created, param count: 68366632
Running inference benchmark on cait_s36_384.fb_dist_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 172.50 samples/sec. 1484.019 ms/step.
Infer [16/40]. 172.50 samples/sec. 1484.025 ms/step.
Infer [24/40]. 172.50 samples/sec. 1484.019 ms/step.
Infer [32/40]. 172.51 samples/sec. 1484.006 ms/step.
Infer [40/40]. 172.51 samples/sec. 1484.002 ms/step.
Inference benchmark of cait_s36_384.fb_dist_in1k done. 172.50 samples/sec, 1484.00 ms/step
Model cait_s36_384.fb_dist_in1k created, param count: 68366632
Running train benchmark on cait_s36_384.fb_dist_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 1.27 GiB. GPU 0 has a total capacty of 23.65 GiB of which 992.06 MiB is free. Including non-PyTorch memory, this process has 22.67 GiB memory in use. Of the allocated memory 22.09 GiB is allocated by PyTorch, and 85.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model cait_s36_384.fb_dist_in1k created, param count: 68366632
Running train benchmark on cait_s36_384.fb_dist_in1k for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 972.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 272.06 MiB is free. Including non-PyTorch memory, this process has 23.38 GiB memory in use. Of the allocated memory 22.79 GiB is allocated by PyTorch, and 87.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model cait_s36_384.fb_dist_in1k created, param count: 68366632
Running train benchmark on cait_s36_384.fb_dist_in1k for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 648.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 284.06 MiB is free. Including non-PyTorch memory, this process has 23.36 GiB memory in use. Of the allocated memory 22.83 GiB is allocated by PyTorch, and 38.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model cait_s36_384.fb_dist_in1k created, param count: 68366632
Running train benchmark on cait_s36_384.fb_dist_in1k for 40 steps w/ input size (3, 384, 384) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 486.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 164.06 MiB is free. Including non-PyTorch memory, this process has 23.48 GiB memory in use. Of the allocated memory 22.86 GiB is allocated by PyTorch, and 129.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model cait_s36_384.fb_dist_in1k created, param count: 68366632
Running train benchmark on cait_s36_384.fb_dist_in1k for 40 steps w/ input size (3, 384, 384) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 324.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 40.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 23.09 GiB is allocated by PyTorch, and 17.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model cait_s36_384.fb_dist_in1k created, param count: 68366632
Running train benchmark on cait_s36_384.fb_dist_in1k for 40 steps w/ input size (3, 384, 384) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 244.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 142.06 MiB is free. Including non-PyTorch memory, this process has 23.50 GiB memory in use. Of the allocated memory 22.86 GiB is allocated by PyTorch, and 151.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model cait_s36_384.fb_dist_in1k created, param count: 68366632
Running train benchmark on cait_s36_384.fb_dist_in1k for 40 steps w/ input size (3, 384, 384) and batch size 32.
ERROR: "CUDA out of memory. Tried to allocate 162.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 130.06 MiB is free. Including non-PyTorch memory, this process has 23.51 GiB memory in use. Of the allocated memory 22.94 GiB is allocated by PyTorch, and 74.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model cait_s36_384.fb_dist_in1k created, param count: 68366632
Running train benchmark on cait_s36_384.fb_dist_in1k for 40 steps w/ input size (3, 384, 384) and batch size 24.
ERROR: "CUDA out of memory. Tried to allocate 122.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 112.06 MiB is free. Including non-PyTorch memory, this process has 23.53 GiB memory in use. Of the allocated memory 22.76 GiB is allocated by PyTorch, and 283.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model cait_s36_384.fb_dist_in1k created, param count: 68366632
Running train benchmark on cait_s36_384.fb_dist_in1k for 40 steps w/ input size (3, 384, 384) and batch size 16.
Train [8/40]. 68.25 samples/sec. 234.449 ms/step.
Train [16/40]. 68.24 samples/sec. 234.460 ms/step.
Train [24/40]. 68.24 samples/sec. 234.472 ms/step.
Train [32/40]. 68.24 samples/sec. 234.478 ms/step.
Train [40/40]. 68.23 samples/sec. 234.485 ms/step.
Train benchmark of cait_s36_384.fb_dist_in1k done. 67.48 samples/sec, 234.49 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model cait_xs24_384.fb_dist_in1k created, param count: 26670088
Running inference benchmark on cait_xs24_384.fb_dist_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 336.56 samples/sec. 760.642 ms/step.
Infer [16/40]. 336.56 samples/sec. 760.636 ms/step.
Infer [24/40]. 336.56 samples/sec. 760.645 ms/step.
Infer [32/40]. 336.55 samples/sec. 760.652 ms/step.
Infer [40/40]. 336.55 samples/sec. 760.652 ms/step.
Inference benchmark of cait_xs24_384.fb_dist_in1k done. 336.54 samples/sec, 760.65 ms/step
Model cait_xs24_384.fb_dist_in1k created, param count: 26670088
Running train benchmark on cait_xs24_384.fb_dist_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 972.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 262.06 MiB is free. Including non-PyTorch memory, this process has 23.38 GiB memory in use. Of the allocated memory 22.77 GiB is allocated by PyTorch, and 120.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model cait_xs24_384.fb_dist_in1k created, param count: 26670088
Running train benchmark on cait_xs24_384.fb_dist_in1k for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 730.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 570.06 MiB is free. Including non-PyTorch memory, this process has 23.08 GiB memory in use. Of the allocated memory 22.45 GiB is allocated by PyTorch, and 140.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model cait_xs24_384.fb_dist_in1k created, param count: 26670088
Running train benchmark on cait_xs24_384.fb_dist_in1k for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 486.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 290.06 MiB is free. Including non-PyTorch memory, this process has 23.36 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 53.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model cait_xs24_384.fb_dist_in1k created, param count: 26670088
Running train benchmark on cait_xs24_384.fb_dist_in1k for 40 steps w/ input size (3, 384, 384) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 366.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 332.06 MiB is free. Including non-PyTorch memory, this process has 23.32 GiB memory in use. Of the allocated memory 22.65 GiB is allocated by PyTorch, and 174.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model cait_xs24_384.fb_dist_in1k created, param count: 26670088
Running train benchmark on cait_xs24_384.fb_dist_in1k for 40 steps w/ input size (3, 384, 384) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 244.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 206.06 MiB is free. Including non-PyTorch memory, this process has 23.44 GiB memory in use. Of the allocated memory 22.79 GiB is allocated by PyTorch, and 151.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model cait_xs24_384.fb_dist_in1k created, param count: 26670088
Running train benchmark on cait_xs24_384.fb_dist_in1k for 40 steps w/ input size (3, 384, 384) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 184.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 104.06 MiB is free. Including non-PyTorch memory, this process has 23.54 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 236.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model cait_xs24_384.fb_dist_in1k created, param count: 26670088
Running train benchmark on cait_xs24_384.fb_dist_in1k for 40 steps w/ input size (3, 384, 384) and batch size 32.
Train [8/40]. 133.13 samples/sec. 240.361 ms/step.
Train [16/40]. 133.14 samples/sec. 240.355 ms/step.
Train [24/40]. 133.14 samples/sec. 240.353 ms/step.
Train [32/40]. 133.14 samples/sec. 240.351 ms/step.
Train [40/40]. 133.14 samples/sec. 240.353 ms/step.
Train benchmark of cait_xs24_384.fb_dist_in1k done. 132.02 samples/sec, 240.35 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model cait_xxs24_224.fb_dist_in1k created, param count: 11956264
Running inference benchmark on cait_xxs24_224.fb_dist_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 2982.01 samples/sec. 85.848 ms/step.
Infer [16/40]. 2981.63 samples/sec. 85.859 ms/step.
Infer [24/40]. 2981.60 samples/sec. 85.860 ms/step.
Infer [32/40]. 2981.42 samples/sec. 85.865 ms/step.
Infer [40/40]. 2981.25 samples/sec. 85.870 ms/step.
Inference benchmark of cait_xxs24_224.fb_dist_in1k done. 2980.34 samples/sec, 85.87 ms/step
Model cait_xxs24_224.fb_dist_in1k created, param count: 11956264
Running train benchmark on cait_xxs24_224.fb_dist_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1100.51 samples/sec. 232.620 ms/step.
Train [16/40]. 1100.55 samples/sec. 232.611 ms/step.
Train [24/40]. 1100.58 samples/sec. 232.604 ms/step.
Train [32/40]. 1100.60 samples/sec. 232.600 ms/step.
Train [40/40]. 1100.62 samples/sec. 232.597 ms/step.
Train benchmark of cait_xxs24_224.fb_dist_in1k done. 1091.18 samples/sec, 232.60 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model cait_xxs24_384.fb_dist_in1k created, param count: 12029224
Running inference benchmark on cait_xxs24_384.fb_dist_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 471.42 samples/sec. 543.036 ms/step.
Infer [16/40]. 471.42 samples/sec. 543.035 ms/step.
Infer [24/40]. 471.42 samples/sec. 543.039 ms/step.
Infer [32/40]. 471.42 samples/sec. 543.039 ms/step.
Infer [40/40]. 471.42 samples/sec. 543.039 ms/step.
Inference benchmark of cait_xxs24_384.fb_dist_in1k done. 471.40 samples/sec, 543.04 ms/step
Model cait_xxs24_384.fb_dist_in1k created, param count: 12029224
Running train benchmark on cait_xxs24_384.fb_dist_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 648.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 250.06 MiB is free. Including non-PyTorch memory, this process has 23.40 GiB memory in use. Of the allocated memory 22.83 GiB is allocated by PyTorch, and 66.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model cait_xxs24_384.fb_dist_in1k created, param count: 12029224
Running train benchmark on cait_xxs24_384.fb_dist_in1k for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 486.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 230.06 MiB is free. Including non-PyTorch memory, this process has 23.42 GiB memory in use. Of the allocated memory 22.84 GiB is allocated by PyTorch, and 82.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model cait_xxs24_384.fb_dist_in1k created, param count: 12029224
Running train benchmark on cait_xxs24_384.fb_dist_in1k for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 324.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 68.06 MiB is free. Including non-PyTorch memory, this process has 23.57 GiB memory in use. Of the allocated memory 23.04 GiB is allocated by PyTorch, and 36.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model cait_xxs24_384.fb_dist_in1k created, param count: 12029224
Running train benchmark on cait_xxs24_384.fb_dist_in1k for 40 steps w/ input size (3, 384, 384) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 244.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 216.06 MiB is free. Including non-PyTorch memory, this process has 23.43 GiB memory in use. Of the allocated memory 22.80 GiB is allocated by PyTorch, and 137.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model cait_xxs24_384.fb_dist_in1k created, param count: 12029224
Running train benchmark on cait_xxs24_384.fb_dist_in1k for 40 steps w/ input size (3, 384, 384) and batch size 64.
Train [8/40]. 191.32 samples/sec. 334.511 ms/step.
Train [16/40]. 191.32 samples/sec. 334.515 ms/step.
Train [24/40]. 191.33 samples/sec. 334.509 ms/step.
Train [32/40]. 191.32 samples/sec. 334.510 ms/step.
Train [40/40]. 191.33 samples/sec. 334.509 ms/step.
Train benchmark of cait_xxs24_384.fb_dist_in1k done. 190.18 samples/sec, 334.51 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model cait_xxs36_224.fb_dist_in1k created, param count: 17299720
Running inference benchmark on cait_xxs36_224.fb_dist_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1997.02 samples/sec. 128.191 ms/step.
Infer [16/40]. 1996.92 samples/sec. 128.197 ms/step.
Infer [24/40]. 1996.87 samples/sec. 128.201 ms/step.
Infer [32/40]. 1996.90 samples/sec. 128.199 ms/step.
Infer [40/40]. 1996.83 samples/sec. 128.203 ms/step.
Inference benchmark of cait_xxs36_224.fb_dist_in1k done. 1996.43 samples/sec, 128.20 ms/step
Model cait_xxs36_224.fb_dist_in1k created, param count: 17299720
Running train benchmark on cait_xxs36_224.fb_dist_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 8.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.62 GiB is allocated by PyTorch, and 531.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model cait_xxs36_224.fb_dist_in1k created, param count: 17299720
Running train benchmark on cait_xxs36_224.fb_dist_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
Train [8/40]. 771.47 samples/sec. 248.877 ms/step.
Train [16/40]. 771.55 samples/sec. 248.851 ms/step.
Train [24/40]. 771.51 samples/sec. 248.864 ms/step.
Train [32/40]. 771.05 samples/sec. 249.012 ms/step.
Train [40/40]. 771.14 samples/sec. 248.981 ms/step.
Train benchmark of cait_xxs36_224.fb_dist_in1k done. 762.97 samples/sec, 248.98 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model cait_xxs36_384.fb_dist_in1k created, param count: 17372680
Running inference benchmark on cait_xxs36_384.fb_dist_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 314.92 samples/sec. 812.911 ms/step.
Infer [16/40]. 314.92 samples/sec. 812.916 ms/step.
Infer [24/40]. 314.91 samples/sec. 812.918 ms/step.
Infer [32/40]. 314.91 samples/sec. 812.918 ms/step.
Infer [40/40]. 314.89 samples/sec. 812.984 ms/step.
Inference benchmark of cait_xxs36_384.fb_dist_in1k done. 314.88 samples/sec, 812.98 ms/step
Model cait_xxs36_384.fb_dist_in1k created, param count: 17372680
Running train benchmark on cait_xxs36_384.fb_dist_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 648.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 240.06 MiB is free. Including non-PyTorch memory, this process has 23.41 GiB memory in use. Of the allocated memory 22.84 GiB is allocated by PyTorch, and 65.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model cait_xxs36_384.fb_dist_in1k created, param count: 17372680
Running train benchmark on cait_xxs36_384.fb_dist_in1k for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 486.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 220.06 MiB is free. Including non-PyTorch memory, this process has 23.43 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 82.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model cait_xxs36_384.fb_dist_in1k created, param count: 17372680
Running train benchmark on cait_xxs36_384.fb_dist_in1k for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 324.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 58.06 MiB is free. Including non-PyTorch memory, this process has 23.58 GiB memory in use. Of the allocated memory 23.05 GiB is allocated by PyTorch, and 36.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model cait_xxs36_384.fb_dist_in1k created, param count: 17372680
Running train benchmark on cait_xxs36_384.fb_dist_in1k for 40 steps w/ input size (3, 384, 384) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 244.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 200.06 MiB is free. Including non-PyTorch memory, this process has 23.45 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 143.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model cait_xxs36_384.fb_dist_in1k created, param count: 17372680
Running train benchmark on cait_xxs36_384.fb_dist_in1k for 40 steps w/ input size (3, 384, 384) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 162.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 44.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 23.04 GiB is allocated by PyTorch, and 63.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model cait_xxs36_384.fb_dist_in1k created, param count: 17372680
Running train benchmark on cait_xxs36_384.fb_dist_in1k for 40 steps w/ input size (3, 384, 384) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 122.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 78.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 265.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model cait_xxs36_384.fb_dist_in1k created, param count: 17372680
Running train benchmark on cait_xxs36_384.fb_dist_in1k for 40 steps w/ input size (3, 384, 384) and batch size 32.
Train [8/40]. 127.80 samples/sec. 250.383 ms/step.
Train [16/40]. 127.80 samples/sec. 250.396 ms/step.
Train [24/40]. 127.80 samples/sec. 250.400 ms/step.
Train [32/40]. 127.79 samples/sec. 250.402 ms/step.
Train [40/40]. 127.79 samples/sec. 250.405 ms/step.
Train benchmark of cait_xxs36_384.fb_dist_in1k done. 126.46 samples/sec, 250.41 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model coat_lite_medium.in1k created, param count: 44571048
Running inference benchmark on coat_lite_medium.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1583.59 samples/sec. 161.658 ms/step.
Infer [16/40]. 1583.58 samples/sec. 161.659 ms/step.
Infer [24/40]. 1583.55 samples/sec. 161.662 ms/step.
Infer [32/40]. 1583.64 samples/sec. 161.653 ms/step.
Infer [40/40]. 1583.59 samples/sec. 161.658 ms/step.
Inference benchmark of coat_lite_medium.in1k done. 1583.31 samples/sec, 161.66 ms/step
Model coat_lite_medium.in1k created, param count: 44571048
Running train benchmark on coat_lite_medium.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 394.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 168.06 MiB is free. Including non-PyTorch memory, this process has 23.48 GiB memory in use. Of the allocated memory 22.66 GiB is allocated by PyTorch, and 324.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model coat_lite_medium.in1k created, param count: 44571048
Running train benchmark on coat_lite_medium.in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 70.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 14.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 23.01 GiB is allocated by PyTorch, and 122.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model coat_lite_medium.in1k created, param count: 44571048
Running train benchmark on coat_lite_medium.in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
Train [8/40]. 485.46 samples/sec. 263.669 ms/step.
Train [16/40]. 485.46 samples/sec. 263.666 ms/step.
Train [24/40]. 485.47 samples/sec. 263.663 ms/step.
Train [32/40]. 485.46 samples/sec. 263.668 ms/step.
Train [40/40]. 485.47 samples/sec. 263.662 ms/step.
Train benchmark of coat_lite_medium.in1k done. 481.54 samples/sec, 263.66 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model coat_lite_medium_384.in1k created, param count: 44571048
Running inference benchmark on coat_lite_medium_384.in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 478.03 samples/sec. 535.535 ms/step.
Infer [16/40]. 478.03 samples/sec. 535.533 ms/step.
Infer [24/40]. 478.03 samples/sec. 535.526 ms/step.
Infer [32/40]. 478.03 samples/sec. 535.530 ms/step.
Infer [40/40]. 478.02 samples/sec. 535.539 ms/step.
Inference benchmark of coat_lite_medium_384.in1k done. 477.98 samples/sec, 535.54 ms/step
Model coat_lite_medium_384.in1k created, param count: 44571048
Running train benchmark on coat_lite_medium_384.in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 2.25 GiB. GPU 0 has a total capacty of 23.65 GiB of which 308.06 MiB is free. Including non-PyTorch memory, this process has 23.34 GiB memory in use. Of the allocated memory 22.34 GiB is allocated by PyTorch, and 512.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model coat_lite_medium_384.in1k created, param count: 44571048
Running train benchmark on coat_lite_medium_384.in1k for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 162.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 162.06 MiB is free. Including non-PyTorch memory, this process has 23.48 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 181.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model coat_lite_medium_384.in1k created, param count: 44571048
Running train benchmark on coat_lite_medium_384.in1k for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 434.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 374.06 MiB is free. Including non-PyTorch memory, this process has 23.28 GiB memory in use. Of the allocated memory 22.37 GiB is allocated by PyTorch, and 419.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model coat_lite_medium_384.in1k created, param count: 44571048
Running train benchmark on coat_lite_medium_384.in1k for 40 steps w/ input size (3, 384, 384) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 434.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 48.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 22.72 GiB is allocated by PyTorch, and 384.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model coat_lite_medium_384.in1k created, param count: 44571048
Running train benchmark on coat_lite_medium_384.in1k for 40 steps w/ input size (3, 384, 384) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 92.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 68.06 MiB is free. Including non-PyTorch memory, this process has 23.57 GiB memory in use. Of the allocated memory 22.68 GiB is allocated by PyTorch, and 403.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model coat_lite_medium_384.in1k created, param count: 44571048
Running train benchmark on coat_lite_medium_384.in1k for 40 steps w/ input size (3, 384, 384) and batch size 48.
Train [8/40]. 163.82 samples/sec. 292.999 ms/step.
Train [16/40]. 163.83 samples/sec. 292.981 ms/step.
Train [24/40]. 163.83 samples/sec. 292.994 ms/step.
Train [32/40]. 163.83 samples/sec. 292.990 ms/step.
Train [40/40]. 163.83 samples/sec. 292.991 ms/step.
Train benchmark of coat_lite_medium_384.in1k done. 162.65 samples/sec, 292.99 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model coat_lite_mini.in1k created, param count: 11011560
Running inference benchmark on coat_lite_mini.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 5334.23 samples/sec. 47.992 ms/step.
Infer [16/40]. 5334.35 samples/sec. 47.991 ms/step.
Infer [24/40]. 5334.51 samples/sec. 47.989 ms/step.
Infer [32/40]. 5334.57 samples/sec. 47.989 ms/step.
Infer [40/40]. 5334.23 samples/sec. 47.992 ms/step.
Inference benchmark of coat_lite_mini.in1k done. 5331.40 samples/sec, 47.99 ms/step
Model coat_lite_mini.in1k created, param count: 11011560
Running train benchmark on coat_lite_mini.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1558.77 samples/sec. 164.232 ms/step.
Train [16/40]. 1558.73 samples/sec. 164.236 ms/step.
Train [24/40]. 1558.74 samples/sec. 164.235 ms/step.
Train [32/40]. 1558.74 samples/sec. 164.235 ms/step.
Train [40/40]. 1558.76 samples/sec. 164.233 ms/step.
Train benchmark of coat_lite_mini.in1k done. 1551.08 samples/sec, 164.23 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model coat_lite_small.in1k created, param count: 19838504
Running inference benchmark on coat_lite_small.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3079.79 samples/sec. 83.123 ms/step.
Infer [16/40]. 3079.57 samples/sec. 83.128 ms/step.
Infer [24/40]. 3079.44 samples/sec. 83.132 ms/step.
Infer [32/40]. 3079.45 samples/sec. 83.132 ms/step.
Infer [40/40]. 3079.43 samples/sec. 83.132 ms/step.
Inference benchmark of coat_lite_small.in1k done. 3078.44 samples/sec, 83.13 ms/step
Model coat_lite_small.in1k created, param count: 19838504
Running train benchmark on coat_lite_small.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 898.52 samples/sec. 284.914 ms/step.
Train [16/40]. 898.52 samples/sec. 284.912 ms/step.
Train [24/40]. 898.50 samples/sec. 284.918 ms/step.
Train [32/40]. 898.52 samples/sec. 284.912 ms/step.
Train [40/40]. 898.53 samples/sec. 284.911 ms/step.
Train benchmark of coat_lite_small.in1k done. 893.59 samples/sec, 284.91 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model coat_lite_tiny.in1k created, param count: 5721960
Running inference benchmark on coat_lite_tiny.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 5625.07 samples/sec. 45.511 ms/step.
Infer [16/40]. 5625.64 samples/sec. 45.506 ms/step.
Infer [24/40]. 5625.70 samples/sec. 45.505 ms/step.
Infer [32/40]. 5626.26 samples/sec. 45.501 ms/step.
Infer [40/40]. 5626.19 samples/sec. 45.502 ms/step.
Inference benchmark of coat_lite_tiny.in1k done. 5623.09 samples/sec, 45.50 ms/step
Model coat_lite_tiny.in1k created, param count: 5721960
Running train benchmark on coat_lite_tiny.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1643.77 samples/sec. 155.739 ms/step.
Train [16/40]. 1643.79 samples/sec. 155.738 ms/step.
Train [24/40]. 1643.79 samples/sec. 155.737 ms/step.
Train [32/40]. 1643.81 samples/sec. 155.736 ms/step.
Train [40/40]. 1643.79 samples/sec. 155.737 ms/step.
Train benchmark of coat_lite_tiny.in1k done. 1635.40 samples/sec, 155.74 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model coat_mini.in1k created, param count: 10337004
Running inference benchmark on coat_mini.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1569.27 samples/sec. 163.133 ms/step.
Infer [16/40]. 1569.41 samples/sec. 163.119 ms/step.
Infer [24/40]. 1569.32 samples/sec. 163.128 ms/step.
Infer [32/40]. 1569.26 samples/sec. 163.134 ms/step.
Infer [40/40]. 1569.22 samples/sec. 163.138 ms/step.
Inference benchmark of coat_mini.in1k done. 1568.94 samples/sec, 163.14 ms/step
Model coat_mini.in1k created, param count: 10337004
Running train benchmark on coat_mini.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 84.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 50.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 199.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model coat_mini.in1k created, param count: 10337004
Running train benchmark on coat_mini.in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
Train [8/40]. 474.71 samples/sec. 404.459 ms/step.
Train [16/40]. 474.72 samples/sec. 404.450 ms/step.
Train [24/40]. 474.73 samples/sec. 404.440 ms/step.
Train [32/40]. 474.73 samples/sec. 404.440 ms/step.
Train [40/40]. 474.73 samples/sec. 404.441 ms/step.
Train benchmark of coat_mini.in1k done. 472.31 samples/sec, 404.44 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model coat_small.in1k created, param count: 21693908
Running inference benchmark on coat_small.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1221.91 samples/sec. 209.508 ms/step.
Infer [16/40]. 1221.98 samples/sec. 209.497 ms/step.
Infer [24/40]. 1221.98 samples/sec. 209.497 ms/step.
Infer [32/40]. 1221.96 samples/sec. 209.500 ms/step.
Infer [40/40]. 1221.91 samples/sec. 209.509 ms/step.
Inference benchmark of coat_small.in1k done. 1221.72 samples/sec, 209.51 ms/step
Model coat_small.in1k created, param count: 21693908
Running train benchmark on coat_small.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 124.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 124.06 MiB is free. Including non-PyTorch memory, this process has 23.52 GiB memory in use. Of the allocated memory 22.74 GiB is allocated by PyTorch, and 283.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model coat_small.in1k created, param count: 21693908
Running train benchmark on coat_small.in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 368.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 196.06 MiB is free. Including non-PyTorch memory, this process has 23.45 GiB memory in use. Of the allocated memory 22.71 GiB is allocated by PyTorch, and 249.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model coat_small.in1k created, param count: 21693908
Running train benchmark on coat_small.in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
Train [8/40]. 359.87 samples/sec. 355.683 ms/step.
Train [16/40]. 359.88 samples/sec. 355.678 ms/step.
Train [24/40]. 359.89 samples/sec. 355.665 ms/step.
Train [32/40]. 359.89 samples/sec. 355.665 ms/step.
Train [40/40]. 359.89 samples/sec. 355.666 ms/step.
Train benchmark of coat_small.in1k done. 357.89 samples/sec, 355.67 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model coat_tiny.in1k created, param count: 5498540
Running inference benchmark on coat_tiny.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 2018.54 samples/sec. 126.825 ms/step.
Infer [16/40]. 2018.25 samples/sec. 126.843 ms/step.
Infer [24/40]. 2018.24 samples/sec. 126.843 ms/step.
Infer [32/40]. 2018.25 samples/sec. 126.843 ms/step.
Infer [40/40]. 2018.17 samples/sec. 126.848 ms/step.
Inference benchmark of coat_tiny.in1k done. 2017.75 samples/sec, 126.85 ms/step
Model coat_tiny.in1k created, param count: 5498540
Running train benchmark on coat_tiny.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 234.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 100.06 MiB is free. Including non-PyTorch memory, this process has 23.54 GiB memory in use. Of the allocated memory 22.72 GiB is allocated by PyTorch, and 330.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model coat_tiny.in1k created, param count: 5498540
Running train benchmark on coat_tiny.in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
Train [8/40]. 609.90 samples/sec. 314.806 ms/step.
Train [16/40]. 609.88 samples/sec. 314.817 ms/step.
Train [24/40]. 609.90 samples/sec. 314.808 ms/step.
Train [32/40]. 609.89 samples/sec. 314.810 ms/step.
Train [40/40]. 609.88 samples/sec. 314.814 ms/step.
Train benchmark of coat_tiny.in1k done. 606.18 samples/sec, 314.81 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model coatnet_0_rw_224.sw_in1k created, param count: 27435562
Running inference benchmark on coatnet_0_rw_224.sw_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3007.24 samples/sec. 85.128 ms/step.
Infer [16/40]. 3007.14 samples/sec. 85.131 ms/step.
Infer [24/40]. 3007.09 samples/sec. 85.132 ms/step.
Infer [32/40]. 3007.03 samples/sec. 85.134 ms/step.
Infer [40/40]. 3007.00 samples/sec. 85.135 ms/step.
Inference benchmark of coatnet_0_rw_224.sw_in1k done. 3006.07 samples/sec, 85.14 ms/step
Model coatnet_0_rw_224.sw_in1k created, param count: 27435562
Running train benchmark on coatnet_0_rw_224.sw_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 873.97 samples/sec. 292.915 ms/step.
Train [16/40]. 873.97 samples/sec. 292.918 ms/step.
Train [24/40]. 873.98 samples/sec. 292.914 ms/step.
Train [32/40]. 873.96 samples/sec. 292.919 ms/step.
Train [40/40]. 873.98 samples/sec. 292.914 ms/step.
Train benchmark of coatnet_0_rw_224.sw_in1k done. 871.07 samples/sec, 292.91 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model coatnet_1_rw_224.sw_in1k created, param count: 41721502
Running inference benchmark on coatnet_1_rw_224.sw_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1672.42 samples/sec. 153.072 ms/step.
Infer [16/40]. 1672.40 samples/sec. 153.073 ms/step.
Infer [24/40]. 1672.38 samples/sec. 153.076 ms/step.
Infer [32/40]. 1672.37 samples/sec. 153.076 ms/step.
Infer [40/40]. 1672.35 samples/sec. 153.078 ms/step.
Inference benchmark of coatnet_1_rw_224.sw_in1k done. 1672.03 samples/sec, 153.08 ms/step
Model coatnet_1_rw_224.sw_in1k created, param count: 41721502
Running train benchmark on coatnet_1_rw_224.sw_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 294.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 2.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 42.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model coatnet_1_rw_224.sw_in1k created, param count: 41721502
Running train benchmark on coatnet_1_rw_224.sw_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 106.06 MiB is free. Including non-PyTorch memory, this process has 23.54 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 341.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model coatnet_1_rw_224.sw_in1k created, param count: 41721502
Running train benchmark on coatnet_1_rw_224.sw_in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
Train [8/40]. 502.28 samples/sec. 254.838 ms/step.
Train [16/40]. 502.24 samples/sec. 254.857 ms/step.
Train [24/40]. 502.24 samples/sec. 254.859 ms/step.
Train [32/40]. 502.24 samples/sec. 254.856 ms/step.
Train [40/40]. 502.24 samples/sec. 254.856 ms/step.
Train benchmark of coatnet_1_rw_224.sw_in1k done. 499.55 samples/sec, 254.86 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model coatnet_2_rw_224.sw_in12k created, param count: 84959925
Running inference benchmark on coatnet_2_rw_224.sw_in12k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1133.16 samples/sec. 225.916 ms/step.
Infer [16/40]. 1133.06 samples/sec. 225.936 ms/step.
Infer [24/40]. 1133.01 samples/sec. 225.948 ms/step.
Infer [32/40]. 1132.96 samples/sec. 225.957 ms/step.
Infer [40/40]. 1132.92 samples/sec. 225.964 ms/step.
Inference benchmark of coatnet_2_rw_224.sw_in12k done. 1132.75 samples/sec, 225.96 ms/step
Model coatnet_2_rw_224.sw_in12k created, param count: 84959925
Running train benchmark on coatnet_2_rw_224.sw_in12k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 356.06 MiB is free. Including non-PyTorch memory, this process has 23.29 GiB memory in use. Of the allocated memory 22.60 GiB is allocated by PyTorch, and 194.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model coatnet_2_rw_224.sw_in12k created, param count: 84959925
Running train benchmark on coatnet_2_rw_224.sw_in12k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 294.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 240.06 MiB is free. Including non-PyTorch memory, this process has 23.41 GiB memory in use. Of the allocated memory 22.82 GiB is allocated by PyTorch, and 86.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model coatnet_2_rw_224.sw_in12k created, param count: 84959925
Running train benchmark on coatnet_2_rw_224.sw_in12k for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 74.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 48.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 141.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model coatnet_2_rw_224.sw_in12k created, param count: 84959925
Running train benchmark on coatnet_2_rw_224.sw_in12k for 40 steps w/ input size (3, 224, 224) and batch size 96.
Train [8/40]. 341.55 samples/sec. 281.076 ms/step.
Train [16/40]. 341.55 samples/sec. 281.074 ms/step.
Train [24/40]. 341.54 samples/sec. 281.077 ms/step.
Train [32/40]. 341.53 samples/sec. 281.087 ms/step.
Train [40/40]. 341.53 samples/sec. 281.091 ms/step.
Train benchmark of coatnet_2_rw_224.sw_in12k done. 339.82 samples/sec, 281.09 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model coatnet_2_rw_224.sw_in12k_ft_in1k created, param count: 73868400
Running inference benchmark on coatnet_2_rw_224.sw_in12k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1133.78 samples/sec. 225.794 ms/step.
Infer [16/40]. 1133.71 samples/sec. 225.807 ms/step.
Infer [24/40]. 1133.65 samples/sec. 225.820 ms/step.
Infer [32/40]. 1133.61 samples/sec. 225.827 ms/step.
Infer [40/40]. 1133.56 samples/sec. 225.837 ms/step.
Inference benchmark of coatnet_2_rw_224.sw_in12k_ft_in1k done. 1133.38 samples/sec, 225.84 ms/step
Model coatnet_2_rw_224.sw_in12k_ft_in1k created, param count: 73868400
Running train benchmark on coatnet_2_rw_224.sw_in12k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 356.06 MiB is free. Including non-PyTorch memory, this process has 23.29 GiB memory in use. Of the allocated memory 22.58 GiB is allocated by PyTorch, and 215.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model coatnet_2_rw_224.sw_in12k_ft_in1k created, param count: 73868400
Running train benchmark on coatnet_2_rw_224.sw_in12k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 294.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 240.06 MiB is free. Including non-PyTorch memory, this process has 23.41 GiB memory in use. Of the allocated memory 22.80 GiB is allocated by PyTorch, and 107.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model coatnet_2_rw_224.sw_in12k_ft_in1k created, param count: 73868400
Running train benchmark on coatnet_2_rw_224.sw_in12k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 74.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 60.06 MiB is free. Including non-PyTorch memory, this process has 23.58 GiB memory in use. Of the allocated memory 22.93 GiB is allocated by PyTorch, and 152.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model coatnet_2_rw_224.sw_in12k_ft_in1k created, param count: 73868400
Running train benchmark on coatnet_2_rw_224.sw_in12k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 96.
Train [8/40]. 341.97 samples/sec. 280.724 ms/step.
Train [16/40]. 341.98 samples/sec. 280.720 ms/step.
Train [24/40]. 341.99 samples/sec. 280.710 ms/step.
Train [32/40]. 341.99 samples/sec. 280.711 ms/step.
Train [40/40]. 341.99 samples/sec. 280.710 ms/step.
Train benchmark of coatnet_2_rw_224.sw_in12k_ft_in1k done. 340.23 samples/sec, 280.71 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model coatnet_3_rw_224.sw_in12k created, param count: 181805305
Running inference benchmark on coatnet_3_rw_224.sw_in12k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 673.84 samples/sec. 379.910 ms/step.
Infer [16/40]. 673.84 samples/sec. 379.914 ms/step.
Infer [24/40]. 673.82 samples/sec. 379.926 ms/step.
Infer [32/40]. 673.80 samples/sec. 379.936 ms/step.
Infer [40/40]. 673.79 samples/sec. 379.938 ms/step.
Inference benchmark of coatnet_3_rw_224.sw_in12k done. 673.70 samples/sec, 379.94 ms/step
Model coatnet_3_rw_224.sw_in12k created, param count: 181805305
Running train benchmark on coatnet_3_rw_224.sw_in12k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 1.15 GiB. GPU 0 has a total capacty of 23.65 GiB of which 500.06 MiB is free. Including non-PyTorch memory, this process has 23.15 GiB memory in use. Of the allocated memory 21.97 GiB is allocated by PyTorch, and 699.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model coatnet_3_rw_224.sw_in12k created, param count: 181805305
Running train benchmark on coatnet_3_rw_224.sw_in12k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 882.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 786.06 MiB is free. Including non-PyTorch memory, this process has 22.87 GiB memory in use. Of the allocated memory 22.17 GiB is allocated by PyTorch, and 208.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model coatnet_3_rw_224.sw_in12k created, param count: 181805305
Running train benchmark on coatnet_3_rw_224.sw_in12k for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 294.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 60.06 MiB is free. Including non-PyTorch memory, this process has 23.58 GiB memory in use. Of the allocated memory 22.98 GiB is allocated by PyTorch, and 97.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model coatnet_3_rw_224.sw_in12k created, param count: 181805305
Running train benchmark on coatnet_3_rw_224.sw_in12k for 40 steps w/ input size (3, 224, 224) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 222.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 26.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.77 GiB is allocated by PyTorch, and 345.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model coatnet_3_rw_224.sw_in12k created, param count: 181805305
Running train benchmark on coatnet_3_rw_224.sw_in12k for 40 steps w/ input size (3, 224, 224) and batch size 64.
Train [8/40]. 204.45 samples/sec. 313.034 ms/step.
Train [16/40]. 204.45 samples/sec. 313.039 ms/step.
Train [24/40]. 204.45 samples/sec. 313.030 ms/step.
Train [32/40]. 204.45 samples/sec. 313.034 ms/step.
Train [40/40]. 204.45 samples/sec. 313.037 ms/step.
Train benchmark of coatnet_3_rw_224.sw_in12k done. 203.48 samples/sec, 313.04 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model coatnet_bn_0_rw_224.sw_in1k created, param count: 27435562
Running inference benchmark on coatnet_bn_0_rw_224.sw_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 2761.81 samples/sec. 92.693 ms/step.
Infer [16/40]. 2761.87 samples/sec. 92.691 ms/step.
Infer [24/40]. 2761.88 samples/sec. 92.691 ms/step.
Infer [32/40]. 2761.93 samples/sec. 92.689 ms/step.
Infer [40/40]. 2761.91 samples/sec. 92.689 ms/step.
Inference benchmark of coatnet_bn_0_rw_224.sw_in1k done. 2761.04 samples/sec, 92.69 ms/step
Model coatnet_bn_0_rw_224.sw_in1k created, param count: 27435562
Running train benchmark on coatnet_bn_0_rw_224.sw_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 148.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 108.06 MiB is free. Including non-PyTorch memory, this process has 23.54 GiB memory in use. Of the allocated memory 22.84 GiB is allocated by PyTorch, and 196.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model coatnet_bn_0_rw_224.sw_in1k created, param count: 27435562
Running train benchmark on coatnet_bn_0_rw_224.sw_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
Train [8/40]. 768.64 samples/sec. 249.790 ms/step.
Train [16/40]. 768.66 samples/sec. 249.785 ms/step.
Train [24/40]. 768.65 samples/sec. 249.790 ms/step.
Train [32/40]. 768.63 samples/sec. 249.795 ms/step.
Train [40/40]. 768.62 samples/sec. 249.800 ms/step.
Train benchmark of coatnet_bn_0_rw_224.sw_in1k done. 765.60 samples/sec, 249.80 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model coatnet_nano_rw_224.sw_in1k created, param count: 15141244
Running inference benchmark on coatnet_nano_rw_224.sw_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3844.05 samples/sec. 66.596 ms/step.
Infer [16/40]. 3844.16 samples/sec. 66.594 ms/step.
Infer [24/40]. 3844.01 samples/sec. 66.597 ms/step.
Infer [32/40]. 3843.89 samples/sec. 66.599 ms/step.
Infer [40/40]. 3843.95 samples/sec. 66.598 ms/step.
Inference benchmark of coatnet_nano_rw_224.sw_in1k done. 3842.42 samples/sec, 66.60 ms/step
Model coatnet_nano_rw_224.sw_in1k created, param count: 15141244
Running train benchmark on coatnet_nano_rw_224.sw_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1057.90 samples/sec. 241.990 ms/step.
Train [16/40]. 1057.92 samples/sec. 241.984 ms/step.
Train [24/40]. 1057.93 samples/sec. 241.981 ms/step.
Train [32/40]. 1057.96 samples/sec. 241.975 ms/step.
Train [40/40]. 1057.96 samples/sec. 241.975 ms/step.
Train benchmark of coatnet_nano_rw_224.sw_in1k done. 1053.64 samples/sec, 241.97 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model coatnet_rmlp_1_rw2_224.sw_in12k created, param count: 50045971
Running inference benchmark on coatnet_rmlp_1_rw2_224.sw_in12k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1669.82 samples/sec. 153.310 ms/step.
Infer [16/40]. 1669.72 samples/sec. 153.319 ms/step.
Infer [24/40]. 1669.71 samples/sec. 153.320 ms/step.
Infer [32/40]. 1669.66 samples/sec. 153.325 ms/step.
Infer [40/40]. 1669.66 samples/sec. 153.324 ms/step.
Inference benchmark of coatnet_rmlp_1_rw2_224.sw_in12k done. 1669.34 samples/sec, 153.32 ms/step
Model coatnet_rmlp_1_rw2_224.sw_in12k created, param count: 50045971
Running train benchmark on coatnet_rmlp_1_rw2_224.sw_in12k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 294.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 294.06 MiB is free. Including non-PyTorch memory, this process has 23.35 GiB memory in use. Of the allocated memory 22.82 GiB is allocated by PyTorch, and 26.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model coatnet_rmlp_1_rw2_224.sw_in12k created, param count: 50045971
Running train benchmark on coatnet_rmlp_1_rw2_224.sw_in12k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 56.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 22.73 GiB is allocated by PyTorch, and 364.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model coatnet_rmlp_1_rw2_224.sw_in12k created, param count: 50045971
Running train benchmark on coatnet_rmlp_1_rw2_224.sw_in12k for 40 steps w/ input size (3, 224, 224) and batch size 128.
Train [8/40]. 499.85 samples/sec. 256.077 ms/step.
Train [16/40]. 499.86 samples/sec. 256.074 ms/step.
Train [24/40]. 499.84 samples/sec. 256.082 ms/step.
Train [32/40]. 499.85 samples/sec. 256.079 ms/step.
Train [40/40]. 499.84 samples/sec. 256.083 ms/step.
Train benchmark of coatnet_rmlp_1_rw2_224.sw_in12k done. 496.80 samples/sec, 256.08 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model coatnet_rmlp_1_rw2_224.sw_in12k_ft_in1k created, param count: 41724622
Running inference benchmark on coatnet_rmlp_1_rw2_224.sw_in12k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1669.95 samples/sec. 153.298 ms/step.
Infer [16/40]. 1669.93 samples/sec. 153.300 ms/step.
Infer [24/40]. 1669.87 samples/sec. 153.305 ms/step.
Infer [32/40]. 1669.83 samples/sec. 153.309 ms/step.
Infer [40/40]. 1669.82 samples/sec. 153.310 ms/step.
Inference benchmark of coatnet_rmlp_1_rw2_224.sw_in12k_ft_in1k done. 1669.49 samples/sec, 153.31 ms/step
Model coatnet_rmlp_1_rw2_224.sw_in12k_ft_in1k created, param count: 41724622
Running train benchmark on coatnet_rmlp_1_rw2_224.sw_in12k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 294.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 294.06 MiB is free. Including non-PyTorch memory, this process has 23.35 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 42.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model coatnet_rmlp_1_rw2_224.sw_in12k_ft_in1k created, param count: 41724622
Running train benchmark on coatnet_rmlp_1_rw2_224.sw_in12k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 44.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 22.71 GiB is allocated by PyTorch, and 393.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model coatnet_rmlp_1_rw2_224.sw_in12k_ft_in1k created, param count: 41724622
Running train benchmark on coatnet_rmlp_1_rw2_224.sw_in12k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
Train [8/40]. 500.49 samples/sec. 255.747 ms/step.
Train [16/40]. 500.49 samples/sec. 255.752 ms/step.
Train [24/40]. 500.50 samples/sec. 255.746 ms/step.
Train [32/40]. 500.50 samples/sec. 255.745 ms/step.
Train [40/40]. 500.49 samples/sec. 255.747 ms/step.
Train benchmark of coatnet_rmlp_1_rw2_224.sw_in12k_ft_in1k done. 497.61 samples/sec, 255.75 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model coatnet_rmlp_1_rw_224.sw_in1k created, param count: 41691982
Running inference benchmark on coatnet_rmlp_1_rw_224.sw_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1792.87 samples/sec. 142.788 ms/step.
Infer [16/40]. 1792.87 samples/sec. 142.788 ms/step.
Infer [24/40]. 1792.83 samples/sec. 142.791 ms/step.
Infer [32/40]. 1792.82 samples/sec. 142.792 ms/step.
Infer [40/40]. 1792.80 samples/sec. 142.793 ms/step.
Inference benchmark of coatnet_rmlp_1_rw_224.sw_in1k done. 1792.42 samples/sec, 142.79 ms/step
Model coatnet_rmlp_1_rw_224.sw_in1k created, param count: 41691982
Running train benchmark on coatnet_rmlp_1_rw_224.sw_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 148.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 12.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.98 GiB is allocated by PyTorch, and 148.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model coatnet_rmlp_1_rw_224.sw_in1k created, param count: 41691982
Running train benchmark on coatnet_rmlp_1_rw_224.sw_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 170.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 138.06 MiB is free. Including non-PyTorch memory, this process has 23.51 GiB memory in use. Of the allocated memory 22.64 GiB is allocated by PyTorch, and 370.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model coatnet_rmlp_1_rw_224.sw_in1k created, param count: 41691982
Running train benchmark on coatnet_rmlp_1_rw_224.sw_in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
Train [8/40]. 550.70 samples/sec. 232.433 ms/step.
Train [16/40]. 550.68 samples/sec. 232.442 ms/step.
Train [24/40]. 550.66 samples/sec. 232.446 ms/step.
Train [32/40]. 550.65 samples/sec. 232.451 ms/step.
Train [40/40]. 550.63 samples/sec. 232.459 ms/step.
Train benchmark of coatnet_rmlp_1_rw_224.sw_in1k done. 547.12 samples/sec, 232.46 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model coatnet_rmlp_2_rw_224.sw_in1k created, param count: 73881264
Running inference benchmark on coatnet_rmlp_2_rw_224.sw_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1119.53 samples/sec. 228.667 ms/step.
Infer [16/40]. 1119.47 samples/sec. 228.679 ms/step.
Infer [24/40]. 1119.43 samples/sec. 228.687 ms/step.
Infer [32/40]. 1119.42 samples/sec. 228.689 ms/step.
Infer [40/40]. 1119.41 samples/sec. 228.692 ms/step.
Inference benchmark of coatnet_rmlp_2_rw_224.sw_in1k done. 1119.25 samples/sec, 228.69 ms/step
Model coatnet_rmlp_2_rw_224.sw_in1k created, param count: 73881264
Running train benchmark on coatnet_rmlp_2_rw_224.sw_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 356.06 MiB is free. Including non-PyTorch memory, this process has 23.29 GiB memory in use. Of the allocated memory 22.58 GiB is allocated by PyTorch, and 215.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model coatnet_rmlp_2_rw_224.sw_in1k created, param count: 73881264
Running train benchmark on coatnet_rmlp_2_rw_224.sw_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 294.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 246.06 MiB is free. Including non-PyTorch memory, this process has 23.40 GiB memory in use. Of the allocated memory 22.80 GiB is allocated by PyTorch, and 101.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model coatnet_rmlp_2_rw_224.sw_in1k created, param count: 73881264
Running train benchmark on coatnet_rmlp_2_rw_224.sw_in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 152.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 134.06 MiB is free. Including non-PyTorch memory, this process has 23.51 GiB memory in use. Of the allocated memory 22.89 GiB is allocated by PyTorch, and 116.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model coatnet_rmlp_2_rw_224.sw_in1k created, param count: 73881264
Running train benchmark on coatnet_rmlp_2_rw_224.sw_in1k for 40 steps w/ input size (3, 224, 224) and batch size 96.
Train [8/40]. 336.53 samples/sec. 285.260 ms/step.
Train [16/40]. 336.56 samples/sec. 285.243 ms/step.
Train [24/40]. 336.54 samples/sec. 285.257 ms/step.
Train [32/40]. 336.53 samples/sec. 285.262 ms/step.
Train [40/40]. 336.54 samples/sec. 285.260 ms/step.
Train benchmark of coatnet_rmlp_2_rw_224.sw_in1k done. 334.66 samples/sec, 285.26 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model coatnet_rmlp_2_rw_224.sw_in12k created, param count: 84972789
Running inference benchmark on coatnet_rmlp_2_rw_224.sw_in12k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1118.96 samples/sec. 228.783 ms/step.
Infer [16/40]. 1118.91 samples/sec. 228.794 ms/step.
Infer [24/40]. 1118.93 samples/sec. 228.789 ms/step.
Infer [32/40]. 1118.91 samples/sec. 228.795 ms/step.
Infer [40/40]. 1118.84 samples/sec. 228.808 ms/step.
Inference benchmark of coatnet_rmlp_2_rw_224.sw_in12k done. 1118.68 samples/sec, 228.81 ms/step
Model coatnet_rmlp_2_rw_224.sw_in12k created, param count: 84972789
Running train benchmark on coatnet_rmlp_2_rw_224.sw_in12k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 356.06 MiB is free. Including non-PyTorch memory, this process has 23.29 GiB memory in use. Of the allocated memory 22.60 GiB is allocated by PyTorch, and 194.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model coatnet_rmlp_2_rw_224.sw_in12k created, param count: 84972789
Running train benchmark on coatnet_rmlp_2_rw_224.sw_in12k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 294.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 246.06 MiB is free. Including non-PyTorch memory, this process has 23.40 GiB memory in use. Of the allocated memory 22.82 GiB is allocated by PyTorch, and 80.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model coatnet_rmlp_2_rw_224.sw_in12k created, param count: 84972789
Running train benchmark on coatnet_rmlp_2_rw_224.sw_in12k for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 152.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 108.06 MiB is free. Including non-PyTorch memory, this process has 23.54 GiB memory in use. Of the allocated memory 22.91 GiB is allocated by PyTorch, and 123.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model coatnet_rmlp_2_rw_224.sw_in12k created, param count: 84972789
Running train benchmark on coatnet_rmlp_2_rw_224.sw_in12k for 40 steps w/ input size (3, 224, 224) and batch size 96.
Train [8/40]. 336.32 samples/sec. 285.439 ms/step.
Train [16/40]. 336.32 samples/sec. 285.443 ms/step.
Train [24/40]. 336.32 samples/sec. 285.447 ms/step.
Train [32/40]. 336.24 samples/sec. 285.511 ms/step.
Train [40/40]. 336.19 samples/sec. 285.552 ms/step.
Train benchmark of coatnet_rmlp_2_rw_224.sw_in12k done. 334.21 samples/sec, 285.55 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model coatnet_rmlp_2_rw_224.sw_in12k_ft_in1k created, param count: 73881264
Running inference benchmark on coatnet_rmlp_2_rw_224.sw_in12k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1119.57 samples/sec. 228.659 ms/step.
Infer [16/40]. 1119.58 samples/sec. 228.657 ms/step.
Infer [24/40]. 1119.55 samples/sec. 228.663 ms/step.
Infer [32/40]. 1119.53 samples/sec. 228.668 ms/step.
Infer [40/40]. 1119.50 samples/sec. 228.673 ms/step.
Inference benchmark of coatnet_rmlp_2_rw_224.sw_in12k_ft_in1k done. 1119.33 samples/sec, 228.67 ms/step
Model coatnet_rmlp_2_rw_224.sw_in12k_ft_in1k created, param count: 73881264
Running train benchmark on coatnet_rmlp_2_rw_224.sw_in12k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 356.06 MiB is free. Including non-PyTorch memory, this process has 23.29 GiB memory in use. Of the allocated memory 22.58 GiB is allocated by PyTorch, and 215.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model coatnet_rmlp_2_rw_224.sw_in12k_ft_in1k created, param count: 73881264
Running train benchmark on coatnet_rmlp_2_rw_224.sw_in12k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 294.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 246.06 MiB is free. Including non-PyTorch memory, this process has 23.40 GiB memory in use. Of the allocated memory 22.80 GiB is allocated by PyTorch, and 101.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model coatnet_rmlp_2_rw_224.sw_in12k_ft_in1k created, param count: 73881264
Running train benchmark on coatnet_rmlp_2_rw_224.sw_in12k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 152.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 134.06 MiB is free. Including non-PyTorch memory, this process has 23.51 GiB memory in use. Of the allocated memory 22.89 GiB is allocated by PyTorch, and 116.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model coatnet_rmlp_2_rw_224.sw_in12k_ft_in1k created, param count: 73881264
Running train benchmark on coatnet_rmlp_2_rw_224.sw_in12k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 96.
Train [8/40]. 336.40 samples/sec. 285.375 ms/step.
Train [16/40]. 336.42 samples/sec. 285.354 ms/step.
Train [24/40]. 336.42 samples/sec. 285.359 ms/step.
Train [32/40]. 336.42 samples/sec. 285.354 ms/step.
Train [40/40]. 336.43 samples/sec. 285.349 ms/step.
Train benchmark of coatnet_rmlp_2_rw_224.sw_in12k_ft_in1k done. 334.49 samples/sec, 285.35 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model coatnet_rmlp_2_rw_384.sw_in12k_ft_in1k created, param count: 73881264
Running inference benchmark on coatnet_rmlp_2_rw_384.sw_in12k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 9.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 8.80 GiB is free. Including non-PyTorch memory, this process has 14.84 GiB memory in use. Of the allocated memory 12.21 GiB is allocated by PyTorch, and 2.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model coatnet_rmlp_2_rw_384.sw_in12k_ft_in1k created, param count: 73881264
Running inference benchmark on coatnet_rmlp_2_rw_384.sw_in12k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 192.
Infer [8/40]. 290.79 samples/sec. 660.268 ms/step.
Infer [16/40]. 290.73 samples/sec. 660.418 ms/step.
Infer [24/40]. 290.72 samples/sec. 660.440 ms/step.
Infer [32/40]. 290.70 samples/sec. 660.474 ms/step.
Infer [40/40]. 290.69 samples/sec. 660.499 ms/step.
Inference benchmark of coatnet_rmlp_2_rw_384.sw_in12k_ft_in1k done. 290.67 samples/sec, 660.50 ms/step
Model coatnet_rmlp_2_rw_384.sw_in12k_ft_in1k created, param count: 73881264
Running train benchmark on coatnet_rmlp_2_rw_384.sw_in12k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 9.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 2.05 GiB is free. Including non-PyTorch memory, this process has 21.59 GiB memory in use. Of the allocated memory 20.09 GiB is allocated by PyTorch, and 1.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model coatnet_rmlp_2_rw_384.sw_in12k_ft_in1k created, param count: 73881264
Running train benchmark on coatnet_rmlp_2_rw_384.sw_in12k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 6.75 GiB. GPU 0 has a total capacty of 23.65 GiB of which 364.06 MiB is free. Including non-PyTorch memory, this process has 23.29 GiB memory in use. Of the allocated memory 21.86 GiB is allocated by PyTorch, and 940.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model coatnet_rmlp_2_rw_384.sw_in12k_ft_in1k created, param count: 73881264
Running train benchmark on coatnet_rmlp_2_rw_384.sw_in12k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 1.12 GiB. GPU 0 has a total capacty of 23.65 GiB of which 462.06 MiB is free. Including non-PyTorch memory, this process has 23.19 GiB memory in use. Of the allocated memory 21.39 GiB is allocated by PyTorch, and 1.30 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model coatnet_rmlp_2_rw_384.sw_in12k_ft_in1k created, param count: 73881264
Running train benchmark on coatnet_rmlp_2_rw_384.sw_in12k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 864.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 574.06 MiB is free. Including non-PyTorch memory, this process has 23.08 GiB memory in use. Of the allocated memory 22.42 GiB is allocated by PyTorch, and 163.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model coatnet_rmlp_2_rw_384.sw_in12k_ft_in1k created, param count: 73881264
Running train benchmark on coatnet_rmlp_2_rw_384.sw_in12k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 288.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 144.06 MiB is free. Including non-PyTorch memory, this process has 23.50 GiB memory in use. Of the allocated memory 22.92 GiB is allocated by PyTorch, and 79.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model coatnet_rmlp_2_rw_384.sw_in12k_ft_in1k created, param count: 73881264
Running train benchmark on coatnet_rmlp_2_rw_384.sw_in12k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 244.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 226.06 MiB is free. Including non-PyTorch memory, this process has 23.42 GiB memory in use. Of the allocated memory 22.84 GiB is allocated by PyTorch, and 78.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model coatnet_rmlp_2_rw_384.sw_in12k_ft_in1k created, param count: 73881264
Running train benchmark on coatnet_rmlp_2_rw_384.sw_in12k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 32.
ERROR: "CUDA out of memory. Tried to allocate 324.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 130.06 MiB is free. Including non-PyTorch memory, this process has 23.51 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 46.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model coatnet_rmlp_2_rw_384.sw_in12k_ft_in1k created, param count: 73881264
Running train benchmark on coatnet_rmlp_2_rw_384.sw_in12k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 24.
Train [8/40]. 95.21 samples/sec. 252.077 ms/step.
Train [16/40]. 95.21 samples/sec. 252.082 ms/step.
Train [24/40]. 95.21 samples/sec. 252.078 ms/step.
Train [32/40]. 95.21 samples/sec. 252.078 ms/step.
Train [40/40]. 95.21 samples/sec. 252.071 ms/step.
Train benchmark of coatnet_rmlp_2_rw_384.sw_in12k_ft_in1k done. 94.60 samples/sec, 252.07 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model coatnet_rmlp_nano_rw_224.sw_in1k created, param count: 15145116
Running inference benchmark on coatnet_rmlp_nano_rw_224.sw_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3318.79 samples/sec. 77.136 ms/step.
Infer [16/40]. 3318.70 samples/sec. 77.139 ms/step.
Infer [24/40]. 3318.74 samples/sec. 77.138 ms/step.
Infer [32/40]. 3318.63 samples/sec. 77.140 ms/step.
Infer [40/40]. 3318.75 samples/sec. 77.138 ms/step.
Inference benchmark of coatnet_rmlp_nano_rw_224.sw_in1k done. 3317.55 samples/sec, 77.14 ms/step
Model coatnet_rmlp_nano_rw_224.sw_in1k created, param count: 15145116
Running train benchmark on coatnet_rmlp_nano_rw_224.sw_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 50.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 22.91 GiB is allocated by PyTorch, and 187.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model coatnet_rmlp_nano_rw_224.sw_in1k created, param count: 15145116
Running train benchmark on coatnet_rmlp_nano_rw_224.sw_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
Train [8/40]. 901.40 samples/sec. 213.002 ms/step.
Train [16/40]. 901.39 samples/sec. 213.003 ms/step.
Train [24/40]. 901.39 samples/sec. 213.005 ms/step.
Train [32/40]. 901.40 samples/sec. 213.001 ms/step.
Train [40/40]. 901.39 samples/sec. 213.004 ms/step.
Train benchmark of coatnet_rmlp_nano_rw_224.sw_in1k done. 896.87 samples/sec, 213.00 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model coatnext_nano_rw_224.sw_in1k created, param count: 14701244
Running inference benchmark on coatnext_nano_rw_224.sw_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 4694.63 samples/sec. 54.530 ms/step.
Infer [16/40]. 4694.35 samples/sec. 54.534 ms/step.
Infer [24/40]. 4694.26 samples/sec. 54.535 ms/step.
Infer [32/40]. 4694.31 samples/sec. 54.534 ms/step.
Infer [40/40]. 4694.34 samples/sec. 54.534 ms/step.
Inference benchmark of coatnext_nano_rw_224.sw_in1k done. 4692.11 samples/sec, 54.53 ms/step
Model coatnext_nano_rw_224.sw_in1k created, param count: 14701244
Running train benchmark on coatnext_nano_rw_224.sw_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1447.69 samples/sec. 176.834 ms/step.
Train [16/40]. 1446.75 samples/sec. 176.948 ms/step.
Train [24/40]. 1446.45 samples/sec. 176.985 ms/step.
Train [32/40]. 1446.29 samples/sec. 177.004 ms/step.
Train [40/40]. 1446.21 samples/sec. 177.014 ms/step.
Train benchmark of coatnext_nano_rw_224.sw_in1k done. 1439.42 samples/sec, 177.01 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convformer_b36.sail_in1k created, param count: 99882616
Running inference benchmark on convformer_b36.sail_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 927.76 samples/sec. 275.933 ms/step.
Infer [16/40]. 927.66 samples/sec. 275.963 ms/step.
Infer [24/40]. 927.59 samples/sec. 275.984 ms/step.
Infer [32/40]. 927.55 samples/sec. 275.997 ms/step.
Infer [40/40]. 927.52 samples/sec. 276.005 ms/step.
Inference benchmark of convformer_b36.sail_in1k done. 927.40 samples/sec, 276.00 ms/step
Model convformer_b36.sail_in1k created, param count: 99882616
Running train benchmark on convformer_b36.sail_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 392.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 242.06 MiB is free. Including non-PyTorch memory, this process has 23.40 GiB memory in use. Of the allocated memory 22.73 GiB is allocated by PyTorch, and 176.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convformer_b36.sail_in1k created, param count: 99882616
Running train benchmark on convformer_b36.sail_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 294.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 130.06 MiB is free. Including non-PyTorch memory, this process has 23.51 GiB memory in use. Of the allocated memory 22.86 GiB is allocated by PyTorch, and 156.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convformer_b36.sail_in1k created, param count: 99882616
Running train benchmark on convformer_b36.sail_in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 30.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 23.09 GiB is allocated by PyTorch, and 18.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model convformer_b36.sail_in1k created, param count: 99882616
Running train benchmark on convformer_b36.sail_in1k for 40 steps w/ input size (3, 224, 224) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 74.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 56.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 23.01 GiB is allocated by PyTorch, and 77.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model convformer_b36.sail_in1k created, param count: 99882616
Running train benchmark on convformer_b36.sail_in1k for 40 steps w/ input size (3, 224, 224) and batch size 64.
Train [8/40]. 297.96 samples/sec. 214.796 ms/step.
Train [16/40]. 297.95 samples/sec. 214.805 ms/step.
Train [24/40]. 297.94 samples/sec. 214.809 ms/step.
Train [32/40]. 297.93 samples/sec. 214.814 ms/step.
Train [40/40]. 297.93 samples/sec. 214.817 ms/step.
Train benchmark of convformer_b36.sail_in1k done. 295.79 samples/sec, 214.82 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convformer_b36.sail_in1k_384 created, param count: 99882616
Running inference benchmark on convformer_b36.sail_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 317.84 samples/sec. 805.429 ms/step.
Infer [16/40]. 317.82 samples/sec. 805.493 ms/step.
Infer [24/40]. 317.80 samples/sec. 805.528 ms/step.
Infer [32/40]. 317.79 samples/sec. 805.564 ms/step.
Infer [40/40]. 317.78 samples/sec. 805.594 ms/step.
Inference benchmark of convformer_b36.sail_in1k_384 done. 317.76 samples/sec, 805.59 ms/step
Model convformer_b36.sail_in1k_384 created, param count: 99882616
Running train benchmark on convformer_b36.sail_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 2.25 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.45 GiB is free. Including non-PyTorch memory, this process has 22.19 GiB memory in use. Of the allocated memory 21.31 GiB is allocated by PyTorch, and 381.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convformer_b36.sail_in1k_384 created, param count: 99882616
Running train benchmark on convformer_b36.sail_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 1.69 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.59 GiB is free. Including non-PyTorch memory, this process has 22.05 GiB memory in use. Of the allocated memory 21.10 GiB is allocated by PyTorch, and 461.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convformer_b36.sail_in1k_384 created, param count: 99882616
Running train benchmark on convformer_b36.sail_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 1.12 GiB. GPU 0 has a total capacty of 23.65 GiB of which 906.06 MiB is free. Including non-PyTorch memory, this process has 22.76 GiB memory in use. Of the allocated memory 22.03 GiB is allocated by PyTorch, and 232.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model convformer_b36.sail_in1k_384 created, param count: 99882616
Running train benchmark on convformer_b36.sail_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 432.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 106.06 MiB is free. Including non-PyTorch memory, this process has 23.54 GiB memory in use. Of the allocated memory 22.92 GiB is allocated by PyTorch, and 120.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model convformer_b36.sail_in1k_384 created, param count: 99882616
Running train benchmark on convformer_b36.sail_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 288.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 26.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 167.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model convformer_b36.sail_in1k_384 created, param count: 99882616
Running train benchmark on convformer_b36.sail_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 216.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 94.06 MiB is free. Including non-PyTorch memory, this process has 23.55 GiB memory in use. Of the allocated memory 22.97 GiB is allocated by PyTorch, and 81.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model convformer_b36.sail_in1k_384 created, param count: 99882616
Running train benchmark on convformer_b36.sail_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 32.
ERROR: "CUDA out of memory. Tried to allocate 36.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 26.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 23.07 GiB is allocated by PyTorch, and 45.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model convformer_b36.sail_in1k_384 created, param count: 99882616
Running train benchmark on convformer_b36.sail_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 24.
Train [8/40]. 102.13 samples/sec. 235.005 ms/step.
Train [16/40]. 102.12 samples/sec. 235.008 ms/step.
Train [24/40]. 102.13 samples/sec. 235.003 ms/step.
Train [32/40]. 102.12 samples/sec. 235.010 ms/step.
Train [40/40]. 102.11 samples/sec. 235.033 ms/step.
Train benchmark of convformer_b36.sail_in1k_384 done. 101.43 samples/sec, 235.03 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convformer_b36.sail_in22k created, param count: 163927009
Running inference benchmark on convformer_b36.sail_in22k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 926.46 samples/sec. 276.321 ms/step.
Infer [16/40]. 926.43 samples/sec. 276.328 ms/step.
Infer [24/40]. 926.39 samples/sec. 276.342 ms/step.
Infer [32/40]. 926.32 samples/sec. 276.362 ms/step.
Infer [40/40]. 926.28 samples/sec. 276.376 ms/step.
Inference benchmark of convformer_b36.sail_in22k done. 926.14 samples/sec, 276.38 ms/step
Model convformer_b36.sail_in22k created, param count: 163927009
Running train benchmark on convformer_b36.sail_in22k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 392.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 242.06 MiB is free. Including non-PyTorch memory, this process has 23.40 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 54.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convformer_b36.sail_in22k created, param count: 163927009
Running train benchmark on convformer_b36.sail_in22k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 294.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 276.06 MiB is free. Including non-PyTorch memory, this process has 23.37 GiB memory in use. Of the allocated memory 22.69 GiB is allocated by PyTorch, and 180.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convformer_b36.sail_in22k created, param count: 163927009
Running train benchmark on convformer_b36.sail_in22k for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 88.06 MiB is free. Including non-PyTorch memory, this process has 23.55 GiB memory in use. Of the allocated memory 23.01 GiB is allocated by PyTorch, and 37.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model convformer_b36.sail_in22k created, param count: 163927009
Running train benchmark on convformer_b36.sail_in22k for 40 steps w/ input size (3, 224, 224) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 74.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 24.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 23.06 GiB is allocated by PyTorch, and 57.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model convformer_b36.sail_in22k created, param count: 163927009
Running train benchmark on convformer_b36.sail_in22k for 40 steps w/ input size (3, 224, 224) and batch size 64.
Train [8/40]. 295.04 samples/sec. 216.922 ms/step.
Train [16/40]. 295.04 samples/sec. 216.923 ms/step.
Train [24/40]. 295.03 samples/sec. 216.929 ms/step.
Train [32/40]. 295.02 samples/sec. 216.933 ms/step.
Train [40/40]. 295.02 samples/sec. 216.932 ms/step.
Train benchmark of convformer_b36.sail_in22k done. 292.89 samples/sec, 216.93 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convformer_b36.sail_in22k_ft_in1k created, param count: 99882616
Running inference benchmark on convformer_b36.sail_in22k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 927.47 samples/sec. 276.019 ms/step.
Infer [16/40]. 927.40 samples/sec. 276.039 ms/step.
Infer [24/40]. 927.38 samples/sec. 276.047 ms/step.
Infer [32/40]. 927.36 samples/sec. 276.052 ms/step.
Infer [40/40]. 927.36 samples/sec. 276.053 ms/step.
Inference benchmark of convformer_b36.sail_in22k_ft_in1k done. 927.23 samples/sec, 276.05 ms/step
Model convformer_b36.sail_in22k_ft_in1k created, param count: 99882616
Running train benchmark on convformer_b36.sail_in22k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 392.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 242.06 MiB is free. Including non-PyTorch memory, this process has 23.40 GiB memory in use. Of the allocated memory 22.73 GiB is allocated by PyTorch, and 176.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convformer_b36.sail_in22k_ft_in1k created, param count: 99882616
Running train benchmark on convformer_b36.sail_in22k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 294.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 130.06 MiB is free. Including non-PyTorch memory, this process has 23.51 GiB memory in use. Of the allocated memory 22.86 GiB is allocated by PyTorch, and 156.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convformer_b36.sail_in22k_ft_in1k created, param count: 99882616
Running train benchmark on convformer_b36.sail_in22k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 36.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 23.09 GiB is allocated by PyTorch, and 15.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model convformer_b36.sail_in22k_ft_in1k created, param count: 99882616
Running train benchmark on convformer_b36.sail_in22k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 74.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 60.06 MiB is free. Including non-PyTorch memory, this process has 23.58 GiB memory in use. Of the allocated memory 23.01 GiB is allocated by PyTorch, and 72.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model convformer_b36.sail_in22k_ft_in1k created, param count: 99882616
Running train benchmark on convformer_b36.sail_in22k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 64.
Train [8/40]. 297.74 samples/sec. 214.956 ms/step.
Train [16/40]. 297.73 samples/sec. 214.961 ms/step.
Train [24/40]. 297.73 samples/sec. 214.962 ms/step.
Train [32/40]. 297.72 samples/sec. 214.968 ms/step.
Train [40/40]. 297.72 samples/sec. 214.969 ms/step.
Train benchmark of convformer_b36.sail_in22k_ft_in1k done. 295.46 samples/sec, 214.97 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convformer_b36.sail_in22k_ft_in1k_384 created, param count: 99882616
Running inference benchmark on convformer_b36.sail_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 317.86 samples/sec. 805.397 ms/step.
Infer [16/40]. 317.82 samples/sec. 805.476 ms/step.
Infer [24/40]. 317.78 samples/sec. 805.592 ms/step.
Infer [32/40]. 317.75 samples/sec. 805.666 ms/step.
Infer [40/40]. 317.73 samples/sec. 805.716 ms/step.
Inference benchmark of convformer_b36.sail_in22k_ft_in1k_384 done. 317.71 samples/sec, 805.72 ms/step
Model convformer_b36.sail_in22k_ft_in1k_384 created, param count: 99882616
Running train benchmark on convformer_b36.sail_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 2.25 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.45 GiB is free. Including non-PyTorch memory, this process has 22.19 GiB memory in use. Of the allocated memory 21.31 GiB is allocated by PyTorch, and 381.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convformer_b36.sail_in22k_ft_in1k_384 created, param count: 99882616
Running train benchmark on convformer_b36.sail_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 1.69 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.59 GiB is free. Including non-PyTorch memory, this process has 22.05 GiB memory in use. Of the allocated memory 21.10 GiB is allocated by PyTorch, and 461.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convformer_b36.sail_in22k_ft_in1k_384 created, param count: 99882616
Running train benchmark on convformer_b36.sail_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 1.12 GiB. GPU 0 has a total capacty of 23.65 GiB of which 906.06 MiB is free. Including non-PyTorch memory, this process has 22.76 GiB memory in use. Of the allocated memory 22.03 GiB is allocated by PyTorch, and 232.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model convformer_b36.sail_in22k_ft_in1k_384 created, param count: 99882616
Running train benchmark on convformer_b36.sail_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 432.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 106.06 MiB is free. Including non-PyTorch memory, this process has 23.54 GiB memory in use. Of the allocated memory 22.92 GiB is allocated by PyTorch, and 120.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model convformer_b36.sail_in22k_ft_in1k_384 created, param count: 99882616
Running train benchmark on convformer_b36.sail_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 288.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 26.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 167.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model convformer_b36.sail_in22k_ft_in1k_384 created, param count: 99882616
Running train benchmark on convformer_b36.sail_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 216.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 94.06 MiB is free. Including non-PyTorch memory, this process has 23.55 GiB memory in use. Of the allocated memory 22.97 GiB is allocated by PyTorch, and 81.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model convformer_b36.sail_in22k_ft_in1k_384 created, param count: 99882616
Running train benchmark on convformer_b36.sail_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 32.
ERROR: "CUDA out of memory. Tried to allocate 36.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 26.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 23.07 GiB is allocated by PyTorch, and 45.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model convformer_b36.sail_in22k_ft_in1k_384 created, param count: 99882616
Running train benchmark on convformer_b36.sail_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 24.
Train [8/40]. 102.11 samples/sec. 235.045 ms/step.
Train [16/40]. 102.11 samples/sec. 235.040 ms/step.
Train [24/40]. 102.11 samples/sec. 235.044 ms/step.
Train [32/40]. 102.11 samples/sec. 235.052 ms/step.
Train [40/40]. 102.10 samples/sec. 235.058 ms/step.
Train benchmark of convformer_b36.sail_in22k_ft_in1k_384 done. 101.43 samples/sec, 235.06 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convformer_m36.sail_in1k created, param count: 57051640
Running inference benchmark on convformer_m36.sail_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1272.43 samples/sec. 201.190 ms/step.
Infer [16/40]. 1272.28 samples/sec. 201.214 ms/step.
Infer [24/40]. 1272.16 samples/sec. 201.232 ms/step.
Infer [32/40]. 1272.08 samples/sec. 201.245 ms/step.
Infer [40/40]. 1272.01 samples/sec. 201.257 ms/step.
Inference benchmark of convformer_m36.sail_in1k done. 1271.80 samples/sec, 201.26 ms/step
Model convformer_m36.sail_in1k created, param count: 57051640
Running train benchmark on convformer_m36.sail_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 294.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 292.06 MiB is free. Including non-PyTorch memory, this process has 23.36 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 39.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convformer_m36.sail_in1k created, param count: 57051640
Running train benchmark on convformer_m36.sail_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 222.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 156.06 MiB is free. Including non-PyTorch memory, this process has 23.49 GiB memory in use. Of the allocated memory 22.75 GiB is allocated by PyTorch, and 236.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convformer_m36.sail_in1k created, param count: 57051640
Running train benchmark on convformer_m36.sail_in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 74.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 40.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 23.02 GiB is allocated by PyTorch, and 78.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model convformer_m36.sail_in1k created, param count: 57051640
Running train benchmark on convformer_m36.sail_in1k for 40 steps w/ input size (3, 224, 224) and batch size 96.
Train [8/40]. 400.66 samples/sec. 239.605 ms/step.
Train [16/40]. 400.65 samples/sec. 239.611 ms/step.
Train [24/40]. 400.65 samples/sec. 239.608 ms/step.
Train [32/40]. 400.65 samples/sec. 239.608 ms/step.
Train [40/40]. 400.65 samples/sec. 239.613 ms/step.
Train benchmark of convformer_m36.sail_in1k done. 397.99 samples/sec, 239.61 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convformer_m36.sail_in1k_384 created, param count: 57051640
Running inference benchmark on convformer_m36.sail_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 435.14 samples/sec. 588.311 ms/step.
Infer [16/40]. 435.12 samples/sec. 588.347 ms/step.
Infer [24/40]. 435.11 samples/sec. 588.363 ms/step.
Infer [32/40]. 435.09 samples/sec. 588.388 ms/step.
Infer [40/40]. 435.07 samples/sec. 588.414 ms/step.
Inference benchmark of convformer_m36.sail_in1k_384 done. 435.03 samples/sec, 588.41 ms/step
Model convformer_m36.sail_in1k_384 created, param count: 57051640
Running train benchmark on convformer_m36.sail_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 1.69 GiB. GPU 0 has a total capacty of 23.65 GiB of which 358.06 MiB is free. Including non-PyTorch memory, this process has 23.29 GiB memory in use. Of the allocated memory 22.78 GiB is allocated by PyTorch, and 5.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convformer_m36.sail_in1k_384 created, param count: 57051640
Running train benchmark on convformer_m36.sail_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 1.27 GiB. GPU 0 has a total capacty of 23.65 GiB of which 768.06 MiB is free. Including non-PyTorch memory, this process has 22.89 GiB memory in use. Of the allocated memory 22.20 GiB is allocated by PyTorch, and 186.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convformer_m36.sail_in1k_384 created, param count: 57051640
Running train benchmark on convformer_m36.sail_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 432.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 122.06 MiB is free. Including non-PyTorch memory, this process has 23.52 GiB memory in use. Of the allocated memory 22.88 GiB is allocated by PyTorch, and 137.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model convformer_m36.sail_in1k_384 created, param count: 57051640
Running train benchmark on convformer_m36.sail_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 324.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 108.06 MiB is free. Including non-PyTorch memory, this process has 23.54 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 135.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model convformer_m36.sail_in1k_384 created, param count: 57051640
Running train benchmark on convformer_m36.sail_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 216.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 110.06 MiB is free. Including non-PyTorch memory, this process has 23.53 GiB memory in use. Of the allocated memory 22.91 GiB is allocated by PyTorch, and 118.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model convformer_m36.sail_in1k_384 created, param count: 57051640
Running train benchmark on convformer_m36.sail_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 82.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 74.06 MiB is free. Including non-PyTorch memory, this process has 23.57 GiB memory in use. Of the allocated memory 23.01 GiB is allocated by PyTorch, and 57.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model convformer_m36.sail_in1k_384 created, param count: 57051640
Running train benchmark on convformer_m36.sail_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 32.
Train [8/40]. 137.42 samples/sec. 232.856 ms/step.
Train [16/40]. 137.42 samples/sec. 232.867 ms/step.
Train [24/40]. 137.42 samples/sec. 232.871 ms/step.
Train [32/40]. 137.41 samples/sec. 232.873 ms/step.
Train [40/40]. 137.42 samples/sec. 232.871 ms/step.
Train benchmark of convformer_m36.sail_in1k_384 done. 136.48 samples/sec, 232.87 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convformer_m36.sail_in22k created, param count: 105090145
Running inference benchmark on convformer_m36.sail_in22k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1269.73 samples/sec. 201.618 ms/step.
Infer [16/40]. 1269.70 samples/sec. 201.623 ms/step.
Infer [24/40]. 1269.66 samples/sec. 201.629 ms/step.
Infer [32/40]. 1269.64 samples/sec. 201.632 ms/step.
Infer [40/40]. 1269.65 samples/sec. 201.631 ms/step.
Inference benchmark of convformer_m36.sail_in22k done. 1269.44 samples/sec, 201.63 ms/step
Model convformer_m36.sail_in22k created, param count: 105090145
Running train benchmark on convformer_m36.sail_in22k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 294.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 144.06 MiB is free. Including non-PyTorch memory, this process has 23.50 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 94.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convformer_m36.sail_in22k created, param count: 105090145
Running train benchmark on convformer_m36.sail_in22k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 222.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 46.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 22.84 GiB is allocated by PyTorch, and 254.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convformer_m36.sail_in22k created, param count: 105090145
Running train benchmark on convformer_m36.sail_in22k for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 74.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 42.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 23.04 GiB is allocated by PyTorch, and 57.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model convformer_m36.sail_in22k created, param count: 105090145
Running train benchmark on convformer_m36.sail_in22k for 40 steps w/ input size (3, 224, 224) and batch size 96.
Train [8/40]. 397.74 samples/sec. 241.363 ms/step.
Train [16/40]. 397.74 samples/sec. 241.362 ms/step.
Train [24/40]. 397.75 samples/sec. 241.360 ms/step.
Train [32/40]. 397.75 samples/sec. 241.360 ms/step.
Train [40/40]. 397.74 samples/sec. 241.363 ms/step.
Train benchmark of convformer_m36.sail_in22k done. 395.15 samples/sec, 241.36 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convformer_m36.sail_in22k_ft_in1k created, param count: 57051640
Running inference benchmark on convformer_m36.sail_in22k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1271.71 samples/sec. 201.304 ms/step.
Infer [16/40]. 1271.66 samples/sec. 201.312 ms/step.
Infer [24/40]. 1271.68 samples/sec. 201.308 ms/step.
Infer [32/40]. 1271.69 samples/sec. 201.307 ms/step.
Infer [40/40]. 1271.67 samples/sec. 201.310 ms/step.
Inference benchmark of convformer_m36.sail_in22k_ft_in1k done. 1271.48 samples/sec, 201.31 ms/step
Model convformer_m36.sail_in22k_ft_in1k created, param count: 57051640
Running train benchmark on convformer_m36.sail_in22k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 294.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 292.06 MiB is free. Including non-PyTorch memory, this process has 23.36 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 39.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convformer_m36.sail_in22k_ft_in1k created, param count: 57051640
Running train benchmark on convformer_m36.sail_in22k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 222.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 156.06 MiB is free. Including non-PyTorch memory, this process has 23.49 GiB memory in use. Of the allocated memory 22.75 GiB is allocated by PyTorch, and 236.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convformer_m36.sail_in22k_ft_in1k created, param count: 57051640
Running train benchmark on convformer_m36.sail_in22k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 74.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 40.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 23.02 GiB is allocated by PyTorch, and 78.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model convformer_m36.sail_in22k_ft_in1k created, param count: 57051640
Running train benchmark on convformer_m36.sail_in22k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 96.
Train [8/40]. 400.78 samples/sec. 239.533 ms/step.
Train [16/40]. 400.78 samples/sec. 239.532 ms/step.
Train [24/40]. 400.77 samples/sec. 239.540 ms/step.
Train [32/40]. 400.76 samples/sec. 239.544 ms/step.
Train [40/40]. 400.75 samples/sec. 239.550 ms/step.
Train benchmark of convformer_m36.sail_in22k_ft_in1k done. 398.15 samples/sec, 239.55 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convformer_m36.sail_in22k_ft_in1k_384 created, param count: 57051640
Running inference benchmark on convformer_m36.sail_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 435.16 samples/sec. 588.290 ms/step.
Infer [16/40]. 435.17 samples/sec. 588.281 ms/step.
Infer [24/40]. 435.14 samples/sec. 588.319 ms/step.
Infer [32/40]. 435.12 samples/sec. 588.345 ms/step.
Infer [40/40]. 435.10 samples/sec. 588.369 ms/step.
Inference benchmark of convformer_m36.sail_in22k_ft_in1k_384 done. 435.07 samples/sec, 588.37 ms/step
Model convformer_m36.sail_in22k_ft_in1k_384 created, param count: 57051640
Running train benchmark on convformer_m36.sail_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 1.69 GiB. GPU 0 has a total capacty of 23.65 GiB of which 358.06 MiB is free. Including non-PyTorch memory, this process has 23.29 GiB memory in use. Of the allocated memory 22.78 GiB is allocated by PyTorch, and 5.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convformer_m36.sail_in22k_ft_in1k_384 created, param count: 57051640
Running train benchmark on convformer_m36.sail_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 1.27 GiB. GPU 0 has a total capacty of 23.65 GiB of which 768.06 MiB is free. Including non-PyTorch memory, this process has 22.89 GiB memory in use. Of the allocated memory 22.20 GiB is allocated by PyTorch, and 186.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convformer_m36.sail_in22k_ft_in1k_384 created, param count: 57051640
Running train benchmark on convformer_m36.sail_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 432.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 122.06 MiB is free. Including non-PyTorch memory, this process has 23.52 GiB memory in use. Of the allocated memory 22.88 GiB is allocated by PyTorch, and 137.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model convformer_m36.sail_in22k_ft_in1k_384 created, param count: 57051640
Running train benchmark on convformer_m36.sail_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 324.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 108.06 MiB is free. Including non-PyTorch memory, this process has 23.54 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 135.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model convformer_m36.sail_in22k_ft_in1k_384 created, param count: 57051640
Running train benchmark on convformer_m36.sail_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 216.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 110.06 MiB is free. Including non-PyTorch memory, this process has 23.53 GiB memory in use. Of the allocated memory 22.91 GiB is allocated by PyTorch, and 118.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model convformer_m36.sail_in22k_ft_in1k_384 created, param count: 57051640
Running train benchmark on convformer_m36.sail_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 2.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 23.09 GiB is allocated by PyTorch, and 45.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model convformer_m36.sail_in22k_ft_in1k_384 created, param count: 57051640
Running train benchmark on convformer_m36.sail_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 32.
Train [8/40]. 137.40 samples/sec. 232.896 ms/step.
Train [16/40]. 137.39 samples/sec. 232.906 ms/step.
Train [24/40]. 137.39 samples/sec. 232.911 ms/step.
Train [32/40]. 137.39 samples/sec. 232.912 ms/step.
Train [40/40]. 137.39 samples/sec. 232.913 ms/step.
Train benchmark of convformer_m36.sail_in22k_ft_in1k_384 done. 136.47 samples/sec, 232.91 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convformer_s18.sail_in1k created, param count: 26774448
Running inference benchmark on convformer_s18.sail_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3282.71 samples/sec. 77.984 ms/step.
Infer [16/40]. 3282.60 samples/sec. 77.987 ms/step.
Infer [24/40]. 3282.62 samples/sec. 77.987 ms/step.
Infer [32/40]. 3282.55 samples/sec. 77.988 ms/step.
Infer [40/40]. 3282.52 samples/sec. 77.989 ms/step.
Inference benchmark of convformer_s18.sail_in1k done. 3281.38 samples/sec, 77.99 ms/step
Model convformer_s18.sail_in1k created, param count: 26774448
Running train benchmark on convformer_s18.sail_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 966.94 samples/sec. 264.752 ms/step.
Train [16/40]. 966.97 samples/sec. 264.745 ms/step.
Train [24/40]. 966.97 samples/sec. 264.744 ms/step.
Train [32/40]. 966.97 samples/sec. 264.746 ms/step.
Train [40/40]. 966.97 samples/sec. 264.745 ms/step.
Train benchmark of convformer_s18.sail_in1k done. 963.14 samples/sec, 264.75 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convformer_s18.sail_in1k_384 created, param count: 26774448
Running inference benchmark on convformer_s18.sail_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 1106.66 samples/sec. 231.328 ms/step.
Infer [16/40]. 1106.64 samples/sec. 231.332 ms/step.
Infer [24/40]. 1106.62 samples/sec. 231.335 ms/step.
Infer [32/40]. 1106.61 samples/sec. 231.338 ms/step.
Infer [40/40]. 1106.60 samples/sec. 231.339 ms/step.
Inference benchmark of convformer_s18.sail_in1k_384 done. 1106.43 samples/sec, 231.34 ms/step
Model convformer_s18.sail_in1k_384 created, param count: 26774448
Running train benchmark on convformer_s18.sail_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 1.12 GiB. GPU 0 has a total capacty of 23.65 GiB of which 938.06 MiB is free. Including non-PyTorch memory, this process has 22.72 GiB memory in use. Of the allocated memory 22.06 GiB is allocated by PyTorch, and 168.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convformer_s18.sail_in1k_384 created, param count: 26774448
Running train benchmark on convformer_s18.sail_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 432.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 124.06 MiB is free. Including non-PyTorch memory, this process has 23.52 GiB memory in use. Of the allocated memory 22.92 GiB is allocated by PyTorch, and 96.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convformer_s18.sail_in1k_384 created, param count: 26774448
Running train benchmark on convformer_s18.sail_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 160.06 MiB is free. Including non-PyTorch memory, this process has 23.48 GiB memory in use. Of the allocated memory 22.92 GiB is allocated by PyTorch, and 67.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model convformer_s18.sail_in1k_384 created, param count: 26774448
Running train benchmark on convformer_s18.sail_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 96.
Train [8/40]. 329.14 samples/sec. 291.673 ms/step.
Train [16/40]. 329.12 samples/sec. 291.691 ms/step.
Train [24/40]. 329.12 samples/sec. 291.691 ms/step.
Train [32/40]. 329.11 samples/sec. 291.692 ms/step.
Train [40/40]. 329.12 samples/sec. 291.691 ms/step.
Train benchmark of convformer_s18.sail_in1k_384 done. 327.90 samples/sec, 291.69 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convformer_s18.sail_in22k created, param count: 69477657
Running inference benchmark on convformer_s18.sail_in22k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3274.47 samples/sec. 78.181 ms/step.
Infer [16/40]. 3274.27 samples/sec. 78.185 ms/step.
Infer [24/40]. 3274.25 samples/sec. 78.186 ms/step.
Infer [32/40]. 3273.99 samples/sec. 78.192 ms/step.
Infer [40/40]. 3273.65 samples/sec. 78.200 ms/step.
Inference benchmark of convformer_s18.sail_in22k done. 3272.57 samples/sec, 78.20 ms/step
Model convformer_s18.sail_in22k created, param count: 69477657
Running train benchmark on convformer_s18.sail_in22k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 960.15 samples/sec. 266.625 ms/step.
Train [16/40]. 960.10 samples/sec. 266.638 ms/step.
Train [24/40]. 960.10 samples/sec. 266.640 ms/step.
Train [32/40]. 960.09 samples/sec. 266.643 ms/step.
Train [40/40]. 960.09 samples/sec. 266.642 ms/step.
Train benchmark of convformer_s18.sail_in22k done. 956.34 samples/sec, 266.64 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convformer_s18.sail_in22k_ft_in1k created, param count: 26774448
Running inference benchmark on convformer_s18.sail_in22k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3283.22 samples/sec. 77.972 ms/step.
Infer [16/40]. 3282.90 samples/sec. 77.980 ms/step.
Infer [24/40]. 3282.84 samples/sec. 77.981 ms/step.
Infer [32/40]. 3282.83 samples/sec. 77.982 ms/step.
Infer [40/40]. 3282.56 samples/sec. 77.988 ms/step.
Inference benchmark of convformer_s18.sail_in22k_ft_in1k done. 3281.46 samples/sec, 77.99 ms/step
Model convformer_s18.sail_in22k_ft_in1k created, param count: 26774448
Running train benchmark on convformer_s18.sail_in22k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 967.10 samples/sec. 264.708 ms/step.
Train [16/40]. 967.09 samples/sec. 264.712 ms/step.
Train [24/40]. 967.10 samples/sec. 264.710 ms/step.
Train [32/40]. 967.05 samples/sec. 264.723 ms/step.
Train [40/40]. 966.96 samples/sec. 264.746 ms/step.
Train benchmark of convformer_s18.sail_in22k_ft_in1k done. 963.16 samples/sec, 264.75 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convformer_s18.sail_in22k_ft_in1k_384 created, param count: 26774448
Running inference benchmark on convformer_s18.sail_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 1106.64 samples/sec. 231.332 ms/step.
Infer [16/40]. 1106.65 samples/sec. 231.329 ms/step.
Infer [24/40]. 1106.63 samples/sec. 231.332 ms/step.
Infer [32/40]. 1106.61 samples/sec. 231.338 ms/step.
Infer [40/40]. 1106.61 samples/sec. 231.338 ms/step.
Inference benchmark of convformer_s18.sail_in22k_ft_in1k_384 done. 1106.43 samples/sec, 231.34 ms/step
Model convformer_s18.sail_in22k_ft_in1k_384 created, param count: 26774448
Running train benchmark on convformer_s18.sail_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 1.12 GiB. GPU 0 has a total capacty of 23.65 GiB of which 938.06 MiB is free. Including non-PyTorch memory, this process has 22.72 GiB memory in use. Of the allocated memory 22.06 GiB is allocated by PyTorch, and 168.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convformer_s18.sail_in22k_ft_in1k_384 created, param count: 26774448
Running train benchmark on convformer_s18.sail_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 432.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 124.06 MiB is free. Including non-PyTorch memory, this process has 23.52 GiB memory in use. Of the allocated memory 22.92 GiB is allocated by PyTorch, and 96.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convformer_s18.sail_in22k_ft_in1k_384 created, param count: 26774448
Running train benchmark on convformer_s18.sail_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 160.06 MiB is free. Including non-PyTorch memory, this process has 23.48 GiB memory in use. Of the allocated memory 22.92 GiB is allocated by PyTorch, and 67.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model convformer_s18.sail_in22k_ft_in1k_384 created, param count: 26774448
Running train benchmark on convformer_s18.sail_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 96.
Train [8/40]. 329.18 samples/sec. 291.638 ms/step.
Train [16/40]. 329.13 samples/sec. 291.674 ms/step.
Train [24/40]. 329.12 samples/sec. 291.683 ms/step.
Train [32/40]. 329.13 samples/sec. 291.680 ms/step.
Train [40/40]. 329.13 samples/sec. 291.679 ms/step.
Train benchmark of convformer_s18.sail_in22k_ft_in1k_384 done. 327.93 samples/sec, 291.68 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convformer_s36.sail_in1k created, param count: 40012152
Running inference benchmark on convformer_s36.sail_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1745.47 samples/sec. 146.665 ms/step.
Infer [16/40]. 1745.66 samples/sec. 146.649 ms/step.
Infer [24/40]. 1745.51 samples/sec. 146.662 ms/step.
Infer [32/40]. 1745.46 samples/sec. 146.667 ms/step.
Infer [40/40]. 1745.42 samples/sec. 146.669 ms/step.
Inference benchmark of convformer_s36.sail_in1k done. 1745.05 samples/sec, 146.67 ms/step
Model convformer_s36.sail_in1k created, param count: 40012152
Running train benchmark on convformer_s36.sail_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 62.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 58.06 MiB is free. Including non-PyTorch memory, this process has 23.58 GiB memory in use. Of the allocated memory 23.01 GiB is allocated by PyTorch, and 70.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convformer_s36.sail_in1k created, param count: 40012152
Running train benchmark on convformer_s36.sail_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 92.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 74.06 MiB is free. Including non-PyTorch memory, this process has 23.57 GiB memory in use. Of the allocated memory 23.02 GiB is allocated by PyTorch, and 45.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convformer_s36.sail_in1k created, param count: 40012152
Running train benchmark on convformer_s36.sail_in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
Train [8/40]. 551.01 samples/sec. 232.302 ms/step.
Train [16/40]. 550.98 samples/sec. 232.313 ms/step.
Train [24/40]. 550.97 samples/sec. 232.317 ms/step.
Train [32/40]. 550.96 samples/sec. 232.320 ms/step.
Train [40/40]. 550.96 samples/sec. 232.321 ms/step.
Train benchmark of convformer_s36.sail_in1k done. 547.13 samples/sec, 232.32 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convformer_s36.sail_in1k_384 created, param count: 40012152
Running inference benchmark on convformer_s36.sail_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 590.74 samples/sec. 433.355 ms/step.
Infer [16/40]. 590.72 samples/sec. 433.367 ms/step.
Infer [24/40]. 590.71 samples/sec. 433.379 ms/step.
Infer [32/40]. 590.69 samples/sec. 433.393 ms/step.
Infer [40/40]. 590.68 samples/sec. 433.398 ms/step.
Inference benchmark of convformer_s36.sail_in1k_384 done. 590.62 samples/sec, 433.40 ms/step
Model convformer_s36.sail_in1k_384 created, param count: 40012152
Running train benchmark on convformer_s36.sail_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 1.12 GiB. GPU 0 has a total capacty of 23.65 GiB of which 912.06 MiB is free. Including non-PyTorch memory, this process has 22.75 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 169.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convformer_s36.sail_in1k_384 created, param count: 40012152
Running train benchmark on convformer_s36.sail_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 432.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 78.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 117.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convformer_s36.sail_in1k_384 created, param count: 40012152
Running train benchmark on convformer_s36.sail_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 288.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 150.06 MiB is free. Including non-PyTorch memory, this process has 23.49 GiB memory in use. Of the allocated memory 22.94 GiB is allocated by PyTorch, and 52.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model convformer_s36.sail_in1k_384 created, param count: 40012152
Running train benchmark on convformer_s36.sail_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 216.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 180.06 MiB is free. Including non-PyTorch memory, this process has 23.46 GiB memory in use. Of the allocated memory 22.94 GiB is allocated by PyTorch, and 21.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model convformer_s36.sail_in1k_384 created, param count: 40012152
Running train benchmark on convformer_s36.sail_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 16.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 274.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model convformer_s36.sail_in1k_384 created, param count: 40012152
Running train benchmark on convformer_s36.sail_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 48.
Train [8/40]. 186.19 samples/sec. 257.807 ms/step.
Train [16/40]. 186.19 samples/sec. 257.796 ms/step.
Train [24/40]. 186.19 samples/sec. 257.806 ms/step.
Train [32/40]. 186.19 samples/sec. 257.806 ms/step.
Train [40/40]. 186.19 samples/sec. 257.808 ms/step.
Train benchmark of convformer_s36.sail_in1k_384 done. 185.02 samples/sec, 257.81 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convformer_s36.sail_in22k created, param count: 82715361
Running inference benchmark on convformer_s36.sail_in22k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1743.83 samples/sec. 146.803 ms/step.
Infer [16/40]. 1743.76 samples/sec. 146.809 ms/step.
Infer [24/40]. 1743.76 samples/sec. 146.809 ms/step.
Infer [32/40]. 1743.59 samples/sec. 146.823 ms/step.
Infer [40/40]. 1743.46 samples/sec. 146.834 ms/step.
Inference benchmark of convformer_s36.sail_in22k done. 1743.12 samples/sec, 146.83 ms/step
Model convformer_s36.sail_in22k created, param count: 82715361
Running train benchmark on convformer_s36.sail_in22k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 124.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 120.06 MiB is free. Including non-PyTorch memory, this process has 23.52 GiB memory in use. Of the allocated memory 22.97 GiB is allocated by PyTorch, and 49.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convformer_s36.sail_in22k created, param count: 82715361
Running train benchmark on convformer_s36.sail_in22k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 92.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 42.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 23.01 GiB is allocated by PyTorch, and 86.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convformer_s36.sail_in22k created, param count: 82715361
Running train benchmark on convformer_s36.sail_in22k for 40 steps w/ input size (3, 224, 224) and batch size 128.
Train [8/40]. 547.69 samples/sec. 233.709 ms/step.
Train [16/40]. 547.69 samples/sec. 233.709 ms/step.
Train [24/40]. 547.68 samples/sec. 233.713 ms/step.
Train [32/40]. 547.68 samples/sec. 233.713 ms/step.
Train [40/40]. 547.62 samples/sec. 233.740 ms/step.
Train benchmark of convformer_s36.sail_in22k done. 543.82 samples/sec, 233.74 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convformer_s36.sail_in22k_ft_in1k created, param count: 40012152
Running inference benchmark on convformer_s36.sail_in22k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1746.23 samples/sec. 146.602 ms/step.
Infer [16/40]. 1746.23 samples/sec. 146.602 ms/step.
Infer [24/40]. 1746.19 samples/sec. 146.605 ms/step.
Infer [32/40]. 1746.08 samples/sec. 146.615 ms/step.
Infer [40/40]. 1745.94 samples/sec. 146.626 ms/step.
Inference benchmark of convformer_s36.sail_in22k_ft_in1k done. 1745.58 samples/sec, 146.63 ms/step
Model convformer_s36.sail_in22k_ft_in1k created, param count: 40012152
Running train benchmark on convformer_s36.sail_in22k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 62.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 58.06 MiB is free. Including non-PyTorch memory, this process has 23.58 GiB memory in use. Of the allocated memory 23.01 GiB is allocated by PyTorch, and 70.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convformer_s36.sail_in22k_ft_in1k created, param count: 40012152
Running train benchmark on convformer_s36.sail_in22k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 92.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 52.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 23.02 GiB is allocated by PyTorch, and 67.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convformer_s36.sail_in22k_ft_in1k created, param count: 40012152
Running train benchmark on convformer_s36.sail_in22k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
Train [8/40]. 550.70 samples/sec. 232.431 ms/step.
Train [16/40]. 550.70 samples/sec. 232.432 ms/step.
Train [24/40]. 550.67 samples/sec. 232.443 ms/step.
Train [32/40]. 550.69 samples/sec. 232.438 ms/step.
Train [40/40]. 550.69 samples/sec. 232.438 ms/step.
Train benchmark of convformer_s36.sail_in22k_ft_in1k done. 546.86 samples/sec, 232.44 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convformer_s36.sail_in22k_ft_in1k_384 created, param count: 40012152
Running inference benchmark on convformer_s36.sail_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 590.68 samples/sec. 433.399 ms/step.
Infer [16/40]. 590.67 samples/sec. 433.403 ms/step.
Infer [24/40]. 590.68 samples/sec. 433.401 ms/step.
Infer [32/40]. 590.67 samples/sec. 433.403 ms/step.
Infer [40/40]. 590.67 samples/sec. 433.407 ms/step.
Inference benchmark of convformer_s36.sail_in22k_ft_in1k_384 done. 590.60 samples/sec, 433.41 ms/step
Model convformer_s36.sail_in22k_ft_in1k_384 created, param count: 40012152
Running train benchmark on convformer_s36.sail_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 1.12 GiB. GPU 0 has a total capacty of 23.65 GiB of which 912.06 MiB is free. Including non-PyTorch memory, this process has 22.75 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 169.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convformer_s36.sail_in22k_ft_in1k_384 created, param count: 40012152
Running train benchmark on convformer_s36.sail_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 432.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 78.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 117.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convformer_s36.sail_in22k_ft_in1k_384 created, param count: 40012152
Running train benchmark on convformer_s36.sail_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 288.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 150.06 MiB is free. Including non-PyTorch memory, this process has 23.49 GiB memory in use. Of the allocated memory 22.94 GiB is allocated by PyTorch, and 52.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model convformer_s36.sail_in22k_ft_in1k_384 created, param count: 40012152
Running train benchmark on convformer_s36.sail_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 216.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 180.06 MiB is free. Including non-PyTorch memory, this process has 23.46 GiB memory in use. Of the allocated memory 22.94 GiB is allocated by PyTorch, and 21.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model convformer_s36.sail_in22k_ft_in1k_384 created, param count: 40012152
Running train benchmark on convformer_s36.sail_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 46.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 36.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 22.80 GiB is allocated by PyTorch, and 305.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model convformer_s36.sail_in22k_ft_in1k_384 created, param count: 40012152
Running train benchmark on convformer_s36.sail_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 48.
Train [8/40]. 186.20 samples/sec. 257.793 ms/step.
Train [16/40]. 186.19 samples/sec. 257.799 ms/step.
Train [24/40]. 186.19 samples/sec. 257.797 ms/step.
Train [32/40]. 186.19 samples/sec. 257.800 ms/step.
Train [40/40]. 186.19 samples/sec. 257.803 ms/step.
Train benchmark of convformer_s36.sail_in22k_ft_in1k_384 done. 185.07 samples/sec, 257.80 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convit_base.fb_in1k created, param count: 86540040
Running inference benchmark on convit_base.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "expected mat1 and mat2 to have the same dtype, but got: float != c10::Half" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convit_base.fb_in1k created, param count: 86540040
Running inference benchmark on convit_base.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "expected mat1 and mat2 to have the same dtype, but got: float != c10::Half" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convit_base.fb_in1k created, param count: 86540040
Running inference benchmark on convit_base.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "expected mat1 and mat2 to have the same dtype, but got: float != c10::Half" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model convit_base.fb_in1k created, param count: 86540040
Running inference benchmark on convit_base.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 96.
ERROR: "expected mat1 and mat2 to have the same dtype, but got: float != c10::Half" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model convit_base.fb_in1k created, param count: 86540040
Running inference benchmark on convit_base.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 64.
ERROR: "expected mat1 and mat2 to have the same dtype, but got: float != c10::Half" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model convit_base.fb_in1k created, param count: 86540040
Running inference benchmark on convit_base.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 48.
ERROR: "expected mat1 and mat2 to have the same dtype, but got: float != c10::Half" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model convit_base.fb_in1k created, param count: 86540040
Running inference benchmark on convit_base.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 32.
ERROR: "expected mat1 and mat2 to have the same dtype, but got: float != c10::Half" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model convit_base.fb_in1k created, param count: 86540040
Running inference benchmark on convit_base.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 24.
ERROR: "expected mat1 and mat2 to have the same dtype, but got: float != c10::Half" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model convit_base.fb_in1k created, param count: 86540040
Running inference benchmark on convit_base.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 16.
ERROR: "expected mat1 and mat2 to have the same dtype, but got: float != c10::Half" while running benchmark.
WARNING: Reducing batch size to 12 for retry.
Model convit_base.fb_in1k created, param count: 86540040
Running inference benchmark on convit_base.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 12.
ERROR: "expected mat1 and mat2 to have the same dtype, but got: float != c10::Half" while running benchmark.
WARNING: Reducing batch size to 8 for retry.
Model convit_base.fb_in1k created, param count: 86540040
Running inference benchmark on convit_base.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 8.
ERROR: "expected mat1 and mat2 to have the same dtype, but got: float != c10::Half" while running benchmark.
WARNING: Reducing batch size to 6 for retry.
Model convit_base.fb_in1k created, param count: 86540040
Running inference benchmark on convit_base.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 6.
ERROR: "expected mat1 and mat2 to have the same dtype, but got: float != c10::Half" while running benchmark.
WARNING: Reducing batch size to 4 for retry.
Model convit_base.fb_in1k created, param count: 86540040
Running inference benchmark on convit_base.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 4.
ERROR: "expected mat1 and mat2 to have the same dtype, but got: float != c10::Half" while running benchmark.
WARNING: Reducing batch size to 3 for retry.
Model convit_base.fb_in1k created, param count: 86540040
Running inference benchmark on convit_base.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 3.
ERROR: "expected mat1 and mat2 to have the same dtype, but got: float != c10::Half" while running benchmark.
WARNING: Reducing batch size to 2 for retry.
Model convit_base.fb_in1k created, param count: 86540040
Running inference benchmark on convit_base.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 2.
ERROR: "expected mat1 and mat2 to have the same dtype, but got: float != c10::Half" while running benchmark.
WARNING: Reducing batch size to 1 for retry.
Model convit_base.fb_in1k created, param count: 86540040
Running inference benchmark on convit_base.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 1.
ERROR: "expected mat1 and mat2 to have the same dtype, but got: float != c10::Half" while running benchmark.
WARNING: Reducing batch size to 0 for retry.
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convit_small.fb_in1k created, param count: 27777322
Running inference benchmark on convit_small.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "expected mat1 and mat2 to have the same dtype, but got: float != c10::Half" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convit_small.fb_in1k created, param count: 27777322
Running inference benchmark on convit_small.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "expected mat1 and mat2 to have the same dtype, but got: float != c10::Half" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convit_small.fb_in1k created, param count: 27777322
Running inference benchmark on convit_small.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "expected mat1 and mat2 to have the same dtype, but got: float != c10::Half" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model convit_small.fb_in1k created, param count: 27777322
Running inference benchmark on convit_small.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 96.
ERROR: "expected mat1 and mat2 to have the same dtype, but got: float != c10::Half" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model convit_small.fb_in1k created, param count: 27777322
Running inference benchmark on convit_small.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 64.
ERROR: "expected mat1 and mat2 to have the same dtype, but got: float != c10::Half" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model convit_small.fb_in1k created, param count: 27777322
Running inference benchmark on convit_small.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 48.
ERROR: "expected mat1 and mat2 to have the same dtype, but got: float != c10::Half" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model convit_small.fb_in1k created, param count: 27777322
Running inference benchmark on convit_small.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 32.
ERROR: "expected mat1 and mat2 to have the same dtype, but got: float != c10::Half" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model convit_small.fb_in1k created, param count: 27777322
Running inference benchmark on convit_small.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 24.
ERROR: "expected mat1 and mat2 to have the same dtype, but got: float != c10::Half" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model convit_small.fb_in1k created, param count: 27777322
Running inference benchmark on convit_small.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 16.
ERROR: "expected mat1 and mat2 to have the same dtype, but got: float != c10::Half" while running benchmark.
WARNING: Reducing batch size to 12 for retry.
Model convit_small.fb_in1k created, param count: 27777322
Running inference benchmark on convit_small.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 12.
ERROR: "expected mat1 and mat2 to have the same dtype, but got: float != c10::Half" while running benchmark.
WARNING: Reducing batch size to 8 for retry.
Model convit_small.fb_in1k created, param count: 27777322
Running inference benchmark on convit_small.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 8.
ERROR: "expected mat1 and mat2 to have the same dtype, but got: float != c10::Half" while running benchmark.
WARNING: Reducing batch size to 6 for retry.
Model convit_small.fb_in1k created, param count: 27777322
Running inference benchmark on convit_small.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 6.
ERROR: "expected mat1 and mat2 to have the same dtype, but got: float != c10::Half" while running benchmark.
WARNING: Reducing batch size to 4 for retry.
Model convit_small.fb_in1k created, param count: 27777322
Running inference benchmark on convit_small.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 4.
ERROR: "expected mat1 and mat2 to have the same dtype, but got: float != c10::Half" while running benchmark.
WARNING: Reducing batch size to 3 for retry.
Model convit_small.fb_in1k created, param count: 27777322
Running inference benchmark on convit_small.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 3.
ERROR: "expected mat1 and mat2 to have the same dtype, but got: float != c10::Half" while running benchmark.
WARNING: Reducing batch size to 2 for retry.
Model convit_small.fb_in1k created, param count: 27777322
Running inference benchmark on convit_small.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 2.
ERROR: "expected mat1 and mat2 to have the same dtype, but got: float != c10::Half" while running benchmark.
WARNING: Reducing batch size to 1 for retry.
Model convit_small.fb_in1k created, param count: 27777322
Running inference benchmark on convit_small.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 1.
ERROR: "expected mat1 and mat2 to have the same dtype, but got: float != c10::Half" while running benchmark.
WARNING: Reducing batch size to 0 for retry.
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convit_tiny.fb_in1k created, param count: 5710512
Running inference benchmark on convit_tiny.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "expected mat1 and mat2 to have the same dtype, but got: float != c10::Half" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convit_tiny.fb_in1k created, param count: 5710512
Running inference benchmark on convit_tiny.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "expected mat1 and mat2 to have the same dtype, but got: float != c10::Half" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convit_tiny.fb_in1k created, param count: 5710512
Running inference benchmark on convit_tiny.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "expected mat1 and mat2 to have the same dtype, but got: float != c10::Half" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model convit_tiny.fb_in1k created, param count: 5710512
Running inference benchmark on convit_tiny.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 96.
ERROR: "expected mat1 and mat2 to have the same dtype, but got: float != c10::Half" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model convit_tiny.fb_in1k created, param count: 5710512
Running inference benchmark on convit_tiny.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 64.
ERROR: "expected mat1 and mat2 to have the same dtype, but got: float != c10::Half" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model convit_tiny.fb_in1k created, param count: 5710512
Running inference benchmark on convit_tiny.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 48.
ERROR: "expected mat1 and mat2 to have the same dtype, but got: float != c10::Half" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model convit_tiny.fb_in1k created, param count: 5710512
Running inference benchmark on convit_tiny.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 32.
ERROR: "expected mat1 and mat2 to have the same dtype, but got: float != c10::Half" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model convit_tiny.fb_in1k created, param count: 5710512
Running inference benchmark on convit_tiny.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 24.
ERROR: "expected mat1 and mat2 to have the same dtype, but got: float != c10::Half" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model convit_tiny.fb_in1k created, param count: 5710512
Running inference benchmark on convit_tiny.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 16.
ERROR: "expected mat1 and mat2 to have the same dtype, but got: float != c10::Half" while running benchmark.
WARNING: Reducing batch size to 12 for retry.
Model convit_tiny.fb_in1k created, param count: 5710512
Running inference benchmark on convit_tiny.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 12.
ERROR: "expected mat1 and mat2 to have the same dtype, but got: float != c10::Half" while running benchmark.
WARNING: Reducing batch size to 8 for retry.
Model convit_tiny.fb_in1k created, param count: 5710512
Running inference benchmark on convit_tiny.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 8.
ERROR: "expected mat1 and mat2 to have the same dtype, but got: float != c10::Half" while running benchmark.
WARNING: Reducing batch size to 6 for retry.
Model convit_tiny.fb_in1k created, param count: 5710512
Running inference benchmark on convit_tiny.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 6.
ERROR: "expected mat1 and mat2 to have the same dtype, but got: float != c10::Half" while running benchmark.
WARNING: Reducing batch size to 4 for retry.
Model convit_tiny.fb_in1k created, param count: 5710512
Running inference benchmark on convit_tiny.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 4.
ERROR: "expected mat1 and mat2 to have the same dtype, but got: float != c10::Half" while running benchmark.
WARNING: Reducing batch size to 3 for retry.
Model convit_tiny.fb_in1k created, param count: 5710512
Running inference benchmark on convit_tiny.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 3.
ERROR: "expected mat1 and mat2 to have the same dtype, but got: float != c10::Half" while running benchmark.
WARNING: Reducing batch size to 2 for retry.
Model convit_tiny.fb_in1k created, param count: 5710512
Running inference benchmark on convit_tiny.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 2.
ERROR: "expected mat1 and mat2 to have the same dtype, but got: float != c10::Half" while running benchmark.
WARNING: Reducing batch size to 1 for retry.
Model convit_tiny.fb_in1k created, param count: 5710512
Running inference benchmark on convit_tiny.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 1.
ERROR: "expected mat1 and mat2 to have the same dtype, but got: float != c10::Half" while running benchmark.
WARNING: Reducing batch size to 0 for retry.
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convmixer_768_32.in1k created, param count: 21110248
Running inference benchmark on convmixer_768_32.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 541.05 samples/sec. 473.152 ms/step.
Infer [16/40]. 541.01 samples/sec. 473.185 ms/step.
Infer [24/40]. 541.01 samples/sec. 473.193 ms/step.
Infer [32/40]. 540.99 samples/sec. 473.204 ms/step.
Infer [40/40]. 540.98 samples/sec. 473.214 ms/step.
Inference benchmark of convmixer_768_32.in1k done. 540.93 samples/sec, 473.21 ms/step
Model convmixer_768_32.in1k created, param count: 21110248
Running train benchmark on convmixer_768_32.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 384.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 198.06 MiB is free. Including non-PyTorch memory, this process has 23.45 GiB memory in use. Of the allocated memory 22.63 GiB is allocated by PyTorch, and 322.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convmixer_768_32.in1k created, param count: 21110248
Running train benchmark on convmixer_768_32.in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 288.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 100.06 MiB is free. Including non-PyTorch memory, this process has 23.54 GiB memory in use. Of the allocated memory 22.89 GiB is allocated by PyTorch, and 151.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convmixer_768_32.in1k created, param count: 21110248
Running train benchmark on convmixer_768_32.in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 2.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 22.97 GiB is allocated by PyTorch, and 170.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model convmixer_768_32.in1k created, param count: 21110248
Running train benchmark on convmixer_768_32.in1k for 40 steps w/ input size (3, 224, 224) and batch size 96.
Train [8/40]. 168.09 samples/sec. 571.119 ms/step.
Train [16/40]. 168.10 samples/sec. 571.075 ms/step.
Train [24/40]. 168.09 samples/sec. 571.112 ms/step.
Train [32/40]. 168.08 samples/sec. 571.140 ms/step.
Train [40/40]. 168.07 samples/sec. 571.197 ms/step.
Train benchmark of convmixer_768_32.in1k done. 167.72 samples/sec, 571.20 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convmixer_1024_20_ks9_p14.in1k created, param count: 24383464
Running inference benchmark on convmixer_1024_20_ks9_p14.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 2050.25 samples/sec. 124.863 ms/step.
Infer [16/40]. 2049.12 samples/sec. 124.932 ms/step.
Infer [24/40]. 2048.72 samples/sec. 124.956 ms/step.
Infer [32/40]. 2048.48 samples/sec. 124.971 ms/step.
Infer [40/40]. 2048.32 samples/sec. 124.981 ms/step.
Inference benchmark of convmixer_1024_20_ks9_p14.in1k done. 2047.85 samples/sec, 124.98 ms/step
Model convmixer_1024_20_ks9_p14.in1k created, param count: 24383464
Running train benchmark on convmixer_1024_20_ks9_p14.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 598.82 samples/sec. 427.505 ms/step.
Train [16/40]. 598.86 samples/sec. 427.475 ms/step.
Train [24/40]. 598.85 samples/sec. 427.489 ms/step.
Train [32/40]. 598.76 samples/sec. 427.547 ms/step.
Train [40/40]. 598.72 samples/sec. 427.581 ms/step.
Train benchmark of convmixer_1024_20_ks9_p14.in1k done. 597.45 samples/sec, 427.58 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convmixer_1536_20.in1k created, param count: 51625960
Running inference benchmark on convmixer_1536_20.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 325.21 samples/sec. 787.185 ms/step.
Infer [16/40]. 325.16 samples/sec. 787.298 ms/step.
Infer [24/40]. 325.15 samples/sec. 787.337 ms/step.
Infer [32/40]. 325.13 samples/sec. 787.386 ms/step.
Infer [40/40]. 325.11 samples/sec. 787.435 ms/step.
Inference benchmark of convmixer_1536_20.in1k done. 325.09 samples/sec, 787.43 ms/step
Model convmixer_1536_20.in1k created, param count: 51625960
Running train benchmark on convmixer_1536_20.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 768.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 580.06 MiB is free. Including non-PyTorch memory, this process has 23.07 GiB memory in use. Of the allocated memory 21.94 GiB is allocated by PyTorch, and 650.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convmixer_1536_20.in1k created, param count: 51625960
Running train benchmark on convmixer_1536_20.in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 576.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 386.06 MiB is free. Including non-PyTorch memory, this process has 23.26 GiB memory in use. Of the allocated memory 22.10 GiB is allocated by PyTorch, and 671.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convmixer_1536_20.in1k created, param count: 51625960
Running train benchmark on convmixer_1536_20.in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 384.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 194.06 MiB is free. Including non-PyTorch memory, this process has 23.45 GiB memory in use. Of the allocated memory 22.65 GiB is allocated by PyTorch, and 305.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model convmixer_1536_20.in1k created, param count: 51625960
Running train benchmark on convmixer_1536_20.in1k for 40 steps w/ input size (3, 224, 224) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 288.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 96.06 MiB is free. Including non-PyTorch memory, this process has 23.55 GiB memory in use. Of the allocated memory 22.92 GiB is allocated by PyTorch, and 124.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model convmixer_1536_20.in1k created, param count: 51625960
Running train benchmark on convmixer_1536_20.in1k for 40 steps w/ input size (3, 224, 224) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 192.06 MiB is free. Including non-PyTorch memory, this process has 23.45 GiB memory in use. Of the allocated memory 22.82 GiB is allocated by PyTorch, and 133.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model convmixer_1536_20.in1k created, param count: 51625960
Running train benchmark on convmixer_1536_20.in1k for 40 steps w/ input size (3, 224, 224) and batch size 48.
Train [8/40]. 95.71 samples/sec. 501.513 ms/step.
Train [16/40]. 95.63 samples/sec. 501.957 ms/step.
Train [24/40]. 95.59 samples/sec. 502.153 ms/step.
Train [32/40]. 95.56 samples/sec. 502.297 ms/step.
Train [40/40]. 95.54 samples/sec. 502.414 ms/step.
Train benchmark of convmixer_1536_20.in1k done. 95.37 samples/sec, 502.41 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnext_atto.d2_in1k created, param count: 3695520
Running inference benchmark on convnext_atto.d2_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 9199.17 samples/sec. 27.829 ms/step.
Infer [16/40]. 9198.34 samples/sec. 27.831 ms/step.
Infer [24/40]. 9198.27 samples/sec. 27.831 ms/step.
Infer [32/40]. 9198.31 samples/sec. 27.831 ms/step.
Infer [40/40]. 9198.20 samples/sec. 27.832 ms/step.
Inference benchmark of convnext_atto.d2_in1k done. 9190.19 samples/sec, 27.83 ms/step
Model convnext_atto.d2_in1k created, param count: 3695520
Running train benchmark on convnext_atto.d2_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 2913.97 samples/sec. 87.853 ms/step.
Train [16/40]. 2914.04 samples/sec. 87.851 ms/step.
Train [24/40]. 2914.00 samples/sec. 87.852 ms/step.
Train [32/40]. 2913.99 samples/sec. 87.852 ms/step.
Train [40/40]. 2914.04 samples/sec. 87.851 ms/step.
Train benchmark of convnext_atto.d2_in1k done. 2898.33 samples/sec, 87.85 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnext_atto_ols.a2_in1k created, param count: 3702912
Running inference benchmark on convnext_atto_ols.a2_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 8602.35 samples/sec. 29.759 ms/step.
Infer [16/40]. 8602.63 samples/sec. 29.758 ms/step.
Infer [24/40]. 8602.51 samples/sec. 29.759 ms/step.
Infer [32/40]. 8602.86 samples/sec. 29.758 ms/step.
Infer [40/40]. 8602.78 samples/sec. 29.758 ms/step.
Inference benchmark of convnext_atto_ols.a2_in1k done. 8595.79 samples/sec, 29.76 ms/step
Model convnext_atto_ols.a2_in1k created, param count: 3702912
Running train benchmark on convnext_atto_ols.a2_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 2739.12 samples/sec. 93.461 ms/step.
Train [16/40]. 2739.15 samples/sec. 93.460 ms/step.
Train [24/40]. 2739.09 samples/sec. 93.462 ms/step.
Train [32/40]. 2739.10 samples/sec. 93.461 ms/step.
Train [40/40]. 2739.08 samples/sec. 93.462 ms/step.
Train benchmark of convnext_atto_ols.a2_in1k done. 2725.14 samples/sec, 93.46 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnext_base.clip_laion2b created, param count: 88222464
Running inference benchmark on convnext_base.clip_laion2b for 40 steps w/ input size (3, 256, 256) and batch size 256.
Infer [8/40]. 1489.55 samples/sec. 171.864 ms/step.
Infer [16/40]. 1489.31 samples/sec. 171.892 ms/step.
Infer [24/40]. 1489.11 samples/sec. 171.915 ms/step.
Infer [32/40]. 1488.95 samples/sec. 171.933 ms/step.
Infer [40/40]. 1488.93 samples/sec. 171.936 ms/step.
Inference benchmark of convnext_base.clip_laion2b done. 1488.66 samples/sec, 171.94 ms/step
Model convnext_base.clip_laion2b created, param count: 88222464
Running train benchmark on convnext_base.clip_laion2b for 40 steps w/ input size (3, 256, 256) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 18.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.39 GiB is allocated by PyTorch, and 9.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convnext_base.clip_laion2b created, param count: 88222464
Running train benchmark on convnext_base.clip_laion2b for 40 steps w/ input size (3, 256, 256) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 162.06 MiB is free. Including non-PyTorch memory, this process has 23.48 GiB memory in use. Of the allocated memory 22.19 GiB is allocated by PyTorch, and 74.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convnext_base.clip_laion2b created, param count: 88222464
Running train benchmark on convnext_base.clip_laion2b for 40 steps w/ input size (3, 256, 256) and batch size 128.
Train [8/40]. 544.12 samples/sec. 235.243 ms/step.
Train [16/40]. 544.11 samples/sec. 235.245 ms/step.
Train [24/40]. 544.13 samples/sec. 235.240 ms/step.
Train [32/40]. 544.13 samples/sec. 235.237 ms/step.
Train [40/40]. 544.15 samples/sec. 235.229 ms/step.
Train benchmark of convnext_base.clip_laion2b done. 541.42 samples/sec, 235.23 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnext_base.clip_laion2b_augreg created, param count: 88222464
Running inference benchmark on convnext_base.clip_laion2b_augreg for 40 steps w/ input size (3, 256, 256) and batch size 256.
Infer [8/40]. 1489.28 samples/sec. 171.895 ms/step.
Infer [16/40]. 1489.19 samples/sec. 171.906 ms/step.
Infer [24/40]. 1489.06 samples/sec. 171.921 ms/step.
Infer [32/40]. 1489.09 samples/sec. 171.917 ms/step.
Infer [40/40]. 1489.07 samples/sec. 171.919 ms/step.
Inference benchmark of convnext_base.clip_laion2b_augreg done. 1488.82 samples/sec, 171.92 ms/step
Model convnext_base.clip_laion2b_augreg created, param count: 88222464
Running train benchmark on convnext_base.clip_laion2b_augreg for 40 steps w/ input size (3, 256, 256) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 18.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.39 GiB is allocated by PyTorch, and 9.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convnext_base.clip_laion2b_augreg created, param count: 88222464
Running train benchmark on convnext_base.clip_laion2b_augreg for 40 steps w/ input size (3, 256, 256) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 162.06 MiB is free. Including non-PyTorch memory, this process has 23.48 GiB memory in use. Of the allocated memory 22.19 GiB is allocated by PyTorch, and 74.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convnext_base.clip_laion2b_augreg created, param count: 88222464
Running train benchmark on convnext_base.clip_laion2b_augreg for 40 steps w/ input size (3, 256, 256) and batch size 128.
Train [8/40]. 544.13 samples/sec. 235.239 ms/step.
Train [16/40]. 544.00 samples/sec. 235.296 ms/step.
Train [24/40]. 543.94 samples/sec. 235.321 ms/step.
Train [32/40]. 543.94 samples/sec. 235.318 ms/step.
Train [40/40]. 543.94 samples/sec. 235.320 ms/step.
Train benchmark of convnext_base.clip_laion2b_augreg done. 541.09 samples/sec, 235.32 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnext_base.clip_laion2b_augreg_ft_in1k created, param count: 88591464
Running inference benchmark on convnext_base.clip_laion2b_augreg_ft_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
Infer [8/40]. 1489.97 samples/sec. 171.815 ms/step.
Infer [16/40]. 1489.70 samples/sec. 171.847 ms/step.
Infer [24/40]. 1489.53 samples/sec. 171.866 ms/step.
Infer [32/40]. 1489.41 samples/sec. 171.880 ms/step.
Infer [40/40]. 1489.29 samples/sec. 171.894 ms/step.
Inference benchmark of convnext_base.clip_laion2b_augreg_ft_in1k done. 1489.00 samples/sec, 171.89 ms/step
Model convnext_base.clip_laion2b_augreg_ft_in1k created, param count: 88591464
Running train benchmark on convnext_base.clip_laion2b_augreg_ft_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 18.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.39 GiB is allocated by PyTorch, and 9.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convnext_base.clip_laion2b_augreg_ft_in1k created, param count: 88591464
Running train benchmark on convnext_base.clip_laion2b_augreg_ft_in1k for 40 steps w/ input size (3, 256, 256) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 162.06 MiB is free. Including non-PyTorch memory, this process has 23.48 GiB memory in use. Of the allocated memory 22.19 GiB is allocated by PyTorch, and 73.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convnext_base.clip_laion2b_augreg_ft_in1k created, param count: 88591464
Running train benchmark on convnext_base.clip_laion2b_augreg_ft_in1k for 40 steps w/ input size (3, 256, 256) and batch size 128.
Train [8/40]. 544.03 samples/sec. 235.282 ms/step.
Train [16/40]. 544.00 samples/sec. 235.292 ms/step.
Train [24/40]. 543.97 samples/sec. 235.308 ms/step.
Train [32/40]. 543.96 samples/sec. 235.312 ms/step.
Train [40/40]. 543.96 samples/sec. 235.313 ms/step.
Train benchmark of convnext_base.clip_laion2b_augreg_ft_in1k done. 541.14 samples/sec, 235.31 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnext_base.clip_laion2b_augreg_ft_in12k created, param count: 99682989
Running inference benchmark on convnext_base.clip_laion2b_augreg_ft_in12k for 40 steps w/ input size (3, 256, 256) and batch size 256.
Infer [8/40]. 1488.02 samples/sec. 172.041 ms/step.
Infer [16/40]. 1488.01 samples/sec. 172.042 ms/step.
Infer [24/40]. 1488.06 samples/sec. 172.036 ms/step.
Infer [32/40]. 1488.03 samples/sec. 172.039 ms/step.
Infer [40/40]. 1488.01 samples/sec. 172.042 ms/step.
Inference benchmark of convnext_base.clip_laion2b_augreg_ft_in12k done. 1487.75 samples/sec, 172.04 ms/step
Model convnext_base.clip_laion2b_augreg_ft_in12k created, param count: 99682989
Running train benchmark on convnext_base.clip_laion2b_augreg_ft_in12k for 40 steps w/ input size (3, 256, 256) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 62.06 MiB is free. Including non-PyTorch memory, this process has 23.58 GiB memory in use. Of the allocated memory 22.35 GiB is allocated by PyTorch, and 7.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convnext_base.clip_laion2b_augreg_ft_in12k created, param count: 99682989
Running train benchmark on convnext_base.clip_laion2b_augreg_ft_in12k for 40 steps w/ input size (3, 256, 256) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 162.06 MiB is free. Including non-PyTorch memory, this process has 23.48 GiB memory in use. Of the allocated memory 22.21 GiB is allocated by PyTorch, and 52.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convnext_base.clip_laion2b_augreg_ft_in12k created, param count: 99682989
Running train benchmark on convnext_base.clip_laion2b_augreg_ft_in12k for 40 steps w/ input size (3, 256, 256) and batch size 128.
Train [8/40]. 543.11 samples/sec. 235.681 ms/step.
Train [16/40]. 543.06 samples/sec. 235.701 ms/step.
Train [24/40]. 543.03 samples/sec. 235.715 ms/step.
Train [32/40]. 543.00 samples/sec. 235.727 ms/step.
Train [40/40]. 543.01 samples/sec. 235.725 ms/step.
Train benchmark of convnext_base.clip_laion2b_augreg_ft_in12k done. 540.23 samples/sec, 235.72 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnext_base.clip_laion2b_augreg_ft_in12k_in1k created, param count: 88591464
Running inference benchmark on convnext_base.clip_laion2b_augreg_ft_in12k_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
Infer [8/40]. 1490.16 samples/sec. 171.794 ms/step.
Infer [16/40]. 1489.97 samples/sec. 171.815 ms/step.
Infer [24/40]. 1489.64 samples/sec. 171.854 ms/step.
Infer [32/40]. 1489.45 samples/sec. 171.875 ms/step.
Infer [40/40]. 1489.40 samples/sec. 171.882 ms/step.
Inference benchmark of convnext_base.clip_laion2b_augreg_ft_in12k_in1k done. 1489.13 samples/sec, 171.88 ms/step
Model convnext_base.clip_laion2b_augreg_ft_in12k_in1k created, param count: 88591464
Running train benchmark on convnext_base.clip_laion2b_augreg_ft_in12k_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 18.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.39 GiB is allocated by PyTorch, and 9.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convnext_base.clip_laion2b_augreg_ft_in12k_in1k created, param count: 88591464
Running train benchmark on convnext_base.clip_laion2b_augreg_ft_in12k_in1k for 40 steps w/ input size (3, 256, 256) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 162.06 MiB is free. Including non-PyTorch memory, this process has 23.48 GiB memory in use. Of the allocated memory 22.19 GiB is allocated by PyTorch, and 73.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convnext_base.clip_laion2b_augreg_ft_in12k_in1k created, param count: 88591464
Running train benchmark on convnext_base.clip_laion2b_augreg_ft_in12k_in1k for 40 steps w/ input size (3, 256, 256) and batch size 128.
Train [8/40]. 544.09 samples/sec. 235.256 ms/step.
Train [16/40]. 544.01 samples/sec. 235.289 ms/step.
Train [24/40]. 543.98 samples/sec. 235.302 ms/step.
Train [32/40]. 543.96 samples/sec. 235.314 ms/step.
Train [40/40]. 543.94 samples/sec. 235.320 ms/step.
Train benchmark of convnext_base.clip_laion2b_augreg_ft_in12k_in1k done. 541.24 samples/sec, 235.32 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnext_base.clip_laion2b_augreg_ft_in12k_in1k_384 created, param count: 88591464
Running inference benchmark on convnext_base.clip_laion2b_augreg_ft_in12k_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 657.33 samples/sec. 389.454 ms/step.
Infer [16/40]. 657.30 samples/sec. 389.471 ms/step.
Infer [24/40]. 657.30 samples/sec. 389.469 ms/step.
Infer [32/40]. 657.30 samples/sec. 389.472 ms/step.
Infer [40/40]. 657.30 samples/sec. 389.475 ms/step.
Inference benchmark of convnext_base.clip_laion2b_augreg_ft_in12k_in1k_384 done. 657.22 samples/sec, 389.48 ms/step
Model convnext_base.clip_laion2b_augreg_ft_in12k_in1k_384 created, param count: 88591464
Running train benchmark on convnext_base.clip_laion2b_augreg_ft_in12k_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 576.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 224.06 MiB is free. Including non-PyTorch memory, this process has 23.42 GiB memory in use. Of the allocated memory 21.84 GiB is allocated by PyTorch, and 368.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convnext_base.clip_laion2b_augreg_ft_in12k_in1k_384 created, param count: 88591464
Running train benchmark on convnext_base.clip_laion2b_augreg_ft_in12k_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 216.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 78.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 22.14 GiB is allocated by PyTorch, and 205.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convnext_base.clip_laion2b_augreg_ft_in12k_in1k_384 created, param count: 88591464
Running train benchmark on convnext_base.clip_laion2b_augreg_ft_in12k_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 288.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 148.06 MiB is free. Including non-PyTorch memory, this process has 23.50 GiB memory in use. Of the allocated memory 22.21 GiB is allocated by PyTorch, and 62.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model convnext_base.clip_laion2b_augreg_ft_in12k_in1k_384 created, param count: 88591464
Running train benchmark on convnext_base.clip_laion2b_augreg_ft_in12k_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 216.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 174.06 MiB is free. Including non-PyTorch memory, this process has 23.47 GiB memory in use. Of the allocated memory 22.19 GiB is allocated by PyTorch, and 55.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model convnext_base.clip_laion2b_augreg_ft_in12k_in1k_384 created, param count: 88591464
Running train benchmark on convnext_base.clip_laion2b_augreg_ft_in12k_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 64.
Train [8/40]. 241.19 samples/sec. 265.351 ms/step.
Train [16/40]. 241.19 samples/sec. 265.352 ms/step.
Train [24/40]. 241.18 samples/sec. 265.362 ms/step.
Train [32/40]. 241.18 samples/sec. 265.361 ms/step.
Train [40/40]. 241.18 samples/sec. 265.362 ms/step.
Train benchmark of convnext_base.clip_laion2b_augreg_ft_in12k_in1k_384 done. 240.03 samples/sec, 265.36 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnext_base.clip_laiona created, param count: 88222464
Running inference benchmark on convnext_base.clip_laiona for 40 steps w/ input size (3, 256, 256) and batch size 256.
Infer [8/40]. 1489.38 samples/sec. 171.884 ms/step.
Infer [16/40]. 1489.20 samples/sec. 171.904 ms/step.
Infer [24/40]. 1489.18 samples/sec. 171.907 ms/step.
Infer [32/40]. 1489.19 samples/sec. 171.906 ms/step.
Infer [40/40]. 1489.11 samples/sec. 171.915 ms/step.
Inference benchmark of convnext_base.clip_laiona done. 1488.84 samples/sec, 171.91 ms/step
Model convnext_base.clip_laiona created, param count: 88222464
Running train benchmark on convnext_base.clip_laiona for 40 steps w/ input size (3, 256, 256) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 18.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.39 GiB is allocated by PyTorch, and 9.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convnext_base.clip_laiona created, param count: 88222464
Running train benchmark on convnext_base.clip_laiona for 40 steps w/ input size (3, 256, 256) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 162.06 MiB is free. Including non-PyTorch memory, this process has 23.48 GiB memory in use. Of the allocated memory 22.19 GiB is allocated by PyTorch, and 74.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convnext_base.clip_laiona created, param count: 88222464
Running train benchmark on convnext_base.clip_laiona for 40 steps w/ input size (3, 256, 256) and batch size 128.
Train [8/40]. 544.12 samples/sec. 235.242 ms/step.
Train [16/40]. 544.11 samples/sec. 235.247 ms/step.
Train [24/40]. 544.10 samples/sec. 235.252 ms/step.
Train [32/40]. 544.09 samples/sec. 235.254 ms/step.
Train [40/40]. 544.09 samples/sec. 235.257 ms/step.
Train benchmark of convnext_base.clip_laiona done. 541.31 samples/sec, 235.26 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnext_base.clip_laiona_320 created, param count: 88222464
Running inference benchmark on convnext_base.clip_laiona_320 for 40 steps w/ input size (3, 320, 320) and batch size 256.
Infer [8/40]. 939.38 samples/sec. 272.520 ms/step.
Infer [16/40]. 939.37 samples/sec. 272.524 ms/step.
Infer [24/40]. 939.39 samples/sec. 272.517 ms/step.
Infer [32/40]. 939.37 samples/sec. 272.522 ms/step.
Infer [40/40]. 939.37 samples/sec. 272.523 ms/step.
Inference benchmark of convnext_base.clip_laiona_320 done. 939.24 samples/sec, 272.52 ms/step
Model convnext_base.clip_laiona_320 created, param count: 88222464
Running train benchmark on convnext_base.clip_laiona_320 for 40 steps w/ input size (3, 320, 320) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 800.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 712.06 MiB is free. Including non-PyTorch memory, this process has 22.95 GiB memory in use. Of the allocated memory 21.69 GiB is allocated by PyTorch, and 34.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convnext_base.clip_laiona_320 created, param count: 88222464
Running train benchmark on convnext_base.clip_laiona_320 for 40 steps w/ input size (3, 320, 320) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 300.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 36.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 22.27 GiB is allocated by PyTorch, and 113.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convnext_base.clip_laiona_320 created, param count: 88222464
Running train benchmark on convnext_base.clip_laiona_320 for 40 steps w/ input size (3, 320, 320) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 200.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 48.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 22.32 GiB is allocated by PyTorch, and 50.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model convnext_base.clip_laiona_320 created, param count: 88222464
Running train benchmark on convnext_base.clip_laiona_320 for 40 steps w/ input size (3, 320, 320) and batch size 96.
Train [8/40]. 344.00 samples/sec. 279.072 ms/step.
Train [16/40]. 344.01 samples/sec. 279.065 ms/step.
Train [24/40]. 344.01 samples/sec. 279.063 ms/step.
Train [32/40]. 344.01 samples/sec. 279.065 ms/step.
Train [40/40]. 344.01 samples/sec. 279.065 ms/step.
Train benchmark of convnext_base.clip_laiona_320 done. 342.43 samples/sec, 279.06 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnext_base.clip_laiona_augreg_320 created, param count: 88222464
Running inference benchmark on convnext_base.clip_laiona_augreg_320 for 40 steps w/ input size (3, 320, 320) and batch size 256.
Infer [8/40]. 939.37 samples/sec. 272.523 ms/step.
Infer [16/40]. 939.34 samples/sec. 272.531 ms/step.
Infer [24/40]. 939.31 samples/sec. 272.539 ms/step.
Infer [32/40]. 939.30 samples/sec. 272.543 ms/step.
Infer [40/40]. 939.28 samples/sec. 272.550 ms/step.
Inference benchmark of convnext_base.clip_laiona_augreg_320 done. 939.15 samples/sec, 272.55 ms/step
Model convnext_base.clip_laiona_augreg_320 created, param count: 88222464
Running train benchmark on convnext_base.clip_laiona_augreg_320 for 40 steps w/ input size (3, 320, 320) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 800.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 712.06 MiB is free. Including non-PyTorch memory, this process has 22.95 GiB memory in use. Of the allocated memory 21.69 GiB is allocated by PyTorch, and 34.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convnext_base.clip_laiona_augreg_320 created, param count: 88222464
Running train benchmark on convnext_base.clip_laiona_augreg_320 for 40 steps w/ input size (3, 320, 320) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 300.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 36.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 22.27 GiB is allocated by PyTorch, and 113.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convnext_base.clip_laiona_augreg_320 created, param count: 88222464
Running train benchmark on convnext_base.clip_laiona_augreg_320 for 40 steps w/ input size (3, 320, 320) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 200.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 48.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 22.32 GiB is allocated by PyTorch, and 50.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model convnext_base.clip_laiona_augreg_320 created, param count: 88222464
Running train benchmark on convnext_base.clip_laiona_augreg_320 for 40 steps w/ input size (3, 320, 320) and batch size 96.
Train [8/40]. 343.82 samples/sec. 279.216 ms/step.
Train [16/40]. 343.84 samples/sec. 279.203 ms/step.
Train [24/40]. 343.85 samples/sec. 279.190 ms/step.
Train [32/40]. 343.85 samples/sec. 279.188 ms/step.
Train [40/40]. 343.86 samples/sec. 279.184 ms/step.
Train benchmark of convnext_base.clip_laiona_augreg_320 done. 342.25 samples/sec, 279.18 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnext_base.clip_laiona_augreg_ft_in1k_384 created, param count: 88591464
Running inference benchmark on convnext_base.clip_laiona_augreg_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 657.54 samples/sec. 389.331 ms/step.
Infer [16/40]. 657.43 samples/sec. 389.393 ms/step.
Infer [24/40]. 657.35 samples/sec. 389.441 ms/step.
Infer [32/40]. 657.31 samples/sec. 389.466 ms/step.
Infer [40/40]. 657.28 samples/sec. 389.482 ms/step.
Inference benchmark of convnext_base.clip_laiona_augreg_ft_in1k_384 done. 657.21 samples/sec, 389.48 ms/step
Model convnext_base.clip_laiona_augreg_ft_in1k_384 created, param count: 88591464
Running train benchmark on convnext_base.clip_laiona_augreg_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 576.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 224.06 MiB is free. Including non-PyTorch memory, this process has 23.42 GiB memory in use. Of the allocated memory 21.84 GiB is allocated by PyTorch, and 368.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convnext_base.clip_laiona_augreg_ft_in1k_384 created, param count: 88591464
Running train benchmark on convnext_base.clip_laiona_augreg_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 216.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 78.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 22.14 GiB is allocated by PyTorch, and 205.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convnext_base.clip_laiona_augreg_ft_in1k_384 created, param count: 88591464
Running train benchmark on convnext_base.clip_laiona_augreg_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 288.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 148.06 MiB is free. Including non-PyTorch memory, this process has 23.50 GiB memory in use. Of the allocated memory 22.21 GiB is allocated by PyTorch, and 62.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model convnext_base.clip_laiona_augreg_ft_in1k_384 created, param count: 88591464
Running train benchmark on convnext_base.clip_laiona_augreg_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 216.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 174.06 MiB is free. Including non-PyTorch memory, this process has 23.47 GiB memory in use. Of the allocated memory 22.19 GiB is allocated by PyTorch, and 55.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model convnext_base.clip_laiona_augreg_ft_in1k_384 created, param count: 88591464
Running train benchmark on convnext_base.clip_laiona_augreg_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 64.
Train [8/40]. 241.31 samples/sec. 265.218 ms/step.
Train [16/40]. 241.32 samples/sec. 265.206 ms/step.
Train [24/40]. 241.32 samples/sec. 265.209 ms/step.
Train [32/40]. 241.31 samples/sec. 265.214 ms/step.
Train [40/40]. 241.29 samples/sec. 265.236 ms/step.
Train benchmark of convnext_base.clip_laiona_augreg_ft_in1k_384 done. 240.17 samples/sec, 265.24 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnext_base.fb_in1k created, param count: 88591464
Running inference benchmark on convnext_base.fb_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 1151.82 samples/sec. 222.256 ms/step.
Infer [16/40]. 1151.81 samples/sec. 222.259 ms/step.
Infer [24/40]. 1151.80 samples/sec. 222.262 ms/step.
Infer [32/40]. 1151.78 samples/sec. 222.264 ms/step.
Infer [40/40]. 1151.77 samples/sec. 222.267 ms/step.
Inference benchmark of convnext_base.fb_in1k done. 1151.60 samples/sec, 222.27 ms/step
Model convnext_base.fb_in1k created, param count: 88591464
Running train benchmark on convnext_base.fb_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 324.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 226.06 MiB is free. Including non-PyTorch memory, this process has 23.42 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 68.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convnext_base.fb_in1k created, param count: 88591464
Running train benchmark on convnext_base.fb_in1k for 40 steps w/ input size (3, 288, 288) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 244.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 188.06 MiB is free. Including non-PyTorch memory, this process has 23.46 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 100.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convnext_base.fb_in1k created, param count: 88591464
Running train benchmark on convnext_base.fb_in1k for 40 steps w/ input size (3, 288, 288) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 162.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 136.06 MiB is free. Including non-PyTorch memory, this process has 23.51 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 203.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model convnext_base.fb_in1k created, param count: 88591464
Running train benchmark on convnext_base.fb_in1k for 40 steps w/ input size (3, 288, 288) and batch size 96.
Train [8/40]. 426.44 samples/sec. 225.121 ms/step.
Train [16/40]. 426.37 samples/sec. 225.155 ms/step.
Train [24/40]. 426.34 samples/sec. 225.171 ms/step.
Train [32/40]. 426.33 samples/sec. 225.177 ms/step.
Train [40/40]. 426.31 samples/sec. 225.187 ms/step.
Train benchmark of convnext_base.fb_in1k done. 424.04 samples/sec, 225.19 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnext_base.fb_in22k created, param count: 109953489
Running inference benchmark on convnext_base.fb_in22k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1938.92 samples/sec. 132.032 ms/step.
Infer [16/40]. 1938.97 samples/sec. 132.029 ms/step.
Infer [24/40]. 1938.91 samples/sec. 132.033 ms/step.
Infer [32/40]. 1938.80 samples/sec. 132.040 ms/step.
Infer [40/40]. 1938.61 samples/sec. 132.053 ms/step.
Inference benchmark of convnext_base.fb_in22k done. 1938.19 samples/sec, 132.05 ms/step
Model convnext_base.fb_in22k created, param count: 109953489
Running train benchmark on convnext_base.fb_in22k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 22.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.37 GiB is allocated by PyTorch, and 22.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convnext_base.fb_in22k created, param count: 109953489
Running train benchmark on convnext_base.fb_in22k for 40 steps w/ input size (3, 224, 224) and batch size 192.
Train [8/40]. 694.79 samples/sec. 276.341 ms/step.
Train [16/40]. 694.79 samples/sec. 276.342 ms/step.
Train [24/40]. 694.77 samples/sec. 276.349 ms/step.
Train [32/40]. 694.78 samples/sec. 276.347 ms/step.
Train [40/40]. 694.78 samples/sec. 276.347 ms/step.
Train benchmark of convnext_base.fb_in22k done. 691.53 samples/sec, 276.35 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnext_base.fb_in22k_ft_in1k created, param count: 88591464
Running inference benchmark on convnext_base.fb_in22k_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 1151.61 samples/sec. 222.297 ms/step.
Infer [16/40]. 1151.58 samples/sec. 222.304 ms/step.
Infer [24/40]. 1151.62 samples/sec. 222.295 ms/step.
Infer [32/40]. 1151.61 samples/sec. 222.298 ms/step.
Infer [40/40]. 1151.59 samples/sec. 222.302 ms/step.
Inference benchmark of convnext_base.fb_in22k_ft_in1k done. 1151.40 samples/sec, 222.30 ms/step
Model convnext_base.fb_in22k_ft_in1k created, param count: 88591464
Running train benchmark on convnext_base.fb_in22k_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 324.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 226.06 MiB is free. Including non-PyTorch memory, this process has 23.42 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 68.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convnext_base.fb_in22k_ft_in1k created, param count: 88591464
Running train benchmark on convnext_base.fb_in22k_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 244.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 188.06 MiB is free. Including non-PyTorch memory, this process has 23.46 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 100.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convnext_base.fb_in22k_ft_in1k created, param count: 88591464
Running train benchmark on convnext_base.fb_in22k_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 162.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 136.06 MiB is free. Including non-PyTorch memory, this process has 23.51 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 203.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model convnext_base.fb_in22k_ft_in1k created, param count: 88591464
Running train benchmark on convnext_base.fb_in22k_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 96.
Train [8/40]. 426.03 samples/sec. 225.335 ms/step.
Train [16/40]. 426.03 samples/sec. 225.336 ms/step.
Train [24/40]. 426.02 samples/sec. 225.339 ms/step.
Train [32/40]. 426.03 samples/sec. 225.338 ms/step.
Train [40/40]. 426.03 samples/sec. 225.336 ms/step.
Train benchmark of convnext_base.fb_in22k_ft_in1k done. 423.57 samples/sec, 225.34 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnext_base.fb_in22k_ft_in1k_384 created, param count: 88591464
Running inference benchmark on convnext_base.fb_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 657.28 samples/sec. 389.485 ms/step.
Infer [16/40]. 657.25 samples/sec. 389.500 ms/step.
Infer [24/40]. 657.25 samples/sec. 389.499 ms/step.
Infer [32/40]. 657.24 samples/sec. 389.506 ms/step.
Infer [40/40]. 657.23 samples/sec. 389.513 ms/step.
Inference benchmark of convnext_base.fb_in22k_ft_in1k_384 done. 657.15 samples/sec, 389.51 ms/step
Model convnext_base.fb_in22k_ft_in1k_384 created, param count: 88591464
Running train benchmark on convnext_base.fb_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 576.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 224.06 MiB is free. Including non-PyTorch memory, this process has 23.42 GiB memory in use. Of the allocated memory 21.84 GiB is allocated by PyTorch, and 368.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convnext_base.fb_in22k_ft_in1k_384 created, param count: 88591464
Running train benchmark on convnext_base.fb_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 216.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 78.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 22.14 GiB is allocated by PyTorch, and 205.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convnext_base.fb_in22k_ft_in1k_384 created, param count: 88591464
Running train benchmark on convnext_base.fb_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 288.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 148.06 MiB is free. Including non-PyTorch memory, this process has 23.50 GiB memory in use. Of the allocated memory 22.21 GiB is allocated by PyTorch, and 62.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model convnext_base.fb_in22k_ft_in1k_384 created, param count: 88591464
Running train benchmark on convnext_base.fb_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 216.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 174.06 MiB is free. Including non-PyTorch memory, this process has 23.47 GiB memory in use. Of the allocated memory 22.19 GiB is allocated by PyTorch, and 55.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model convnext_base.fb_in22k_ft_in1k_384 created, param count: 88591464
Running train benchmark on convnext_base.fb_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 64.
Train [8/40]. 241.34 samples/sec. 265.182 ms/step.
Train [16/40]. 241.30 samples/sec. 265.228 ms/step.
Train [24/40]. 241.27 samples/sec. 265.263 ms/step.
Train [32/40]. 241.26 samples/sec. 265.276 ms/step.
Train [40/40]. 241.25 samples/sec. 265.288 ms/step.
Train benchmark of convnext_base.fb_in22k_ft_in1k_384 done. 240.09 samples/sec, 265.29 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnext_femto.d1_in1k created, param count: 5217784
Running inference benchmark on convnext_femto.d1_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 7895.45 samples/sec. 32.424 ms/step.
Infer [16/40]. 7894.93 samples/sec. 32.426 ms/step.
Infer [24/40]. 7894.74 samples/sec. 32.427 ms/step.
Infer [32/40]. 7894.89 samples/sec. 32.426 ms/step.
Infer [40/40]. 7894.87 samples/sec. 32.426 ms/step.
Inference benchmark of convnext_femto.d1_in1k done. 7888.81 samples/sec, 32.43 ms/step
Model convnext_femto.d1_in1k created, param count: 5217784
Running train benchmark on convnext_femto.d1_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 2565.67 samples/sec. 99.779 ms/step.
Train [16/40]. 2565.88 samples/sec. 99.771 ms/step.
Train [24/40]. 2565.92 samples/sec. 99.769 ms/step.
Train [32/40]. 2565.86 samples/sec. 99.772 ms/step.
Train [40/40]. 2565.92 samples/sec. 99.769 ms/step.
Train benchmark of convnext_femto.d1_in1k done. 2553.85 samples/sec, 99.77 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnext_femto_ols.d1_in1k created, param count: 5226520
Running inference benchmark on convnext_femto_ols.d1_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 7435.61 samples/sec. 34.429 ms/step.
Infer [16/40]. 7435.31 samples/sec. 34.430 ms/step.
Infer [24/40]. 7435.10 samples/sec. 34.431 ms/step.
Infer [32/40]. 7434.94 samples/sec. 34.432 ms/step.
Infer [40/40]. 7434.53 samples/sec. 34.434 ms/step.
Inference benchmark of convnext_femto_ols.d1_in1k done. 7429.59 samples/sec, 34.43 ms/step
Model convnext_femto_ols.d1_in1k created, param count: 5226520
Running train benchmark on convnext_femto_ols.d1_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 2427.43 samples/sec. 105.462 ms/step.
Train [16/40]. 2427.36 samples/sec. 105.464 ms/step.
Train [24/40]. 2427.46 samples/sec. 105.460 ms/step.
Train [32/40]. 2427.38 samples/sec. 105.464 ms/step.
Train [40/40]. 2427.40 samples/sec. 105.463 ms/step.
Train benchmark of convnext_femto_ols.d1_in1k done. 2416.36 samples/sec, 105.46 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnext_large.fb_in1k created, param count: 197767336
Running inference benchmark on convnext_large.fb_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 676.61 samples/sec. 378.358 ms/step.
Infer [16/40]. 676.60 samples/sec. 378.363 ms/step.
Infer [24/40]. 676.59 samples/sec. 378.370 ms/step.
Infer [32/40]. 676.39 samples/sec. 378.479 ms/step.
Infer [40/40]. 676.21 samples/sec. 378.581 ms/step.
Inference benchmark of convnext_large.fb_in1k done. 676.13 samples/sec, 378.58 ms/step
Model convnext_large.fb_in1k created, param count: 197767336
Running train benchmark on convnext_large.fb_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 244.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 240.06 MiB is free. Including non-PyTorch memory, this process has 23.41 GiB memory in use. Of the allocated memory 22.15 GiB is allocated by PyTorch, and 27.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convnext_large.fb_in1k created, param count: 197767336
Running train benchmark on convnext_large.fb_in1k for 40 steps w/ input size (3, 288, 288) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 92.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 34.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 22.24 GiB is allocated by PyTorch, and 145.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convnext_large.fb_in1k created, param count: 197767336
Running train benchmark on convnext_large.fb_in1k for 40 steps w/ input size (3, 288, 288) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 244.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 12.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.29 GiB is allocated by PyTorch, and 114.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model convnext_large.fb_in1k created, param count: 197767336
Running train benchmark on convnext_large.fb_in1k for 40 steps w/ input size (3, 288, 288) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 184.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 72.06 MiB is free. Including non-PyTorch memory, this process has 23.57 GiB memory in use. Of the allocated memory 22.18 GiB is allocated by PyTorch, and 169.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model convnext_large.fb_in1k created, param count: 197767336
Running train benchmark on convnext_large.fb_in1k for 40 steps w/ input size (3, 288, 288) and batch size 64.
Train [8/40]. 242.73 samples/sec. 263.672 ms/step.
Train [16/40]. 242.72 samples/sec. 263.677 ms/step.
Train [24/40]. 242.72 samples/sec. 263.679 ms/step.
Train [32/40]. 242.72 samples/sec. 263.674 ms/step.
Train [40/40]. 242.73 samples/sec. 263.672 ms/step.
Train benchmark of convnext_large.fb_in1k done. 241.58 samples/sec, 263.67 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnext_large.fb_in22k created, param count: 229799953
Running inference benchmark on convnext_large.fb_in22k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1100.34 samples/sec. 232.656 ms/step.
Infer [16/40]. 1100.36 samples/sec. 232.652 ms/step.
Infer [24/40]. 1100.38 samples/sec. 232.647 ms/step.
Infer [32/40]. 1100.35 samples/sec. 232.653 ms/step.
Infer [40/40]. 1100.25 samples/sec. 232.675 ms/step.
Inference benchmark of convnext_large.fb_in22k done. 1100.09 samples/sec, 232.68 ms/step
Model convnext_large.fb_in22k created, param count: 229799953
Running train benchmark on convnext_large.fb_in22k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 74.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 8.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.40 GiB is allocated by PyTorch, and 6.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convnext_large.fb_in22k created, param count: 229799953
Running train benchmark on convnext_large.fb_in22k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 52.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 22.22 GiB is allocated by PyTorch, and 144.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convnext_large.fb_in22k created, param count: 229799953
Running train benchmark on convnext_large.fb_in22k for 40 steps w/ input size (3, 224, 224) and batch size 128.
Train [8/40]. 397.77 samples/sec. 321.792 ms/step.
Train [16/40]. 397.78 samples/sec. 321.787 ms/step.
Train [24/40]. 397.74 samples/sec. 321.816 ms/step.
Train [32/40]. 397.71 samples/sec. 321.846 ms/step.
Train [40/40]. 397.69 samples/sec. 321.859 ms/step.
Train benchmark of convnext_large.fb_in22k done. 396.03 samples/sec, 321.86 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnext_large.fb_in22k_ft_in1k created, param count: 197767336
Running inference benchmark on convnext_large.fb_in22k_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 676.24 samples/sec. 378.563 ms/step.
Infer [16/40]. 676.22 samples/sec. 378.573 ms/step.
Infer [24/40]. 676.20 samples/sec. 378.584 ms/step.
Infer [32/40]. 676.18 samples/sec. 378.597 ms/step.
Infer [40/40]. 676.18 samples/sec. 378.599 ms/step.
Inference benchmark of convnext_large.fb_in22k_ft_in1k done. 676.09 samples/sec, 378.60 ms/step
Model convnext_large.fb_in22k_ft_in1k created, param count: 197767336
Running train benchmark on convnext_large.fb_in22k_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 244.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 240.06 MiB is free. Including non-PyTorch memory, this process has 23.41 GiB memory in use. Of the allocated memory 22.15 GiB is allocated by PyTorch, and 27.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convnext_large.fb_in22k_ft_in1k created, param count: 197767336
Running train benchmark on convnext_large.fb_in22k_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 92.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 34.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 22.24 GiB is allocated by PyTorch, and 145.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convnext_large.fb_in22k_ft_in1k created, param count: 197767336
Running train benchmark on convnext_large.fb_in22k_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 62.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 38.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 22.23 GiB is allocated by PyTorch, and 148.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model convnext_large.fb_in22k_ft_in1k created, param count: 197767336
Running train benchmark on convnext_large.fb_in22k_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 184.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 72.06 MiB is free. Including non-PyTorch memory, this process has 23.57 GiB memory in use. Of the allocated memory 22.18 GiB is allocated by PyTorch, and 169.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model convnext_large.fb_in22k_ft_in1k created, param count: 197767336
Running train benchmark on convnext_large.fb_in22k_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 64.
Train [8/40]. 242.78 samples/sec. 263.615 ms/step.
Train [16/40]. 242.78 samples/sec. 263.617 ms/step.
Train [24/40]. 242.77 samples/sec. 263.619 ms/step.
Train [32/40]. 242.77 samples/sec. 263.621 ms/step.
Train [40/40]. 242.77 samples/sec. 263.621 ms/step.
Train benchmark of convnext_large.fb_in22k_ft_in1k done. 241.58 samples/sec, 263.62 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnext_large.fb_in22k_ft_in1k_384 created, param count: 197767336
Running inference benchmark on convnext_large.fb_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 384.61 samples/sec. 665.603 ms/step.
Infer [16/40]. 384.60 samples/sec. 665.632 ms/step.
Infer [24/40]. 384.58 samples/sec. 665.660 ms/step.
Infer [32/40]. 384.57 samples/sec. 665.684 ms/step.
Infer [40/40]. 384.55 samples/sec. 665.705 ms/step.
Inference benchmark of convnext_large.fb_in22k_ft_in1k_384 done. 384.53 samples/sec, 665.71 ms/step
Model convnext_large.fb_in22k_ft_in1k_384 created, param count: 197767336
Running train benchmark on convnext_large.fb_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 864.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 514.06 MiB is free. Including non-PyTorch memory, this process has 23.14 GiB memory in use. Of the allocated memory 21.74 GiB is allocated by PyTorch, and 175.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convnext_large.fb_in22k_ft_in1k_384 created, param count: 197767336
Running train benchmark on convnext_large.fb_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 2.53 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.20 GiB is free. Including non-PyTorch memory, this process has 22.45 GiB memory in use. Of the allocated memory 20.85 GiB is allocated by PyTorch, and 384.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convnext_large.fb_in22k_ft_in1k_384 created, param count: 197767336
Running train benchmark on convnext_large.fb_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 216.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 36.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 22.27 GiB is allocated by PyTorch, and 115.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model convnext_large.fb_in22k_ft_in1k_384 created, param count: 197767336
Running train benchmark on convnext_large.fb_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 324.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 18.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.27 GiB is allocated by PyTorch, and 127.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model convnext_large.fb_in22k_ft_in1k_384 created, param count: 197767336
Running train benchmark on convnext_large.fb_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 54.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 32.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 22.30 GiB is allocated by PyTorch, and 85.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model convnext_large.fb_in22k_ft_in1k_384 created, param count: 197767336
Running train benchmark on convnext_large.fb_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 162.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 144.06 MiB is free. Including non-PyTorch memory, this process has 23.50 GiB memory in use. Of the allocated memory 22.10 GiB is allocated by PyTorch, and 179.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model convnext_large.fb_in22k_ft_in1k_384 created, param count: 197767336
Running train benchmark on convnext_large.fb_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 32.
Train [8/40]. 134.67 samples/sec. 237.619 ms/step.
Train [16/40]. 134.67 samples/sec. 237.610 ms/step.
Train [24/40]. 134.68 samples/sec. 237.608 ms/step.
Train [32/40]. 134.67 samples/sec. 237.610 ms/step.
Train [40/40]. 134.67 samples/sec. 237.612 ms/step.
Train benchmark of convnext_large.fb_in22k_ft_in1k_384 done. 134.01 samples/sec, 237.61 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnext_large_mlp.clip_laion2b_augreg created, param count: 199771584
Running inference benchmark on convnext_large_mlp.clip_laion2b_augreg for 40 steps w/ input size (3, 256, 256) and batch size 256.
Infer [8/40]. 856.39 samples/sec. 298.929 ms/step.
Infer [16/40]. 856.39 samples/sec. 298.929 ms/step.
Infer [24/40]. 856.39 samples/sec. 298.928 ms/step.
Infer [32/40]. 856.40 samples/sec. 298.926 ms/step.
Infer [40/40]. 856.29 samples/sec. 298.965 ms/step.
Inference benchmark of convnext_large_mlp.clip_laion2b_augreg done. 856.17 samples/sec, 298.96 ms/step
Model convnext_large_mlp.clip_laion2b_augreg created, param count: 199771584
Running train benchmark on convnext_large_mlp.clip_laion2b_augreg for 40 steps w/ input size (3, 256, 256) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 26.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.37 GiB is allocated by PyTorch, and 17.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convnext_large_mlp.clip_laion2b_augreg created, param count: 199771584
Running train benchmark on convnext_large_mlp.clip_laion2b_augreg for 40 steps w/ input size (3, 256, 256) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 288.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 252.06 MiB is free. Including non-PyTorch memory, this process has 23.39 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 88.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convnext_large_mlp.clip_laion2b_augreg created, param count: 199771584
Running train benchmark on convnext_large_mlp.clip_laion2b_augreg for 40 steps w/ input size (3, 256, 256) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 24.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.36 GiB is allocated by PyTorch, and 37.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model convnext_large_mlp.clip_laion2b_augreg created, param count: 199771584
Running train benchmark on convnext_large_mlp.clip_laion2b_augreg for 40 steps w/ input size (3, 256, 256) and batch size 96.
Train [8/40]. 306.12 samples/sec. 313.606 ms/step.
Train [16/40]. 306.11 samples/sec. 313.611 ms/step.
Train [24/40]. 306.11 samples/sec. 313.617 ms/step.
Train [32/40]. 306.11 samples/sec. 313.612 ms/step.
Train [40/40]. 306.11 samples/sec. 313.611 ms/step.
Train benchmark of convnext_large_mlp.clip_laion2b_augreg done. 304.82 samples/sec, 313.61 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnext_large_mlp.clip_laion2b_augreg_ft_in1k created, param count: 200128168
Running inference benchmark on convnext_large_mlp.clip_laion2b_augreg_ft_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
Infer [8/40]. 856.29 samples/sec. 298.964 ms/step.
Infer [16/40]. 856.30 samples/sec. 298.960 ms/step.
Infer [24/40]. 856.31 samples/sec. 298.959 ms/step.
Infer [32/40]. 856.29 samples/sec. 298.965 ms/step.
Infer [40/40]. 856.28 samples/sec. 298.967 ms/step.
Inference benchmark of convnext_large_mlp.clip_laion2b_augreg_ft_in1k done. 856.16 samples/sec, 298.97 ms/step
Model convnext_large_mlp.clip_laion2b_augreg_ft_in1k created, param count: 200128168
Running train benchmark on convnext_large_mlp.clip_laion2b_augreg_ft_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 26.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.37 GiB is allocated by PyTorch, and 16.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convnext_large_mlp.clip_laion2b_augreg_ft_in1k created, param count: 200128168
Running train benchmark on convnext_large_mlp.clip_laion2b_augreg_ft_in1k for 40 steps w/ input size (3, 256, 256) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 288.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 252.06 MiB is free. Including non-PyTorch memory, this process has 23.39 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 87.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convnext_large_mlp.clip_laion2b_augreg_ft_in1k created, param count: 200128168
Running train benchmark on convnext_large_mlp.clip_laion2b_augreg_ft_in1k for 40 steps w/ input size (3, 256, 256) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 6.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.36 GiB is allocated by PyTorch, and 55.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model convnext_large_mlp.clip_laion2b_augreg_ft_in1k created, param count: 200128168
Running train benchmark on convnext_large_mlp.clip_laion2b_augreg_ft_in1k for 40 steps w/ input size (3, 256, 256) and batch size 96.
Train [8/40]. 306.17 samples/sec. 313.552 ms/step.
Train [16/40]. 306.17 samples/sec. 313.556 ms/step.
Train [24/40]. 306.17 samples/sec. 313.555 ms/step.
Train [32/40]. 306.16 samples/sec. 313.557 ms/step.
Train [40/40]. 306.17 samples/sec. 313.556 ms/step.
Train benchmark of convnext_large_mlp.clip_laion2b_augreg_ft_in1k done. 304.82 samples/sec, 313.56 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnext_large_mlp.clip_laion2b_augreg_ft_in1k_384 created, param count: 200128168
Running inference benchmark on convnext_large_mlp.clip_laion2b_augreg_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 384.53 samples/sec. 665.742 ms/step.
Infer [16/40]. 384.51 samples/sec. 665.790 ms/step.
Infer [24/40]. 384.50 samples/sec. 665.801 ms/step.
Infer [32/40]. 384.48 samples/sec. 665.829 ms/step.
Infer [40/40]. 384.47 samples/sec. 665.850 ms/step.
Inference benchmark of convnext_large_mlp.clip_laion2b_augreg_ft_in1k_384 done. 384.44 samples/sec, 665.85 ms/step
Model convnext_large_mlp.clip_laion2b_augreg_ft_in1k_384 created, param count: 200128168
Running train benchmark on convnext_large_mlp.clip_laion2b_augreg_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 864.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 514.06 MiB is free. Including non-PyTorch memory, this process has 23.14 GiB memory in use. Of the allocated memory 21.75 GiB is allocated by PyTorch, and 170.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convnext_large_mlp.clip_laion2b_augreg_ft_in1k_384 created, param count: 200128168
Running train benchmark on convnext_large_mlp.clip_laion2b_augreg_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 2.53 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.18 GiB is free. Including non-PyTorch memory, this process has 22.46 GiB memory in use. Of the allocated memory 20.85 GiB is allocated by PyTorch, and 399.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convnext_large_mlp.clip_laion2b_augreg_ft_in1k_384 created, param count: 200128168
Running train benchmark on convnext_large_mlp.clip_laion2b_augreg_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 216.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 214.06 MiB is free. Including non-PyTorch memory, this process has 23.43 GiB memory in use. Of the allocated memory 22.06 GiB is allocated by PyTorch, and 148.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model convnext_large_mlp.clip_laion2b_augreg_ft_in1k_384 created, param count: 200128168
Running train benchmark on convnext_large_mlp.clip_laion2b_augreg_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 324.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 322.06 MiB is free. Including non-PyTorch memory, this process has 23.33 GiB memory in use. Of the allocated memory 21.96 GiB is allocated by PyTorch, and 143.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model convnext_large_mlp.clip_laion2b_augreg_ft_in1k_384 created, param count: 200128168
Running train benchmark on convnext_large_mlp.clip_laion2b_augreg_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 54.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 12.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.31 GiB is allocated by PyTorch, and 99.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model convnext_large_mlp.clip_laion2b_augreg_ft_in1k_384 created, param count: 200128168
Running train benchmark on convnext_large_mlp.clip_laion2b_augreg_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 162.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 82.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 22.10 GiB is allocated by PyTorch, and 236.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model convnext_large_mlp.clip_laion2b_augreg_ft_in1k_384 created, param count: 200128168
Running train benchmark on convnext_large_mlp.clip_laion2b_augreg_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 32.
Train [8/40]. 134.59 samples/sec. 237.755 ms/step.
Train [16/40]. 134.59 samples/sec. 237.759 ms/step.
Train [24/40]. 134.59 samples/sec. 237.758 ms/step.
Train [32/40]. 134.59 samples/sec. 237.759 ms/step.
Train [40/40]. 134.59 samples/sec. 237.759 ms/step.
Train benchmark of convnext_large_mlp.clip_laion2b_augreg_ft_in1k_384 done. 133.91 samples/sec, 237.76 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnext_large_mlp.clip_laion2b_augreg_ft_in12k_384 created, param count: 216760045
Running inference benchmark on convnext_large_mlp.clip_laion2b_augreg_ft_in12k_384 for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 384.82 samples/sec. 665.253 ms/step.
Infer [16/40]. 384.57 samples/sec. 665.679 ms/step.
Infer [24/40]. 384.43 samples/sec. 665.920 ms/step.
Infer [32/40]. 384.35 samples/sec. 666.068 ms/step.
Infer [40/40]. 384.30 samples/sec. 666.155 ms/step.
Inference benchmark of convnext_large_mlp.clip_laion2b_augreg_ft_in12k_384 done. 384.27 samples/sec, 666.15 ms/step
Model convnext_large_mlp.clip_laion2b_augreg_ft_in12k_384 created, param count: 216760045
Running train benchmark on convnext_large_mlp.clip_laion2b_augreg_ft_in12k_384 for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 864.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 514.06 MiB is free. Including non-PyTorch memory, this process has 23.14 GiB memory in use. Of the allocated memory 21.78 GiB is allocated by PyTorch, and 138.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convnext_large_mlp.clip_laion2b_augreg_ft_in12k_384 created, param count: 216760045
Running train benchmark on convnext_large_mlp.clip_laion2b_augreg_ft_in12k_384 for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 2.53 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.10 GiB is free. Including non-PyTorch memory, this process has 22.54 GiB memory in use. Of the allocated memory 20.88 GiB is allocated by PyTorch, and 444.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convnext_large_mlp.clip_laion2b_augreg_ft_in12k_384 created, param count: 216760045
Running train benchmark on convnext_large_mlp.clip_laion2b_augreg_ft_in12k_384 for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 216.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 156.06 MiB is free. Including non-PyTorch memory, this process has 23.49 GiB memory in use. Of the allocated memory 22.10 GiB is allocated by PyTorch, and 172.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model convnext_large_mlp.clip_laion2b_augreg_ft_in12k_384 created, param count: 216760045
Running train benchmark on convnext_large_mlp.clip_laion2b_augreg_ft_in12k_384 for 40 steps w/ input size (3, 384, 384) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 324.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 2.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 22.31 GiB is allocated by PyTorch, and 105.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model convnext_large_mlp.clip_laion2b_augreg_ft_in12k_384 created, param count: 216760045
Running train benchmark on convnext_large_mlp.clip_laion2b_augreg_ft_in12k_384 for 40 steps w/ input size (3, 384, 384) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 54.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 44.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 22.23 GiB is allocated by PyTorch, and 143.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model convnext_large_mlp.clip_laion2b_augreg_ft_in12k_384 created, param count: 216760045
Running train benchmark on convnext_large_mlp.clip_laion2b_augreg_ft_in12k_384 for 40 steps w/ input size (3, 384, 384) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 162.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 88.06 MiB is free. Including non-PyTorch memory, this process has 23.55 GiB memory in use. Of the allocated memory 22.14 GiB is allocated by PyTorch, and 199.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model convnext_large_mlp.clip_laion2b_augreg_ft_in12k_384 created, param count: 216760045
Running train benchmark on convnext_large_mlp.clip_laion2b_augreg_ft_in12k_384 for 40 steps w/ input size (3, 384, 384) and batch size 32.
Train [8/40]. 134.34 samples/sec. 238.197 ms/step.
Train [16/40]. 134.34 samples/sec. 238.194 ms/step.
Train [24/40]. 134.34 samples/sec. 238.200 ms/step.
Train [32/40]. 134.34 samples/sec. 238.203 ms/step.
Train [40/40]. 134.34 samples/sec. 238.204 ms/step.
Train benchmark of convnext_large_mlp.clip_laion2b_augreg_ft_in12k_384 done. 133.66 samples/sec, 238.20 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnext_large_mlp.clip_laion2b_ft_320 created, param count: 199771584
Running inference benchmark on convnext_large_mlp.clip_laion2b_ft_320 for 40 steps w/ input size (3, 320, 320) and batch size 256.
Infer [8/40]. 551.29 samples/sec. 464.367 ms/step.
Infer [16/40]. 550.97 samples/sec. 464.634 ms/step.
Infer [24/40]. 550.82 samples/sec. 464.766 ms/step.
Infer [32/40]. 550.72 samples/sec. 464.845 ms/step.
Infer [40/40]. 550.67 samples/sec. 464.888 ms/step.
Inference benchmark of convnext_large_mlp.clip_laion2b_ft_320 done. 550.62 samples/sec, 464.89 ms/step
Model convnext_large_mlp.clip_laion2b_ft_320 created, param count: 199771584
Running train benchmark on convnext_large_mlp.clip_laion2b_ft_320 for 40 steps w/ input size (3, 320, 320) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 600.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 514.06 MiB is free. Including non-PyTorch memory, this process has 23.14 GiB memory in use. Of the allocated memory 21.68 GiB is allocated by PyTorch, and 241.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convnext_large_mlp.clip_laion2b_ft_320 created, param count: 199771584
Running train benchmark on convnext_large_mlp.clip_laion2b_ft_320 for 40 steps w/ input size (3, 320, 320) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 900.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 412.06 MiB is free. Including non-PyTorch memory, this process has 23.24 GiB memory in use. Of the allocated memory 21.87 GiB is allocated by PyTorch, and 149.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convnext_large_mlp.clip_laion2b_ft_320 created, param count: 199771584
Running train benchmark on convnext_large_mlp.clip_laion2b_ft_320 for 40 steps w/ input size (3, 320, 320) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 300.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 256.06 MiB is free. Including non-PyTorch memory, this process has 23.39 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 35.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model convnext_large_mlp.clip_laion2b_ft_320 created, param count: 199771584
Running train benchmark on convnext_large_mlp.clip_laion2b_ft_320 for 40 steps w/ input size (3, 320, 320) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 226.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 218.06 MiB is free. Including non-PyTorch memory, this process has 23.43 GiB memory in use. Of the allocated memory 21.99 GiB is allocated by PyTorch, and 216.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model convnext_large_mlp.clip_laion2b_ft_320 created, param count: 199771584
Running train benchmark on convnext_large_mlp.clip_laion2b_ft_320 for 40 steps w/ input size (3, 320, 320) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 80.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 21.94 GiB is allocated by PyTorch, and 408.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model convnext_large_mlp.clip_laion2b_ft_320 created, param count: 199771584
Running train benchmark on convnext_large_mlp.clip_laion2b_ft_320 for 40 steps w/ input size (3, 320, 320) and batch size 48.
Train [8/40]. 192.11 samples/sec. 249.851 ms/step.
Train [16/40]. 192.11 samples/sec. 249.854 ms/step.
Train [24/40]. 192.11 samples/sec. 249.853 ms/step.
Train [32/40]. 192.11 samples/sec. 249.852 ms/step.
Train [40/40]. 192.11 samples/sec. 249.857 ms/step.
Train benchmark of convnext_large_mlp.clip_laion2b_ft_320 done. 191.17 samples/sec, 249.86 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnext_large_mlp.clip_laion2b_ft_soup_320 created, param count: 199771584
Running inference benchmark on convnext_large_mlp.clip_laion2b_ft_soup_320 for 40 steps w/ input size (3, 320, 320) and batch size 256.
Infer [8/40]. 551.28 samples/sec. 464.371 ms/step.
Infer [16/40]. 551.13 samples/sec. 464.499 ms/step.
Infer [24/40]. 551.06 samples/sec. 464.562 ms/step.
Infer [32/40]. 551.01 samples/sec. 464.600 ms/step.
Infer [40/40]. 550.99 samples/sec. 464.621 ms/step.
Inference benchmark of convnext_large_mlp.clip_laion2b_ft_soup_320 done. 550.93 samples/sec, 464.62 ms/step
Model convnext_large_mlp.clip_laion2b_ft_soup_320 created, param count: 199771584
Running train benchmark on convnext_large_mlp.clip_laion2b_ft_soup_320 for 40 steps w/ input size (3, 320, 320) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 600.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 514.06 MiB is free. Including non-PyTorch memory, this process has 23.14 GiB memory in use. Of the allocated memory 21.68 GiB is allocated by PyTorch, and 241.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convnext_large_mlp.clip_laion2b_ft_soup_320 created, param count: 199771584
Running train benchmark on convnext_large_mlp.clip_laion2b_ft_soup_320 for 40 steps w/ input size (3, 320, 320) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 900.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 412.06 MiB is free. Including non-PyTorch memory, this process has 23.24 GiB memory in use. Of the allocated memory 21.87 GiB is allocated by PyTorch, and 149.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convnext_large_mlp.clip_laion2b_ft_soup_320 created, param count: 199771584
Running train benchmark on convnext_large_mlp.clip_laion2b_ft_soup_320 for 40 steps w/ input size (3, 320, 320) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 300.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 220.06 MiB is free. Including non-PyTorch memory, this process has 23.43 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 72.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model convnext_large_mlp.clip_laion2b_ft_soup_320 created, param count: 199771584
Running train benchmark on convnext_large_mlp.clip_laion2b_ft_soup_320 for 40 steps w/ input size (3, 320, 320) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 226.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 218.06 MiB is free. Including non-PyTorch memory, this process has 23.43 GiB memory in use. Of the allocated memory 21.99 GiB is allocated by PyTorch, and 216.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model convnext_large_mlp.clip_laion2b_ft_soup_320 created, param count: 199771584
Running train benchmark on convnext_large_mlp.clip_laion2b_ft_soup_320 for 40 steps w/ input size (3, 320, 320) and batch size 64.
Train [8/40]. 196.60 samples/sec. 325.536 ms/step.
Train [16/40]. 196.58 samples/sec. 325.574 ms/step.
Train [24/40]. 196.56 samples/sec. 325.597 ms/step.
Train [32/40]. 196.55 samples/sec. 325.612 ms/step.
Train [40/40]. 196.54 samples/sec. 325.633 ms/step.
Train benchmark of convnext_large_mlp.clip_laion2b_ft_soup_320 done. 195.74 samples/sec, 325.63 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnext_large_mlp.clip_laion2b_soup_ft_in12k_320 created, param count: 216760045
Running inference benchmark on convnext_large_mlp.clip_laion2b_soup_ft_in12k_320 for 40 steps w/ input size (3, 320, 320) and batch size 256.
Infer [8/40]. 550.91 samples/sec. 464.684 ms/step.
Infer [16/40]. 550.87 samples/sec. 464.716 ms/step.
Infer [24/40]. 550.83 samples/sec. 464.753 ms/step.
Infer [32/40]. 550.81 samples/sec. 464.774 ms/step.
Infer [40/40]. 550.79 samples/sec. 464.790 ms/step.
Inference benchmark of convnext_large_mlp.clip_laion2b_soup_ft_in12k_320 done. 550.73 samples/sec, 464.79 ms/step
Model convnext_large_mlp.clip_laion2b_soup_ft_in12k_320 created, param count: 216760045
Running train benchmark on convnext_large_mlp.clip_laion2b_soup_ft_in12k_320 for 40 steps w/ input size (3, 320, 320) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 600.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 514.06 MiB is free. Including non-PyTorch memory, this process has 23.14 GiB memory in use. Of the allocated memory 21.71 GiB is allocated by PyTorch, and 208.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convnext_large_mlp.clip_laion2b_soup_ft_in12k_320 created, param count: 216760045
Running train benchmark on convnext_large_mlp.clip_laion2b_soup_ft_in12k_320 for 40 steps w/ input size (3, 320, 320) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 900.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 338.06 MiB is free. Including non-PyTorch memory, this process has 23.31 GiB memory in use. Of the allocated memory 21.90 GiB is allocated by PyTorch, and 190.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convnext_large_mlp.clip_laion2b_soup_ft_in12k_320 created, param count: 216760045
Running train benchmark on convnext_large_mlp.clip_laion2b_soup_ft_in12k_320 for 40 steps w/ input size (3, 320, 320) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 300.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 164.06 MiB is free. Including non-PyTorch memory, this process has 23.48 GiB memory in use. Of the allocated memory 22.16 GiB is allocated by PyTorch, and 95.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model convnext_large_mlp.clip_laion2b_soup_ft_in12k_320 created, param count: 216760045
Running train benchmark on convnext_large_mlp.clip_laion2b_soup_ft_in12k_320 for 40 steps w/ input size (3, 320, 320) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 226.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 200.06 MiB is free. Including non-PyTorch memory, this process has 23.45 GiB memory in use. Of the allocated memory 22.03 GiB is allocated by PyTorch, and 200.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model convnext_large_mlp.clip_laion2b_soup_ft_in12k_320 created, param count: 216760045
Running train benchmark on convnext_large_mlp.clip_laion2b_soup_ft_in12k_320 for 40 steps w/ input size (3, 320, 320) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 76.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 58.06 MiB is free. Including non-PyTorch memory, this process has 23.58 GiB memory in use. Of the allocated memory 22.12 GiB is allocated by PyTorch, and 242.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model convnext_large_mlp.clip_laion2b_soup_ft_in12k_320 created, param count: 216760045
Running train benchmark on convnext_large_mlp.clip_laion2b_soup_ft_in12k_320 for 40 steps w/ input size (3, 320, 320) and batch size 48.
Train [8/40]. 191.69 samples/sec. 250.404 ms/step.
Train [16/40]. 191.69 samples/sec. 250.405 ms/step.
Train [24/40]. 191.69 samples/sec. 250.407 ms/step.
Train [32/40]. 191.69 samples/sec. 250.409 ms/step.
Train [40/40]. 191.68 samples/sec. 250.411 ms/step.
Train benchmark of convnext_large_mlp.clip_laion2b_soup_ft_in12k_320 done. 190.70 samples/sec, 250.41 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnext_large_mlp.clip_laion2b_soup_ft_in12k_384 created, param count: 216760045
Running inference benchmark on convnext_large_mlp.clip_laion2b_soup_ft_in12k_384 for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 384.54 samples/sec. 665.729 ms/step.
Infer [16/40]. 384.51 samples/sec. 665.785 ms/step.
Infer [24/40]. 384.49 samples/sec. 665.820 ms/step.
Infer [32/40]. 384.47 samples/sec. 665.845 ms/step.
Infer [40/40]. 384.46 samples/sec. 665.875 ms/step.
Inference benchmark of convnext_large_mlp.clip_laion2b_soup_ft_in12k_384 done. 384.43 samples/sec, 665.88 ms/step
Model convnext_large_mlp.clip_laion2b_soup_ft_in12k_384 created, param count: 216760045
Running train benchmark on convnext_large_mlp.clip_laion2b_soup_ft_in12k_384 for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 864.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 514.06 MiB is free. Including non-PyTorch memory, this process has 23.14 GiB memory in use. Of the allocated memory 21.78 GiB is allocated by PyTorch, and 138.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convnext_large_mlp.clip_laion2b_soup_ft_in12k_384 created, param count: 216760045
Running train benchmark on convnext_large_mlp.clip_laion2b_soup_ft_in12k_384 for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 2.53 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.10 GiB is free. Including non-PyTorch memory, this process has 22.54 GiB memory in use. Of the allocated memory 20.88 GiB is allocated by PyTorch, and 444.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convnext_large_mlp.clip_laion2b_soup_ft_in12k_384 created, param count: 216760045
Running train benchmark on convnext_large_mlp.clip_laion2b_soup_ft_in12k_384 for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 216.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 156.06 MiB is free. Including non-PyTorch memory, this process has 23.49 GiB memory in use. Of the allocated memory 22.10 GiB is allocated by PyTorch, and 172.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model convnext_large_mlp.clip_laion2b_soup_ft_in12k_384 created, param count: 216760045
Running train benchmark on convnext_large_mlp.clip_laion2b_soup_ft_in12k_384 for 40 steps w/ input size (3, 384, 384) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 324.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 2.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 22.31 GiB is allocated by PyTorch, and 105.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model convnext_large_mlp.clip_laion2b_soup_ft_in12k_384 created, param count: 216760045
Running train benchmark on convnext_large_mlp.clip_laion2b_soup_ft_in12k_384 for 40 steps w/ input size (3, 384, 384) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 54.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 44.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 22.23 GiB is allocated by PyTorch, and 143.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model convnext_large_mlp.clip_laion2b_soup_ft_in12k_384 created, param count: 216760045
Running train benchmark on convnext_large_mlp.clip_laion2b_soup_ft_in12k_384 for 40 steps w/ input size (3, 384, 384) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 162.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 88.06 MiB is free. Including non-PyTorch memory, this process has 23.55 GiB memory in use. Of the allocated memory 22.14 GiB is allocated by PyTorch, and 199.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model convnext_large_mlp.clip_laion2b_soup_ft_in12k_384 created, param count: 216760045
Running train benchmark on convnext_large_mlp.clip_laion2b_soup_ft_in12k_384 for 40 steps w/ input size (3, 384, 384) and batch size 32.
Train [8/40]. 134.32 samples/sec. 238.243 ms/step.
Train [16/40]. 134.32 samples/sec. 238.238 ms/step.
Train [24/40]. 134.32 samples/sec. 238.233 ms/step.
Train [32/40]. 134.32 samples/sec. 238.238 ms/step.
Train [40/40]. 134.32 samples/sec. 238.237 ms/step.
Train benchmark of convnext_large_mlp.clip_laion2b_soup_ft_in12k_384 done. 133.63 samples/sec, 238.24 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnext_large_mlp.clip_laion2b_soup_ft_in12k_in1k_320 created, param count: 200128168
Running inference benchmark on convnext_large_mlp.clip_laion2b_soup_ft_in12k_in1k_320 for 40 steps w/ input size (3, 320, 320) and batch size 256.
Infer [8/40]. 551.08 samples/sec. 464.539 ms/step.
Infer [16/40]. 551.07 samples/sec. 464.549 ms/step.
Infer [24/40]. 551.05 samples/sec. 464.569 ms/step.
Infer [32/40]. 550.94 samples/sec. 464.663 ms/step.
Infer [40/40]. 550.82 samples/sec. 464.765 ms/step.
Inference benchmark of convnext_large_mlp.clip_laion2b_soup_ft_in12k_in1k_320 done. 550.76 samples/sec, 464.76 ms/step
Model convnext_large_mlp.clip_laion2b_soup_ft_in12k_in1k_320 created, param count: 200128168
Running train benchmark on convnext_large_mlp.clip_laion2b_soup_ft_in12k_in1k_320 for 40 steps w/ input size (3, 320, 320) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 600.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 514.06 MiB is free. Including non-PyTorch memory, this process has 23.14 GiB memory in use. Of the allocated memory 21.68 GiB is allocated by PyTorch, and 240.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convnext_large_mlp.clip_laion2b_soup_ft_in12k_in1k_320 created, param count: 200128168
Running train benchmark on convnext_large_mlp.clip_laion2b_soup_ft_in12k_in1k_320 for 40 steps w/ input size (3, 320, 320) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 900.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 412.06 MiB is free. Including non-PyTorch memory, this process has 23.24 GiB memory in use. Of the allocated memory 21.87 GiB is allocated by PyTorch, and 148.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convnext_large_mlp.clip_laion2b_soup_ft_in12k_in1k_320 created, param count: 200128168
Running train benchmark on convnext_large_mlp.clip_laion2b_soup_ft_in12k_in1k_320 for 40 steps w/ input size (3, 320, 320) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 300.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 220.06 MiB is free. Including non-PyTorch memory, this process has 23.43 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 72.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model convnext_large_mlp.clip_laion2b_soup_ft_in12k_in1k_320 created, param count: 200128168
Running train benchmark on convnext_large_mlp.clip_laion2b_soup_ft_in12k_in1k_320 for 40 steps w/ input size (3, 320, 320) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 226.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 218.06 MiB is free. Including non-PyTorch memory, this process has 23.43 GiB memory in use. Of the allocated memory 21.99 GiB is allocated by PyTorch, and 214.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model convnext_large_mlp.clip_laion2b_soup_ft_in12k_in1k_320 created, param count: 200128168
Running train benchmark on convnext_large_mlp.clip_laion2b_soup_ft_in12k_in1k_320 for 40 steps w/ input size (3, 320, 320) and batch size 64.
Train [8/40]. 196.56 samples/sec. 325.593 ms/step.
Train [16/40]. 196.57 samples/sec. 325.591 ms/step.
Train [24/40]. 196.56 samples/sec. 325.604 ms/step.
Train [32/40]. 196.53 samples/sec. 325.652 ms/step.
Train [40/40]. 196.51 samples/sec. 325.690 ms/step.
Train benchmark of convnext_large_mlp.clip_laion2b_soup_ft_in12k_in1k_320 done. 195.70 samples/sec, 325.69 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnext_large_mlp.clip_laion2b_soup_ft_in12k_in1k_384 created, param count: 200128168
Running inference benchmark on convnext_large_mlp.clip_laion2b_soup_ft_in12k_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 384.42 samples/sec. 665.932 ms/step.
Infer [16/40]. 384.31 samples/sec. 666.135 ms/step.
Infer [24/40]. 384.25 samples/sec. 666.240 ms/step.
Infer [32/40]. 384.21 samples/sec. 666.305 ms/step.
Infer [40/40]. 384.18 samples/sec. 666.348 ms/step.
Inference benchmark of convnext_large_mlp.clip_laion2b_soup_ft_in12k_in1k_384 done. 384.15 samples/sec, 666.35 ms/step
Model convnext_large_mlp.clip_laion2b_soup_ft_in12k_in1k_384 created, param count: 200128168
Running train benchmark on convnext_large_mlp.clip_laion2b_soup_ft_in12k_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 864.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 514.06 MiB is free. Including non-PyTorch memory, this process has 23.14 GiB memory in use. Of the allocated memory 21.75 GiB is allocated by PyTorch, and 170.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convnext_large_mlp.clip_laion2b_soup_ft_in12k_in1k_384 created, param count: 200128168
Running train benchmark on convnext_large_mlp.clip_laion2b_soup_ft_in12k_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 2.53 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.18 GiB is free. Including non-PyTorch memory, this process has 22.46 GiB memory in use. Of the allocated memory 20.85 GiB is allocated by PyTorch, and 399.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convnext_large_mlp.clip_laion2b_soup_ft_in12k_in1k_384 created, param count: 200128168
Running train benchmark on convnext_large_mlp.clip_laion2b_soup_ft_in12k_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 216.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 214.06 MiB is free. Including non-PyTorch memory, this process has 23.43 GiB memory in use. Of the allocated memory 22.06 GiB is allocated by PyTorch, and 148.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model convnext_large_mlp.clip_laion2b_soup_ft_in12k_in1k_384 created, param count: 200128168
Running train benchmark on convnext_large_mlp.clip_laion2b_soup_ft_in12k_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 324.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 322.06 MiB is free. Including non-PyTorch memory, this process has 23.33 GiB memory in use. Of the allocated memory 21.96 GiB is allocated by PyTorch, and 143.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model convnext_large_mlp.clip_laion2b_soup_ft_in12k_in1k_384 created, param count: 200128168
Running train benchmark on convnext_large_mlp.clip_laion2b_soup_ft_in12k_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 54.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 12.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.31 GiB is allocated by PyTorch, and 99.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model convnext_large_mlp.clip_laion2b_soup_ft_in12k_in1k_384 created, param count: 200128168
Running train benchmark on convnext_large_mlp.clip_laion2b_soup_ft_in12k_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 162.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 82.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 22.10 GiB is allocated by PyTorch, and 236.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model convnext_large_mlp.clip_laion2b_soup_ft_in12k_in1k_384 created, param count: 200128168
Running train benchmark on convnext_large_mlp.clip_laion2b_soup_ft_in12k_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 32.
Train [8/40]. 134.59 samples/sec. 237.755 ms/step.
Train [16/40]. 134.59 samples/sec. 237.762 ms/step.
Train [24/40]. 134.59 samples/sec. 237.768 ms/step.
Train [32/40]. 134.58 samples/sec. 237.773 ms/step.
Train [40/40]. 134.58 samples/sec. 237.773 ms/step.
Train benchmark of convnext_large_mlp.clip_laion2b_soup_ft_in12k_in1k_384 done. 133.89 samples/sec, 237.77 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnext_nano.d1h_in1k created, param count: 15593560
Running inference benchmark on convnext_nano.d1h_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 4362.13 samples/sec. 58.687 ms/step.
Infer [16/40]. 4362.08 samples/sec. 58.688 ms/step.
Infer [24/40]. 4361.90 samples/sec. 58.690 ms/step.
Infer [32/40]. 4361.88 samples/sec. 58.690 ms/step.
Infer [40/40]. 4361.78 samples/sec. 58.692 ms/step.
Inference benchmark of convnext_nano.d1h_in1k done. 4359.81 samples/sec, 58.69 ms/step
Model convnext_nano.d1h_in1k created, param count: 15593560
Running train benchmark on convnext_nano.d1h_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 1490.82 samples/sec. 171.717 ms/step.
Train [16/40]. 1490.79 samples/sec. 171.721 ms/step.
Train [24/40]. 1490.81 samples/sec. 171.719 ms/step.
Train [32/40]. 1490.83 samples/sec. 171.716 ms/step.
Train [40/40]. 1490.82 samples/sec. 171.718 ms/step.
Train benchmark of convnext_nano.d1h_in1k done. 1485.69 samples/sec, 171.72 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnext_nano.in12k created, param count: 22529821
Running inference benchmark on convnext_nano.in12k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 7373.01 samples/sec. 34.721 ms/step.
Infer [16/40]. 7372.46 samples/sec. 34.724 ms/step.
Infer [24/40]. 7372.25 samples/sec. 34.725 ms/step.
Infer [32/40]. 7372.05 samples/sec. 34.726 ms/step.
Infer [40/40]. 7371.94 samples/sec. 34.726 ms/step.
Inference benchmark of convnext_nano.in12k done. 7366.65 samples/sec, 34.73 ms/step
Model convnext_nano.in12k created, param count: 22529821
Running train benchmark on convnext_nano.in12k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 2482.53 samples/sec. 103.120 ms/step.
Train [16/40]. 2482.75 samples/sec. 103.112 ms/step.
Train [24/40]. 2482.75 samples/sec. 103.112 ms/step.
Train [32/40]. 2482.81 samples/sec. 103.109 ms/step.
Train [40/40]. 2482.79 samples/sec. 103.110 ms/step.
Train benchmark of convnext_nano.in12k done. 2469.72 samples/sec, 103.11 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnext_nano.in12k_ft_in1k created, param count: 15593560
Running inference benchmark on convnext_nano.in12k_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 4361.88 samples/sec. 58.690 ms/step.
Infer [16/40]. 4361.68 samples/sec. 58.693 ms/step.
Infer [24/40]. 4361.55 samples/sec. 58.695 ms/step.
Infer [32/40]. 4361.56 samples/sec. 58.695 ms/step.
Infer [40/40]. 4361.45 samples/sec. 58.696 ms/step.
Inference benchmark of convnext_nano.in12k_ft_in1k done. 4359.41 samples/sec, 58.70 ms/step
Model convnext_nano.in12k_ft_in1k created, param count: 15593560
Running train benchmark on convnext_nano.in12k_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 1490.89 samples/sec. 171.709 ms/step.
Train [16/40]. 1491.09 samples/sec. 171.686 ms/step.
Train [24/40]. 1491.08 samples/sec. 171.688 ms/step.
Train [32/40]. 1491.04 samples/sec. 171.692 ms/step.
Train [40/40]. 1491.08 samples/sec. 171.688 ms/step.
Train benchmark of convnext_nano.in12k_ft_in1k done. 1485.86 samples/sec, 171.69 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnext_nano_ols.d1h_in1k created, param count: 15649560
Running inference benchmark on convnext_nano_ols.d1h_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 3904.95 samples/sec. 65.558 ms/step.
Infer [16/40]. 3904.87 samples/sec. 65.559 ms/step.
Infer [24/40]. 3904.76 samples/sec. 65.561 ms/step.
Infer [32/40]. 3904.70 samples/sec. 65.562 ms/step.
Infer [40/40]. 3904.66 samples/sec. 65.563 ms/step.
Inference benchmark of convnext_nano_ols.d1h_in1k done. 3903.03 samples/sec, 65.56 ms/step
Model convnext_nano_ols.d1h_in1k created, param count: 15649560
Running train benchmark on convnext_nano_ols.d1h_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 1344.17 samples/sec. 190.453 ms/step.
Train [16/40]. 1344.18 samples/sec. 190.451 ms/step.
Train [24/40]. 1344.17 samples/sec. 190.452 ms/step.
Train [32/40]. 1344.18 samples/sec. 190.451 ms/step.
Train [40/40]. 1344.12 samples/sec. 190.459 ms/step.
Train benchmark of convnext_nano_ols.d1h_in1k done. 1339.68 samples/sec, 190.46 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnext_pico.d1_in1k created, param count: 9045672
Running inference benchmark on convnext_pico.d1_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 6049.28 samples/sec. 42.319 ms/step.
Infer [16/40]. 6049.25 samples/sec. 42.319 ms/step.
Infer [24/40]. 6049.01 samples/sec. 42.321 ms/step.
Infer [32/40]. 6048.98 samples/sec. 42.321 ms/step.
Infer [40/40]. 6048.55 samples/sec. 42.324 ms/step.
Inference benchmark of convnext_pico.d1_in1k done. 6044.77 samples/sec, 42.32 ms/step
Model convnext_pico.d1_in1k created, param count: 9045672
Running train benchmark on convnext_pico.d1_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 1996.32 samples/sec. 128.236 ms/step.
Train [16/40]. 1996.43 samples/sec. 128.229 ms/step.
Train [24/40]. 1996.44 samples/sec. 128.228 ms/step.
Train [32/40]. 1996.45 samples/sec. 128.228 ms/step.
Train [40/40]. 1996.44 samples/sec. 128.228 ms/step.
Train benchmark of convnext_pico.d1_in1k done. 1988.28 samples/sec, 128.23 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnext_pico_ols.d1_in1k created, param count: 9061928
Running inference benchmark on convnext_pico_ols.d1_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 5695.35 samples/sec. 44.949 ms/step.
Infer [16/40]. 5693.73 samples/sec. 44.962 ms/step.
Infer [24/40]. 5692.97 samples/sec. 44.968 ms/step.
Infer [32/40]. 5692.52 samples/sec. 44.971 ms/step.
Infer [40/40]. 5692.23 samples/sec. 44.974 ms/step.
Inference benchmark of convnext_pico_ols.d1_in1k done. 5688.89 samples/sec, 44.97 ms/step
Model convnext_pico_ols.d1_in1k created, param count: 9061928
Running train benchmark on convnext_pico_ols.d1_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 1888.48 samples/sec. 135.559 ms/step.
Train [16/40]. 1888.42 samples/sec. 135.563 ms/step.
Train [24/40]. 1888.45 samples/sec. 135.561 ms/step.
Train [32/40]. 1888.48 samples/sec. 135.559 ms/step.
Train [40/40]. 1888.46 samples/sec. 135.560 ms/step.
Train benchmark of convnext_pico_ols.d1_in1k done. 1881.39 samples/sec, 135.56 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnext_small.fb_in1k created, param count: 50223688
Running inference benchmark on convnext_small.fb_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 1661.99 samples/sec. 154.032 ms/step.
Infer [16/40]. 1661.84 samples/sec. 154.046 ms/step.
Infer [24/40]. 1661.79 samples/sec. 154.051 ms/step.
Infer [32/40]. 1661.76 samples/sec. 154.054 ms/step.
Infer [40/40]. 1661.75 samples/sec. 154.055 ms/step.
Inference benchmark of convnext_small.fb_in1k done. 1661.43 samples/sec, 154.06 ms/step
Model convnext_small.fb_in1k created, param count: 50223688
Running train benchmark on convnext_small.fb_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 244.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 196.06 MiB is free. Including non-PyTorch memory, this process has 23.45 GiB memory in use. Of the allocated memory 22.11 GiB is allocated by PyTorch, and 117.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convnext_small.fb_in1k created, param count: 50223688
Running train benchmark on convnext_small.fb_in1k for 40 steps w/ input size (3, 288, 288) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 184.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 110.06 MiB is free. Including non-PyTorch memory, this process has 23.53 GiB memory in use. Of the allocated memory 22.15 GiB is allocated by PyTorch, and 158.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convnext_small.fb_in1k created, param count: 50223688
Running train benchmark on convnext_small.fb_in1k for 40 steps w/ input size (3, 288, 288) and batch size 128.
Train [8/40]. 590.32 samples/sec. 216.830 ms/step.
Train [16/40]. 590.33 samples/sec. 216.829 ms/step.
Train [24/40]. 590.32 samples/sec. 216.831 ms/step.
Train [32/40]. 590.32 samples/sec. 216.832 ms/step.
Train [40/40]. 590.32 samples/sec. 216.833 ms/step.
Train benchmark of convnext_small.fb_in1k done. 587.21 samples/sec, 216.83 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnext_small.fb_in22k created, param count: 66250417
Running inference benchmark on convnext_small.fb_in22k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 2709.01 samples/sec. 94.499 ms/step.
Infer [16/40]. 2709.08 samples/sec. 94.497 ms/step.
Infer [24/40]. 2709.08 samples/sec. 94.497 ms/step.
Infer [32/40]. 2709.11 samples/sec. 94.496 ms/step.
Infer [40/40]. 2709.07 samples/sec. 94.497 ms/step.
Inference benchmark of convnext_small.fb_in22k done. 2708.30 samples/sec, 94.50 ms/step
Model convnext_small.fb_in22k created, param count: 66250417
Running train benchmark on convnext_small.fb_in22k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 961.38 samples/sec. 266.285 ms/step.
Train [16/40]. 961.40 samples/sec. 266.279 ms/step.
Train [24/40]. 961.42 samples/sec. 266.274 ms/step.
Train [32/40]. 961.41 samples/sec. 266.275 ms/step.
Train [40/40]. 961.42 samples/sec. 266.274 ms/step.
Train benchmark of convnext_small.fb_in22k done. 956.91 samples/sec, 266.27 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnext_small.fb_in22k_ft_in1k created, param count: 50223688
Running inference benchmark on convnext_small.fb_in22k_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 1661.61 samples/sec. 154.068 ms/step.
Infer [16/40]. 1661.57 samples/sec. 154.071 ms/step.
Infer [24/40]. 1661.58 samples/sec. 154.070 ms/step.
Infer [32/40]. 1661.60 samples/sec. 154.068 ms/step.
Infer [40/40]. 1661.60 samples/sec. 154.069 ms/step.
Inference benchmark of convnext_small.fb_in22k_ft_in1k done. 1661.28 samples/sec, 154.07 ms/step
Model convnext_small.fb_in22k_ft_in1k created, param count: 50223688
Running train benchmark on convnext_small.fb_in22k_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 244.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 196.06 MiB is free. Including non-PyTorch memory, this process has 23.45 GiB memory in use. Of the allocated memory 22.11 GiB is allocated by PyTorch, and 117.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convnext_small.fb_in22k_ft_in1k created, param count: 50223688
Running train benchmark on convnext_small.fb_in22k_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 184.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 110.06 MiB is free. Including non-PyTorch memory, this process has 23.53 GiB memory in use. Of the allocated memory 22.15 GiB is allocated by PyTorch, and 158.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convnext_small.fb_in22k_ft_in1k created, param count: 50223688
Running train benchmark on convnext_small.fb_in22k_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 128.
Train [8/40]. 590.37 samples/sec. 216.812 ms/step.
Train [16/40]. 590.34 samples/sec. 216.823 ms/step.
Train [24/40]. 590.35 samples/sec. 216.820 ms/step.
Train [32/40]. 590.34 samples/sec. 216.824 ms/step.
Train [40/40]. 590.33 samples/sec. 216.827 ms/step.
Train benchmark of convnext_small.fb_in22k_ft_in1k done. 587.11 samples/sec, 216.83 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnext_small.fb_in22k_ft_in1k_384 created, param count: 50223688
Running inference benchmark on convnext_small.fb_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 929.48 samples/sec. 275.422 ms/step.
Infer [16/40]. 929.48 samples/sec. 275.422 ms/step.
Infer [24/40]. 929.47 samples/sec. 275.426 ms/step.
Infer [32/40]. 929.47 samples/sec. 275.425 ms/step.
Infer [40/40]. 929.46 samples/sec. 275.428 ms/step.
Inference benchmark of convnext_small.fb_in22k_ft_in1k_384 done. 929.33 samples/sec, 275.43 ms/step
Model convnext_small.fb_in22k_ft_in1k_384 created, param count: 50223688
Running train benchmark on convnext_small.fb_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 216.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 42.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 22.36 GiB is allocated by PyTorch, and 19.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convnext_small.fb_in22k_ft_in1k_384 created, param count: 50223688
Running train benchmark on convnext_small.fb_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 324.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 226.06 MiB is free. Including non-PyTorch memory, this process has 23.42 GiB memory in use. Of the allocated memory 22.12 GiB is allocated by PyTorch, and 79.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convnext_small.fb_in22k_ft_in1k_384 created, param count: 50223688
Running train benchmark on convnext_small.fb_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 216.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 20.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.37 GiB is allocated by PyTorch, and 23.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model convnext_small.fb_in22k_ft_in1k_384 created, param count: 50223688
Running train benchmark on convnext_small.fb_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 162.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 134.06 MiB is free. Including non-PyTorch memory, this process has 23.51 GiB memory in use. Of the allocated memory 22.05 GiB is allocated by PyTorch, and 244.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model convnext_small.fb_in22k_ft_in1k_384 created, param count: 50223688
Running train benchmark on convnext_small.fb_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 64.
Train [8/40]. 340.20 samples/sec. 188.127 ms/step.
Train [16/40]. 340.19 samples/sec. 188.133 ms/step.
Train [24/40]. 340.19 samples/sec. 188.128 ms/step.
Train [32/40]. 340.19 samples/sec. 188.130 ms/step.
Train [40/40]. 340.18 samples/sec. 188.135 ms/step.
Train benchmark of convnext_small.fb_in22k_ft_in1k_384 done. 338.10 samples/sec, 188.13 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnext_small.in12k created, param count: 58545037
Running inference benchmark on convnext_small.in12k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 2708.42 samples/sec. 94.520 ms/step.
Infer [16/40]. 2708.51 samples/sec. 94.517 ms/step.
Infer [24/40]. 2708.45 samples/sec. 94.519 ms/step.
Infer [32/40]. 2708.46 samples/sec. 94.519 ms/step.
Infer [40/40]. 2708.42 samples/sec. 94.520 ms/step.
Inference benchmark of convnext_small.in12k done. 2707.64 samples/sec, 94.52 ms/step
Model convnext_small.in12k created, param count: 58545037
Running train benchmark on convnext_small.in12k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 962.33 samples/sec. 266.021 ms/step.
Train [16/40]. 962.34 samples/sec. 266.017 ms/step.
Train [24/40]. 962.33 samples/sec. 266.022 ms/step.
Train [32/40]. 962.31 samples/sec. 266.025 ms/step.
Train [40/40]. 962.31 samples/sec. 266.026 ms/step.
Train benchmark of convnext_small.in12k done. 957.73 samples/sec, 266.03 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnext_small.in12k_ft_in1k created, param count: 50223688
Running inference benchmark on convnext_small.in12k_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 1661.43 samples/sec. 154.084 ms/step.
Infer [16/40]. 1661.50 samples/sec. 154.078 ms/step.
Infer [24/40]. 1661.50 samples/sec. 154.078 ms/step.
Infer [32/40]. 1661.49 samples/sec. 154.079 ms/step.
Infer [40/40]. 1661.49 samples/sec. 154.079 ms/step.
Inference benchmark of convnext_small.in12k_ft_in1k done. 1661.16 samples/sec, 154.08 ms/step
Model convnext_small.in12k_ft_in1k created, param count: 50223688
Running train benchmark on convnext_small.in12k_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 244.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 196.06 MiB is free. Including non-PyTorch memory, this process has 23.45 GiB memory in use. Of the allocated memory 22.11 GiB is allocated by PyTorch, and 117.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convnext_small.in12k_ft_in1k created, param count: 50223688
Running train benchmark on convnext_small.in12k_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 184.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 110.06 MiB is free. Including non-PyTorch memory, this process has 23.53 GiB memory in use. Of the allocated memory 22.15 GiB is allocated by PyTorch, and 158.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convnext_small.in12k_ft_in1k created, param count: 50223688
Running train benchmark on convnext_small.in12k_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 128.
Train [8/40]. 590.29 samples/sec. 216.843 ms/step.
Train [16/40]. 590.28 samples/sec. 216.844 ms/step.
Train [24/40]. 590.27 samples/sec. 216.850 ms/step.
Train [32/40]. 590.25 samples/sec. 216.857 ms/step.
Train [40/40]. 590.25 samples/sec. 216.859 ms/step.
Train benchmark of convnext_small.in12k_ft_in1k done. 587.04 samples/sec, 216.86 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnext_small.in12k_ft_in1k_384 created, param count: 50223688
Running inference benchmark on convnext_small.in12k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 929.58 samples/sec. 275.394 ms/step.
Infer [16/40]. 929.55 samples/sec. 275.402 ms/step.
Infer [24/40]. 929.55 samples/sec. 275.403 ms/step.
Infer [32/40]. 929.54 samples/sec. 275.406 ms/step.
Infer [40/40]. 929.53 samples/sec. 275.408 ms/step.
Inference benchmark of convnext_small.in12k_ft_in1k_384 done. 929.41 samples/sec, 275.41 ms/step
Model convnext_small.in12k_ft_in1k_384 created, param count: 50223688
Running train benchmark on convnext_small.in12k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 216.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 42.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 22.36 GiB is allocated by PyTorch, and 19.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convnext_small.in12k_ft_in1k_384 created, param count: 50223688
Running train benchmark on convnext_small.in12k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 324.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 226.06 MiB is free. Including non-PyTorch memory, this process has 23.42 GiB memory in use. Of the allocated memory 22.12 GiB is allocated by PyTorch, and 79.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convnext_small.in12k_ft_in1k_384 created, param count: 50223688
Running train benchmark on convnext_small.in12k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 216.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 20.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.37 GiB is allocated by PyTorch, and 23.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model convnext_small.in12k_ft_in1k_384 created, param count: 50223688
Running train benchmark on convnext_small.in12k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 162.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 134.06 MiB is free. Including non-PyTorch memory, this process has 23.51 GiB memory in use. Of the allocated memory 22.05 GiB is allocated by PyTorch, and 244.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model convnext_small.in12k_ft_in1k_384 created, param count: 50223688
Running train benchmark on convnext_small.in12k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 64.
Train [8/40]. 340.14 samples/sec. 188.158 ms/step.
Train [16/40]. 340.11 samples/sec. 188.172 ms/step.
Train [24/40]. 340.12 samples/sec. 188.166 ms/step.
Train [32/40]. 340.11 samples/sec. 188.174 ms/step.
Train [40/40]. 340.10 samples/sec. 188.178 ms/step.
Train benchmark of convnext_small.in12k_ft_in1k_384 done. 338.09 samples/sec, 188.18 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnext_tiny.fb_in1k created, param count: 28589128
Running inference benchmark on convnext_tiny.fb_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 2691.79 samples/sec. 95.104 ms/step.
Infer [16/40]. 2691.74 samples/sec. 95.106 ms/step.
Infer [24/40]. 2691.56 samples/sec. 95.112 ms/step.
Infer [32/40]. 2691.46 samples/sec. 95.116 ms/step.
Infer [40/40]. 2691.39 samples/sec. 95.118 ms/step.
Inference benchmark of convnext_tiny.fb_in1k done. 2690.64 samples/sec, 95.12 ms/step
Model convnext_tiny.fb_in1k created, param count: 28589128
Running train benchmark on convnext_tiny.fb_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 943.88 samples/sec. 271.220 ms/step.
Train [16/40]. 943.87 samples/sec. 271.224 ms/step.
Train [24/40]. 943.87 samples/sec. 271.223 ms/step.
Train [32/40]. 943.88 samples/sec. 271.221 ms/step.
Train [40/40]. 943.89 samples/sec. 271.219 ms/step.
Train benchmark of convnext_tiny.fb_in1k done. 940.93 samples/sec, 271.22 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnext_tiny.fb_in22k created, param count: 44615857
Running inference benchmark on convnext_tiny.fb_in22k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 4394.78 samples/sec. 58.251 ms/step.
Infer [16/40]. 4394.56 samples/sec. 58.254 ms/step.
Infer [24/40]. 4394.60 samples/sec. 58.253 ms/step.
Infer [32/40]. 4394.65 samples/sec. 58.253 ms/step.
Infer [40/40]. 4394.53 samples/sec. 58.254 ms/step.
Inference benchmark of convnext_tiny.fb_in22k done. 4392.60 samples/sec, 58.25 ms/step
Model convnext_tiny.fb_in22k created, param count: 44615857
Running train benchmark on convnext_tiny.fb_in22k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1534.90 samples/sec. 166.786 ms/step.
Train [16/40]. 1534.83 samples/sec. 166.794 ms/step.
Train [24/40]. 1534.82 samples/sec. 166.795 ms/step.
Train [32/40]. 1534.80 samples/sec. 166.797 ms/step.
Train [40/40]. 1534.77 samples/sec. 166.800 ms/step.
Train benchmark of convnext_tiny.fb_in22k done. 1528.16 samples/sec, 166.80 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnext_tiny.fb_in22k_ft_in1k created, param count: 28589128
Running inference benchmark on convnext_tiny.fb_in22k_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 2691.64 samples/sec. 95.109 ms/step.
Infer [16/40]. 2691.53 samples/sec. 95.113 ms/step.
Infer [24/40]. 2691.47 samples/sec. 95.115 ms/step.
Infer [32/40]. 2691.45 samples/sec. 95.116 ms/step.
Infer [40/40]. 2691.38 samples/sec. 95.118 ms/step.
Inference benchmark of convnext_tiny.fb_in22k_ft_in1k done. 2690.53 samples/sec, 95.12 ms/step
Model convnext_tiny.fb_in22k_ft_in1k created, param count: 28589128
Running train benchmark on convnext_tiny.fb_in22k_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 943.58 samples/sec. 271.306 ms/step.
Train [16/40]. 943.58 samples/sec. 271.308 ms/step.
Train [24/40]. 943.50 samples/sec. 271.330 ms/step.
Train [32/40]. 943.46 samples/sec. 271.342 ms/step.
Train [40/40]. 943.45 samples/sec. 271.345 ms/step.
Train benchmark of convnext_tiny.fb_in22k_ft_in1k done. 940.09 samples/sec, 271.35 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnext_tiny.fb_in22k_ft_in1k_384 created, param count: 28589128
Running inference benchmark on convnext_tiny.fb_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 1508.20 samples/sec. 169.739 ms/step.
Infer [16/40]. 1508.17 samples/sec. 169.742 ms/step.
Infer [24/40]. 1508.05 samples/sec. 169.755 ms/step.
Infer [32/40]. 1508.04 samples/sec. 169.757 ms/step.
Infer [40/40]. 1508.03 samples/sec. 169.758 ms/step.
Inference benchmark of convnext_tiny.fb_in22k_ft_in1k_384 done. 1507.71 samples/sec, 169.76 ms/step
Model convnext_tiny.fb_in22k_ft_in1k_384 created, param count: 28589128
Running train benchmark on convnext_tiny.fb_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 216.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 84.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 22.32 GiB is allocated by PyTorch, and 19.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convnext_tiny.fb_in22k_ft_in1k_384 created, param count: 28589128
Running train benchmark on convnext_tiny.fb_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 324.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 308.06 MiB is free. Including non-PyTorch memory, this process has 23.34 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 37.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convnext_tiny.fb_in22k_ft_in1k_384 created, param count: 28589128
Running train benchmark on convnext_tiny.fb_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 128.
Train [8/40]. 527.68 samples/sec. 242.572 ms/step.
Train [16/40]. 527.68 samples/sec. 242.571 ms/step.
Train [24/40]. 527.68 samples/sec. 242.569 ms/step.
Train [32/40]. 527.67 samples/sec. 242.577 ms/step.
Train [40/40]. 527.60 samples/sec. 242.607 ms/step.
Train benchmark of convnext_tiny.fb_in22k_ft_in1k_384 done. 525.47 samples/sec, 242.61 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnext_tiny.in12k created, param count: 36910477
Running inference benchmark on convnext_tiny.in12k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 4400.52 samples/sec. 58.175 ms/step.
Infer [16/40]. 4398.94 samples/sec. 58.196 ms/step.
Infer [24/40]. 4398.35 samples/sec. 58.204 ms/step.
Infer [32/40]. 4398.07 samples/sec. 58.207 ms/step.
Infer [40/40]. 4397.82 samples/sec. 58.211 ms/step.
Inference benchmark of convnext_tiny.in12k done. 4395.81 samples/sec, 58.21 ms/step
Model convnext_tiny.in12k created, param count: 36910477
Running train benchmark on convnext_tiny.in12k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1537.35 samples/sec. 166.520 ms/step.
Train [16/40]. 1537.35 samples/sec. 166.521 ms/step.
Train [24/40]. 1537.35 samples/sec. 166.521 ms/step.
Train [32/40]. 1537.34 samples/sec. 166.522 ms/step.
Train [40/40]. 1537.35 samples/sec. 166.520 ms/step.
Train benchmark of convnext_tiny.in12k done. 1529.50 samples/sec, 166.52 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnext_tiny.in12k_ft_in1k created, param count: 28589128
Running inference benchmark on convnext_tiny.in12k_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 2692.22 samples/sec. 95.089 ms/step.
Infer [16/40]. 2691.91 samples/sec. 95.100 ms/step.
Infer [24/40]. 2691.92 samples/sec. 95.099 ms/step.
Infer [32/40]. 2691.89 samples/sec. 95.101 ms/step.
Infer [40/40]. 2691.93 samples/sec. 95.099 ms/step.
Inference benchmark of convnext_tiny.in12k_ft_in1k done. 2691.15 samples/sec, 95.10 ms/step
Model convnext_tiny.in12k_ft_in1k created, param count: 28589128
Running train benchmark on convnext_tiny.in12k_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 943.36 samples/sec. 271.371 ms/step.
Train [16/40]. 943.41 samples/sec. 271.357 ms/step.
Train [24/40]. 943.37 samples/sec. 271.367 ms/step.
Train [32/40]. 943.36 samples/sec. 271.369 ms/step.
Train [40/40]. 943.37 samples/sec. 271.369 ms/step.
Train benchmark of convnext_tiny.in12k_ft_in1k done. 939.76 samples/sec, 271.37 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnext_tiny.in12k_ft_in1k_384 created, param count: 28589128
Running inference benchmark on convnext_tiny.in12k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 1508.30 samples/sec. 169.728 ms/step.
Infer [16/40]. 1508.28 samples/sec. 169.729 ms/step.
Infer [24/40]. 1508.30 samples/sec. 169.727 ms/step.
Infer [32/40]. 1508.15 samples/sec. 169.744 ms/step.
Infer [40/40]. 1508.10 samples/sec. 169.750 ms/step.
Inference benchmark of convnext_tiny.in12k_ft_in1k_384 done. 1507.76 samples/sec, 169.75 ms/step
Model convnext_tiny.in12k_ft_in1k_384 created, param count: 28589128
Running train benchmark on convnext_tiny.in12k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 216.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 84.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 22.32 GiB is allocated by PyTorch, and 19.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convnext_tiny.in12k_ft_in1k_384 created, param count: 28589128
Running train benchmark on convnext_tiny.in12k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 324.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 308.06 MiB is free. Including non-PyTorch memory, this process has 23.34 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 37.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convnext_tiny.in12k_ft_in1k_384 created, param count: 28589128
Running train benchmark on convnext_tiny.in12k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 128.
Train [8/40]. 527.36 samples/sec. 242.719 ms/step.
Train [16/40]. 527.36 samples/sec. 242.719 ms/step.
Train [24/40]. 527.36 samples/sec. 242.719 ms/step.
Train [32/40]. 527.36 samples/sec. 242.719 ms/step.
Train [40/40]. 527.36 samples/sec. 242.719 ms/step.
Train benchmark of convnext_tiny.in12k_ft_in1k_384 done. 525.29 samples/sec, 242.72 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnext_tiny_hnf.a2h_in1k created, param count: 28589128
Running inference benchmark on convnext_tiny_hnf.a2h_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 2723.49 samples/sec. 93.997 ms/step.
Infer [16/40]. 2723.20 samples/sec. 94.007 ms/step.
Infer [24/40]. 2723.16 samples/sec. 94.008 ms/step.
Infer [32/40]. 2723.06 samples/sec. 94.012 ms/step.
Infer [40/40]. 2722.94 samples/sec. 94.016 ms/step.
Inference benchmark of convnext_tiny_hnf.a2h_in1k done. 2722.14 samples/sec, 94.02 ms/step
Model convnext_tiny_hnf.a2h_in1k created, param count: 28589128
Running train benchmark on convnext_tiny_hnf.a2h_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 955.05 samples/sec. 268.050 ms/step.
Train [16/40]. 955.04 samples/sec. 268.050 ms/step.
Train [24/40]. 955.02 samples/sec. 268.057 ms/step.
Train [32/40]. 955.00 samples/sec. 268.063 ms/step.
Train [40/40]. 954.99 samples/sec. 268.065 ms/step.
Train benchmark of convnext_tiny_hnf.a2h_in1k done. 951.77 samples/sec, 268.06 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnext_xlarge.fb_in22k created, param count: 392900177
Running inference benchmark on convnext_xlarge.fb_in22k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 731.35 samples/sec. 350.037 ms/step.
Infer [16/40]. 731.21 samples/sec. 350.106 ms/step.
Infer [24/40]. 731.15 samples/sec. 350.134 ms/step.
Infer [32/40]. 731.12 samples/sec. 350.145 ms/step.
Infer [40/40]. 731.11 samples/sec. 350.151 ms/step.
Inference benchmark of convnext_xlarge.fb_in22k done. 731.02 samples/sec, 350.15 ms/step
Model convnext_xlarge.fb_in22k created, param count: 392900177
Running train benchmark on convnext_xlarge.fb_in22k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 668.06 MiB is free. Including non-PyTorch memory, this process has 22.99 GiB memory in use. Of the allocated memory 21.72 GiB is allocated by PyTorch, and 47.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convnext_xlarge.fb_in22k created, param count: 392900177
Running train benchmark on convnext_xlarge.fb_in22k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 294.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 210.06 MiB is free. Including non-PyTorch memory, this process has 23.44 GiB memory in use. Of the allocated memory 22.03 GiB is allocated by PyTorch, and 185.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convnext_xlarge.fb_in22k created, param count: 392900177
Running train benchmark on convnext_xlarge.fb_in22k for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 24.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.32 GiB is allocated by PyTorch, and 78.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model convnext_xlarge.fb_in22k created, param count: 392900177
Running train benchmark on convnext_xlarge.fb_in22k for 40 steps w/ input size (3, 224, 224) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 14.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 333.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model convnext_xlarge.fb_in22k created, param count: 392900177
Running train benchmark on convnext_xlarge.fb_in22k for 40 steps w/ input size (3, 224, 224) and batch size 64.
Train [8/40]. 259.60 samples/sec. 246.535 ms/step.
Train [16/40]. 259.57 samples/sec. 246.566 ms/step.
Train [24/40]. 259.56 samples/sec. 246.568 ms/step.
Train [32/40]. 259.55 samples/sec. 246.585 ms/step.
Train [40/40]. 259.52 samples/sec. 246.608 ms/step.
Train benchmark of convnext_xlarge.fb_in22k done. 258.04 samples/sec, 246.61 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnext_xlarge.fb_in22k_ft_in1k created, param count: 350196968
Running inference benchmark on convnext_xlarge.fb_in22k_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 445.10 samples/sec. 575.150 ms/step.
Infer [16/40]. 445.09 samples/sec. 575.161 ms/step.
Infer [24/40]. 445.09 samples/sec. 575.161 ms/step.
Infer [32/40]. 445.03 samples/sec. 575.248 ms/step.
Infer [40/40]. 444.91 samples/sec. 575.403 ms/step.
Inference benchmark of convnext_xlarge.fb_in22k_ft_in1k done. 444.87 samples/sec, 575.40 ms/step
Model convnext_xlarge.fb_in22k_ft_in1k created, param count: 350196968
Running train benchmark on convnext_xlarge.fb_in22k_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 2.53 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.32 GiB is free. Including non-PyTorch memory, this process has 22.32 GiB memory in use. Of the allocated memory 21.08 GiB is allocated by PyTorch, and 17.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convnext_xlarge.fb_in22k_ft_in1k created, param count: 350196968
Running train benchmark on convnext_xlarge.fb_in22k_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 244.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 236.06 MiB is free. Including non-PyTorch memory, this process has 23.41 GiB memory in use. Of the allocated memory 21.92 GiB is allocated by PyTorch, and 270.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convnext_xlarge.fb_in22k_ft_in1k created, param count: 350196968
Running train benchmark on convnext_xlarge.fb_in22k_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 324.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 140.06 MiB is free. Including non-PyTorch memory, this process has 23.50 GiB memory in use. Of the allocated memory 22.21 GiB is allocated by PyTorch, and 71.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model convnext_xlarge.fb_in22k_ft_in1k created, param count: 350196968
Running train benchmark on convnext_xlarge.fb_in22k_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 244.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 160.06 MiB is free. Including non-PyTorch memory, this process has 23.48 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 187.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model convnext_xlarge.fb_in22k_ft_in1k created, param count: 350196968
Running train benchmark on convnext_xlarge.fb_in22k_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 162.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 96.06 MiB is free. Including non-PyTorch memory, this process has 23.55 GiB memory in use. Of the allocated memory 22.05 GiB is allocated by PyTorch, and 279.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model convnext_xlarge.fb_in22k_ft_in1k created, param count: 350196968
Running train benchmark on convnext_xlarge.fb_in22k_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 48.
Train [8/40]. 159.86 samples/sec. 300.267 ms/step.
Train [16/40]. 159.84 samples/sec. 300.296 ms/step.
Train [24/40]. 159.82 samples/sec. 300.331 ms/step.
Train [32/40]. 159.82 samples/sec. 300.347 ms/step.
Train [40/40]. 159.81 samples/sec. 300.360 ms/step.
Train benchmark of convnext_xlarge.fb_in22k_ft_in1k done. 159.06 samples/sec, 300.36 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnext_xlarge.fb_in22k_ft_in1k_384 created, param count: 350196968
Running inference benchmark on convnext_xlarge.fb_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 251.39 samples/sec. 1018.330 ms/step.
Infer [16/40]. 250.69 samples/sec. 1021.168 ms/step.
Infer [24/40]. 250.51 samples/sec. 1021.933 ms/step.
Infer [32/40]. 249.92 samples/sec. 1024.338 ms/step.
Infer [40/40]. 250.13 samples/sec. 1023.450 ms/step.
Inference benchmark of convnext_xlarge.fb_in22k_ft_in1k_384 done. 250.12 samples/sec, 1023.45 ms/step
Model convnext_xlarge.fb_in22k_ft_in1k_384 created, param count: 350196968
Running train benchmark on convnext_xlarge.fb_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 4.50 GiB. GPU 0 has a total capacty of 23.65 GiB of which 3.38 GiB is free. Including non-PyTorch memory, this process has 20.26 GiB memory in use. Of the allocated memory 18.93 GiB is allocated by PyTorch, and 105.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convnext_xlarge.fb_in22k_ft_in1k_384 created, param count: 350196968
Running train benchmark on convnext_xlarge.fb_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 864.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 568.06 MiB is free. Including non-PyTorch memory, this process has 23.09 GiB memory in use. Of the allocated memory 21.12 GiB is allocated by PyTorch, and 763.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convnext_xlarge.fb_in22k_ft_in1k_384 created, param count: 350196968
Running train benchmark on convnext_xlarge.fb_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 576.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 458.06 MiB is free. Including non-PyTorch memory, this process has 23.19 GiB memory in use. Of the allocated memory 21.62 GiB is allocated by PyTorch, and 355.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model convnext_xlarge.fb_in22k_ft_in1k_384 created, param count: 350196968
Running train benchmark on convnext_xlarge.fb_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 216.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 76.06 MiB is free. Including non-PyTorch memory, this process has 23.57 GiB memory in use. Of the allocated memory 22.09 GiB is allocated by PyTorch, and 254.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model convnext_xlarge.fb_in22k_ft_in1k_384 created, param count: 350196968
Running train benchmark on convnext_xlarge.fb_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 72.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 42.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 22.20 GiB is allocated by PyTorch, and 184.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model convnext_xlarge.fb_in22k_ft_in1k_384 created, param count: 350196968
Running train benchmark on convnext_xlarge.fb_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 216.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 80.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 22.20 GiB is allocated by PyTorch, and 144.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model convnext_xlarge.fb_in22k_ft_in1k_384 created, param count: 350196968
Running train benchmark on convnext_xlarge.fb_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 32.
Train [8/40]. 90.41 samples/sec. 353.962 ms/step.
Train [16/40]. 90.41 samples/sec. 353.962 ms/step.
Train [24/40]. 90.41 samples/sec. 353.956 ms/step.
Train [32/40]. 90.41 samples/sec. 353.954 ms/step.
Train [40/40]. 90.41 samples/sec. 353.955 ms/step.
Train benchmark of convnext_xlarge.fb_in22k_ft_in1k_384 done. 90.02 samples/sec, 353.95 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnext_xxlarge.clip_laion2b_rewind created, param count: 846544768
Running inference benchmark on convnext_xxlarge.clip_laion2b_rewind for 40 steps w/ input size (3, 256, 256) and batch size 256.
Infer [8/40]. 269.57 samples/sec. 949.644 ms/step.
Infer [16/40]. 269.55 samples/sec. 949.723 ms/step.
Infer [24/40]. 269.53 samples/sec. 949.786 ms/step.
Infer [32/40]. 269.51 samples/sec. 949.877 ms/step.
Infer [40/40]. 269.46 samples/sec. 950.046 ms/step.
Inference benchmark of convnext_xxlarge.clip_laion2b_rewind done. 269.45 samples/sec, 950.05 ms/step
Model convnext_xxlarge.clip_laion2b_rewind created, param count: 846544768
Running train benchmark on convnext_xxlarge.clip_laion2b_rewind for 40 steps w/ input size (3, 256, 256) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 768.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 448.06 MiB is free. Including non-PyTorch memory, this process has 23.20 GiB memory in use. Of the allocated memory 21.96 GiB is allocated by PyTorch, and 17.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convnext_xxlarge.clip_laion2b_rewind created, param count: 846544768
Running train benchmark on convnext_xxlarge.clip_laion2b_rewind for 40 steps w/ input size (3, 256, 256) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 576.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 248.06 MiB is free. Including non-PyTorch memory, this process has 23.40 GiB memory in use. Of the allocated memory 21.94 GiB is allocated by PyTorch, and 240.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convnext_xxlarge.clip_laion2b_rewind created, param count: 846544768
Running train benchmark on convnext_xxlarge.clip_laion2b_rewind for 40 steps w/ input size (3, 256, 256) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 768.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 234.06 MiB is free. Including non-PyTorch memory, this process has 23.41 GiB memory in use. Of the allocated memory 22.10 GiB is allocated by PyTorch, and 85.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model convnext_xxlarge.clip_laion2b_rewind created, param count: 846544768
Running train benchmark on convnext_xxlarge.clip_laion2b_rewind for 40 steps w/ input size (3, 256, 256) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 72.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 10.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.25 GiB is allocated by PyTorch, and 157.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model convnext_xxlarge.clip_laion2b_rewind created, param count: 846544768
Running train benchmark on convnext_xxlarge.clip_laion2b_rewind for 40 steps w/ input size (3, 256, 256) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 116.06 MiB is free. Including non-PyTorch memory, this process has 23.53 GiB memory in use. Of the allocated memory 22.21 GiB is allocated by PyTorch, and 95.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model convnext_xxlarge.clip_laion2b_rewind created, param count: 846544768
Running train benchmark on convnext_xxlarge.clip_laion2b_rewind for 40 steps w/ input size (3, 256, 256) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 144.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 76.06 MiB is free. Including non-PyTorch memory, this process has 23.57 GiB memory in use. Of the allocated memory 22.26 GiB is allocated by PyTorch, and 80.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model convnext_xxlarge.clip_laion2b_rewind created, param count: 846544768
Running train benchmark on convnext_xxlarge.clip_laion2b_rewind for 40 steps w/ input size (3, 256, 256) and batch size 32.
Train [8/40]. 93.79 samples/sec. 341.196 ms/step.
Train [16/40]. 93.79 samples/sec. 341.204 ms/step.
Train [24/40]. 93.78 samples/sec. 341.207 ms/step.
Train [32/40]. 93.78 samples/sec. 341.211 ms/step.
Train [40/40]. 93.78 samples/sec. 341.212 ms/step.
Train benchmark of convnext_xxlarge.clip_laion2b_rewind done. 93.34 samples/sec, 341.21 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnext_xxlarge.clip_laion2b_soup created, param count: 846544768
Running inference benchmark on convnext_xxlarge.clip_laion2b_soup for 40 steps w/ input size (3, 256, 256) and batch size 256.
Infer [8/40]. 269.87 samples/sec. 948.603 ms/step.
Infer [16/40]. 269.78 samples/sec. 948.908 ms/step.
Infer [24/40]. 269.59 samples/sec. 949.582 ms/step.
Infer [32/40]. 269.49 samples/sec. 949.949 ms/step.
Infer [40/40]. 269.42 samples/sec. 950.190 ms/step.
Inference benchmark of convnext_xxlarge.clip_laion2b_soup done. 269.41 samples/sec, 950.19 ms/step
Model convnext_xxlarge.clip_laion2b_soup created, param count: 846544768
Running train benchmark on convnext_xxlarge.clip_laion2b_soup for 40 steps w/ input size (3, 256, 256) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 768.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 448.06 MiB is free. Including non-PyTorch memory, this process has 23.20 GiB memory in use. Of the allocated memory 21.96 GiB is allocated by PyTorch, and 17.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convnext_xxlarge.clip_laion2b_soup created, param count: 846544768
Running train benchmark on convnext_xxlarge.clip_laion2b_soup for 40 steps w/ input size (3, 256, 256) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 576.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 248.06 MiB is free. Including non-PyTorch memory, this process has 23.40 GiB memory in use. Of the allocated memory 21.94 GiB is allocated by PyTorch, and 240.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convnext_xxlarge.clip_laion2b_soup created, param count: 846544768
Running train benchmark on convnext_xxlarge.clip_laion2b_soup for 40 steps w/ input size (3, 256, 256) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 768.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 234.06 MiB is free. Including non-PyTorch memory, this process has 23.41 GiB memory in use. Of the allocated memory 22.10 GiB is allocated by PyTorch, and 85.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model convnext_xxlarge.clip_laion2b_soup created, param count: 846544768
Running train benchmark on convnext_xxlarge.clip_laion2b_soup for 40 steps w/ input size (3, 256, 256) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 72.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 10.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.25 GiB is allocated by PyTorch, and 157.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model convnext_xxlarge.clip_laion2b_soup created, param count: 846544768
Running train benchmark on convnext_xxlarge.clip_laion2b_soup for 40 steps w/ input size (3, 256, 256) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 20.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.16 GiB is allocated by PyTorch, and 239.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model convnext_xxlarge.clip_laion2b_soup created, param count: 846544768
Running train benchmark on convnext_xxlarge.clip_laion2b_soup for 40 steps w/ input size (3, 256, 256) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 144.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 76.06 MiB is free. Including non-PyTorch memory, this process has 23.57 GiB memory in use. Of the allocated memory 22.26 GiB is allocated by PyTorch, and 80.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model convnext_xxlarge.clip_laion2b_soup created, param count: 846544768
Running train benchmark on convnext_xxlarge.clip_laion2b_soup for 40 steps w/ input size (3, 256, 256) and batch size 32.
Train [8/40]. 93.80 samples/sec. 341.156 ms/step.
Train [16/40]. 93.79 samples/sec. 341.170 ms/step.
Train [24/40]. 93.79 samples/sec. 341.178 ms/step.
Train [32/40]. 93.79 samples/sec. 341.180 ms/step.
Train [40/40]. 93.79 samples/sec. 341.180 ms/step.
Train benchmark of convnext_xxlarge.clip_laion2b_soup done. 93.35 samples/sec, 341.18 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnext_xxlarge.clip_laion2b_soup_ft_in1k created, param count: 846471016
Running inference benchmark on convnext_xxlarge.clip_laion2b_soup_ft_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
Infer [8/40]. 269.87 samples/sec. 948.603 ms/step.
Infer [16/40]. 269.78 samples/sec. 948.914 ms/step.
Infer [24/40]. 269.59 samples/sec. 949.582 ms/step.
Infer [32/40]. 269.49 samples/sec. 949.944 ms/step.
Infer [40/40]. 269.42 samples/sec. 950.194 ms/step.
Inference benchmark of convnext_xxlarge.clip_laion2b_soup_ft_in1k done. 269.40 samples/sec, 950.19 ms/step
Model convnext_xxlarge.clip_laion2b_soup_ft_in1k created, param count: 846471016
Running train benchmark on convnext_xxlarge.clip_laion2b_soup_ft_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 768.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 448.06 MiB is free. Including non-PyTorch memory, this process has 23.20 GiB memory in use. Of the allocated memory 21.96 GiB is allocated by PyTorch, and 17.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convnext_xxlarge.clip_laion2b_soup_ft_in1k created, param count: 846471016
Running train benchmark on convnext_xxlarge.clip_laion2b_soup_ft_in1k for 40 steps w/ input size (3, 256, 256) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 576.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 230.06 MiB is free. Including non-PyTorch memory, this process has 23.42 GiB memory in use. Of the allocated memory 21.94 GiB is allocated by PyTorch, and 258.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convnext_xxlarge.clip_laion2b_soup_ft_in1k created, param count: 846471016
Running train benchmark on convnext_xxlarge.clip_laion2b_soup_ft_in1k for 40 steps w/ input size (3, 256, 256) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 768.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 234.06 MiB is free. Including non-PyTorch memory, this process has 23.41 GiB memory in use. Of the allocated memory 22.10 GiB is allocated by PyTorch, and 85.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model convnext_xxlarge.clip_laion2b_soup_ft_in1k created, param count: 846471016
Running train benchmark on convnext_xxlarge.clip_laion2b_soup_ft_in1k for 40 steps w/ input size (3, 256, 256) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 72.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 10.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.25 GiB is allocated by PyTorch, and 156.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model convnext_xxlarge.clip_laion2b_soup_ft_in1k created, param count: 846471016
Running train benchmark on convnext_xxlarge.clip_laion2b_soup_ft_in1k for 40 steps w/ input size (3, 256, 256) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 20.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.16 GiB is allocated by PyTorch, and 239.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model convnext_xxlarge.clip_laion2b_soup_ft_in1k created, param count: 846471016
Running train benchmark on convnext_xxlarge.clip_laion2b_soup_ft_in1k for 40 steps w/ input size (3, 256, 256) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 144.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 76.06 MiB is free. Including non-PyTorch memory, this process has 23.57 GiB memory in use. Of the allocated memory 22.26 GiB is allocated by PyTorch, and 80.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model convnext_xxlarge.clip_laion2b_soup_ft_in1k created, param count: 846471016
Running train benchmark on convnext_xxlarge.clip_laion2b_soup_ft_in1k for 40 steps w/ input size (3, 256, 256) and batch size 32.
Train [8/40]. 93.80 samples/sec. 341.155 ms/step.
Train [16/40]. 93.80 samples/sec. 341.161 ms/step.
Train [24/40]. 93.80 samples/sec. 341.166 ms/step.
Train [32/40]. 93.79 samples/sec. 341.171 ms/step.
Train [40/40]. 93.79 samples/sec. 341.174 ms/step.
Train benchmark of convnext_xxlarge.clip_laion2b_soup_ft_in1k done. 93.37 samples/sec, 341.17 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnextv2_atto.fcmae created, param count: 3387400
Running inference benchmark on convnextv2_atto.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 10030.77 samples/sec. 25.521 ms/step.
Infer [16/40]. 10030.09 samples/sec. 25.523 ms/step.
Infer [24/40]. 10030.04 samples/sec. 25.523 ms/step.
Infer [32/40]. 10030.01 samples/sec. 25.523 ms/step.
Infer [40/40]. 10030.09 samples/sec. 25.523 ms/step.
Inference benchmark of convnextv2_atto.fcmae done. 10019.33 samples/sec, 25.52 ms/step
Model convnextv2_atto.fcmae created, param count: 3387400
Running train benchmark on convnextv2_atto.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convnextv2_atto.fcmae created, param count: 3387400
Running train benchmark on convnextv2_atto.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convnextv2_atto.fcmae created, param count: 3387400
Running train benchmark on convnextv2_atto.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model convnextv2_atto.fcmae created, param count: 3387400
Running train benchmark on convnextv2_atto.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 96.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model convnextv2_atto.fcmae created, param count: 3387400
Running train benchmark on convnextv2_atto.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 64.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model convnextv2_atto.fcmae created, param count: 3387400
Running train benchmark on convnextv2_atto.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 48.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model convnextv2_atto.fcmae created, param count: 3387400
Running train benchmark on convnextv2_atto.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 32.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model convnextv2_atto.fcmae created, param count: 3387400
Running train benchmark on convnextv2_atto.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 24.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model convnextv2_atto.fcmae created, param count: 3387400
Running train benchmark on convnextv2_atto.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 16.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 12 for retry.
Model convnextv2_atto.fcmae created, param count: 3387400
Running train benchmark on convnextv2_atto.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 12.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 8 for retry.
Model convnextv2_atto.fcmae created, param count: 3387400
Running train benchmark on convnextv2_atto.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 8.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 6 for retry.
Model convnextv2_atto.fcmae created, param count: 3387400
Running train benchmark on convnextv2_atto.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 6.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 4 for retry.
Model convnextv2_atto.fcmae created, param count: 3387400
Running train benchmark on convnextv2_atto.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 4.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 3 for retry.
Model convnextv2_atto.fcmae created, param count: 3387400
Running train benchmark on convnextv2_atto.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 3.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 2 for retry.
Model convnextv2_atto.fcmae created, param count: 3387400
Running train benchmark on convnextv2_atto.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 2.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 1 for retry.
Model convnextv2_atto.fcmae created, param count: 3387400
Running train benchmark on convnextv2_atto.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 1.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 0 for retry.
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnextv2_atto.fcmae_ft_in1k created, param count: 3708400
Running inference benchmark on convnextv2_atto.fcmae_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 5815.25 samples/sec. 44.022 ms/step.
Infer [16/40]. 5815.09 samples/sec. 44.023 ms/step.
Infer [24/40]. 5815.10 samples/sec. 44.023 ms/step.
Infer [32/40]. 5815.13 samples/sec. 44.023 ms/step.
Infer [40/40]. 5815.13 samples/sec. 44.023 ms/step.
Inference benchmark of convnextv2_atto.fcmae_ft_in1k done. 5811.80 samples/sec, 44.02 ms/step
Model convnextv2_atto.fcmae_ft_in1k created, param count: 3708400
Running train benchmark on convnextv2_atto.fcmae_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 1594.49 samples/sec. 160.553 ms/step.
Train [16/40]. 1594.49 samples/sec. 160.553 ms/step.
Train [24/40]. 1594.46 samples/sec. 160.556 ms/step.
Train [32/40]. 1594.46 samples/sec. 160.556 ms/step.
Train [40/40]. 1594.45 samples/sec. 160.557 ms/step.
Train benchmark of convnextv2_atto.fcmae_ft_in1k done. 1587.98 samples/sec, 160.56 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnextv2_base.fcmae created, param count: 87692800
Running inference benchmark on convnextv2_base.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1215.42 samples/sec. 210.627 ms/step.
Infer [16/40]. 1215.43 samples/sec. 210.625 ms/step.
Infer [24/40]. 1215.45 samples/sec. 210.622 ms/step.
Infer [32/40]. 1215.43 samples/sec. 210.625 ms/step.
Infer [40/40]. 1215.44 samples/sec. 210.624 ms/step.
Inference benchmark of convnextv2_base.fcmae done. 1215.20 samples/sec, 210.62 ms/step
Model convnextv2_base.fcmae created, param count: 87692800
Running train benchmark on convnextv2_base.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 196.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 76.06 MiB is free. Including non-PyTorch memory, this process has 23.57 GiB memory in use. Of the allocated memory 22.12 GiB is allocated by PyTorch, and 223.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convnextv2_base.fcmae created, param count: 87692800
Running train benchmark on convnextv2_base.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 148.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 68.06 MiB is free. Including non-PyTorch memory, this process has 23.57 GiB memory in use. Of the allocated memory 22.17 GiB is allocated by PyTorch, and 187.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convnextv2_base.fcmae created, param count: 87692800
Running train benchmark on convnextv2_base.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model convnextv2_base.fcmae created, param count: 87692800
Running train benchmark on convnextv2_base.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 96.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model convnextv2_base.fcmae created, param count: 87692800
Running train benchmark on convnextv2_base.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 64.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model convnextv2_base.fcmae created, param count: 87692800
Running train benchmark on convnextv2_base.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 48.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model convnextv2_base.fcmae created, param count: 87692800
Running train benchmark on convnextv2_base.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 32.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model convnextv2_base.fcmae created, param count: 87692800
Running train benchmark on convnextv2_base.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 24.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model convnextv2_base.fcmae created, param count: 87692800
Running train benchmark on convnextv2_base.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 16.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 12 for retry.
Model convnextv2_base.fcmae created, param count: 87692800
Running train benchmark on convnextv2_base.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 12.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 8 for retry.
Model convnextv2_base.fcmae created, param count: 87692800
Running train benchmark on convnextv2_base.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 8.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 6 for retry.
Model convnextv2_base.fcmae created, param count: 87692800
Running train benchmark on convnextv2_base.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 6.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 4 for retry.
Model convnextv2_base.fcmae created, param count: 87692800
Running train benchmark on convnextv2_base.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 4.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 3 for retry.
Model convnextv2_base.fcmae created, param count: 87692800
Running train benchmark on convnextv2_base.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 3.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 2 for retry.
Model convnextv2_base.fcmae created, param count: 87692800
Running train benchmark on convnextv2_base.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 2.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 1 for retry.
Model convnextv2_base.fcmae created, param count: 87692800
Running train benchmark on convnextv2_base.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 1.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 0 for retry.
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnextv2_base.fcmae_ft_in1k created, param count: 88717800
Running inference benchmark on convnextv2_base.fcmae_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 728.73 samples/sec. 351.298 ms/step.
Infer [16/40]. 728.71 samples/sec. 351.303 ms/step.
Infer [24/40]. 728.71 samples/sec. 351.306 ms/step.
Infer [32/40]. 728.71 samples/sec. 351.307 ms/step.
Infer [40/40]. 728.65 samples/sec. 351.336 ms/step.
Inference benchmark of convnextv2_base.fcmae_ft_in1k done. 728.56 samples/sec, 351.34 ms/step
Model convnextv2_base.fcmae_ft_in1k created, param count: 88717800
Running train benchmark on convnextv2_base.fcmae_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 648.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 18.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.34 GiB is allocated by PyTorch, and 54.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convnextv2_base.fcmae_ft_in1k created, param count: 88717800
Running train benchmark on convnextv2_base.fcmae_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 244.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 38.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 22.22 GiB is allocated by PyTorch, and 167.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convnextv2_base.fcmae_ft_in1k created, param count: 88717800
Running train benchmark on convnextv2_base.fcmae_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 162.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 62.06 MiB is free. Including non-PyTorch memory, this process has 23.58 GiB memory in use. Of the allocated memory 22.24 GiB is allocated by PyTorch, and 122.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model convnextv2_base.fcmae_ft_in1k created, param count: 88717800
Running train benchmark on convnextv2_base.fcmae_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 122.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 38.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 22.31 GiB is allocated by PyTorch, and 72.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model convnextv2_base.fcmae_ft_in1k created, param count: 88717800
Running train benchmark on convnextv2_base.fcmae_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 64.
Train [8/40]. 218.66 samples/sec. 292.688 ms/step.
Train [16/40]. 218.67 samples/sec. 292.684 ms/step.
Train [24/40]. 218.66 samples/sec. 292.692 ms/step.
Train [32/40]. 218.63 samples/sec. 292.735 ms/step.
Train [40/40]. 218.61 samples/sec. 292.761 ms/step.
Train benchmark of convnextv2_base.fcmae_ft_in1k done. 217.33 samples/sec, 292.76 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnextv2_base.fcmae_ft_in22k_in1k created, param count: 88717800
Running inference benchmark on convnextv2_base.fcmae_ft_in22k_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 728.73 samples/sec. 351.295 ms/step.
Infer [16/40]. 728.56 samples/sec. 351.380 ms/step.
Infer [24/40]. 728.46 samples/sec. 351.426 ms/step.
Infer [32/40]. 728.41 samples/sec. 351.452 ms/step.
Infer [40/40]. 728.38 samples/sec. 351.465 ms/step.
Inference benchmark of convnextv2_base.fcmae_ft_in22k_in1k done. 728.29 samples/sec, 351.46 ms/step
Model convnextv2_base.fcmae_ft_in22k_in1k created, param count: 88717800
Running train benchmark on convnextv2_base.fcmae_ft_in22k_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 648.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 18.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.34 GiB is allocated by PyTorch, and 54.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convnextv2_base.fcmae_ft_in22k_in1k created, param count: 88717800
Running train benchmark on convnextv2_base.fcmae_ft_in22k_in1k for 40 steps w/ input size (3, 288, 288) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 244.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 38.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 22.22 GiB is allocated by PyTorch, and 167.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convnextv2_base.fcmae_ft_in22k_in1k created, param count: 88717800
Running train benchmark on convnextv2_base.fcmae_ft_in22k_in1k for 40 steps w/ input size (3, 288, 288) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 162.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 62.06 MiB is free. Including non-PyTorch memory, this process has 23.58 GiB memory in use. Of the allocated memory 22.24 GiB is allocated by PyTorch, and 122.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model convnextv2_base.fcmae_ft_in22k_in1k created, param count: 88717800
Running train benchmark on convnextv2_base.fcmae_ft_in22k_in1k for 40 steps w/ input size (3, 288, 288) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 122.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 38.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 22.31 GiB is allocated by PyTorch, and 72.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model convnextv2_base.fcmae_ft_in22k_in1k created, param count: 88717800
Running train benchmark on convnextv2_base.fcmae_ft_in22k_in1k for 40 steps w/ input size (3, 288, 288) and batch size 64.
Train [8/40]. 218.65 samples/sec. 292.701 ms/step.
Train [16/40]. 218.59 samples/sec. 292.788 ms/step.
Train [24/40]. 218.57 samples/sec. 292.813 ms/step.
Train [32/40]. 218.56 samples/sec. 292.830 ms/step.
Train [40/40]. 218.55 samples/sec. 292.837 ms/step.
Train benchmark of convnextv2_base.fcmae_ft_in22k_in1k done. 217.27 samples/sec, 292.84 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnextv2_base.fcmae_ft_in22k_in1k_384 created, param count: 88717800
Running inference benchmark on convnextv2_base.fcmae_ft_in22k_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 415.03 samples/sec. 616.829 ms/step.
Infer [16/40]. 414.95 samples/sec. 616.943 ms/step.
Infer [24/40]. 414.92 samples/sec. 616.989 ms/step.
Infer [32/40]. 414.91 samples/sec. 617.007 ms/step.
Infer [40/40]. 414.90 samples/sec. 617.022 ms/step.
Inference benchmark of convnextv2_base.fcmae_ft_in22k_in1k_384 done. 414.86 samples/sec, 617.02 ms/step
Model convnextv2_base.fcmae_ft_in22k_in1k_384 created, param count: 88717800
Running train benchmark on convnextv2_base.fcmae_ft_in22k_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 2.25 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.34 GiB is free. Including non-PyTorch memory, this process has 22.30 GiB memory in use. Of the allocated memory 20.13 GiB is allocated by PyTorch, and 963.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convnextv2_base.fcmae_ft_in22k_in1k_384 created, param count: 88717800
Running train benchmark on convnextv2_base.fcmae_ft_in22k_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 1.69 GiB. GPU 0 has a total capacty of 23.65 GiB of which 942.06 MiB is free. Including non-PyTorch memory, this process has 22.72 GiB memory in use. Of the allocated memory 19.80 GiB is allocated by PyTorch, and 1.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convnextv2_base.fcmae_ft_in22k_in1k_384 created, param count: 88717800
Running train benchmark on convnextv2_base.fcmae_ft_in22k_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 576.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 30.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 21.99 GiB is allocated by PyTorch, and 402.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model convnextv2_base.fcmae_ft_in22k_in1k_384 created, param count: 88717800
Running train benchmark on convnextv2_base.fcmae_ft_in22k_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 216.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 140.06 MiB is free. Including non-PyTorch memory, this process has 23.50 GiB memory in use. Of the allocated memory 22.19 GiB is allocated by PyTorch, and 90.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model convnextv2_base.fcmae_ft_in22k_in1k_384 created, param count: 88717800
Running train benchmark on convnextv2_base.fcmae_ft_in22k_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 144.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 110.06 MiB is free. Including non-PyTorch memory, this process has 23.53 GiB memory in use. Of the allocated memory 22.21 GiB is allocated by PyTorch, and 100.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model convnextv2_base.fcmae_ft_in22k_in1k_384 created, param count: 88717800
Running train benchmark on convnextv2_base.fcmae_ft_in22k_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 108.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 84.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 22.22 GiB is allocated by PyTorch, and 114.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model convnextv2_base.fcmae_ft_in22k_in1k_384 created, param count: 88717800
Running train benchmark on convnextv2_base.fcmae_ft_in22k_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 32.
Train [8/40]. 124.06 samples/sec. 257.947 ms/step.
Train [16/40]. 124.06 samples/sec. 257.950 ms/step.
Train [24/40]. 124.05 samples/sec. 257.951 ms/step.
Train [32/40]. 124.04 samples/sec. 257.980 ms/step.
Train [40/40]. 124.03 samples/sec. 258.003 ms/step.
Train benchmark of convnextv2_base.fcmae_ft_in22k_in1k_384 done. 123.24 samples/sec, 258.00 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnextv2_femto.fcmae created, param count: 4848240
Running inference benchmark on convnextv2_femto.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 8368.01 samples/sec. 30.593 ms/step.
Infer [16/40]. 8368.59 samples/sec. 30.591 ms/step.
Infer [24/40]. 8368.78 samples/sec. 30.590 ms/step.
Infer [32/40]. 8368.83 samples/sec. 30.590 ms/step.
Infer [40/40]. 8368.80 samples/sec. 30.590 ms/step.
Inference benchmark of convnextv2_femto.fcmae done. 8361.46 samples/sec, 30.59 ms/step
Model convnextv2_femto.fcmae created, param count: 4848240
Running train benchmark on convnextv2_femto.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convnextv2_femto.fcmae created, param count: 4848240
Running train benchmark on convnextv2_femto.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convnextv2_femto.fcmae created, param count: 4848240
Running train benchmark on convnextv2_femto.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model convnextv2_femto.fcmae created, param count: 4848240
Running train benchmark on convnextv2_femto.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 96.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model convnextv2_femto.fcmae created, param count: 4848240
Running train benchmark on convnextv2_femto.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 64.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model convnextv2_femto.fcmae created, param count: 4848240
Running train benchmark on convnextv2_femto.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 48.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model convnextv2_femto.fcmae created, param count: 4848240
Running train benchmark on convnextv2_femto.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 32.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model convnextv2_femto.fcmae created, param count: 4848240
Running train benchmark on convnextv2_femto.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 24.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model convnextv2_femto.fcmae created, param count: 4848240
Running train benchmark on convnextv2_femto.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 16.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 12 for retry.
Model convnextv2_femto.fcmae created, param count: 4848240
Running train benchmark on convnextv2_femto.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 12.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 8 for retry.
Model convnextv2_femto.fcmae created, param count: 4848240
Running train benchmark on convnextv2_femto.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 8.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 6 for retry.
Model convnextv2_femto.fcmae created, param count: 4848240
Running train benchmark on convnextv2_femto.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 6.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 4 for retry.
Model convnextv2_femto.fcmae created, param count: 4848240
Running train benchmark on convnextv2_femto.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 4.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 3 for retry.
Model convnextv2_femto.fcmae created, param count: 4848240
Running train benchmark on convnextv2_femto.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 3.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 2 for retry.
Model convnextv2_femto.fcmae created, param count: 4848240
Running train benchmark on convnextv2_femto.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 2.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 1 for retry.
Model convnextv2_femto.fcmae created, param count: 4848240
Running train benchmark on convnextv2_femto.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 1.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 0 for retry.
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnextv2_femto.fcmae_ft_in1k created, param count: 5233240
Running inference benchmark on convnextv2_femto.fcmae_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 4915.44 samples/sec. 52.081 ms/step.
Infer [16/40]. 4914.32 samples/sec. 52.093 ms/step.
Infer [24/40]. 4913.85 samples/sec. 52.098 ms/step.
Infer [32/40]. 4913.52 samples/sec. 52.101 ms/step.
Infer [40/40]. 4913.41 samples/sec. 52.102 ms/step.
Inference benchmark of convnextv2_femto.fcmae_ft_in1k done. 4910.97 samples/sec, 52.10 ms/step
Model convnextv2_femto.fcmae_ft_in1k created, param count: 5233240
Running train benchmark on convnextv2_femto.fcmae_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 1367.64 samples/sec. 187.183 ms/step.
Train [16/40]. 1367.68 samples/sec. 187.179 ms/step.
Train [24/40]. 1367.66 samples/sec. 187.181 ms/step.
Train [32/40]. 1367.63 samples/sec. 187.185 ms/step.
Train [40/40]. 1367.62 samples/sec. 187.186 ms/step.
Train benchmark of convnextv2_femto.fcmae_ft_in1k done. 1362.29 samples/sec, 187.19 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnextv2_huge.fcmae created, param count: 657472640
Running inference benchmark on convnextv2_huge.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 324.10 samples/sec. 789.888 ms/step.
Infer [16/40]. 324.02 samples/sec. 790.071 ms/step.
Infer [24/40]. 323.98 samples/sec. 790.166 ms/step.
Infer [32/40]. 323.96 samples/sec. 790.212 ms/step.
Infer [40/40]. 323.95 samples/sec. 790.240 ms/step.
Inference benchmark of convnextv2_huge.fcmae done. 323.93 samples/sec, 790.24 ms/step
Model convnextv2_huge.fcmae created, param count: 657472640
Running train benchmark on convnextv2_huge.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 2.11 GiB. GPU 0 has a total capacty of 23.65 GiB of which 2.10 GiB is free. Including non-PyTorch memory, this process has 21.54 GiB memory in use. Of the allocated memory 19.76 GiB is allocated by PyTorch, and 566.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convnextv2_huge.fcmae created, param count: 657472640
Running train benchmark on convnextv2_huge.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 1.58 GiB. GPU 0 has a total capacty of 23.65 GiB of which 96.06 MiB is free. Including non-PyTorch memory, this process has 23.55 GiB memory in use. Of the allocated memory 21.11 GiB is allocated by PyTorch, and 1.21 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convnextv2_huge.fcmae created, param count: 657472640
Running train benchmark on convnextv2_huge.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 540.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 42.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 22.10 GiB is allocated by PyTorch, and 280.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model convnextv2_huge.fcmae created, param count: 657472640
Running train benchmark on convnextv2_huge.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 204.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 54.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 21.97 GiB is allocated by PyTorch, and 402.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model convnextv2_huge.fcmae created, param count: 657472640
Running train benchmark on convnextv2_huge.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 136.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 84.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 22.07 GiB is allocated by PyTorch, and 268.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model convnextv2_huge.fcmae created, param count: 657472640
Running train benchmark on convnextv2_huge.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 102.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 32.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 22.17 GiB is allocated by PyTorch, and 219.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model convnextv2_huge.fcmae created, param count: 657472640
Running train benchmark on convnextv2_huge.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 32.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model convnextv2_huge.fcmae created, param count: 657472640
Running train benchmark on convnextv2_huge.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 24.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model convnextv2_huge.fcmae created, param count: 657472640
Running train benchmark on convnextv2_huge.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 16.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 12 for retry.
Model convnextv2_huge.fcmae created, param count: 657472640
Running train benchmark on convnextv2_huge.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 12.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 8 for retry.
Model convnextv2_huge.fcmae created, param count: 657472640
Running train benchmark on convnextv2_huge.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 8.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 6 for retry.
Model convnextv2_huge.fcmae created, param count: 657472640
Running train benchmark on convnextv2_huge.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 6.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 4 for retry.
Model convnextv2_huge.fcmae created, param count: 657472640
Running train benchmark on convnextv2_huge.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 4.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 3 for retry.
Model convnextv2_huge.fcmae created, param count: 657472640
Running train benchmark on convnextv2_huge.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 3.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 2 for retry.
Model convnextv2_huge.fcmae created, param count: 657472640
Running train benchmark on convnextv2_huge.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 2.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 1 for retry.
Model convnextv2_huge.fcmae created, param count: 657472640
Running train benchmark on convnextv2_huge.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 1.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 0 for retry.
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnextv2_huge.fcmae_ft_in1k created, param count: 660289640
Running inference benchmark on convnextv2_huge.fcmae_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 196.71 samples/sec. 1301.429 ms/step.
Infer [16/40]. 196.72 samples/sec. 1301.353 ms/step.
Infer [24/40]. 196.72 samples/sec. 1301.323 ms/step.
Infer [32/40]. 196.72 samples/sec. 1301.315 ms/step.
Infer [40/40]. 196.73 samples/sec. 1301.300 ms/step.
Inference benchmark of convnextv2_huge.fcmae_ft_in1k done. 196.72 samples/sec, 1301.30 ms/step
Model convnextv2_huge.fcmae_ft_in1k created, param count: 660289640
Running train benchmark on convnextv2_huge.fcmae_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 3.48 GiB. GPU 0 has a total capacty of 23.65 GiB of which 104.06 MiB is free. Including non-PyTorch memory, this process has 23.54 GiB memory in use. Of the allocated memory 21.42 GiB is allocated by PyTorch, and 920.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convnextv2_huge.fcmae_ft_in1k created, param count: 660289640
Running train benchmark on convnextv2_huge.fcmae_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 2.61 GiB. GPU 0 has a total capacty of 23.65 GiB of which 2.55 GiB is free. Including non-PyTorch memory, this process has 21.09 GiB memory in use. Of the allocated memory 19.03 GiB is allocated by PyTorch, and 852.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convnextv2_huge.fcmae_ft_in1k created, param count: 660289640
Running train benchmark on convnextv2_huge.fcmae_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 1.74 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.31 GiB is free. Including non-PyTorch memory, this process has 22.33 GiB memory in use. Of the allocated memory 19.60 GiB is allocated by PyTorch, and 1.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model convnextv2_huge.fcmae_ft_in1k created, param count: 660289640
Running train benchmark on convnextv2_huge.fcmae_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 1.31 GiB. GPU 0 has a total capacty of 23.65 GiB of which 884.06 MiB is free. Including non-PyTorch memory, this process has 22.78 GiB memory in use. Of the allocated memory 20.28 GiB is allocated by PyTorch, and 1.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model convnextv2_huge.fcmae_ft_in1k created, param count: 660289640
Running train benchmark on convnextv2_huge.fcmae_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 140.06 MiB is free. Including non-PyTorch memory, this process has 23.50 GiB memory in use. Of the allocated memory 21.99 GiB is allocated by PyTorch, and 301.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model convnextv2_huge.fcmae_ft_in1k created, param count: 660289640
Running train benchmark on convnextv2_huge.fcmae_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 168.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 20.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.16 GiB is allocated by PyTorch, and 237.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model convnextv2_huge.fcmae_ft_in1k created, param count: 660289640
Running train benchmark on convnextv2_huge.fcmae_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 32.
ERROR: "CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 88.06 MiB is free. Including non-PyTorch memory, this process has 23.55 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 202.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model convnextv2_huge.fcmae_ft_in1k created, param count: 660289640
Running train benchmark on convnextv2_huge.fcmae_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 24.
Train [8/40]. 58.10 samples/sec. 413.110 ms/step.
Train [16/40]. 58.09 samples/sec. 413.148 ms/step.
Train [24/40]. 58.09 samples/sec. 413.164 ms/step.
Train [32/40]. 58.09 samples/sec. 413.173 ms/step.
Train [40/40]. 58.09 samples/sec. 413.180 ms/step.
Train benchmark of convnextv2_huge.fcmae_ft_in1k done. 57.83 samples/sec, 413.18 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnextv2_huge.fcmae_ft_in22k_in1k_384 created, param count: 660289640
Running inference benchmark on convnextv2_huge.fcmae_ft_in22k_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 6.19 GiB. GPU 0 has a total capacty of 23.65 GiB of which 5.47 GiB is free. Including non-PyTorch memory, this process has 18.17 GiB memory in use. Of the allocated memory 16.93 GiB is allocated by PyTorch, and 20.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convnextv2_huge.fcmae_ft_in22k_in1k_384 created, param count: 660289640
Running inference benchmark on convnextv2_huge.fcmae_ft_in22k_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 4.64 GiB. GPU 0 has a total capacty of 23.65 GiB of which 3.85 GiB is free. Including non-PyTorch memory, this process has 19.79 GiB memory in use. Of the allocated memory 14.21 GiB is allocated by PyTorch, and 4.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convnextv2_huge.fcmae_ft_in22k_in1k_384 created, param count: 660289640
Running inference benchmark on convnextv2_huge.fcmae_ft_in22k_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 128.
Infer [8/40]. 110.64 samples/sec. 1156.934 ms/step.
Infer [16/40]. 110.63 samples/sec. 1157.059 ms/step.
Infer [24/40]. 110.62 samples/sec. 1157.106 ms/step.
Infer [32/40]. 110.62 samples/sec. 1157.118 ms/step.
Infer [40/40]. 110.62 samples/sec. 1157.130 ms/step.
Inference benchmark of convnextv2_huge.fcmae_ft_in22k_in1k_384 done. 110.61 samples/sec, 1157.13 ms/step
Model convnextv2_huge.fcmae_ft_in22k_in1k_384 created, param count: 660289640
Running train benchmark on convnextv2_huge.fcmae_ft_in22k_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 6.19 GiB. GPU 0 has a total capacty of 23.65 GiB of which 2.33 GiB is free. Including non-PyTorch memory, this process has 21.31 GiB memory in use. Of the allocated memory 20.06 GiB is allocated by PyTorch, and 24.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convnextv2_huge.fcmae_ft_in22k_in1k_384 created, param count: 660289640
Running train benchmark on convnextv2_huge.fcmae_ft_in22k_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 4.64 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.50 GiB is free. Including non-PyTorch memory, this process has 22.14 GiB memory in use. Of the allocated memory 20.04 GiB is allocated by PyTorch, and 896.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convnextv2_huge.fcmae_ft_in22k_in1k_384 created, param count: 660289640
Running train benchmark on convnextv2_huge.fcmae_ft_in22k_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 3.09 GiB. GPU 0 has a total capacty of 23.65 GiB of which 2.38 GiB is free. Including non-PyTorch memory, this process has 21.26 GiB memory in use. Of the allocated memory 19.17 GiB is allocated by PyTorch, and 887.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model convnextv2_huge.fcmae_ft_in22k_in1k_384 created, param count: 660289640
Running train benchmark on convnextv2_huge.fcmae_ft_in22k_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 2.32 GiB. GPU 0 has a total capacty of 23.65 GiB of which 2.15 GiB is free. Including non-PyTorch memory, this process has 21.49 GiB memory in use. Of the allocated memory 19.37 GiB is allocated by PyTorch, and 910.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model convnextv2_huge.fcmae_ft_in22k_in1k_384 created, param count: 660289640
Running train benchmark on convnextv2_huge.fcmae_ft_in22k_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 1.55 GiB. GPU 0 has a total capacty of 23.65 GiB of which 484.06 MiB is free. Including non-PyTorch memory, this process has 23.17 GiB memory in use. Of the allocated memory 20.66 GiB is allocated by PyTorch, and 1.29 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model convnextv2_huge.fcmae_ft_in22k_in1k_384 created, param count: 660289640
Running train benchmark on convnextv2_huge.fcmae_ft_in22k_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 594.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 94.06 MiB is free. Including non-PyTorch memory, this process has 23.55 GiB memory in use. Of the allocated memory 21.94 GiB is allocated by PyTorch, and 389.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model convnextv2_huge.fcmae_ft_in22k_in1k_384 created, param count: 660289640
Running train benchmark on convnextv2_huge.fcmae_ft_in22k_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 32.
ERROR: "CUDA out of memory. Tried to allocate 198.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 20.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 323.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model convnextv2_huge.fcmae_ft_in22k_in1k_384 created, param count: 660289640
Running train benchmark on convnextv2_huge.fcmae_ft_in22k_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 24.
ERROR: "CUDA out of memory. Tried to allocate 150.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 110.06 MiB is free. Including non-PyTorch memory, this process has 23.53 GiB memory in use. Of the allocated memory 21.89 GiB is allocated by PyTorch, and 425.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model convnextv2_huge.fcmae_ft_in22k_in1k_384 created, param count: 660289640
Running train benchmark on convnextv2_huge.fcmae_ft_in22k_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 16.
ERROR: "CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 90.06 MiB is free. Including non-PyTorch memory, this process has 23.55 GiB memory in use. Of the allocated memory 22.17 GiB is allocated by PyTorch, and 165.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 12 for retry.
Model convnextv2_huge.fcmae_ft_in22k_in1k_384 created, param count: 660289640
Running train benchmark on convnextv2_huge.fcmae_ft_in22k_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 12.
Train [8/40]. 32.51 samples/sec. 369.115 ms/step.
Train [16/40]. 32.51 samples/sec. 369.099 ms/step.
Train [24/40]. 32.51 samples/sec. 369.093 ms/step.
Train [32/40]. 32.51 samples/sec. 369.092 ms/step.
Train [40/40]. 32.51 samples/sec. 369.091 ms/step.
Train benchmark of convnextv2_huge.fcmae_ft_in22k_in1k_384 done. 32.36 samples/sec, 369.09 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnextv2_huge.fcmae_ft_in22k_in1k_512 created, param count: 660289640
Running inference benchmark on convnextv2_huge.fcmae_ft_in22k_in1k_512 for 40 steps w/ input size (3, 512, 512) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 11.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 4.27 GiB is free. Including non-PyTorch memory, this process has 19.37 GiB memory in use. Of the allocated memory 18.12 GiB is allocated by PyTorch, and 19.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convnextv2_huge.fcmae_ft_in22k_in1k_512 created, param count: 660289640
Running inference benchmark on convnextv2_huge.fcmae_ft_in22k_in1k_512 for 40 steps w/ input size (3, 512, 512) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 8.25 GiB. GPU 0 has a total capacty of 23.65 GiB of which 7.55 GiB is free. Including non-PyTorch memory, this process has 16.09 GiB memory in use. Of the allocated memory 13.95 GiB is allocated by PyTorch, and 934.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convnextv2_huge.fcmae_ft_in22k_in1k_512 created, param count: 660289640
Running inference benchmark on convnextv2_huge.fcmae_ft_in22k_in1k_512 for 40 steps w/ input size (3, 512, 512) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 5.50 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.58 GiB is free. Including non-PyTorch memory, this process has 22.06 GiB memory in use. Of the allocated memory 16.56 GiB is allocated by PyTorch, and 4.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model convnextv2_huge.fcmae_ft_in22k_in1k_512 created, param count: 660289640
Running inference benchmark on convnextv2_huge.fcmae_ft_in22k_in1k_512 for 40 steps w/ input size (3, 512, 512) and batch size 96.
Infer [8/40]. 62.41 samples/sec. 1538.252 ms/step.
Infer [16/40]. 62.41 samples/sec. 1538.238 ms/step.
Infer [24/40]. 62.41 samples/sec. 1538.218 ms/step.
Infer [32/40]. 62.40 samples/sec. 1538.358 ms/step.
Infer [40/40]. 62.39 samples/sec. 1538.598 ms/step.
Inference benchmark of convnextv2_huge.fcmae_ft_in22k_in1k_512 done. 62.39 samples/sec, 1538.60 ms/step
Model convnextv2_huge.fcmae_ft_in22k_in1k_512 created, param count: 660289640
Running train benchmark on convnextv2_huge.fcmae_ft_in22k_in1k_512 for 40 steps w/ input size (3, 512, 512) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 11.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 9.71 GiB is free. Including non-PyTorch memory, this process has 13.93 GiB memory in use. Of the allocated memory 12.69 GiB is allocated by PyTorch, and 19.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convnextv2_huge.fcmae_ft_in22k_in1k_512 created, param count: 660289640
Running train benchmark on convnextv2_huge.fcmae_ft_in22k_in1k_512 for 40 steps w/ input size (3, 512, 512) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 8.25 GiB. GPU 0 has a total capacty of 23.65 GiB of which 3.35 GiB is free. Including non-PyTorch memory, this process has 20.29 GiB memory in use. Of the allocated memory 18.12 GiB is allocated by PyTorch, and 966.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convnextv2_huge.fcmae_ft_in22k_in1k_512 created, param count: 660289640
Running train benchmark on convnextv2_huge.fcmae_ft_in22k_in1k_512 for 40 steps w/ input size (3, 512, 512) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 5.50 GiB. GPU 0 has a total capacty of 23.65 GiB of which 4.31 GiB is free. Including non-PyTorch memory, this process has 19.33 GiB memory in use. Of the allocated memory 17.97 GiB is allocated by PyTorch, and 142.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model convnextv2_huge.fcmae_ft_in22k_in1k_512 created, param count: 660289640
Running train benchmark on convnextv2_huge.fcmae_ft_in22k_in1k_512 for 40 steps w/ input size (3, 512, 512) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 4.12 GiB. GPU 0 has a total capacty of 23.65 GiB of which 3.54 GiB is free. Including non-PyTorch memory, this process has 20.10 GiB memory in use. Of the allocated memory 17.95 GiB is allocated by PyTorch, and 947.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model convnextv2_huge.fcmae_ft_in22k_in1k_512 created, param count: 660289640
Running train benchmark on convnextv2_huge.fcmae_ft_in22k_in1k_512 for 40 steps w/ input size (3, 512, 512) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 2.75 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.57 GiB is free. Including non-PyTorch memory, this process has 22.07 GiB memory in use. Of the allocated memory 19.93 GiB is allocated by PyTorch, and 937.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model convnextv2_huge.fcmae_ft_in22k_in1k_512 created, param count: 660289640
Running train benchmark on convnextv2_huge.fcmae_ft_in22k_in1k_512 for 40 steps w/ input size (3, 512, 512) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 2.06 GiB. GPU 0 has a total capacty of 23.65 GiB of which 24.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 21.48 GiB is allocated by PyTorch, and 933.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model convnextv2_huge.fcmae_ft_in22k_in1k_512 created, param count: 660289640
Running train benchmark on convnextv2_huge.fcmae_ft_in22k_in1k_512 for 40 steps w/ input size (3, 512, 512) and batch size 32.
ERROR: "CUDA out of memory. Tried to allocate 1.38 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.23 GiB is free. Including non-PyTorch memory, this process has 22.41 GiB memory in use. Of the allocated memory 19.87 GiB is allocated by PyTorch, and 1.32 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model convnextv2_huge.fcmae_ft_in22k_in1k_512 created, param count: 660289640
Running train benchmark on convnextv2_huge.fcmae_ft_in22k_in1k_512 for 40 steps w/ input size (3, 512, 512) and batch size 24.
ERROR: "CUDA out of memory. Tried to allocate 528.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 274.06 MiB is free. Including non-PyTorch memory, this process has 23.37 GiB memory in use. Of the allocated memory 21.70 GiB is allocated by PyTorch, and 459.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model convnextv2_huge.fcmae_ft_in22k_in1k_512 created, param count: 660289640
Running train benchmark on convnextv2_huge.fcmae_ft_in22k_in1k_512 for 40 steps w/ input size (3, 512, 512) and batch size 16.
ERROR: "CUDA out of memory. Tried to allocate 176.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 140.06 MiB is free. Including non-PyTorch memory, this process has 23.50 GiB memory in use. Of the allocated memory 22.04 GiB is allocated by PyTorch, and 246.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 12 for retry.
Model convnextv2_huge.fcmae_ft_in22k_in1k_512 created, param count: 660289640
Running train benchmark on convnextv2_huge.fcmae_ft_in22k_in1k_512 for 40 steps w/ input size (3, 512, 512) and batch size 12.
ERROR: "CUDA out of memory. Tried to allocate 132.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 6.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.27 GiB is allocated by PyTorch, and 141.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 8 for retry.
Model convnextv2_huge.fcmae_ft_in22k_in1k_512 created, param count: 660289640
Running train benchmark on convnextv2_huge.fcmae_ft_in22k_in1k_512 for 40 steps w/ input size (3, 512, 512) and batch size 8.
ERROR: "CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 86.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 21.81 GiB is allocated by PyTorch, and 532.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 6 for retry.
Model convnextv2_huge.fcmae_ft_in22k_in1k_512 created, param count: 660289640
Running train benchmark on convnextv2_huge.fcmae_ft_in22k_in1k_512 for 40 steps w/ input size (3, 512, 512) and batch size 6.
Train [8/40]. 17.73 samples/sec. 338.371 ms/step.
Train [16/40]. 17.73 samples/sec. 338.393 ms/step.
Train [24/40]. 17.73 samples/sec. 338.396 ms/step.
Train [32/40]. 17.73 samples/sec. 338.401 ms/step.
Train [40/40]. 17.73 samples/sec. 338.402 ms/step.
Train benchmark of convnextv2_huge.fcmae_ft_in22k_in1k_512 done. 17.64 samples/sec, 338.40 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnextv2_large.fcmae created, param count: 196419840
Running inference benchmark on convnextv2_large.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 731.26 samples/sec. 350.080 ms/step.
Infer [16/40]. 731.26 samples/sec. 350.081 ms/step.
Infer [24/40]. 731.25 samples/sec. 350.085 ms/step.
Infer [32/40]. 731.25 samples/sec. 350.085 ms/step.
Infer [40/40]. 731.25 samples/sec. 350.088 ms/step.
Inference benchmark of convnextv2_large.fcmae done. 731.14 samples/sec, 350.09 ms/step
Model convnextv2_large.fcmae created, param count: 196419840
Running train benchmark on convnextv2_large.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 588.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 258.06 MiB is free. Including non-PyTorch memory, this process has 23.39 GiB memory in use. Of the allocated memory 22.03 GiB is allocated by PyTorch, and 141.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convnextv2_large.fcmae created, param count: 196419840
Running train benchmark on convnextv2_large.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 222.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 174.06 MiB is free. Including non-PyTorch memory, this process has 23.47 GiB memory in use. Of the allocated memory 21.97 GiB is allocated by PyTorch, and 286.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convnextv2_large.fcmae created, param count: 196419840
Running train benchmark on convnextv2_large.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 148.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 70.06 MiB is free. Including non-PyTorch memory, this process has 23.57 GiB memory in use. Of the allocated memory 22.23 GiB is allocated by PyTorch, and 115.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model convnextv2_large.fcmae created, param count: 196419840
Running train benchmark on convnextv2_large.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 94.06 MiB is free. Including non-PyTorch memory, this process has 23.55 GiB memory in use. Of the allocated memory 22.06 GiB is allocated by PyTorch, and 273.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model convnextv2_large.fcmae created, param count: 196419840
Running train benchmark on convnextv2_large.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 64.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model convnextv2_large.fcmae created, param count: 196419840
Running train benchmark on convnextv2_large.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 48.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model convnextv2_large.fcmae created, param count: 196419840
Running train benchmark on convnextv2_large.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 32.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model convnextv2_large.fcmae created, param count: 196419840
Running train benchmark on convnextv2_large.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 24.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model convnextv2_large.fcmae created, param count: 196419840
Running train benchmark on convnextv2_large.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 16.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 12 for retry.
Model convnextv2_large.fcmae created, param count: 196419840
Running train benchmark on convnextv2_large.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 12.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 8 for retry.
Model convnextv2_large.fcmae created, param count: 196419840
Running train benchmark on convnextv2_large.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 8.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 6 for retry.
Model convnextv2_large.fcmae created, param count: 196419840
Running train benchmark on convnextv2_large.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 6.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 4 for retry.
Model convnextv2_large.fcmae created, param count: 196419840
Running train benchmark on convnextv2_large.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 4.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 3 for retry.
Model convnextv2_large.fcmae created, param count: 196419840
Running train benchmark on convnextv2_large.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 3.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 2 for retry.
Model convnextv2_large.fcmae created, param count: 196419840
Running train benchmark on convnextv2_large.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 2.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 1 for retry.
Model convnextv2_large.fcmae created, param count: 196419840
Running train benchmark on convnextv2_large.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 1.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 0 for retry.
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnextv2_large.fcmae_ft_in1k created, param count: 197956840
Running inference benchmark on convnextv2_large.fcmae_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 448.48 samples/sec. 570.822 ms/step.
Infer [16/40]. 448.41 samples/sec. 570.902 ms/step.
Infer [24/40]. 448.39 samples/sec. 570.935 ms/step.
Infer [32/40]. 448.36 samples/sec. 570.967 ms/step.
Infer [40/40]. 448.34 samples/sec. 570.989 ms/step.
Inference benchmark of convnextv2_large.fcmae_ft_in1k done. 448.31 samples/sec, 570.99 ms/step
Model convnextv2_large.fcmae_ft_in1k created, param count: 197956840
Running train benchmark on convnextv2_large.fcmae_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 1.90 GiB. GPU 0 has a total capacty of 23.65 GiB of which 152.06 MiB is free. Including non-PyTorch memory, this process has 23.49 GiB memory in use. Of the allocated memory 20.48 GiB is allocated by PyTorch, and 1.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convnextv2_large.fcmae_ft_in1k created, param count: 197956840
Running train benchmark on convnextv2_large.fcmae_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 1.42 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.18 GiB is free. Including non-PyTorch memory, this process has 22.46 GiB memory in use. Of the allocated memory 19.73 GiB is allocated by PyTorch, and 1.51 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convnextv2_large.fcmae_ft_in1k created, param count: 197956840
Running train benchmark on convnextv2_large.fcmae_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 486.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 444.06 MiB is free. Including non-PyTorch memory, this process has 23.21 GiB memory in use. Of the allocated memory 21.48 GiB is allocated by PyTorch, and 512.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model convnextv2_large.fcmae_ft_in1k created, param count: 197956840
Running train benchmark on convnextv2_large.fcmae_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 184.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 142.06 MiB is free. Including non-PyTorch memory, this process has 23.50 GiB memory in use. Of the allocated memory 21.97 GiB is allocated by PyTorch, and 314.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model convnextv2_large.fcmae_ft_in1k created, param count: 197956840
Running train benchmark on convnextv2_large.fcmae_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 122.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 42.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 22.28 GiB is allocated by PyTorch, and 95.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model convnextv2_large.fcmae_ft_in1k created, param count: 197956840
Running train benchmark on convnextv2_large.fcmae_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 48.
Train [8/40]. 133.48 samples/sec. 359.595 ms/step.
Train [16/40]. 133.45 samples/sec. 359.689 ms/step.
Train [24/40]. 133.44 samples/sec. 359.719 ms/step.
Train [32/40]. 133.43 samples/sec. 359.739 ms/step.
Train [40/40]. 133.43 samples/sec. 359.749 ms/step.
Train benchmark of convnextv2_large.fcmae_ft_in1k done. 132.75 samples/sec, 359.75 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnextv2_large.fcmae_ft_in22k_in1k created, param count: 197956840
Running inference benchmark on convnextv2_large.fcmae_ft_in22k_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 448.34 samples/sec. 570.997 ms/step.
Infer [16/40]. 448.33 samples/sec. 571.008 ms/step.
Infer [24/40]. 448.32 samples/sec. 571.020 ms/step.
Infer [32/40]. 448.31 samples/sec. 571.031 ms/step.
Infer [40/40]. 448.30 samples/sec. 571.044 ms/step.
Inference benchmark of convnextv2_large.fcmae_ft_in22k_in1k done. 448.27 samples/sec, 571.04 ms/step
Model convnextv2_large.fcmae_ft_in22k_in1k created, param count: 197956840
Running train benchmark on convnextv2_large.fcmae_ft_in22k_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 1.90 GiB. GPU 0 has a total capacty of 23.65 GiB of which 152.06 MiB is free. Including non-PyTorch memory, this process has 23.49 GiB memory in use. Of the allocated memory 20.48 GiB is allocated by PyTorch, and 1.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convnextv2_large.fcmae_ft_in22k_in1k created, param count: 197956840
Running train benchmark on convnextv2_large.fcmae_ft_in22k_in1k for 40 steps w/ input size (3, 288, 288) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 1.42 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.18 GiB is free. Including non-PyTorch memory, this process has 22.46 GiB memory in use. Of the allocated memory 19.73 GiB is allocated by PyTorch, and 1.51 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convnextv2_large.fcmae_ft_in22k_in1k created, param count: 197956840
Running train benchmark on convnextv2_large.fcmae_ft_in22k_in1k for 40 steps w/ input size (3, 288, 288) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 486.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 444.06 MiB is free. Including non-PyTorch memory, this process has 23.21 GiB memory in use. Of the allocated memory 21.48 GiB is allocated by PyTorch, and 512.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model convnextv2_large.fcmae_ft_in22k_in1k created, param count: 197956840
Running train benchmark on convnextv2_large.fcmae_ft_in22k_in1k for 40 steps w/ input size (3, 288, 288) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 184.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 142.06 MiB is free. Including non-PyTorch memory, this process has 23.50 GiB memory in use. Of the allocated memory 21.97 GiB is allocated by PyTorch, and 314.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model convnextv2_large.fcmae_ft_in22k_in1k created, param count: 197956840
Running train benchmark on convnextv2_large.fcmae_ft_in22k_in1k for 40 steps w/ input size (3, 288, 288) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 122.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 42.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 22.28 GiB is allocated by PyTorch, and 95.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model convnextv2_large.fcmae_ft_in22k_in1k created, param count: 197956840
Running train benchmark on convnextv2_large.fcmae_ft_in22k_in1k for 40 steps w/ input size (3, 288, 288) and batch size 48.
Train [8/40]. 133.50 samples/sec. 359.543 ms/step.
Train [16/40]. 133.46 samples/sec. 359.648 ms/step.
Train [24/40]. 133.45 samples/sec. 359.685 ms/step.
Train [32/40]. 133.44 samples/sec. 359.703 ms/step.
Train [40/40]. 133.44 samples/sec. 359.715 ms/step.
Train benchmark of convnextv2_large.fcmae_ft_in22k_in1k done. 132.77 samples/sec, 359.71 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnextv2_large.fcmae_ft_in22k_in1k_384 created, param count: 197956840
Running inference benchmark on convnextv2_large.fcmae_ft_in22k_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 254.41 samples/sec. 1006.263 ms/step.
Infer [16/40]. 254.36 samples/sec. 1006.463 ms/step.
Infer [24/40]. 254.33 samples/sec. 1006.559 ms/step.
Infer [32/40]. 254.31 samples/sec. 1006.629 ms/step.
Infer [40/40]. 254.30 samples/sec. 1006.672 ms/step.
Inference benchmark of convnextv2_large.fcmae_ft_in22k_in1k_384 done. 254.29 samples/sec, 1006.67 ms/step
Model convnextv2_large.fcmae_ft_in22k_in1k_384 created, param count: 197956840
Running train benchmark on convnextv2_large.fcmae_ft_in22k_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 3.38 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.34 GiB is free. Including non-PyTorch memory, this process has 22.30 GiB memory in use. Of the allocated memory 20.06 GiB is allocated by PyTorch, and 1.02 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convnextv2_large.fcmae_ft_in22k_in1k_384 created, param count: 197956840
Running train benchmark on convnextv2_large.fcmae_ft_in22k_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 2.53 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.20 GiB is free. Including non-PyTorch memory, this process has 22.45 GiB memory in use. Of the allocated memory 20.20 GiB is allocated by PyTorch, and 1.02 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convnextv2_large.fcmae_ft_in22k_in1k_384 created, param count: 197956840
Running train benchmark on convnextv2_large.fcmae_ft_in22k_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 1.69 GiB. GPU 0 has a total capacty of 23.65 GiB of which 904.06 MiB is free. Including non-PyTorch memory, this process has 22.76 GiB memory in use. Of the allocated memory 19.94 GiB is allocated by PyTorch, and 1.60 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model convnextv2_large.fcmae_ft_in22k_in1k_384 created, param count: 197956840
Running train benchmark on convnextv2_large.fcmae_ft_in22k_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 648.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 360.06 MiB is free. Including non-PyTorch memory, this process has 23.29 GiB memory in use. Of the allocated memory 21.86 GiB is allocated by PyTorch, and 209.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model convnextv2_large.fcmae_ft_in22k_in1k_384 created, param count: 197956840
Running train benchmark on convnextv2_large.fcmae_ft_in22k_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 216.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 104.06 MiB is free. Including non-PyTorch memory, this process has 23.54 GiB memory in use. Of the allocated memory 22.14 GiB is allocated by PyTorch, and 174.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model convnextv2_large.fcmae_ft_in22k_in1k_384 created, param count: 197956840
Running train benchmark on convnextv2_large.fcmae_ft_in22k_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 162.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 82.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 22.25 GiB is allocated by PyTorch, and 89.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model convnextv2_large.fcmae_ft_in22k_in1k_384 created, param count: 197956840
Running train benchmark on convnextv2_large.fcmae_ft_in22k_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 32.
ERROR: "CUDA out of memory. Tried to allocate 108.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 50.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 22.19 GiB is allocated by PyTorch, and 177.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model convnextv2_large.fcmae_ft_in22k_in1k_384 created, param count: 197956840
Running train benchmark on convnextv2_large.fcmae_ft_in22k_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 24.
Train [8/40]. 75.08 samples/sec. 319.642 ms/step.
Train [16/40]. 75.06 samples/sec. 319.729 ms/step.
Train [24/40]. 75.06 samples/sec. 319.765 ms/step.
Train [32/40]. 75.05 samples/sec. 319.787 ms/step.
Train [40/40]. 75.05 samples/sec. 319.799 ms/step.
Train benchmark of convnextv2_large.fcmae_ft_in22k_in1k_384 done. 74.64 samples/sec, 319.80 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnextv2_nano.fcmae created, param count: 14982800
Running inference benchmark on convnextv2_nano.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 4512.82 samples/sec. 56.727 ms/step.
Infer [16/40]. 4512.59 samples/sec. 56.730 ms/step.
Infer [24/40]. 4512.30 samples/sec. 56.734 ms/step.
Infer [32/40]. 4512.23 samples/sec. 56.735 ms/step.
Infer [40/40]. 4512.18 samples/sec. 56.735 ms/step.
Inference benchmark of convnextv2_nano.fcmae done. 4509.93 samples/sec, 56.73 ms/step
Model convnextv2_nano.fcmae created, param count: 14982800
Running train benchmark on convnextv2_nano.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convnextv2_nano.fcmae created, param count: 14982800
Running train benchmark on convnextv2_nano.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convnextv2_nano.fcmae created, param count: 14982800
Running train benchmark on convnextv2_nano.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model convnextv2_nano.fcmae created, param count: 14982800
Running train benchmark on convnextv2_nano.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 96.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model convnextv2_nano.fcmae created, param count: 14982800
Running train benchmark on convnextv2_nano.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 64.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model convnextv2_nano.fcmae created, param count: 14982800
Running train benchmark on convnextv2_nano.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 48.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model convnextv2_nano.fcmae created, param count: 14982800
Running train benchmark on convnextv2_nano.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 32.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model convnextv2_nano.fcmae created, param count: 14982800
Running train benchmark on convnextv2_nano.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 24.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model convnextv2_nano.fcmae created, param count: 14982800
Running train benchmark on convnextv2_nano.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 16.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 12 for retry.
Model convnextv2_nano.fcmae created, param count: 14982800
Running train benchmark on convnextv2_nano.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 12.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 8 for retry.
Model convnextv2_nano.fcmae created, param count: 14982800
Running train benchmark on convnextv2_nano.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 8.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 6 for retry.
Model convnextv2_nano.fcmae created, param count: 14982800
Running train benchmark on convnextv2_nano.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 6.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 4 for retry.
Model convnextv2_nano.fcmae created, param count: 14982800
Running train benchmark on convnextv2_nano.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 4.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 3 for retry.
Model convnextv2_nano.fcmae created, param count: 14982800
Running train benchmark on convnextv2_nano.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 3.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 2 for retry.
Model convnextv2_nano.fcmae created, param count: 14982800
Running train benchmark on convnextv2_nano.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 2.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 1 for retry.
Model convnextv2_nano.fcmae created, param count: 14982800
Running train benchmark on convnextv2_nano.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 1.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 0 for retry.
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnextv2_nano.fcmae_ft_in1k created, param count: 15623800
Running inference benchmark on convnextv2_nano.fcmae_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 2690.86 samples/sec. 95.137 ms/step.
Infer [16/40]. 2690.82 samples/sec. 95.138 ms/step.
Infer [24/40]. 2690.77 samples/sec. 95.140 ms/step.
Infer [32/40]. 2690.79 samples/sec. 95.139 ms/step.
Infer [40/40]. 2690.73 samples/sec. 95.142 ms/step.
Inference benchmark of convnextv2_nano.fcmae_ft_in1k done. 2689.95 samples/sec, 95.14 ms/step
Model convnextv2_nano.fcmae_ft_in1k created, param count: 15623800
Running train benchmark on convnextv2_nano.fcmae_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 770.95 samples/sec. 332.056 ms/step.
Train [16/40]. 770.95 samples/sec. 332.057 ms/step.
Train [24/40]. 770.95 samples/sec. 332.056 ms/step.
Train [32/40]. 770.95 samples/sec. 332.057 ms/step.
Train [40/40]. 770.95 samples/sec. 332.058 ms/step.
Train benchmark of convnextv2_nano.fcmae_ft_in1k done. 768.75 samples/sec, 332.06 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnextv2_nano.fcmae_ft_in22k_in1k created, param count: 15623800
Running inference benchmark on convnextv2_nano.fcmae_ft_in22k_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 2690.58 samples/sec. 95.147 ms/step.
Infer [16/40]. 2690.53 samples/sec. 95.149 ms/step.
Infer [24/40]. 2690.40 samples/sec. 95.153 ms/step.
Infer [32/40]. 2690.40 samples/sec. 95.153 ms/step.
Infer [40/40]. 2690.38 samples/sec. 95.154 ms/step.
Inference benchmark of convnextv2_nano.fcmae_ft_in22k_in1k done. 2689.58 samples/sec, 95.15 ms/step
Model convnextv2_nano.fcmae_ft_in22k_in1k created, param count: 15623800
Running train benchmark on convnextv2_nano.fcmae_ft_in22k_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 770.92 samples/sec. 332.069 ms/step.
Train [16/40]. 770.90 samples/sec. 332.080 ms/step.
Train [24/40]. 770.90 samples/sec. 332.079 ms/step.
Train [32/40]. 770.89 samples/sec. 332.083 ms/step.
Train [40/40]. 770.89 samples/sec. 332.082 ms/step.
Train benchmark of convnextv2_nano.fcmae_ft_in22k_in1k done. 768.64 samples/sec, 332.08 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnextv2_nano.fcmae_ft_in22k_in1k_384 created, param count: 15623800
Running inference benchmark on convnextv2_nano.fcmae_ft_in22k_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 1504.20 samples/sec. 170.190 ms/step.
Infer [16/40]. 1504.15 samples/sec. 170.196 ms/step.
Infer [24/40]. 1504.13 samples/sec. 170.198 ms/step.
Infer [32/40]. 1504.11 samples/sec. 170.201 ms/step.
Infer [40/40]. 1504.05 samples/sec. 170.207 ms/step.
Inference benchmark of convnextv2_nano.fcmae_ft_in22k_in1k_384 done. 1503.75 samples/sec, 170.21 ms/step
Model convnextv2_nano.fcmae_ft_in22k_in1k_384 created, param count: 15623800
Running train benchmark on convnextv2_nano.fcmae_ft_in22k_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 720.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 262.06 MiB is free. Including non-PyTorch memory, this process has 23.38 GiB memory in use. Of the allocated memory 21.43 GiB is allocated by PyTorch, and 746.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convnextv2_nano.fcmae_ft_in22k_in1k_384 created, param count: 15623800
Running train benchmark on convnextv2_nano.fcmae_ft_in22k_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 270.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 42.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 22.10 GiB is allocated by PyTorch, and 284.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convnextv2_nano.fcmae_ft_in22k_in1k_384 created, param count: 15623800
Running train benchmark on convnextv2_nano.fcmae_ft_in22k_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 128.
Train [8/40]. 434.47 samples/sec. 294.610 ms/step.
Train [16/40]. 434.47 samples/sec. 294.611 ms/step.
Train [24/40]. 434.46 samples/sec. 294.621 ms/step.
Train [32/40]. 434.45 samples/sec. 294.626 ms/step.
Train [40/40]. 434.45 samples/sec. 294.626 ms/step.
Train benchmark of convnextv2_nano.fcmae_ft_in22k_in1k_384 done. 433.11 samples/sec, 294.63 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnextv2_pico.fcmae created, param count: 8553280
Running inference benchmark on convnextv2_pico.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 6257.03 samples/sec. 40.914 ms/step.
Infer [16/40]. 6256.68 samples/sec. 40.916 ms/step.
Infer [24/40]. 6256.40 samples/sec. 40.918 ms/step.
Infer [32/40]. 6256.36 samples/sec. 40.918 ms/step.
Infer [40/40]. 6256.33 samples/sec. 40.919 ms/step.
Inference benchmark of convnextv2_pico.fcmae done. 6252.31 samples/sec, 40.92 ms/step
Model convnextv2_pico.fcmae created, param count: 8553280
Running train benchmark on convnextv2_pico.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convnextv2_pico.fcmae created, param count: 8553280
Running train benchmark on convnextv2_pico.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convnextv2_pico.fcmae created, param count: 8553280
Running train benchmark on convnextv2_pico.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model convnextv2_pico.fcmae created, param count: 8553280
Running train benchmark on convnextv2_pico.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 96.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model convnextv2_pico.fcmae created, param count: 8553280
Running train benchmark on convnextv2_pico.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 64.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model convnextv2_pico.fcmae created, param count: 8553280
Running train benchmark on convnextv2_pico.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 48.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model convnextv2_pico.fcmae created, param count: 8553280
Running train benchmark on convnextv2_pico.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 32.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model convnextv2_pico.fcmae created, param count: 8553280
Running train benchmark on convnextv2_pico.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 24.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model convnextv2_pico.fcmae created, param count: 8553280
Running train benchmark on convnextv2_pico.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 16.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 12 for retry.
Model convnextv2_pico.fcmae created, param count: 8553280
Running train benchmark on convnextv2_pico.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 12.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 8 for retry.
Model convnextv2_pico.fcmae created, param count: 8553280
Running train benchmark on convnextv2_pico.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 8.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 6 for retry.
Model convnextv2_pico.fcmae created, param count: 8553280
Running train benchmark on convnextv2_pico.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 6.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 4 for retry.
Model convnextv2_pico.fcmae created, param count: 8553280
Running train benchmark on convnextv2_pico.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 4.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 3 for retry.
Model convnextv2_pico.fcmae created, param count: 8553280
Running train benchmark on convnextv2_pico.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 3.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 2 for retry.
Model convnextv2_pico.fcmae created, param count: 8553280
Running train benchmark on convnextv2_pico.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 2.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 1 for retry.
Model convnextv2_pico.fcmae created, param count: 8553280
Running train benchmark on convnextv2_pico.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 1.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 0 for retry.
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnextv2_pico.fcmae_ft_in1k created, param count: 9066280
Running inference benchmark on convnextv2_pico.fcmae_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 3722.39 samples/sec. 68.773 ms/step.
Infer [16/40]. 3722.65 samples/sec. 68.768 ms/step.
Infer [24/40]. 3722.63 samples/sec. 68.769 ms/step.
Infer [32/40]. 3722.72 samples/sec. 68.767 ms/step.
Infer [40/40]. 3722.74 samples/sec. 68.767 ms/step.
Inference benchmark of convnextv2_pico.fcmae_ft_in1k done. 3721.30 samples/sec, 68.77 ms/step
Model convnextv2_pico.fcmae_ft_in1k created, param count: 9066280
Running train benchmark on convnextv2_pico.fcmae_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 1045.77 samples/sec. 244.796 ms/step.
Train [16/40]. 1045.76 samples/sec. 244.799 ms/step.
Train [24/40]. 1045.74 samples/sec. 244.802 ms/step.
Train [32/40]. 1045.76 samples/sec. 244.799 ms/step.
Train [40/40]. 1045.76 samples/sec. 244.798 ms/step.
Train benchmark of convnextv2_pico.fcmae_ft_in1k done. 1042.34 samples/sec, 244.80 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnextv2_tiny.fcmae created, param count: 27866496
Running inference benchmark on convnextv2_tiny.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 2715.68 samples/sec. 94.267 ms/step.
Infer [16/40]. 2715.63 samples/sec. 94.269 ms/step.
Infer [24/40]. 2715.64 samples/sec. 94.269 ms/step.
Infer [32/40]. 2715.54 samples/sec. 94.272 ms/step.
Infer [40/40]. 2715.47 samples/sec. 94.275 ms/step.
Inference benchmark of convnextv2_tiny.fcmae done. 2714.61 samples/sec, 94.28 ms/step
Model convnextv2_tiny.fcmae created, param count: 27866496
Running train benchmark on convnextv2_tiny.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convnextv2_tiny.fcmae created, param count: 27866496
Running train benchmark on convnextv2_tiny.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convnextv2_tiny.fcmae created, param count: 27866496
Running train benchmark on convnextv2_tiny.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model convnextv2_tiny.fcmae created, param count: 27866496
Running train benchmark on convnextv2_tiny.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 96.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model convnextv2_tiny.fcmae created, param count: 27866496
Running train benchmark on convnextv2_tiny.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 64.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model convnextv2_tiny.fcmae created, param count: 27866496
Running train benchmark on convnextv2_tiny.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 48.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model convnextv2_tiny.fcmae created, param count: 27866496
Running train benchmark on convnextv2_tiny.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 32.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model convnextv2_tiny.fcmae created, param count: 27866496
Running train benchmark on convnextv2_tiny.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 24.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model convnextv2_tiny.fcmae created, param count: 27866496
Running train benchmark on convnextv2_tiny.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 16.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 12 for retry.
Model convnextv2_tiny.fcmae created, param count: 27866496
Running train benchmark on convnextv2_tiny.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 12.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 8 for retry.
Model convnextv2_tiny.fcmae created, param count: 27866496
Running train benchmark on convnextv2_tiny.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 8.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 6 for retry.
Model convnextv2_tiny.fcmae created, param count: 27866496
Running train benchmark on convnextv2_tiny.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 6.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 4 for retry.
Model convnextv2_tiny.fcmae created, param count: 27866496
Running train benchmark on convnextv2_tiny.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 4.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 3 for retry.
Model convnextv2_tiny.fcmae created, param count: 27866496
Running train benchmark on convnextv2_tiny.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 3.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 2 for retry.
Model convnextv2_tiny.fcmae created, param count: 27866496
Running train benchmark on convnextv2_tiny.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 2.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 1 for retry.
Model convnextv2_tiny.fcmae created, param count: 27866496
Running train benchmark on convnextv2_tiny.fcmae for 40 steps w/ input size (3, 224, 224) and batch size 1.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 0 for retry.
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnextv2_tiny.fcmae_ft_in1k created, param count: 28635496
Running inference benchmark on convnextv2_tiny.fcmae_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 1659.38 samples/sec. 154.274 ms/step.
Infer [16/40]. 1659.41 samples/sec. 154.272 ms/step.
Infer [24/40]. 1659.39 samples/sec. 154.274 ms/step.
Infer [32/40]. 1659.27 samples/sec. 154.284 ms/step.
Infer [40/40]. 1658.99 samples/sec. 154.310 ms/step.
Inference benchmark of convnextv2_tiny.fcmae_ft_in1k done. 1658.63 samples/sec, 154.31 ms/step
Model convnextv2_tiny.fcmae_ft_in1k created, param count: 28635496
Running train benchmark on convnextv2_tiny.fcmae_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 244.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 244.06 MiB is free. Including non-PyTorch memory, this process has 23.40 GiB memory in use. Of the allocated memory 22.15 GiB is allocated by PyTorch, and 31.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convnextv2_tiny.fcmae_ft_in1k created, param count: 28635496
Running train benchmark on convnextv2_tiny.fcmae_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 184.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 136.06 MiB is free. Including non-PyTorch memory, this process has 23.51 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 210.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convnextv2_tiny.fcmae_ft_in1k created, param count: 28635496
Running train benchmark on convnextv2_tiny.fcmae_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 128.
Train [8/40]. 478.43 samples/sec. 267.541 ms/step.
Train [16/40]. 478.43 samples/sec. 267.540 ms/step.
Train [24/40]. 478.42 samples/sec. 267.547 ms/step.
Train [32/40]. 478.43 samples/sec. 267.542 ms/step.
Train [40/40]. 478.43 samples/sec. 267.543 ms/step.
Train benchmark of convnextv2_tiny.fcmae_ft_in1k done. 476.53 samples/sec, 267.54 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnextv2_tiny.fcmae_ft_in22k_in1k created, param count: 28635496
Running inference benchmark on convnextv2_tiny.fcmae_ft_in22k_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 1658.06 samples/sec. 154.397 ms/step.
Infer [16/40]. 1658.02 samples/sec. 154.401 ms/step.
Infer [24/40]. 1657.96 samples/sec. 154.406 ms/step.
Infer [32/40]. 1657.96 samples/sec. 154.407 ms/step.
Infer [40/40]. 1657.93 samples/sec. 154.410 ms/step.
Inference benchmark of convnextv2_tiny.fcmae_ft_in22k_in1k done. 1657.56 samples/sec, 154.41 ms/step
Model convnextv2_tiny.fcmae_ft_in22k_in1k created, param count: 28635496
Running train benchmark on convnextv2_tiny.fcmae_ft_in22k_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 244.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 244.06 MiB is free. Including non-PyTorch memory, this process has 23.40 GiB memory in use. Of the allocated memory 22.15 GiB is allocated by PyTorch, and 31.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convnextv2_tiny.fcmae_ft_in22k_in1k created, param count: 28635496
Running train benchmark on convnextv2_tiny.fcmae_ft_in22k_in1k for 40 steps w/ input size (3, 288, 288) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 184.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 136.06 MiB is free. Including non-PyTorch memory, this process has 23.51 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 210.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convnextv2_tiny.fcmae_ft_in22k_in1k created, param count: 28635496
Running train benchmark on convnextv2_tiny.fcmae_ft_in22k_in1k for 40 steps w/ input size (3, 288, 288) and batch size 128.
Train [8/40]. 478.33 samples/sec. 267.596 ms/step.
Train [16/40]. 478.37 samples/sec. 267.578 ms/step.
Train [24/40]. 478.38 samples/sec. 267.570 ms/step.
Train [32/40]. 478.38 samples/sec. 267.568 ms/step.
Train [40/40]. 478.39 samples/sec. 267.565 ms/step.
Train benchmark of convnextv2_tiny.fcmae_ft_in22k_in1k done. 476.50 samples/sec, 267.56 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model convnextv2_tiny.fcmae_ft_in22k_in1k_384 created, param count: 28635496
Running inference benchmark on convnextv2_tiny.fcmae_ft_in22k_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 932.77 samples/sec. 274.451 ms/step.
Infer [16/40]. 932.75 samples/sec. 274.456 ms/step.
Infer [24/40]. 932.74 samples/sec. 274.460 ms/step.
Infer [32/40]. 932.72 samples/sec. 274.465 ms/step.
Infer [40/40]. 932.72 samples/sec. 274.467 ms/step.
Inference benchmark of convnextv2_tiny.fcmae_ft_in22k_in1k_384 done. 932.58 samples/sec, 274.47 ms/step
Model convnextv2_tiny.fcmae_ft_in22k_in1k_384 created, param count: 28635496
Running train benchmark on convnextv2_tiny.fcmae_ft_in22k_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 1.69 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.35 GiB is free. Including non-PyTorch memory, this process has 22.29 GiB memory in use. Of the allocated memory 19.76 GiB is allocated by PyTorch, and 1.31 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model convnextv2_tiny.fcmae_ft_in22k_in1k_384 created, param count: 28635496
Running train benchmark on convnextv2_tiny.fcmae_ft_in22k_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 648.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 80.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 22.29 GiB is allocated by PyTorch, and 50.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model convnextv2_tiny.fcmae_ft_in22k_in1k_384 created, param count: 28635496
Running train benchmark on convnextv2_tiny.fcmae_ft_in22k_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 216.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 130.06 MiB is free. Including non-PyTorch memory, this process has 23.51 GiB memory in use. Of the allocated memory 22.12 GiB is allocated by PyTorch, and 172.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model convnextv2_tiny.fcmae_ft_in22k_in1k_384 created, param count: 28635496
Running train benchmark on convnextv2_tiny.fcmae_ft_in22k_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 82.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 22.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.34 GiB is allocated by PyTorch, and 52.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model convnextv2_tiny.fcmae_ft_in22k_in1k_384 created, param count: 28635496
Running train benchmark on convnextv2_tiny.fcmae_ft_in22k_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 64.
Train [8/40]. 270.97 samples/sec. 236.191 ms/step.
Train [16/40]. 270.97 samples/sec. 236.187 ms/step.
Train [24/40]. 270.98 samples/sec. 236.181 ms/step.
Train [32/40]. 270.98 samples/sec. 236.178 ms/step.
Train [40/40]. 270.99 samples/sec. 236.173 ms/step.
Train benchmark of convnextv2_tiny.fcmae_ft_in22k_in1k_384 done. 269.76 samples/sec, 236.17 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model crossvit_9_240.in1k created, param count: 8553296
Running inference benchmark on crossvit_9_240.in1k for 40 steps w/ input size (3, 240, 240) and batch size 256.
Infer [8/40]. 14745.33 samples/sec. 17.361 ms/step.
Infer [16/40]. 14741.45 samples/sec. 17.366 ms/step.
Infer [24/40]. 14741.23 samples/sec. 17.366 ms/step.
Infer [32/40]. 14741.72 samples/sec. 17.366 ms/step.
Infer [40/40]. 14742.27 samples/sec. 17.365 ms/step.
Inference benchmark of crossvit_9_240.in1k done. 14723.14 samples/sec, 17.36 ms/step
Model crossvit_9_240.in1k created, param count: 8553296
Running train benchmark on crossvit_9_240.in1k for 40 steps w/ input size (3, 240, 240) and batch size 256.
Train [8/40]. 3538.08 samples/sec. 72.356 ms/step.
Train [16/40]. 3538.20 samples/sec. 72.353 ms/step.
Train [24/40]. 3537.40 samples/sec. 72.370 ms/step.
Train [32/40]. 3535.76 samples/sec. 72.403 ms/step.
Train [40/40]. 3536.08 samples/sec. 72.396 ms/step.
Train benchmark of crossvit_9_240.in1k done. 3489.83 samples/sec, 72.40 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model crossvit_9_dagger_240.in1k created, param count: 8776592
Running inference benchmark on crossvit_9_dagger_240.in1k for 40 steps w/ input size (3, 240, 240) and batch size 256.
Infer [8/40]. 13356.95 samples/sec. 19.166 ms/step.
Infer [16/40]. 13356.41 samples/sec. 19.167 ms/step.
Infer [24/40]. 13355.79 samples/sec. 19.168 ms/step.
Infer [32/40]. 13351.94 samples/sec. 19.173 ms/step.
Infer [40/40]. 13349.51 samples/sec. 19.177 ms/step.
Inference benchmark of crossvit_9_dagger_240.in1k done. 13333.47 samples/sec, 19.18 ms/step
Model crossvit_9_dagger_240.in1k created, param count: 8776592
Running train benchmark on crossvit_9_dagger_240.in1k for 40 steps w/ input size (3, 240, 240) and batch size 256.
Train [8/40]. 3304.68 samples/sec. 77.466 ms/step.
Train [16/40]. 3304.01 samples/sec. 77.482 ms/step.
Train [24/40]. 3303.40 samples/sec. 77.496 ms/step.
Train [32/40]. 3303.54 samples/sec. 77.493 ms/step.
Train [40/40]. 3303.91 samples/sec. 77.484 ms/step.
Train benchmark of crossvit_9_dagger_240.in1k done. 3262.47 samples/sec, 77.48 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model crossvit_15_240.in1k created, param count: 27528464
Running inference benchmark on crossvit_15_240.in1k for 40 steps w/ input size (3, 240, 240) and batch size 256.
Infer [8/40]. 6063.44 samples/sec. 42.220 ms/step.
Infer [16/40]. 6063.56 samples/sec. 42.219 ms/step.
Infer [24/40]. 6063.10 samples/sec. 42.223 ms/step.
Infer [32/40]. 6062.94 samples/sec. 42.224 ms/step.
Infer [40/40]. 6062.98 samples/sec. 42.223 ms/step.
Inference benchmark of crossvit_15_240.in1k done. 6059.46 samples/sec, 42.22 ms/step
Model crossvit_15_240.in1k created, param count: 27528464
Running train benchmark on crossvit_15_240.in1k for 40 steps w/ input size (3, 240, 240) and batch size 256.
Train [8/40]. 1659.65 samples/sec. 154.249 ms/step.
Train [16/40]. 1659.39 samples/sec. 154.274 ms/step.
Train [24/40]. 1659.39 samples/sec. 154.273 ms/step.
Train [32/40]. 1659.43 samples/sec. 154.270 ms/step.
Train [40/40]. 1659.34 samples/sec. 154.278 ms/step.
Train benchmark of crossvit_15_240.in1k done. 1645.27 samples/sec, 154.28 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model crossvit_15_dagger_240.in1k created, param count: 28209008
Running inference benchmark on crossvit_15_dagger_240.in1k for 40 steps w/ input size (3, 240, 240) and batch size 256.
Infer [8/40]. 5595.12 samples/sec. 45.754 ms/step.
Infer [16/40]. 5594.23 samples/sec. 45.761 ms/step.
Infer [24/40]. 5593.72 samples/sec. 45.766 ms/step.
Infer [32/40]. 5593.67 samples/sec. 45.766 ms/step.
Infer [40/40]. 5593.42 samples/sec. 45.768 ms/step.
Inference benchmark of crossvit_15_dagger_240.in1k done. 5590.36 samples/sec, 45.77 ms/step
Model crossvit_15_dagger_240.in1k created, param count: 28209008
Running train benchmark on crossvit_15_dagger_240.in1k for 40 steps w/ input size (3, 240, 240) and batch size 256.
Train [8/40]. 1548.10 samples/sec. 165.364 ms/step.
Train [16/40]. 1548.06 samples/sec. 165.368 ms/step.
Train [24/40]. 1548.01 samples/sec. 165.373 ms/step.
Train [32/40]. 1547.98 samples/sec. 165.377 ms/step.
Train [40/40]. 1547.97 samples/sec. 165.378 ms/step.
Train benchmark of crossvit_15_dagger_240.in1k done. 1534.89 samples/sec, 165.38 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model crossvit_15_dagger_408.in1k created, param count: 28500080
Running inference benchmark on crossvit_15_dagger_408.in1k for 40 steps w/ input size (3, 408, 408) and batch size 256.
Infer [8/40]. 1622.56 samples/sec. 157.775 ms/step.
Infer [16/40]. 1622.45 samples/sec. 157.786 ms/step.
Infer [24/40]. 1622.41 samples/sec. 157.790 ms/step.
Infer [32/40]. 1622.34 samples/sec. 157.797 ms/step.
Infer [40/40]. 1622.32 samples/sec. 157.798 ms/step.
Inference benchmark of crossvit_15_dagger_408.in1k done. 1621.97 samples/sec, 157.80 ms/step
Model crossvit_15_dagger_408.in1k created, param count: 28500080
Running train benchmark on crossvit_15_dagger_408.in1k for 40 steps w/ input size (3, 408, 408) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 110.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 12.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.06 GiB is allocated by PyTorch, and 352.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model crossvit_15_dagger_408.in1k created, param count: 28500080
Running train benchmark on crossvit_15_dagger_408.in1k for 40 steps w/ input size (3, 408, 408) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 82.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 52.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 22.32 GiB is allocated by PyTorch, and 46.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model crossvit_15_dagger_408.in1k created, param count: 28500080
Running train benchmark on crossvit_15_dagger_408.in1k for 40 steps w/ input size (3, 408, 408) and batch size 128.
Train [8/40]. 448.43 samples/sec. 285.439 ms/step.
Train [16/40]. 448.44 samples/sec. 285.435 ms/step.
Train [24/40]. 448.39 samples/sec. 285.467 ms/step.
Train [32/40]. 448.36 samples/sec. 285.487 ms/step.
Train [40/40]. 448.32 samples/sec. 285.509 ms/step.
Train benchmark of crossvit_15_dagger_408.in1k done. 445.67 samples/sec, 285.51 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model crossvit_18_240.in1k created, param count: 43271408
Running inference benchmark on crossvit_18_240.in1k for 40 steps w/ input size (3, 240, 240) and batch size 256.
Infer [8/40]. 4087.66 samples/sec. 62.628 ms/step.
Infer [16/40]. 4087.21 samples/sec. 62.634 ms/step.
Infer [24/40]. 4087.13 samples/sec. 62.636 ms/step.
Infer [32/40]. 4087.18 samples/sec. 62.635 ms/step.
Infer [40/40]. 4087.32 samples/sec. 62.633 ms/step.
Inference benchmark of crossvit_18_240.in1k done. 4085.65 samples/sec, 62.63 ms/step
Model crossvit_18_240.in1k created, param count: 43271408
Running train benchmark on crossvit_18_240.in1k for 40 steps w/ input size (3, 240, 240) and batch size 256.
Train [8/40]. 1108.78 samples/sec. 230.884 ms/step.
Train [16/40]. 1108.65 samples/sec. 230.911 ms/step.
Train [24/40]. 1108.63 samples/sec. 230.917 ms/step.
Train [32/40]. 1108.61 samples/sec. 230.920 ms/step.
Train [40/40]. 1108.60 samples/sec. 230.923 ms/step.
Train benchmark of crossvit_18_240.in1k done. 1100.62 samples/sec, 230.92 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model crossvit_18_dagger_240.in1k created, param count: 44266976
Running inference benchmark on crossvit_18_dagger_240.in1k for 40 steps w/ input size (3, 240, 240) and batch size 256.
Infer [8/40]. 3815.70 samples/sec. 67.091 ms/step.
Infer [16/40]. 3814.97 samples/sec. 67.104 ms/step.
Infer [24/40]. 3814.16 samples/sec. 67.118 ms/step.
Infer [32/40]. 3813.13 samples/sec. 67.136 ms/step.
Infer [40/40]. 3813.00 samples/sec. 67.139 ms/step.
Inference benchmark of crossvit_18_dagger_240.in1k done. 3811.49 samples/sec, 67.14 ms/step
Model crossvit_18_dagger_240.in1k created, param count: 44266976
Running train benchmark on crossvit_18_dagger_240.in1k for 40 steps w/ input size (3, 240, 240) and batch size 256.
Train [8/40]. 1047.42 samples/sec. 244.409 ms/step.
Train [16/40]. 1047.37 samples/sec. 244.422 ms/step.
Train [24/40]. 1047.36 samples/sec. 244.425 ms/step.
Train [32/40]. 1047.36 samples/sec. 244.424 ms/step.
Train [40/40]. 1047.36 samples/sec. 244.424 ms/step.
Train benchmark of crossvit_18_dagger_240.in1k done. 1040.20 samples/sec, 244.42 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model crossvit_18_dagger_408.in1k created, param count: 44606560
Running inference benchmark on crossvit_18_dagger_408.in1k for 40 steps w/ input size (3, 408, 408) and batch size 256.
Infer [8/40]. 1143.99 samples/sec. 223.777 ms/step.
Infer [16/40]. 1143.74 samples/sec. 223.828 ms/step.
Infer [24/40]. 1143.58 samples/sec. 223.859 ms/step.
Infer [32/40]. 1143.46 samples/sec. 223.882 ms/step.
Infer [40/40]. 1143.43 samples/sec. 223.888 ms/step.
Inference benchmark of crossvit_18_dagger_408.in1k done. 1143.23 samples/sec, 223.89 ms/step
Model crossvit_18_dagger_408.in1k created, param count: 44606560
Running train benchmark on crossvit_18_dagger_408.in1k for 40 steps w/ input size (3, 408, 408) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 128.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 114.06 MiB is free. Including non-PyTorch memory, this process has 23.53 GiB memory in use. Of the allocated memory 22.07 GiB is allocated by PyTorch, and 238.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model crossvit_18_dagger_408.in1k created, param count: 44606560
Running train benchmark on crossvit_18_dagger_408.in1k for 40 steps w/ input size (3, 408, 408) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 8.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 21.97 GiB is allocated by PyTorch, and 446.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model crossvit_18_dagger_408.in1k created, param count: 44606560
Running train benchmark on crossvit_18_dagger_408.in1k for 40 steps w/ input size (3, 408, 408) and batch size 128.
Train [8/40]. 313.35 samples/sec. 408.491 ms/step.
Train [16/40]. 313.30 samples/sec. 408.554 ms/step.
Train [24/40]. 313.29 samples/sec. 408.561 ms/step.
Train [32/40]. 313.28 samples/sec. 408.584 ms/step.
Train [40/40]. 313.28 samples/sec. 408.585 ms/step.
Train benchmark of crossvit_18_dagger_408.in1k done. 311.84 samples/sec, 408.58 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model crossvit_base_240.in1k created, param count: 105025232
Running inference benchmark on crossvit_base_240.in1k for 40 steps w/ input size (3, 240, 240) and batch size 256.
Infer [8/40]. 2371.65 samples/sec. 107.942 ms/step.
Infer [16/40]. 2371.96 samples/sec. 107.928 ms/step.
Infer [24/40]. 2372.03 samples/sec. 107.925 ms/step.
Infer [32/40]. 2372.08 samples/sec. 107.922 ms/step.
Infer [40/40]. 2372.11 samples/sec. 107.921 ms/step.
Inference benchmark of crossvit_base_240.in1k done. 2371.50 samples/sec, 107.92 ms/step
Model crossvit_base_240.in1k created, param count: 105025232
Running train benchmark on crossvit_base_240.in1k for 40 steps w/ input size (3, 240, 240) and batch size 256.
Train [8/40]. 684.27 samples/sec. 374.123 ms/step.
Train [16/40]. 684.26 samples/sec. 374.125 ms/step.
Train [24/40]. 684.24 samples/sec. 374.137 ms/step.
Train [32/40]. 684.24 samples/sec. 374.138 ms/step.
Train [40/40]. 684.24 samples/sec. 374.140 ms/step.
Train benchmark of crossvit_base_240.in1k done. 681.33 samples/sec, 374.14 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model crossvit_small_240.in1k created, param count: 26856272
Running inference benchmark on crossvit_small_240.in1k for 40 steps w/ input size (3, 240, 240) and batch size 256.
Infer [8/40]. 6409.18 samples/sec. 39.943 ms/step.
Infer [16/40]. 6408.44 samples/sec. 39.947 ms/step.
Infer [24/40]. 6407.87 samples/sec. 39.951 ms/step.
Infer [32/40]. 6407.64 samples/sec. 39.952 ms/step.
Infer [40/40]. 6407.34 samples/sec. 39.954 ms/step.
Inference benchmark of crossvit_small_240.in1k done. 6403.36 samples/sec, 39.95 ms/step
Model crossvit_small_240.in1k created, param count: 26856272
Running train benchmark on crossvit_small_240.in1k for 40 steps w/ input size (3, 240, 240) and batch size 256.
Train [8/40]. 1761.38 samples/sec. 145.341 ms/step.
Train [16/40]. 1761.38 samples/sec. 145.340 ms/step.
Train [24/40]. 1761.37 samples/sec. 145.342 ms/step.
Train [32/40]. 1761.34 samples/sec. 145.344 ms/step.
Train [40/40]. 1761.29 samples/sec. 145.348 ms/step.
Train benchmark of crossvit_small_240.in1k done. 1746.95 samples/sec, 145.35 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model crossvit_tiny_240.in1k created, param count: 7014800
Running inference benchmark on crossvit_tiny_240.in1k for 40 steps w/ input size (3, 240, 240) and batch size 256.
Infer [8/40]. 14435.41 samples/sec. 17.734 ms/step.
Infer [16/40]. 14421.79 samples/sec. 17.751 ms/step.
Infer [24/40]. 14421.32 samples/sec. 17.751 ms/step.
Infer [32/40]. 14423.32 samples/sec. 17.749 ms/step.
Infer [40/40]. 14427.93 samples/sec. 17.743 ms/step.
Inference benchmark of crossvit_tiny_240.in1k done. 14410.44 samples/sec, 17.74 ms/step
Model crossvit_tiny_240.in1k created, param count: 7014800
Running train benchmark on crossvit_tiny_240.in1k for 40 steps w/ input size (3, 240, 240) and batch size 256.
Train [8/40]. 3468.43 samples/sec. 73.809 ms/step.
Train [16/40]. 3468.52 samples/sec. 73.807 ms/step.
Train [24/40]. 3467.48 samples/sec. 73.829 ms/step.
Train [32/40]. 3468.14 samples/sec. 73.815 ms/step.
Train [40/40]. 3468.17 samples/sec. 73.814 ms/step.
Train benchmark of crossvit_tiny_240.in1k done. 3419.12 samples/sec, 73.81 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model cs3darknet_focus_l.c2ns_in1k created, param count: 21151720
Running inference benchmark on cs3darknet_focus_l.c2ns_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 4074.73 samples/sec. 62.826 ms/step.
Infer [16/40]. 4074.63 samples/sec. 62.828 ms/step.
Infer [24/40]. 4074.62 samples/sec. 62.828 ms/step.
Infer [32/40]. 4074.49 samples/sec. 62.830 ms/step.
Infer [40/40]. 4074.42 samples/sec. 62.831 ms/step.
Inference benchmark of cs3darknet_focus_l.c2ns_in1k done. 4072.79 samples/sec, 62.83 ms/step
Model cs3darknet_focus_l.c2ns_in1k created, param count: 21151720
Running train benchmark on cs3darknet_focus_l.c2ns_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 1086.83 samples/sec. 235.548 ms/step.
Train [16/40]. 1086.79 samples/sec. 235.556 ms/step.
Train [24/40]. 1086.78 samples/sec. 235.559 ms/step.
Train [32/40]. 1086.77 samples/sec. 235.561 ms/step.
Train [40/40]. 1086.75 samples/sec. 235.564 ms/step.
Train benchmark of cs3darknet_focus_l.c2ns_in1k done. 1082.89 samples/sec, 235.56 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model cs3darknet_focus_m.c2ns_in1k created, param count: 9304360
Running inference benchmark on cs3darknet_focus_m.c2ns_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 7345.86 samples/sec. 34.850 ms/step.
Infer [16/40]. 7342.55 samples/sec. 34.865 ms/step.
Infer [24/40]. 7340.96 samples/sec. 34.873 ms/step.
Infer [32/40]. 7340.10 samples/sec. 34.877 ms/step.
Infer [40/40]. 7339.56 samples/sec. 34.879 ms/step.
Inference benchmark of cs3darknet_focus_m.c2ns_in1k done. 7334.22 samples/sec, 34.88 ms/step
Model cs3darknet_focus_m.c2ns_in1k created, param count: 9304360
Running train benchmark on cs3darknet_focus_m.c2ns_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 1815.02 samples/sec. 141.045 ms/step.
Train [16/40]. 1814.95 samples/sec. 141.051 ms/step.
Train [24/40]. 1814.95 samples/sec. 141.051 ms/step.
Train [32/40]. 1814.75 samples/sec. 141.066 ms/step.
Train [40/40]. 1814.22 samples/sec. 141.107 ms/step.
Train benchmark of cs3darknet_focus_m.c2ns_in1k done. 1806.95 samples/sec, 141.11 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model cs3darknet_l.c2ns_in1k created, param count: 21164168
Running inference benchmark on cs3darknet_l.c2ns_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 3860.86 samples/sec. 66.307 ms/step.
Infer [16/40]. 3860.71 samples/sec. 66.309 ms/step.
Infer [24/40]. 3860.68 samples/sec. 66.309 ms/step.
Infer [32/40]. 3860.63 samples/sec. 66.310 ms/step.
Infer [40/40]. 3860.59 samples/sec. 66.311 ms/step.
Inference benchmark of cs3darknet_l.c2ns_in1k done. 3859.09 samples/sec, 66.31 ms/step
Model cs3darknet_l.c2ns_in1k created, param count: 21164168
Running train benchmark on cs3darknet_l.c2ns_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 999.92 samples/sec. 256.022 ms/step.
Train [16/40]. 999.91 samples/sec. 256.022 ms/step.
Train [24/40]. 999.90 samples/sec. 256.025 ms/step.
Train [32/40]. 999.90 samples/sec. 256.026 ms/step.
Train [40/40]. 999.89 samples/sec. 256.027 ms/step.
Train benchmark of cs3darknet_l.c2ns_in1k done. 996.54 samples/sec, 256.03 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model cs3darknet_m.c2ns_in1k created, param count: 9310240
Running inference benchmark on cs3darknet_m.c2ns_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 6831.88 samples/sec. 37.471 ms/step.
Infer [16/40]. 6831.82 samples/sec. 37.472 ms/step.
Infer [24/40]. 6831.65 samples/sec. 37.473 ms/step.
Infer [32/40]. 6831.53 samples/sec. 37.473 ms/step.
Infer [40/40]. 6831.43 samples/sec. 37.474 ms/step.
Inference benchmark of cs3darknet_m.c2ns_in1k done. 6826.90 samples/sec, 37.47 ms/step
Model cs3darknet_m.c2ns_in1k created, param count: 9310240
Running train benchmark on cs3darknet_m.c2ns_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 1601.11 samples/sec. 159.889 ms/step.
Train [16/40]. 1601.14 samples/sec. 159.886 ms/step.
Train [24/40]. 1601.15 samples/sec. 159.885 ms/step.
Train [32/40]. 1601.16 samples/sec. 159.884 ms/step.
Train [40/40]. 1601.16 samples/sec. 159.884 ms/step.
Train benchmark of cs3darknet_m.c2ns_in1k done. 1595.10 samples/sec, 159.88 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model cs3darknet_x.c2ns_in1k created, param count: 35046320
Running inference benchmark on cs3darknet_x.c2ns_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 2485.04 samples/sec. 103.016 ms/step.
Infer [16/40]. 2484.89 samples/sec. 103.023 ms/step.
Infer [24/40]. 2484.76 samples/sec. 103.028 ms/step.
Infer [32/40]. 2484.74 samples/sec. 103.029 ms/step.
Infer [40/40]. 2484.66 samples/sec. 103.032 ms/step.
Inference benchmark of cs3darknet_x.c2ns_in1k done. 2483.98 samples/sec, 103.03 ms/step
Model cs3darknet_x.c2ns_in1k created, param count: 35046320
Running train benchmark on cs3darknet_x.c2ns_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 685.76 samples/sec. 373.307 ms/step.
Train [16/40]. 685.75 samples/sec. 373.314 ms/step.
Train [24/40]. 685.77 samples/sec. 373.304 ms/step.
Train [32/40]. 685.74 samples/sec. 373.321 ms/step.
Train [40/40]. 685.71 samples/sec. 373.336 ms/step.
Train benchmark of cs3darknet_x.c2ns_in1k done. 683.92 samples/sec, 373.34 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model cs3edgenet_x.c2_in1k created, param count: 47821120
Running inference benchmark on cs3edgenet_x.c2_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 2080.10 samples/sec. 123.071 ms/step.
Infer [16/40]. 2080.02 samples/sec. 123.076 ms/step.
Infer [24/40]. 2080.00 samples/sec. 123.077 ms/step.
Infer [32/40]. 2079.94 samples/sec. 123.081 ms/step.
Infer [40/40]. 2079.99 samples/sec. 123.077 ms/step.
Inference benchmark of cs3edgenet_x.c2_in1k done. 2079.50 samples/sec, 123.08 ms/step
Model cs3edgenet_x.c2_in1k created, param count: 47821120
Running train benchmark on cs3edgenet_x.c2_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 76.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 38.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 22.06 GiB is allocated by PyTorch, and 330.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model cs3edgenet_x.c2_in1k created, param count: 47821120
Running train benchmark on cs3edgenet_x.c2_in1k for 40 steps w/ input size (3, 288, 288) and batch size 192.
Train [8/40]. 587.01 samples/sec. 327.083 ms/step.
Train [16/40]. 586.99 samples/sec. 327.093 ms/step.
Train [24/40]. 586.98 samples/sec. 327.098 ms/step.
Train [32/40]. 586.99 samples/sec. 327.093 ms/step.
Train [40/40]. 586.97 samples/sec. 327.101 ms/step.
Train benchmark of cs3edgenet_x.c2_in1k done. 585.25 samples/sec, 327.10 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model cs3se_edgenet_x.c2ns_in1k created, param count: 50721584
Running inference benchmark on cs3se_edgenet_x.c2ns_in1k for 40 steps w/ input size (3, 320, 320) and batch size 256.
Infer [8/40]. 1586.75 samples/sec. 161.336 ms/step.
Infer [16/40]. 1586.69 samples/sec. 161.342 ms/step.
Infer [24/40]. 1586.64 samples/sec. 161.347 ms/step.
Infer [32/40]. 1586.64 samples/sec. 161.347 ms/step.
Infer [40/40]. 1586.64 samples/sec. 161.347 ms/step.
Inference benchmark of cs3se_edgenet_x.c2ns_in1k done. 1586.32 samples/sec, 161.35 ms/step
Model cs3se_edgenet_x.c2ns_in1k created, param count: 50721584
Running train benchmark on cs3se_edgenet_x.c2ns_in1k for 40 steps w/ input size (3, 320, 320) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 188.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 58.06 MiB is free. Including non-PyTorch memory, this process has 23.58 GiB memory in use. Of the allocated memory 22.14 GiB is allocated by PyTorch, and 224.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model cs3se_edgenet_x.c2ns_in1k created, param count: 50721584
Running train benchmark on cs3se_edgenet_x.c2ns_in1k for 40 steps w/ input size (3, 320, 320) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 24.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 21.95 GiB is allocated by PyTorch, and 453.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model cs3se_edgenet_x.c2ns_in1k created, param count: 50721584
Running train benchmark on cs3se_edgenet_x.c2ns_in1k for 40 steps w/ input size (3, 320, 320) and batch size 128.
Train [8/40]. 441.62 samples/sec. 289.839 ms/step.
Train [16/40]. 441.59 samples/sec. 289.865 ms/step.
Train [24/40]. 441.46 samples/sec. 289.947 ms/step.
Train [32/40]. 441.40 samples/sec. 289.986 ms/step.
Train [40/40]. 441.37 samples/sec. 290.009 ms/step.
Train benchmark of cs3se_edgenet_x.c2ns_in1k done. 439.40 samples/sec, 290.01 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model cs3sedarknet_l.c2ns_in1k created, param count: 21913592
Running inference benchmark on cs3sedarknet_l.c2ns_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 3617.70 samples/sec. 70.763 ms/step.
Infer [16/40]. 3617.49 samples/sec. 70.767 ms/step.
Infer [24/40]. 3617.63 samples/sec. 70.765 ms/step.
Infer [32/40]. 3617.62 samples/sec. 70.765 ms/step.
Infer [40/40]. 3617.58 samples/sec. 70.766 ms/step.
Inference benchmark of cs3sedarknet_l.c2ns_in1k done. 3616.20 samples/sec, 70.77 ms/step
Model cs3sedarknet_l.c2ns_in1k created, param count: 21913592
Running train benchmark on cs3sedarknet_l.c2ns_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 927.09 samples/sec. 276.134 ms/step.
Train [16/40]. 927.13 samples/sec. 276.120 ms/step.
Train [24/40]. 927.14 samples/sec. 276.118 ms/step.
Train [32/40]. 927.15 samples/sec. 276.115 ms/step.
Train [40/40]. 927.15 samples/sec. 276.114 ms/step.
Train benchmark of cs3sedarknet_l.c2ns_in1k done. 923.21 samples/sec, 276.11 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model cs3sedarknet_x.c2ns_in1k created, param count: 35397904
Running inference benchmark on cs3sedarknet_x.c2ns_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 2339.81 samples/sec. 109.411 ms/step.
Infer [16/40]. 2339.72 samples/sec. 109.415 ms/step.
Infer [24/40]. 2339.72 samples/sec. 109.415 ms/step.
Infer [32/40]. 2339.55 samples/sec. 109.423 ms/step.
Infer [40/40]. 2339.34 samples/sec. 109.433 ms/step.
Inference benchmark of cs3sedarknet_x.c2ns_in1k done. 2338.71 samples/sec, 109.43 ms/step
Model cs3sedarknet_x.c2ns_in1k created, param count: 35397904
Running train benchmark on cs3sedarknet_x.c2ns_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 102.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 34.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 21.90 GiB is allocated by PyTorch, and 495.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model cs3sedarknet_x.c2ns_in1k created, param count: 35397904
Running train benchmark on cs3sedarknet_x.c2ns_in1k for 40 steps w/ input size (3, 288, 288) and batch size 192.
Train [8/40]. 643.97 samples/sec. 298.149 ms/step.
Train [16/40]. 643.97 samples/sec. 298.150 ms/step.
Train [24/40]. 643.98 samples/sec. 298.147 ms/step.
Train [32/40]. 643.98 samples/sec. 298.145 ms/step.
Train [40/40]. 643.97 samples/sec. 298.148 ms/step.
Train benchmark of cs3sedarknet_x.c2ns_in1k done. 641.19 samples/sec, 298.15 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model cspdarknet53.ra_in1k created, param count: 27642184
Running inference benchmark on cspdarknet53.ra_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
Infer [8/40]. 2879.36 samples/sec. 88.909 ms/step.
Infer [16/40]. 2879.09 samples/sec. 88.917 ms/step.
Infer [24/40]. 2878.96 samples/sec. 88.921 ms/step.
Infer [32/40]. 2878.93 samples/sec. 88.922 ms/step.
Infer [40/40]. 2878.88 samples/sec. 88.923 ms/step.
Inference benchmark of cspdarknet53.ra_in1k done. 2877.98 samples/sec, 88.92 ms/step
Model cspdarknet53.ra_in1k created, param count: 27642184
Running train benchmark on cspdarknet53.ra_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
Train [8/40]. 747.63 samples/sec. 342.413 ms/step.
Train [16/40]. 747.68 samples/sec. 342.394 ms/step.
Train [24/40]. 747.69 samples/sec. 342.390 ms/step.
Train [32/40]. 747.69 samples/sec. 342.388 ms/step.
Train [40/40]. 747.70 samples/sec. 342.383 ms/step.
Train benchmark of cspdarknet53.ra_in1k done. 745.44 samples/sec, 342.38 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model cspresnet50.ra_in1k created, param count: 21616168
Running inference benchmark on cspresnet50.ra_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
Infer [8/40]. 4484.72 samples/sec. 57.083 ms/step.
Infer [16/40]. 4484.87 samples/sec. 57.081 ms/step.
Infer [24/40]. 4484.52 samples/sec. 57.085 ms/step.
Infer [32/40]. 4483.80 samples/sec. 57.094 ms/step.
Infer [40/40]. 4483.89 samples/sec. 57.093 ms/step.
Inference benchmark of cspresnet50.ra_in1k done. 4481.80 samples/sec, 57.09 ms/step
Model cspresnet50.ra_in1k created, param count: 21616168
Running train benchmark on cspresnet50.ra_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
Train [8/40]. 1283.65 samples/sec. 199.432 ms/step.
Train [16/40]. 1283.67 samples/sec. 199.428 ms/step.
Train [24/40]. 1283.66 samples/sec. 199.429 ms/step.
Train [32/40]. 1283.67 samples/sec. 199.429 ms/step.
Train [40/40]. 1283.63 samples/sec. 199.434 ms/step.
Train benchmark of cspresnet50.ra_in1k done. 1278.48 samples/sec, 199.43 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model cspresnext50.ra_in1k created, param count: 20569896
Running inference benchmark on cspresnext50.ra_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
Infer [8/40]. 3454.13 samples/sec. 74.114 ms/step.
Infer [16/40]. 3454.07 samples/sec. 74.115 ms/step.
Infer [24/40]. 3453.72 samples/sec. 74.123 ms/step.
Infer [32/40]. 3453.54 samples/sec. 74.127 ms/step.
Infer [40/40]. 3453.46 samples/sec. 74.128 ms/step.
Inference benchmark of cspresnext50.ra_in1k done. 3452.21 samples/sec, 74.13 ms/step
Model cspresnext50.ra_in1k created, param count: 20569896
Running train benchmark on cspresnext50.ra_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
Train [8/40]. 1032.72 samples/sec. 247.890 ms/step.
Train [16/40]. 1032.71 samples/sec. 247.891 ms/step.
Train [24/40]. 1032.75 samples/sec. 247.882 ms/step.
Train [32/40]. 1032.59 samples/sec. 247.919 ms/step.
Train [40/40]. 1032.53 samples/sec. 247.934 ms/step.
Train benchmark of cspresnext50.ra_in1k done. 1029.00 samples/sec, 247.93 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model darknet53.c2ns_in1k created, param count: 41609928
Running inference benchmark on darknet53.c2ns_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 2368.34 samples/sec. 108.093 ms/step.
Infer [16/40]. 2368.31 samples/sec. 108.094 ms/step.
Infer [24/40]. 2368.26 samples/sec. 108.096 ms/step.
Infer [32/40]. 2368.23 samples/sec. 108.097 ms/step.
Infer [40/40]. 2368.25 samples/sec. 108.097 ms/step.
Inference benchmark of darknet53.c2ns_in1k done. 2367.63 samples/sec, 108.10 ms/step
Model darknet53.c2ns_in1k created, param count: 41609928
Running train benchmark on darknet53.c2ns_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 638.85 samples/sec. 400.718 ms/step.
Train [16/40]. 638.98 samples/sec. 400.636 ms/step.
Train [24/40]. 638.94 samples/sec. 400.666 ms/step.
Train [32/40]. 638.94 samples/sec. 400.663 ms/step.
Train [40/40]. 638.93 samples/sec. 400.669 ms/step.
Train benchmark of darknet53.c2ns_in1k done. 637.49 samples/sec, 400.67 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model darknetaa53.c2ns_in1k created, param count: 36022984
Running inference benchmark on darknetaa53.c2ns_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 2567.99 samples/sec. 99.689 ms/step.
Infer [16/40]. 2567.93 samples/sec. 99.691 ms/step.
Infer [24/40]. 2567.88 samples/sec. 99.693 ms/step.
Infer [32/40]. 2567.85 samples/sec. 99.694 ms/step.
Infer [40/40]. 2567.83 samples/sec. 99.695 ms/step.
Inference benchmark of darknetaa53.c2ns_in1k done. 2567.10 samples/sec, 99.69 ms/step
Model darknetaa53.c2ns_in1k created, param count: 36022984
Running train benchmark on darknetaa53.c2ns_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 672.01 samples/sec. 380.948 ms/step.
Train [16/40]. 672.01 samples/sec. 380.948 ms/step.
Train [24/40]. 672.00 samples/sec. 380.953 ms/step.
Train [32/40]. 671.99 samples/sec. 380.955 ms/step.
Train [40/40]. 671.99 samples/sec. 380.955 ms/step.
Train benchmark of darknetaa53.c2ns_in1k done. 670.39 samples/sec, 380.95 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model davit_base.msft_in1k created, param count: 87954408
Running inference benchmark on davit_base.msft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1759.97 samples/sec. 145.457 ms/step.
Infer [16/40]. 1759.58 samples/sec. 145.489 ms/step.
Infer [24/40]. 1759.55 samples/sec. 145.492 ms/step.
Infer [32/40]. 1759.60 samples/sec. 145.488 ms/step.
Infer [40/40]. 1759.69 samples/sec. 145.480 ms/step.
Inference benchmark of davit_base.msft_in1k done. 1759.31 samples/sec, 145.48 ms/step
Model davit_base.msft_in1k created, param count: 87954408
Running train benchmark on davit_base.msft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 2.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 22.40 GiB is allocated by PyTorch, and 14.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model davit_base.msft_in1k created, param count: 87954408
Running train benchmark on davit_base.msft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
Train [8/40]. 619.14 samples/sec. 310.108 ms/step.
Train [16/40]. 619.06 samples/sec. 310.149 ms/step.
Train [24/40]. 619.05 samples/sec. 310.154 ms/step.
Train [32/40]. 619.02 samples/sec. 310.168 ms/step.
Train [40/40]. 619.03 samples/sec. 310.162 ms/step.
Train benchmark of davit_base.msft_in1k done. 614.86 samples/sec, 310.16 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model davit_small.msft_in1k created, param count: 49745896
Running inference benchmark on davit_small.msft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 2588.96 samples/sec. 98.882 ms/step.
Infer [16/40]. 2589.11 samples/sec. 98.876 ms/step.
Infer [24/40]. 2589.06 samples/sec. 98.877 ms/step.
Infer [32/40]. 2589.05 samples/sec. 98.878 ms/step.
Infer [40/40]. 2588.99 samples/sec. 98.880 ms/step.
Inference benchmark of davit_small.msft_in1k done. 2588.25 samples/sec, 98.88 ms/step
Model davit_small.msft_in1k created, param count: 49745896
Running train benchmark on davit_small.msft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 857.08 samples/sec. 298.689 ms/step.
Train [16/40]. 857.03 samples/sec. 298.705 ms/step.
Train [24/40]. 857.04 samples/sec. 298.704 ms/step.
Train [32/40]. 857.06 samples/sec. 298.695 ms/step.
Train [40/40]. 857.10 samples/sec. 298.682 ms/step.
Train benchmark of davit_small.msft_in1k done. 851.37 samples/sec, 298.68 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model davit_tiny.msft_in1k created, param count: 28360168
Running inference benchmark on davit_tiny.msft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 4056.52 samples/sec. 63.108 ms/step.
Infer [16/40]. 4056.19 samples/sec. 63.113 ms/step.
Infer [24/40]. 4055.76 samples/sec. 63.120 ms/step.
Infer [32/40]. 4055.67 samples/sec. 63.122 ms/step.
Infer [40/40]. 4055.56 samples/sec. 63.123 ms/step.
Inference benchmark of davit_tiny.msft_in1k done. 4053.83 samples/sec, 63.12 ms/step
Model davit_tiny.msft_in1k created, param count: 28360168
Running train benchmark on davit_tiny.msft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1327.87 samples/sec. 192.790 ms/step.
Train [16/40]. 1327.69 samples/sec. 192.816 ms/step.
Train [24/40]. 1327.63 samples/sec. 192.825 ms/step.
Train [32/40]. 1327.59 samples/sec. 192.830 ms/step.
Train [40/40]. 1327.58 samples/sec. 192.833 ms/step.
Train benchmark of davit_tiny.msft_in1k done. 1319.87 samples/sec, 192.83 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model deit3_base_patch16_224.fb_in1k created, param count: 86585320
Running inference benchmark on deit3_base_patch16_224.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3077.26 samples/sec. 83.191 ms/step.
Infer [16/40]. 3077.13 samples/sec. 83.194 ms/step.
Infer [24/40]. 3077.09 samples/sec. 83.195 ms/step.
Infer [32/40]. 3077.13 samples/sec. 83.194 ms/step.
Infer [40/40]. 3077.10 samples/sec. 83.195 ms/step.
Inference benchmark of deit3_base_patch16_224.fb_in1k done. 3076.12 samples/sec, 83.19 ms/step
Model deit3_base_patch16_224.fb_in1k created, param count: 86585320
Running train benchmark on deit3_base_patch16_224.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 917.53 samples/sec. 279.009 ms/step.
Train [16/40]. 917.53 samples/sec. 279.009 ms/step.
Train [24/40]. 917.50 samples/sec. 279.019 ms/step.
Train [32/40]. 917.45 samples/sec. 279.035 ms/step.
Train [40/40]. 917.43 samples/sec. 279.039 ms/step.
Train benchmark of deit3_base_patch16_224.fb_in1k done. 914.13 samples/sec, 279.04 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model deit3_base_patch16_224.fb_in22k_ft_in1k created, param count: 86585320
Running inference benchmark on deit3_base_patch16_224.fb_in22k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3079.35 samples/sec. 83.135 ms/step.
Infer [16/40]. 3079.37 samples/sec. 83.134 ms/step.
Infer [24/40]. 3079.33 samples/sec. 83.135 ms/step.
Infer [32/40]. 3079.34 samples/sec. 83.135 ms/step.
Infer [40/40]. 3079.25 samples/sec. 83.137 ms/step.
Inference benchmark of deit3_base_patch16_224.fb_in22k_ft_in1k done. 3078.29 samples/sec, 83.14 ms/step
Model deit3_base_patch16_224.fb_in22k_ft_in1k created, param count: 86585320
Running train benchmark on deit3_base_patch16_224.fb_in22k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 917.64 samples/sec. 278.977 ms/step.
Train [16/40]. 917.67 samples/sec. 278.968 ms/step.
Train [24/40]. 917.68 samples/sec. 278.964 ms/step.
Train [32/40]. 917.68 samples/sec. 278.964 ms/step.
Train [40/40]. 917.68 samples/sec. 278.963 ms/step.
Train benchmark of deit3_base_patch16_224.fb_in22k_ft_in1k done. 914.41 samples/sec, 278.96 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model deit3_base_patch16_384.fb_in1k created, param count: 86877160
Running inference benchmark on deit3_base_patch16_384.fb_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 966.00 samples/sec. 265.010 ms/step.
Infer [16/40]. 965.86 samples/sec. 265.048 ms/step.
Infer [24/40]. 965.86 samples/sec. 265.048 ms/step.
Infer [32/40]. 965.72 samples/sec. 265.086 ms/step.
Infer [40/40]. 965.57 samples/sec. 265.129 ms/step.
Inference benchmark of deit3_base_patch16_384.fb_in1k done. 965.41 samples/sec, 265.13 ms/step
Model deit3_base_patch16_384.fb_in1k created, param count: 86877160
Running train benchmark on deit3_base_patch16_384.fb_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 866.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 112.06 MiB is free. Including non-PyTorch memory, this process has 23.53 GiB memory in use. Of the allocated memory 22.22 GiB is allocated by PyTorch, and 87.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model deit3_base_patch16_384.fb_in1k created, param count: 86877160
Running train benchmark on deit3_base_patch16_384.fb_in1k for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 650.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 240.06 MiB is free. Including non-PyTorch memory, this process has 23.41 GiB memory in use. Of the allocated memory 21.80 GiB is allocated by PyTorch, and 391.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model deit3_base_patch16_384.fb_in1k created, param count: 86877160
Running train benchmark on deit3_base_patch16_384.fb_in1k for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 110.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 62.06 MiB is free. Including non-PyTorch memory, this process has 23.58 GiB memory in use. Of the allocated memory 22.10 GiB is allocated by PyTorch, and 256.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model deit3_base_patch16_384.fb_in1k created, param count: 86877160
Running train benchmark on deit3_base_patch16_384.fb_in1k for 40 steps w/ input size (3, 384, 384) and batch size 96.
Train [8/40]. 286.78 samples/sec. 334.756 ms/step.
Train [16/40]. 286.80 samples/sec. 334.726 ms/step.
Train [24/40]. 286.80 samples/sec. 334.723 ms/step.
Train [32/40]. 286.80 samples/sec. 334.725 ms/step.
Train [40/40]. 286.79 samples/sec. 334.736 ms/step.
Train benchmark of deit3_base_patch16_384.fb_in1k done. 285.90 samples/sec, 334.74 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model deit3_base_patch16_384.fb_in22k_ft_in1k created, param count: 86877160
Running inference benchmark on deit3_base_patch16_384.fb_in22k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 964.62 samples/sec. 265.388 ms/step.
Infer [16/40]. 964.43 samples/sec. 265.441 ms/step.
Infer [24/40]. 964.16 samples/sec. 265.517 ms/step.
Infer [32/40]. 963.98 samples/sec. 265.565 ms/step.
Infer [40/40]. 963.92 samples/sec. 265.583 ms/step.
Inference benchmark of deit3_base_patch16_384.fb_in22k_ft_in1k done. 963.77 samples/sec, 265.58 ms/step
Model deit3_base_patch16_384.fb_in22k_ft_in1k created, param count: 86877160
Running train benchmark on deit3_base_patch16_384.fb_in22k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 866.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 112.06 MiB is free. Including non-PyTorch memory, this process has 23.53 GiB memory in use. Of the allocated memory 22.22 GiB is allocated by PyTorch, and 87.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model deit3_base_patch16_384.fb_in22k_ft_in1k created, param count: 86877160
Running train benchmark on deit3_base_patch16_384.fb_in22k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 650.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 240.06 MiB is free. Including non-PyTorch memory, this process has 23.41 GiB memory in use. Of the allocated memory 21.80 GiB is allocated by PyTorch, and 391.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model deit3_base_patch16_384.fb_in22k_ft_in1k created, param count: 86877160
Running train benchmark on deit3_base_patch16_384.fb_in22k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 110.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 62.06 MiB is free. Including non-PyTorch memory, this process has 23.58 GiB memory in use. Of the allocated memory 22.10 GiB is allocated by PyTorch, and 256.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model deit3_base_patch16_384.fb_in22k_ft_in1k created, param count: 86877160
Running train benchmark on deit3_base_patch16_384.fb_in22k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 96.
Train [8/40]. 286.78 samples/sec. 334.750 ms/step.
Train [16/40]. 286.77 samples/sec. 334.758 ms/step.
Train [24/40]. 286.77 samples/sec. 334.766 ms/step.
Train [32/40]. 286.75 samples/sec. 334.784 ms/step.
Train [40/40]. 286.74 samples/sec. 334.798 ms/step.
Train benchmark of deit3_base_patch16_384.fb_in22k_ft_in1k done. 285.85 samples/sec, 334.80 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model deit3_huge_patch14_224.fb_in1k created, param count: 632126440
Running inference benchmark on deit3_huge_patch14_224.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 365.73 samples/sec. 699.977 ms/step.
Infer [16/40]. 365.58 samples/sec. 700.249 ms/step.
Infer [24/40]. 365.45 samples/sec. 700.515 ms/step.
Infer [32/40]. 365.39 samples/sec. 700.619 ms/step.
Infer [40/40]. 365.31 samples/sec. 700.783 ms/step.
Inference benchmark of deit3_huge_patch14_224.fb_in1k done. 365.28 samples/sec, 700.78 ms/step
Model deit3_huge_patch14_224.fb_in1k created, param count: 632126440
Running train benchmark on deit3_huge_patch14_224.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 162.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 138.06 MiB is free. Including non-PyTorch memory, this process has 23.51 GiB memory in use. Of the allocated memory 22.02 GiB is allocated by PyTorch, and 270.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model deit3_huge_patch14_224.fb_in1k created, param count: 632126440
Running train benchmark on deit3_huge_patch14_224.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 482.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 74.06 MiB is free. Including non-PyTorch memory, this process has 23.57 GiB memory in use. Of the allocated memory 21.88 GiB is allocated by PyTorch, and 468.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model deit3_huge_patch14_224.fb_in1k created, param count: 632126440
Running train benchmark on deit3_huge_patch14_224.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 322.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 98.06 MiB is free. Including non-PyTorch memory, this process has 23.54 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 246.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model deit3_huge_patch14_224.fb_in1k created, param count: 632126440
Running train benchmark on deit3_huge_patch14_224.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 62.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 14.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 623.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model deit3_huge_patch14_224.fb_in1k created, param count: 632126440
Running train benchmark on deit3_huge_patch14_224.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 122.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 40.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 21.81 GiB is allocated by PyTorch, and 581.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model deit3_huge_patch14_224.fb_in1k created, param count: 632126440
Running train benchmark on deit3_huge_patch14_224.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 48.
Train [8/40]. 105.10 samples/sec. 456.692 ms/step.
Train [16/40]. 105.11 samples/sec. 456.675 ms/step.
Train [24/40]. 105.11 samples/sec. 456.684 ms/step.
Train [32/40]. 105.10 samples/sec. 456.713 ms/step.
Train [40/40]. 105.09 samples/sec. 456.733 ms/step.
Train benchmark of deit3_huge_patch14_224.fb_in1k done. 104.60 samples/sec, 456.73 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model deit3_huge_patch14_224.fb_in22k_ft_in1k created, param count: 632126440
Running inference benchmark on deit3_huge_patch14_224.fb_in22k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 366.03 samples/sec. 699.391 ms/step.
Infer [16/40]. 365.89 samples/sec. 699.663 ms/step.
Infer [24/40]. 365.81 samples/sec. 699.810 ms/step.
Infer [32/40]. 365.74 samples/sec. 699.949 ms/step.
Infer [40/40]. 365.68 samples/sec. 700.061 ms/step.
Inference benchmark of deit3_huge_patch14_224.fb_in22k_ft_in1k done. 365.66 samples/sec, 700.06 ms/step
Model deit3_huge_patch14_224.fb_in22k_ft_in1k created, param count: 632126440
Running train benchmark on deit3_huge_patch14_224.fb_in22k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 162.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 138.06 MiB is free. Including non-PyTorch memory, this process has 23.51 GiB memory in use. Of the allocated memory 22.02 GiB is allocated by PyTorch, and 270.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model deit3_huge_patch14_224.fb_in22k_ft_in1k created, param count: 632126440
Running train benchmark on deit3_huge_patch14_224.fb_in22k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 482.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 60.06 MiB is free. Including non-PyTorch memory, this process has 23.58 GiB memory in use. Of the allocated memory 21.88 GiB is allocated by PyTorch, and 482.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model deit3_huge_patch14_224.fb_in22k_ft_in1k created, param count: 632126440
Running train benchmark on deit3_huge_patch14_224.fb_in22k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 322.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 84.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 260.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model deit3_huge_patch14_224.fb_in22k_ft_in1k created, param count: 632126440
Running train benchmark on deit3_huge_patch14_224.fb_in22k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 62.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 14.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 623.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model deit3_huge_patch14_224.fb_in22k_ft_in1k created, param count: 632126440
Running train benchmark on deit3_huge_patch14_224.fb_in22k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 122.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 40.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 21.81 GiB is allocated by PyTorch, and 581.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model deit3_huge_patch14_224.fb_in22k_ft_in1k created, param count: 632126440
Running train benchmark on deit3_huge_patch14_224.fb_in22k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 48.
Train [8/40]. 105.25 samples/sec. 456.051 ms/step.
Train [16/40]. 105.25 samples/sec. 456.055 ms/step.
Train [24/40]. 105.25 samples/sec. 456.054 ms/step.
Train [32/40]. 105.20 samples/sec. 456.260 ms/step.
Train [40/40]. 105.17 samples/sec. 456.384 ms/step.
Train benchmark of deit3_huge_patch14_224.fb_in22k_ft_in1k done. 104.68 samples/sec, 456.38 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model deit3_large_patch16_224.fb_in1k created, param count: 304374760
Running inference benchmark on deit3_large_patch16_224.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 985.76 samples/sec. 259.698 ms/step.
Infer [16/40]. 985.52 samples/sec. 259.763 ms/step.
Infer [24/40]. 985.39 samples/sec. 259.795 ms/step.
Infer [32/40]. 985.33 samples/sec. 259.812 ms/step.
Infer [40/40]. 985.21 samples/sec. 259.843 ms/step.
Inference benchmark of deit3_large_patch16_224.fb_in1k done. 985.04 samples/sec, 259.84 ms/step
Model deit3_large_patch16_224.fb_in1k created, param count: 304374760
Running train benchmark on deit3_large_patch16_224.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 2.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 22.26 GiB is allocated by PyTorch, and 153.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model deit3_large_patch16_224.fb_in1k created, param count: 304374760
Running train benchmark on deit3_large_patch16_224.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 296.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 132.06 MiB is free. Including non-PyTorch memory, this process has 23.51 GiB memory in use. Of the allocated memory 22.15 GiB is allocated by PyTorch, and 133.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model deit3_large_patch16_224.fb_in1k created, param count: 304374760
Running train benchmark on deit3_large_patch16_224.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 8.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.24 GiB is allocated by PyTorch, and 168.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model deit3_large_patch16_224.fb_in1k created, param count: 304374760
Running train benchmark on deit3_large_patch16_224.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 96.
Train [8/40]. 289.53 samples/sec. 331.567 ms/step.
Train [16/40]. 289.53 samples/sec. 331.569 ms/step.
Train [24/40]. 289.53 samples/sec. 331.573 ms/step.
Train [32/40]. 289.53 samples/sec. 331.571 ms/step.
Train [40/40]. 289.53 samples/sec. 331.568 ms/step.
Train benchmark of deit3_large_patch16_224.fb_in1k done. 288.09 samples/sec, 331.57 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model deit3_large_patch16_224.fb_in22k_ft_in1k created, param count: 304374760
Running inference benchmark on deit3_large_patch16_224.fb_in22k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 985.58 samples/sec. 259.744 ms/step.
Infer [16/40]. 985.41 samples/sec. 259.791 ms/step.
Infer [24/40]. 985.17 samples/sec. 259.855 ms/step.
Infer [32/40]. 985.00 samples/sec. 259.898 ms/step.
Infer [40/40]. 984.94 samples/sec. 259.914 ms/step.
Inference benchmark of deit3_large_patch16_224.fb_in22k_ft_in1k done. 984.78 samples/sec, 259.91 ms/step
Model deit3_large_patch16_224.fb_in22k_ft_in1k created, param count: 304374760
Running train benchmark on deit3_large_patch16_224.fb_in22k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 2.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 22.26 GiB is allocated by PyTorch, and 153.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model deit3_large_patch16_224.fb_in22k_ft_in1k created, param count: 304374760
Running train benchmark on deit3_large_patch16_224.fb_in22k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 296.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 132.06 MiB is free. Including non-PyTorch memory, this process has 23.51 GiB memory in use. Of the allocated memory 22.15 GiB is allocated by PyTorch, and 133.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model deit3_large_patch16_224.fb_in22k_ft_in1k created, param count: 304374760
Running train benchmark on deit3_large_patch16_224.fb_in22k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 198.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 138.06 MiB is free. Including non-PyTorch memory, this process has 23.51 GiB memory in use. Of the allocated memory 21.94 GiB is allocated by PyTorch, and 347.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model deit3_large_patch16_224.fb_in22k_ft_in1k created, param count: 304374760
Running train benchmark on deit3_large_patch16_224.fb_in22k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 96.
Train [8/40]. 289.43 samples/sec. 331.682 ms/step.
Train [16/40]. 289.39 samples/sec. 331.729 ms/step.
Train [24/40]. 289.39 samples/sec. 331.732 ms/step.
Train [32/40]. 289.38 samples/sec. 331.741 ms/step.
Train [40/40]. 289.37 samples/sec. 331.752 ms/step.
Train benchmark of deit3_large_patch16_224.fb_in22k_ft_in1k done. 287.93 samples/sec, 331.75 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model deit3_large_patch16_384.fb_in1k created, param count: 304763880
Running inference benchmark on deit3_large_patch16_384.fb_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 311.61 samples/sec. 821.547 ms/step.
Infer [16/40]. 311.32 samples/sec. 822.293 ms/step.
Infer [24/40]. 311.10 samples/sec. 822.895 ms/step.
Infer [32/40]. 310.94 samples/sec. 823.316 ms/step.
Infer [40/40]. 310.85 samples/sec. 823.555 ms/step.
Inference benchmark of deit3_large_patch16_384.fb_in1k done. 310.83 samples/sec, 823.55 ms/step
Model deit3_large_patch16_384.fb_in1k created, param count: 304763880
Running train benchmark on deit3_large_patch16_384.fb_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 866.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 420.06 MiB is free. Including non-PyTorch memory, this process has 23.23 GiB memory in use. Of the allocated memory 21.69 GiB is allocated by PyTorch, and 318.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model deit3_large_patch16_384.fb_in1k created, param count: 304763880
Running train benchmark on deit3_large_patch16_384.fb_in1k for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 866.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 442.06 MiB is free. Including non-PyTorch memory, this process has 23.21 GiB memory in use. Of the allocated memory 21.72 GiB is allocated by PyTorch, and 266.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model deit3_large_patch16_384.fb_in1k created, param count: 304763880
Running train benchmark on deit3_large_patch16_384.fb_in1k for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 146.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 32.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 22.17 GiB is allocated by PyTorch, and 218.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model deit3_large_patch16_384.fb_in1k created, param count: 304763880
Running train benchmark on deit3_large_patch16_384.fb_in1k for 40 steps w/ input size (3, 384, 384) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 326.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 132.06 MiB is free. Including non-PyTorch memory, this process has 23.51 GiB memory in use. Of the allocated memory 21.85 GiB is allocated by PyTorch, and 450.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model deit3_large_patch16_384.fb_in1k created, param count: 304763880
Running train benchmark on deit3_large_patch16_384.fb_in1k for 40 steps w/ input size (3, 384, 384) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 290.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 276.06 MiB is free. Including non-PyTorch memory, this process has 23.37 GiB memory in use. Of the allocated memory 21.61 GiB is allocated by PyTorch, and 549.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model deit3_large_patch16_384.fb_in1k created, param count: 304763880
Running train benchmark on deit3_large_patch16_384.fb_in1k for 40 steps w/ input size (3, 384, 384) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 30.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 21.86 GiB is allocated by PyTorch, and 539.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model deit3_large_patch16_384.fb_in1k created, param count: 304763880
Running train benchmark on deit3_large_patch16_384.fb_in1k for 40 steps w/ input size (3, 384, 384) and batch size 32.
Train [8/40]. 90.93 samples/sec. 351.920 ms/step.
Train [16/40]. 90.93 samples/sec. 351.912 ms/step.
Train [24/40]. 90.93 samples/sec. 351.913 ms/step.
Train [32/40]. 90.93 samples/sec. 351.919 ms/step.
Train [40/40]. 90.93 samples/sec. 351.920 ms/step.
Train benchmark of deit3_large_patch16_384.fb_in1k done. 90.49 samples/sec, 351.92 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model deit3_large_patch16_384.fb_in22k_ft_in1k created, param count: 304763880
Running inference benchmark on deit3_large_patch16_384.fb_in22k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 311.58 samples/sec. 821.610 ms/step.
Infer [16/40]. 311.18 samples/sec. 822.672 ms/step.
Infer [24/40]. 311.04 samples/sec. 823.032 ms/step.
Infer [32/40]. 310.92 samples/sec. 823.355 ms/step.
Infer [40/40]. 310.84 samples/sec. 823.582 ms/step.
Inference benchmark of deit3_large_patch16_384.fb_in22k_ft_in1k done. 310.82 samples/sec, 823.58 ms/step
Model deit3_large_patch16_384.fb_in22k_ft_in1k created, param count: 304763880
Running train benchmark on deit3_large_patch16_384.fb_in22k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 866.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 420.06 MiB is free. Including non-PyTorch memory, this process has 23.23 GiB memory in use. Of the allocated memory 21.69 GiB is allocated by PyTorch, and 318.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model deit3_large_patch16_384.fb_in22k_ft_in1k created, param count: 304763880
Running train benchmark on deit3_large_patch16_384.fb_in22k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 866.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 442.06 MiB is free. Including non-PyTorch memory, this process has 23.21 GiB memory in use. Of the allocated memory 21.72 GiB is allocated by PyTorch, and 266.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model deit3_large_patch16_384.fb_in22k_ft_in1k created, param count: 304763880
Running train benchmark on deit3_large_patch16_384.fb_in22k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 146.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 32.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 22.17 GiB is allocated by PyTorch, and 218.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model deit3_large_patch16_384.fb_in22k_ft_in1k created, param count: 304763880
Running train benchmark on deit3_large_patch16_384.fb_in22k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 326.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 132.06 MiB is free. Including non-PyTorch memory, this process has 23.51 GiB memory in use. Of the allocated memory 21.85 GiB is allocated by PyTorch, and 450.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model deit3_large_patch16_384.fb_in22k_ft_in1k created, param count: 304763880
Running train benchmark on deit3_large_patch16_384.fb_in22k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 290.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 276.06 MiB is free. Including non-PyTorch memory, this process has 23.37 GiB memory in use. Of the allocated memory 21.61 GiB is allocated by PyTorch, and 549.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model deit3_large_patch16_384.fb_in22k_ft_in1k created, param count: 304763880
Running train benchmark on deit3_large_patch16_384.fb_in22k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 30.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 21.86 GiB is allocated by PyTorch, and 539.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model deit3_large_patch16_384.fb_in22k_ft_in1k created, param count: 304763880
Running train benchmark on deit3_large_patch16_384.fb_in22k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 32.
Train [8/40]. 90.95 samples/sec. 351.825 ms/step.
Train [16/40]. 90.95 samples/sec. 351.834 ms/step.
Train [24/40]. 90.95 samples/sec. 351.838 ms/step.
Train [32/40]. 90.95 samples/sec. 351.842 ms/step.
Train [40/40]. 90.95 samples/sec. 351.850 ms/step.
Train benchmark of deit3_large_patch16_384.fb_in22k_ft_in1k done. 90.52 samples/sec, 351.85 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model deit3_medium_patch16_224.fb_in1k created, param count: 38849512
Running inference benchmark on deit3_medium_patch16_224.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 5659.14 samples/sec. 45.237 ms/step.
Infer [16/40]. 5657.22 samples/sec. 45.252 ms/step.
Infer [24/40]. 5655.50 samples/sec. 45.266 ms/step.
Infer [32/40]. 5654.42 samples/sec. 45.274 ms/step.
Infer [40/40]. 5654.18 samples/sec. 45.276 ms/step.
Inference benchmark of deit3_medium_patch16_224.fb_in1k done. 5651.04 samples/sec, 45.28 ms/step
Model deit3_medium_patch16_224.fb_in1k created, param count: 38849512
Running train benchmark on deit3_medium_patch16_224.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1651.88 samples/sec. 154.975 ms/step.
Train [16/40]. 1651.19 samples/sec. 155.040 ms/step.
Train [24/40]. 1650.89 samples/sec. 155.068 ms/step.
Train [32/40]. 1650.69 samples/sec. 155.087 ms/step.
Train [40/40]. 1650.61 samples/sec. 155.094 ms/step.
Train benchmark of deit3_medium_patch16_224.fb_in1k done. 1642.02 samples/sec, 155.09 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model deit3_medium_patch16_224.fb_in22k_ft_in1k created, param count: 38849512
Running inference benchmark on deit3_medium_patch16_224.fb_in22k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 5658.43 samples/sec. 45.242 ms/step.
Infer [16/40]. 5656.28 samples/sec. 45.259 ms/step.
Infer [24/40]. 5655.19 samples/sec. 45.268 ms/step.
Infer [32/40]. 5654.98 samples/sec. 45.270 ms/step.
Infer [40/40]. 5654.81 samples/sec. 45.271 ms/step.
Inference benchmark of deit3_medium_patch16_224.fb_in22k_ft_in1k done. 5651.74 samples/sec, 45.27 ms/step
Model deit3_medium_patch16_224.fb_in22k_ft_in1k created, param count: 38849512
Running train benchmark on deit3_medium_patch16_224.fb_in22k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1652.15 samples/sec. 154.949 ms/step.
Train [16/40]. 1651.30 samples/sec. 155.029 ms/step.
Train [24/40]. 1650.98 samples/sec. 155.060 ms/step.
Train [32/40]. 1650.85 samples/sec. 155.071 ms/step.
Train [40/40]. 1650.78 samples/sec. 155.078 ms/step.
Train benchmark of deit3_medium_patch16_224.fb_in22k_ft_in1k done. 1642.30 samples/sec, 155.08 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model deit3_small_patch16_224.fb_in1k created, param count: 22059496
Running inference benchmark on deit3_small_patch16_224.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 8913.25 samples/sec. 28.721 ms/step.
Infer [16/40]. 8911.72 samples/sec. 28.726 ms/step.
Infer [24/40]. 8910.76 samples/sec. 28.729 ms/step.
Infer [32/40]. 8910.98 samples/sec. 28.729 ms/step.
Infer [40/40]. 8910.84 samples/sec. 28.729 ms/step.
Inference benchmark of deit3_small_patch16_224.fb_in1k done. 8903.18 samples/sec, 28.73 ms/step
Model deit3_small_patch16_224.fb_in1k created, param count: 22059496
Running train benchmark on deit3_small_patch16_224.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 2470.97 samples/sec. 103.603 ms/step.
Train [16/40]. 2471.02 samples/sec. 103.601 ms/step.
Train [24/40]. 2471.06 samples/sec. 103.599 ms/step.
Train [32/40]. 2471.06 samples/sec. 103.599 ms/step.
Train [40/40]. 2471.08 samples/sec. 103.598 ms/step.
Train benchmark of deit3_small_patch16_224.fb_in1k done. 2454.83 samples/sec, 103.60 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model deit3_small_patch16_224.fb_in22k_ft_in1k created, param count: 22059496
Running inference benchmark on deit3_small_patch16_224.fb_in22k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 8926.92 samples/sec. 28.677 ms/step.
Infer [16/40]. 8921.75 samples/sec. 28.694 ms/step.
Infer [24/40]. 8919.51 samples/sec. 28.701 ms/step.
Infer [32/40]. 8918.82 samples/sec. 28.703 ms/step.
Infer [40/40]. 8918.16 samples/sec. 28.705 ms/step.
Inference benchmark of deit3_small_patch16_224.fb_in22k_ft_in1k done. 8910.79 samples/sec, 28.70 ms/step
Model deit3_small_patch16_224.fb_in22k_ft_in1k created, param count: 22059496
Running train benchmark on deit3_small_patch16_224.fb_in22k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 2470.57 samples/sec. 103.620 ms/step.
Train [16/40]. 2470.82 samples/sec. 103.609 ms/step.
Train [24/40]. 2470.77 samples/sec. 103.612 ms/step.
Train [32/40]. 2470.81 samples/sec. 103.610 ms/step.
Train [40/40]. 2470.77 samples/sec. 103.611 ms/step.
Train benchmark of deit3_small_patch16_224.fb_in22k_ft_in1k done. 2454.66 samples/sec, 103.61 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model deit3_small_patch16_384.fb_in1k created, param count: 22205416
Running inference benchmark on deit3_small_patch16_384.fb_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 2604.47 samples/sec. 98.293 ms/step.
Infer [16/40]. 2604.14 samples/sec. 98.305 ms/step.
Infer [24/40]. 2604.15 samples/sec. 98.305 ms/step.
Infer [32/40]. 2604.01 samples/sec. 98.310 ms/step.
Infer [40/40]. 2603.90 samples/sec. 98.314 ms/step.
Inference benchmark of deit3_small_patch16_384.fb_in1k done. 2603.17 samples/sec, 98.31 ms/step
Model deit3_small_patch16_384.fb_in1k created, param count: 22205416
Running train benchmark on deit3_small_patch16_384.fb_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 110.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 32.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 22.10 GiB is allocated by PyTorch, and 288.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model deit3_small_patch16_384.fb_in1k created, param count: 22205416
Running train benchmark on deit3_small_patch16_384.fb_in1k for 40 steps w/ input size (3, 384, 384) and batch size 192.
Train [8/40]. 739.37 samples/sec. 259.680 ms/step.
Train [16/40]. 739.32 samples/sec. 259.699 ms/step.
Train [24/40]. 739.28 samples/sec. 259.711 ms/step.
Train [32/40]. 739.28 samples/sec. 259.712 ms/step.
Train [40/40]. 739.27 samples/sec. 259.717 ms/step.
Train benchmark of deit3_small_patch16_384.fb_in1k done. 736.44 samples/sec, 259.72 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model deit3_small_patch16_384.fb_in22k_ft_in1k created, param count: 22205416
Running inference benchmark on deit3_small_patch16_384.fb_in22k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 2604.94 samples/sec. 98.275 ms/step.
Infer [16/40]. 2605.05 samples/sec. 98.271 ms/step.
Infer [24/40]. 2605.08 samples/sec. 98.270 ms/step.
Infer [32/40]. 2604.98 samples/sec. 98.273 ms/step.
Infer [40/40]. 2604.93 samples/sec. 98.275 ms/step.
Inference benchmark of deit3_small_patch16_384.fb_in22k_ft_in1k done. 2604.20 samples/sec, 98.28 ms/step
Model deit3_small_patch16_384.fb_in22k_ft_in1k created, param count: 22205416
Running train benchmark on deit3_small_patch16_384.fb_in22k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 110.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 32.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 22.10 GiB is allocated by PyTorch, and 288.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model deit3_small_patch16_384.fb_in22k_ft_in1k created, param count: 22205416
Running train benchmark on deit3_small_patch16_384.fb_in22k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 192.
Train [8/40]. 739.22 samples/sec. 259.735 ms/step.
Train [16/40]. 739.24 samples/sec. 259.728 ms/step.
Train [24/40]. 739.23 samples/sec. 259.729 ms/step.
Train [32/40]. 739.22 samples/sec. 259.733 ms/step.
Train [40/40]. 739.21 samples/sec. 259.735 ms/step.
Train benchmark of deit3_small_patch16_384.fb_in22k_ft_in1k done. 736.34 samples/sec, 259.74 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model deit_base_distilled_patch16_224.fb_in1k created, param count: 87338192
Running inference benchmark on deit_base_distilled_patch16_224.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3228.54 samples/sec. 79.293 ms/step.
Infer [16/40]. 3228.18 samples/sec. 79.302 ms/step.
Infer [24/40]. 3226.40 samples/sec. 79.345 ms/step.
Infer [32/40]. 3225.34 samples/sec. 79.371 ms/step.
Infer [40/40]. 3224.59 samples/sec. 79.390 ms/step.
Inference benchmark of deit_base_distilled_patch16_224.fb_in1k done. 3223.51 samples/sec, 79.39 ms/step
Model deit_base_distilled_patch16_224.fb_in1k created, param count: 87338192
Running train benchmark on deit_base_distilled_patch16_224.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 972.15 samples/sec. 263.335 ms/step.
Train [16/40]. 972.12 samples/sec. 263.342 ms/step.
Train [24/40]. 971.97 samples/sec. 263.383 ms/step.
Train [32/40]. 971.90 samples/sec. 263.401 ms/step.
Train [40/40]. 971.86 samples/sec. 263.412 ms/step.
Train benchmark of deit_base_distilled_patch16_224.fb_in1k done. 968.44 samples/sec, 263.41 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model deit_base_distilled_patch16_384.fb_in1k created, param count: 87630032
Running inference benchmark on deit_base_distilled_patch16_384.fb_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 1008.53 samples/sec. 253.836 ms/step.
Infer [16/40]. 1008.33 samples/sec. 253.885 ms/step.
Infer [24/40]. 1008.43 samples/sec. 253.861 ms/step.
Infer [32/40]. 1008.43 samples/sec. 253.859 ms/step.
Infer [40/40]. 1008.39 samples/sec. 253.870 ms/step.
Inference benchmark of deit_base_distilled_patch16_384.fb_in1k done. 1008.22 samples/sec, 253.87 ms/step
Model deit_base_distilled_patch16_384.fb_in1k created, param count: 87630032
Running train benchmark on deit_base_distilled_patch16_384.fb_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 218.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 62.06 MiB is free. Including non-PyTorch memory, this process has 23.58 GiB memory in use. Of the allocated memory 22.27 GiB is allocated by PyTorch, and 86.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model deit_base_distilled_patch16_384.fb_in1k created, param count: 87630032
Running train benchmark on deit_base_distilled_patch16_384.fb_in1k for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 652.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 58.06 MiB is free. Including non-PyTorch memory, this process has 23.58 GiB memory in use. Of the allocated memory 21.99 GiB is allocated by PyTorch, and 373.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model deit_base_distilled_patch16_384.fb_in1k created, param count: 87630032
Running train benchmark on deit_base_distilled_patch16_384.fb_in1k for 40 steps w/ input size (3, 384, 384) and batch size 128.
Train [8/40]. 305.91 samples/sec. 418.426 ms/step.
Train [16/40]. 305.83 samples/sec. 418.534 ms/step.
Train [24/40]. 305.75 samples/sec. 418.646 ms/step.
Train [32/40]. 305.71 samples/sec. 418.701 ms/step.
Train [40/40]. 305.68 samples/sec. 418.736 ms/step.
Train benchmark of deit_base_distilled_patch16_384.fb_in1k done. 304.96 samples/sec, 418.74 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model deit_base_patch16_224.fb_in1k created, param count: 86567656
Running inference benchmark on deit_base_patch16_224.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3237.92 samples/sec. 79.063 ms/step.
Infer [16/40]. 3237.21 samples/sec. 79.081 ms/step.
Infer [24/40]. 3236.02 samples/sec. 79.110 ms/step.
Infer [32/40]. 3235.50 samples/sec. 79.122 ms/step.
Infer [40/40]. 3235.64 samples/sec. 79.119 ms/step.
Inference benchmark of deit_base_patch16_224.fb_in1k done. 3234.54 samples/sec, 79.12 ms/step
Model deit_base_patch16_224.fb_in1k created, param count: 86567656
Running train benchmark on deit_base_patch16_224.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 978.27 samples/sec. 261.686 ms/step.
Train [16/40]. 978.27 samples/sec. 261.688 ms/step.
Train [24/40]. 978.25 samples/sec. 261.691 ms/step.
Train [32/40]. 978.24 samples/sec. 261.694 ms/step.
Train [40/40]. 978.24 samples/sec. 261.695 ms/step.
Train benchmark of deit_base_patch16_224.fb_in1k done. 974.78 samples/sec, 261.69 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model deit_base_patch16_384.fb_in1k created, param count: 86859496
Running inference benchmark on deit_base_patch16_384.fb_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 1012.34 samples/sec. 252.879 ms/step.
Infer [16/40]. 1012.41 samples/sec. 252.862 ms/step.
Infer [24/40]. 1012.32 samples/sec. 252.885 ms/step.
Infer [32/40]. 1012.11 samples/sec. 252.936 ms/step.
Infer [40/40]. 1011.63 samples/sec. 253.056 ms/step.
Inference benchmark of deit_base_patch16_384.fb_in1k done. 1011.45 samples/sec, 253.06 ms/step
Model deit_base_patch16_384.fb_in1k created, param count: 86859496
Running train benchmark on deit_base_patch16_384.fb_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 218.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 100.06 MiB is free. Including non-PyTorch memory, this process has 23.54 GiB memory in use. Of the allocated memory 22.23 GiB is allocated by PyTorch, and 89.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model deit_base_patch16_384.fb_in1k created, param count: 86859496
Running train benchmark on deit_base_patch16_384.fb_in1k for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 650.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 90.06 MiB is free. Including non-PyTorch memory, this process has 23.55 GiB memory in use. Of the allocated memory 21.96 GiB is allocated by PyTorch, and 370.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model deit_base_patch16_384.fb_in1k created, param count: 86859496
Running train benchmark on deit_base_patch16_384.fb_in1k for 40 steps w/ input size (3, 384, 384) and batch size 128.
Train [8/40]. 306.10 samples/sec. 418.170 ms/step.
Train [16/40]. 306.03 samples/sec. 418.259 ms/step.
Train [24/40]. 306.06 samples/sec. 418.221 ms/step.
Train [32/40]. 306.06 samples/sec. 418.220 ms/step.
Train [40/40]. 306.05 samples/sec. 418.236 ms/step.
Train benchmark of deit_base_patch16_384.fb_in1k done. 305.33 samples/sec, 418.24 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model deit_small_distilled_patch16_224.fb_in1k created, param count: 22436432
Running inference benchmark on deit_small_distilled_patch16_224.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 9301.39 samples/sec. 27.523 ms/step.
Infer [16/40]. 9301.36 samples/sec. 27.523 ms/step.
Infer [24/40]. 9301.49 samples/sec. 27.522 ms/step.
Infer [32/40]. 9301.41 samples/sec. 27.523 ms/step.
Infer [40/40]. 9301.21 samples/sec. 27.523 ms/step.
Inference benchmark of deit_small_distilled_patch16_224.fb_in1k done. 9292.86 samples/sec, 27.52 ms/step
Model deit_small_distilled_patch16_224.fb_in1k created, param count: 22436432
Running train benchmark on deit_small_distilled_patch16_224.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 2644.11 samples/sec. 96.819 ms/step.
Train [16/40]. 2643.86 samples/sec. 96.828 ms/step.
Train [24/40]. 2643.49 samples/sec. 96.842 ms/step.
Train [32/40]. 2643.32 samples/sec. 96.848 ms/step.
Train [40/40]. 2643.14 samples/sec. 96.855 ms/step.
Train benchmark of deit_small_distilled_patch16_224.fb_in1k done. 2623.91 samples/sec, 96.86 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model deit_small_patch16_224.fb_in1k created, param count: 22050664
Running inference benchmark on deit_small_patch16_224.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 9312.21 samples/sec. 27.491 ms/step.
Infer [16/40]. 9310.90 samples/sec. 27.495 ms/step.
Infer [24/40]. 9309.69 samples/sec. 27.498 ms/step.
Infer [32/40]. 9308.20 samples/sec. 27.503 ms/step.
Infer [40/40]. 9308.83 samples/sec. 27.501 ms/step.
Inference benchmark of deit_small_patch16_224.fb_in1k done. 9300.37 samples/sec, 27.50 ms/step
Model deit_small_patch16_224.fb_in1k created, param count: 22050664
Running train benchmark on deit_small_patch16_224.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 2657.70 samples/sec. 96.324 ms/step.
Train [16/40]. 2657.61 samples/sec. 96.327 ms/step.
Train [24/40]. 2657.53 samples/sec. 96.330 ms/step.
Train [32/40]. 2657.46 samples/sec. 96.333 ms/step.
Train [40/40]. 2657.41 samples/sec. 96.334 ms/step.
Train benchmark of deit_small_patch16_224.fb_in1k done. 2639.86 samples/sec, 96.33 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model deit_tiny_distilled_patch16_224.fb_in1k created, param count: 5910800
Running inference benchmark on deit_tiny_distilled_patch16_224.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 23489.58 samples/sec. 10.898 ms/step.
Infer [16/40]. 23487.82 samples/sec. 10.899 ms/step.
Infer [24/40]. 23486.51 samples/sec. 10.900 ms/step.
Infer [32/40]. 23483.36 samples/sec. 10.901 ms/step.
Infer [40/40]. 23481.80 samples/sec. 10.902 ms/step.
Inference benchmark of deit_tiny_distilled_patch16_224.fb_in1k done. 23436.08 samples/sec, 10.90 ms/step
Model deit_tiny_distilled_patch16_224.fb_in1k created, param count: 5910800
Running train benchmark on deit_tiny_distilled_patch16_224.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 6005.47 samples/sec. 42.628 ms/step.
Train [16/40]. 6007.06 samples/sec. 42.617 ms/step.
Train [24/40]. 6006.90 samples/sec. 42.618 ms/step.
Train [32/40]. 6006.64 samples/sec. 42.620 ms/step.
Train [40/40]. 6006.63 samples/sec. 42.620 ms/step.
Train benchmark of deit_tiny_distilled_patch16_224.fb_in1k done. 5923.04 samples/sec, 42.62 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model deit_tiny_patch16_224.fb_in1k created, param count: 5717416
Running inference benchmark on deit_tiny_patch16_224.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 22467.85 samples/sec. 11.394 ms/step.
Infer [16/40]. 22675.18 samples/sec. 11.290 ms/step.
Infer [24/40]. 22971.47 samples/sec. 11.144 ms/step.
Infer [32/40]. 23118.16 samples/sec. 11.074 ms/step.
Infer [40/40]. 23207.70 samples/sec. 11.031 ms/step.
Inference benchmark of deit_tiny_patch16_224.fb_in1k done. 23162.70 samples/sec, 11.03 ms/step
Model deit_tiny_patch16_224.fb_in1k created, param count: 5717416
Running train benchmark on deit_tiny_patch16_224.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 6047.03 samples/sec. 42.335 ms/step.
Train [16/40]. 6046.07 samples/sec. 42.342 ms/step.
Train [24/40]. 6045.71 samples/sec. 42.344 ms/step.
Train [32/40]. 6045.86 samples/sec. 42.343 ms/step.
Train [40/40]. 6045.95 samples/sec. 42.342 ms/step.
Train benchmark of deit_tiny_patch16_224.fb_in1k done. 5961.37 samples/sec, 42.34 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model densenet121.ra_in1k created, param count: 7978856
Running inference benchmark on densenet121.ra_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 2200.55 samples/sec. 116.334 ms/step.
Infer [16/40]. 2200.57 samples/sec. 116.334 ms/step.
Infer [24/40]. 2200.40 samples/sec. 116.343 ms/step.
Infer [32/40]. 2200.31 samples/sec. 116.347 ms/step.
Infer [40/40]. 2200.34 samples/sec. 116.346 ms/step.
Inference benchmark of densenet121.ra_in1k done. 2199.78 samples/sec, 116.35 ms/step
Model densenet121.ra_in1k created, param count: 7978856
Running train benchmark on densenet121.ra_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 122.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 38.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 22.26 GiB is allocated by PyTorch, and 120.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model densenet121.ra_in1k created, param count: 7978856
Running train benchmark on densenet121.ra_in1k for 40 steps w/ input size (3, 288, 288) and batch size 192.
Train [8/40]. 637.52 samples/sec. 301.169 ms/step.
Train [16/40]. 637.50 samples/sec. 301.175 ms/step.
Train [24/40]. 637.50 samples/sec. 301.176 ms/step.
Train [32/40]. 637.49 samples/sec. 301.180 ms/step.
Train [40/40]. 637.49 samples/sec. 301.181 ms/step.
Train benchmark of densenet121.ra_in1k done. 634.23 samples/sec, 301.18 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model densenet121.tv_in1k created, param count: 7978856
Running inference benchmark on densenet121.tv_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3793.44 samples/sec. 67.485 ms/step.
Infer [16/40]. 3793.06 samples/sec. 67.492 ms/step.
Infer [24/40]. 3793.19 samples/sec. 67.489 ms/step.
Infer [32/40]. 3793.21 samples/sec. 67.489 ms/step.
Infer [40/40]. 3793.20 samples/sec. 67.489 ms/step.
Inference benchmark of densenet121.tv_in1k done. 3791.61 samples/sec, 67.49 ms/step
Model densenet121.tv_in1k created, param count: 7978856
Running train benchmark on densenet121.tv_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1075.10 samples/sec. 238.117 ms/step.
Train [16/40]. 1075.12 samples/sec. 238.114 ms/step.
Train [24/40]. 1075.08 samples/sec. 238.123 ms/step.
Train [32/40]. 1075.05 samples/sec. 238.127 ms/step.
Train [40/40]. 1075.06 samples/sec. 238.127 ms/step.
Train benchmark of densenet121.tv_in1k done. 1068.76 samples/sec, 238.13 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model densenet161.tv_in1k created, param count: 28681000
Running inference benchmark on densenet161.tv_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1870.95 samples/sec. 136.829 ms/step.
Infer [16/40]. 1870.94 samples/sec. 136.830 ms/step.
Infer [24/40]. 1870.93 samples/sec. 136.830 ms/step.
Infer [32/40]. 1870.93 samples/sec. 136.830 ms/step.
Infer [40/40]. 1870.92 samples/sec. 136.831 ms/step.
Inference benchmark of densenet161.tv_in1k done. 1870.50 samples/sec, 136.83 ms/step
Model densenet161.tv_in1k created, param count: 28681000
Running train benchmark on densenet161.tv_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 144.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 110.06 MiB is free. Including non-PyTorch memory, this process has 23.53 GiB memory in use. Of the allocated memory 22.07 GiB is allocated by PyTorch, and 246.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model densenet161.tv_in1k created, param count: 28681000
Running train benchmark on densenet161.tv_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 40.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 16.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.15 GiB is allocated by PyTorch, and 256.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model densenet161.tv_in1k created, param count: 28681000
Running train benchmark on densenet161.tv_in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
Train [8/40]. 576.98 samples/sec. 221.846 ms/step.
Train [16/40]. 577.01 samples/sec. 221.832 ms/step.
Train [24/40]. 577.01 samples/sec. 221.832 ms/step.
Train [32/40]. 577.00 samples/sec. 221.837 ms/step.
Train [40/40]. 577.00 samples/sec. 221.837 ms/step.
Train benchmark of densenet161.tv_in1k done. 572.18 samples/sec, 221.84 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model densenet169.tv_in1k created, param count: 14149480
Running inference benchmark on densenet169.tv_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3139.26 samples/sec. 81.548 ms/step.
Infer [16/40]. 3138.19 samples/sec. 81.576 ms/step.
Infer [24/40]. 3137.91 samples/sec. 81.583 ms/step.
Infer [32/40]. 3137.72 samples/sec. 81.588 ms/step.
Infer [40/40]. 3137.61 samples/sec. 81.591 ms/step.
Inference benchmark of densenet169.tv_in1k done. 3136.61 samples/sec, 81.59 ms/step
Model densenet169.tv_in1k created, param count: 14149480
Running train benchmark on densenet169.tv_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 895.41 samples/sec. 285.903 ms/step.
Train [16/40]. 895.40 samples/sec. 285.906 ms/step.
Train [24/40]. 895.37 samples/sec. 285.916 ms/step.
Train [32/40]. 895.36 samples/sec. 285.920 ms/step.
Train [40/40]. 895.34 samples/sec. 285.925 ms/step.
Train benchmark of densenet169.tv_in1k done. 889.04 samples/sec, 285.93 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model densenet201.tv_in1k created, param count: 20013928
Running inference benchmark on densenet201.tv_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 2448.39 samples/sec. 104.558 ms/step.
Infer [16/40]. 2448.30 samples/sec. 104.562 ms/step.
Infer [24/40]. 2448.28 samples/sec. 104.563 ms/step.
Infer [32/40]. 2448.26 samples/sec. 104.564 ms/step.
Infer [40/40]. 2448.24 samples/sec. 104.565 ms/step.
Inference benchmark of densenet201.tv_in1k done. 2447.59 samples/sec, 104.56 ms/step
Model densenet201.tv_in1k created, param count: 20013928
Running train benchmark on densenet201.tv_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 86.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 36.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 22.05 GiB is allocated by PyTorch, and 337.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model densenet201.tv_in1k created, param count: 20013928
Running train benchmark on densenet201.tv_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
Train [8/40]. 711.59 samples/sec. 269.818 ms/step.
Train [16/40]. 711.55 samples/sec. 269.834 ms/step.
Train [24/40]. 711.55 samples/sec. 269.833 ms/step.
Train [32/40]. 711.55 samples/sec. 269.832 ms/step.
Train [40/40]. 711.54 samples/sec. 269.835 ms/step.
Train benchmark of densenet201.tv_in1k done. 705.80 samples/sec, 269.83 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model densenetblur121d.ra_in1k created, param count: 7998088
Running inference benchmark on densenetblur121d.ra_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 1987.54 samples/sec. 128.802 ms/step.
Infer [16/40]. 1987.53 samples/sec. 128.803 ms/step.
Infer [24/40]. 1987.47 samples/sec. 128.807 ms/step.
Infer [32/40]. 1987.43 samples/sec. 128.810 ms/step.
Infer [40/40]. 1987.44 samples/sec. 128.809 ms/step.
Inference benchmark of densenetblur121d.ra_in1k done. 1986.99 samples/sec, 128.81 ms/step
Model densenetblur121d.ra_in1k created, param count: 7998088
Running train benchmark on densenetblur121d.ra_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 304.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 60.06 MiB is free. Including non-PyTorch memory, this process has 23.58 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 235.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model densenetblur121d.ra_in1k created, param count: 7998088
Running train benchmark on densenetblur121d.ra_in1k for 40 steps w/ input size (3, 288, 288) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 16.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.17 GiB is allocated by PyTorch, and 235.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model densenetblur121d.ra_in1k created, param count: 7998088
Running train benchmark on densenetblur121d.ra_in1k for 40 steps w/ input size (3, 288, 288) and batch size 128.
Train [8/40]. 570.23 samples/sec. 224.473 ms/step.
Train [16/40]. 570.24 samples/sec. 224.466 ms/step.
Train [24/40]. 570.22 samples/sec. 224.475 ms/step.
Train [32/40]. 570.21 samples/sec. 224.479 ms/step.
Train [40/40]. 570.20 samples/sec. 224.481 ms/step.
Train benchmark of densenetblur121d.ra_in1k done. 566.36 samples/sec, 224.48 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model dla34.in1k created, param count: 15742104
Running inference benchmark on dla34.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 7388.70 samples/sec. 34.647 ms/step.
Infer [16/40]. 7388.30 samples/sec. 34.649 ms/step.
Infer [24/40]. 7387.96 samples/sec. 34.651 ms/step.
Infer [32/40]. 7387.77 samples/sec. 34.652 ms/step.
Infer [40/40]. 7387.76 samples/sec. 34.652 ms/step.
Inference benchmark of dla34.in1k done. 7381.65 samples/sec, 34.65 ms/step
Model dla34.in1k created, param count: 15742104
Running train benchmark on dla34.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1559.72 samples/sec. 164.132 ms/step.
Train [16/40]. 1559.80 samples/sec. 164.124 ms/step.
Train [24/40]. 1559.75 samples/sec. 164.128 ms/step.
Train [32/40]. 1559.79 samples/sec. 164.125 ms/step.
Train [40/40]. 1559.79 samples/sec. 164.124 ms/step.
Train benchmark of dla34.in1k done. 1553.53 samples/sec, 164.12 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model dla46_c.in1k created, param count: 1301400
Running inference benchmark on dla46_c.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 11889.85 samples/sec. 21.531 ms/step.
Infer [16/40]. 11886.95 samples/sec. 21.536 ms/step.
Infer [24/40]. 11886.25 samples/sec. 21.537 ms/step.
Infer [32/40]. 11885.58 samples/sec. 21.539 ms/step.
Infer [40/40]. 11885.30 samples/sec. 21.539 ms/step.
Inference benchmark of dla46_c.in1k done. 11870.28 samples/sec, 21.54 ms/step
Model dla46_c.in1k created, param count: 1301400
Running train benchmark on dla46_c.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1938.23 samples/sec. 132.079 ms/step.
Train [16/40]. 1938.02 samples/sec. 132.093 ms/step.
Train [24/40]. 1938.04 samples/sec. 132.092 ms/step.
Train [32/40]. 1938.04 samples/sec. 132.092 ms/step.
Train [40/40]. 1938.06 samples/sec. 132.091 ms/step.
Train benchmark of dla46_c.in1k done. 1928.63 samples/sec, 132.09 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model dla46x_c.in1k created, param count: 1068440
Running inference benchmark on dla46x_c.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 9981.09 samples/sec. 25.648 ms/step.
Infer [16/40]. 9980.53 samples/sec. 25.650 ms/step.
Infer [24/40]. 9980.68 samples/sec. 25.650 ms/step.
Infer [32/40]. 9980.34 samples/sec. 25.650 ms/step.
Infer [40/40]. 9980.38 samples/sec. 25.650 ms/step.
Inference benchmark of dla46x_c.in1k done. 9969.75 samples/sec, 25.65 ms/step
Model dla46x_c.in1k created, param count: 1068440
Running train benchmark on dla46x_c.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1735.59 samples/sec. 147.500 ms/step.
Train [16/40]. 1735.47 samples/sec. 147.511 ms/step.
Train [24/40]. 1735.49 samples/sec. 147.509 ms/step.
Train [32/40]. 1735.47 samples/sec. 147.511 ms/step.
Train [40/40]. 1735.51 samples/sec. 147.507 ms/step.
Train benchmark of dla46x_c.in1k done. 1727.46 samples/sec, 147.51 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model dla60.in1k created, param count: 22036632
Running inference benchmark on dla60.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 4456.33 samples/sec. 57.446 ms/step.
Infer [16/40]. 4455.96 samples/sec. 57.451 ms/step.
Infer [24/40]. 4455.84 samples/sec. 57.453 ms/step.
Infer [32/40]. 4455.80 samples/sec. 57.453 ms/step.
Infer [40/40]. 4455.75 samples/sec. 57.454 ms/step.
Inference benchmark of dla60.in1k done. 4453.56 samples/sec, 57.45 ms/step
Model dla60.in1k created, param count: 22036632
Running train benchmark on dla60.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1064.66 samples/sec. 240.451 ms/step.
Train [16/40]. 1064.65 samples/sec. 240.455 ms/step.
Train [24/40]. 1064.64 samples/sec. 240.456 ms/step.
Train [32/40]. 1064.64 samples/sec. 240.457 ms/step.
Train [40/40]. 1064.64 samples/sec. 240.456 ms/step.
Train benchmark of dla60.in1k done. 1060.45 samples/sec, 240.46 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model dla60_res2net.in1k created, param count: 20848072
Running inference benchmark on dla60_res2net.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3618.60 samples/sec. 70.746 ms/step.
Infer [16/40]. 3618.76 samples/sec. 70.742 ms/step.
Infer [24/40]. 3618.86 samples/sec. 70.741 ms/step.
Infer [32/40]. 3618.82 samples/sec. 70.741 ms/step.
Infer [40/40]. 3618.86 samples/sec. 70.741 ms/step.
Inference benchmark of dla60_res2net.in1k done. 3617.41 samples/sec, 70.74 ms/step
Model dla60_res2net.in1k created, param count: 20848072
Running train benchmark on dla60_res2net.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 869.50 samples/sec. 294.424 ms/step.
Train [16/40]. 869.51 samples/sec. 294.418 ms/step.
Train [24/40]. 869.51 samples/sec. 294.419 ms/step.
Train [32/40]. 869.50 samples/sec. 294.423 ms/step.
Train [40/40]. 869.49 samples/sec. 294.424 ms/step.
Train benchmark of dla60_res2net.in1k done. 865.74 samples/sec, 294.42 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model dla60_res2next.in1k created, param count: 17032984
Running inference benchmark on dla60_res2next.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3507.83 samples/sec. 72.980 ms/step.
Infer [16/40]. 3507.84 samples/sec. 72.979 ms/step.
Infer [24/40]. 3507.91 samples/sec. 72.978 ms/step.
Infer [32/40]. 3507.93 samples/sec. 72.978 ms/step.
Infer [40/40]. 3507.89 samples/sec. 72.978 ms/step.
Inference benchmark of dla60_res2next.in1k done. 3506.51 samples/sec, 72.98 ms/step
Model dla60_res2next.in1k created, param count: 17032984
Running train benchmark on dla60_res2next.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 839.55 samples/sec. 304.925 ms/step.
Train [16/40]. 839.57 samples/sec. 304.917 ms/step.
Train [24/40]. 839.55 samples/sec. 304.926 ms/step.
Train [32/40]. 839.54 samples/sec. 304.929 ms/step.
Train [40/40]. 839.54 samples/sec. 304.930 ms/step.
Train benchmark of dla60_res2next.in1k done. 836.07 samples/sec, 304.93 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model dla60x.in1k created, param count: 17352344
Running inference benchmark on dla60x.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3638.17 samples/sec. 70.365 ms/step.
Infer [16/40]. 3638.20 samples/sec. 70.365 ms/step.
Infer [24/40]. 3638.14 samples/sec. 70.366 ms/step.
Infer [32/40]. 3638.21 samples/sec. 70.364 ms/step.
Infer [40/40]. 3638.20 samples/sec. 70.364 ms/step.
Inference benchmark of dla60x.in1k done. 3636.74 samples/sec, 70.36 ms/step
Model dla60x.in1k created, param count: 17352344
Running train benchmark on dla60x.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 891.27 samples/sec. 287.232 ms/step.
Train [16/40]. 891.21 samples/sec. 287.251 ms/step.
Train [24/40]. 891.19 samples/sec. 287.257 ms/step.
Train [32/40]. 891.19 samples/sec. 287.256 ms/step.
Train [40/40]. 891.19 samples/sec. 287.256 ms/step.
Train benchmark of dla60x.in1k done. 888.12 samples/sec, 287.26 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model dla60x_c.in1k created, param count: 1319832
Running inference benchmark on dla60x_c.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 9549.76 samples/sec. 26.807 ms/step.
Infer [16/40]. 9549.12 samples/sec. 26.809 ms/step.
Infer [24/40]. 9549.12 samples/sec. 26.809 ms/step.
Infer [32/40]. 9548.82 samples/sec. 26.810 ms/step.
Infer [40/40]. 9547.60 samples/sec. 26.813 ms/step.
Inference benchmark of dla60x_c.in1k done. 9538.13 samples/sec, 26.81 ms/step
Model dla60x_c.in1k created, param count: 1319832
Running train benchmark on dla60x_c.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1679.56 samples/sec. 152.421 ms/step.
Train [16/40]. 1679.57 samples/sec. 152.420 ms/step.
Train [24/40]. 1679.58 samples/sec. 152.419 ms/step.
Train [32/40]. 1679.59 samples/sec. 152.418 ms/step.
Train [40/40]. 1679.62 samples/sec. 152.416 ms/step.
Train benchmark of dla60x_c.in1k done. 1671.30 samples/sec, 152.42 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model dla102.in1k created, param count: 33268888
Running inference benchmark on dla102.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 2978.88 samples/sec. 85.938 ms/step.
Infer [16/40]. 2978.86 samples/sec. 85.939 ms/step.
Infer [24/40]. 2978.83 samples/sec. 85.940 ms/step.
Infer [32/40]. 2978.85 samples/sec. 85.939 ms/step.
Infer [40/40]. 2978.75 samples/sec. 85.942 ms/step.
Inference benchmark of dla102.in1k done. 2977.67 samples/sec, 85.94 ms/step
Model dla102.in1k created, param count: 33268888
Running train benchmark on dla102.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 769.43 samples/sec. 332.714 ms/step.
Train [16/40]. 769.41 samples/sec. 332.721 ms/step.
Train [24/40]. 769.41 samples/sec. 332.722 ms/step.
Train [32/40]. 769.40 samples/sec. 332.728 ms/step.
Train [40/40]. 769.39 samples/sec. 332.729 ms/step.
Train benchmark of dla102.in1k done. 766.19 samples/sec, 332.73 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model dla102x2.in1k created, param count: 41282200
Running inference benchmark on dla102x2.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1613.36 samples/sec. 158.675 ms/step.
Infer [16/40]. 1613.45 samples/sec. 158.666 ms/step.
Infer [24/40]. 1613.40 samples/sec. 158.671 ms/step.
Infer [32/40]. 1613.36 samples/sec. 158.675 ms/step.
Infer [40/40]. 1613.34 samples/sec. 158.677 ms/step.
Inference benchmark of dla102x2.in1k done. 1612.96 samples/sec, 158.68 ms/step
Model dla102x2.in1k created, param count: 41282200
Running train benchmark on dla102x2.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 48.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 22.23 GiB is allocated by PyTorch, and 141.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model dla102x2.in1k created, param count: 41282200
Running train benchmark on dla102x2.in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 74.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 74.06 MiB is free. Including non-PyTorch memory, this process has 23.57 GiB memory in use. Of the allocated memory 22.22 GiB is allocated by PyTorch, and 123.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model dla102x2.in1k created, param count: 41282200
Running train benchmark on dla102x2.in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
Train [8/40]. 460.60 samples/sec. 277.901 ms/step.
Train [16/40]. 460.61 samples/sec. 277.891 ms/step.
Train [24/40]. 460.61 samples/sec. 277.891 ms/step.
Train [32/40]. 460.61 samples/sec. 277.892 ms/step.
Train [40/40]. 460.61 samples/sec. 277.891 ms/step.
Train benchmark of dla102x2.in1k done. 458.44 samples/sec, 277.89 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model dla102x.in1k created, param count: 26309272
Running inference benchmark on dla102x.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 2415.20 samples/sec. 105.995 ms/step.
Infer [16/40]. 2415.09 samples/sec. 106.000 ms/step.
Infer [24/40]. 2414.98 samples/sec. 106.005 ms/step.
Infer [32/40]. 2414.99 samples/sec. 106.004 ms/step.
Infer [40/40]. 2414.99 samples/sec. 106.004 ms/step.
Inference benchmark of dla102x.in1k done. 2414.26 samples/sec, 106.00 ms/step
Model dla102x.in1k created, param count: 26309272
Running train benchmark on dla102x.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 633.35 samples/sec. 404.199 ms/step.
Train [16/40]. 633.26 samples/sec. 404.259 ms/step.
Train [24/40]. 633.43 samples/sec. 404.146 ms/step.
Train [32/40]. 633.47 samples/sec. 404.121 ms/step.
Train [40/40]. 633.46 samples/sec. 404.131 ms/step.
Train benchmark of dla102x.in1k done. 631.23 samples/sec, 404.13 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model dla169.in1k created, param count: 53389720
Running inference benchmark on dla169.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 2030.64 samples/sec. 126.069 ms/step.
Infer [16/40]. 2030.03 samples/sec. 126.106 ms/step.
Infer [24/40]. 2029.85 samples/sec. 126.118 ms/step.
Infer [32/40]. 2029.73 samples/sec. 126.125 ms/step.
Infer [40/40]. 2029.68 samples/sec. 126.128 ms/step.
Inference benchmark of dla169.in1k done. 2029.16 samples/sec, 126.13 ms/step
Model dla169.in1k created, param count: 53389720
Running train benchmark on dla169.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 14.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.19 GiB is allocated by PyTorch, and 215.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model dla169.in1k created, param count: 53389720
Running train benchmark on dla169.in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
Train [8/40]. 554.56 samples/sec. 346.219 ms/step.
Train [16/40]. 554.56 samples/sec. 346.222 ms/step.
Train [24/40]. 554.55 samples/sec. 346.226 ms/step.
Train [32/40]. 554.55 samples/sec. 346.227 ms/step.
Train [40/40]. 554.55 samples/sec. 346.228 ms/step.
Train benchmark of dla169.in1k done. 551.44 samples/sec, 346.23 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model dm_nfnet_f0.dm_in1k created, param count: 71489284
Running inference benchmark on dm_nfnet_f0.dm_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
Infer [8/40]. 1612.10 samples/sec. 158.799 ms/step.
Infer [16/40]. 1612.16 samples/sec. 158.793 ms/step.
Infer [24/40]. 1612.15 samples/sec. 158.794 ms/step.
Infer [32/40]. 1612.12 samples/sec. 158.797 ms/step.
Infer [40/40]. 1612.10 samples/sec. 158.799 ms/step.
Inference benchmark of dm_nfnet_f0.dm_in1k done. 1611.76 samples/sec, 158.80 ms/step
Model dm_nfnet_f0.dm_in1k created, param count: 71489284
Running train benchmark on dm_nfnet_f0.dm_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
Train [8/40]. 587.15 samples/sec. 436.007 ms/step.
Train [16/40]. 587.12 samples/sec. 436.028 ms/step.
Train [24/40]. 587.13 samples/sec. 436.021 ms/step.
Train [32/40]. 587.14 samples/sec. 436.015 ms/step.
Train [40/40]. 587.14 samples/sec. 436.014 ms/step.
Train benchmark of dm_nfnet_f0.dm_in1k done. 585.26 samples/sec, 436.01 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model dm_nfnet_f1.dm_in1k created, param count: 132634256
Running inference benchmark on dm_nfnet_f1.dm_in1k for 40 steps w/ input size (3, 320, 320) and batch size 256.
Infer [8/40]. 576.13 samples/sec. 444.343 ms/step.
Infer [16/40]. 576.03 samples/sec. 444.420 ms/step.
Infer [24/40]. 576.02 samples/sec. 444.426 ms/step.
Infer [32/40]. 576.01 samples/sec. 444.434 ms/step.
Infer [40/40]. 575.98 samples/sec. 444.458 ms/step.
Inference benchmark of dm_nfnet_f1.dm_in1k done. 575.92 samples/sec, 444.46 ms/step
Model dm_nfnet_f1.dm_in1k created, param count: 132634256
Running train benchmark on dm_nfnet_f1.dm_in1k for 40 steps w/ input size (3, 320, 320) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 200.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 18.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.27 GiB is allocated by PyTorch, and 124.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model dm_nfnet_f1.dm_in1k created, param count: 132634256
Running train benchmark on dm_nfnet_f1.dm_in1k for 40 steps w/ input size (3, 320, 320) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 474.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 286.06 MiB is free. Including non-PyTorch memory, this process has 23.36 GiB memory in use. Of the allocated memory 21.76 GiB is allocated by PyTorch, and 383.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model dm_nfnet_f1.dm_in1k created, param count: 132634256
Running train benchmark on dm_nfnet_f1.dm_in1k for 40 steps w/ input size (3, 320, 320) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 150.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 38.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 22.32 GiB is allocated by PyTorch, and 60.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model dm_nfnet_f1.dm_in1k created, param count: 132634256
Running train benchmark on dm_nfnet_f1.dm_in1k for 40 steps w/ input size (3, 320, 320) and batch size 96.
Train [8/40]. 195.25 samples/sec. 491.676 ms/step.
Train [16/40]. 195.27 samples/sec. 491.629 ms/step.
Train [24/40]. 195.26 samples/sec. 491.648 ms/step.
Train [32/40]. 195.27 samples/sec. 491.637 ms/step.
Train [40/40]. 195.27 samples/sec. 491.639 ms/step.
Train benchmark of dm_nfnet_f1.dm_in1k done. 194.40 samples/sec, 491.64 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model dm_nfnet_f2.dm_in1k created, param count: 193779228
Running inference benchmark on dm_nfnet_f2.dm_in1k for 40 steps w/ input size (3, 352, 352) and batch size 256.
Infer [8/40]. 334.95 samples/sec. 764.300 ms/step.
Infer [16/40]. 334.95 samples/sec. 764.300 ms/step.
Infer [24/40]. 334.93 samples/sec. 764.341 ms/step.
Infer [32/40]. 334.91 samples/sec. 764.380 ms/step.
Infer [40/40]. 334.90 samples/sec. 764.414 ms/step.
Inference benchmark of dm_nfnet_f2.dm_in1k done. 334.88 samples/sec, 764.41 ms/step
Model dm_nfnet_f2.dm_in1k created, param count: 193779228
Running train benchmark on dm_nfnet_f2.dm_in1k for 40 steps w/ input size (3, 352, 352) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 968.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 522.06 MiB is free. Including non-PyTorch memory, this process has 23.13 GiB memory in use. Of the allocated memory 21.07 GiB is allocated by PyTorch, and 852.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model dm_nfnet_f2.dm_in1k created, param count: 193779228
Running train benchmark on dm_nfnet_f2.dm_in1k for 40 steps w/ input size (3, 352, 352) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 364.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 358.06 MiB is free. Including non-PyTorch memory, this process has 23.29 GiB memory in use. Of the allocated memory 21.77 GiB is allocated by PyTorch, and 299.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model dm_nfnet_f2.dm_in1k created, param count: 193779228
Running train benchmark on dm_nfnet_f2.dm_in1k for 40 steps w/ input size (3, 352, 352) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 122.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 58.06 MiB is free. Including non-PyTorch memory, this process has 23.58 GiB memory in use. Of the allocated memory 22.22 GiB is allocated by PyTorch, and 145.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model dm_nfnet_f2.dm_in1k created, param count: 193779228
Running train benchmark on dm_nfnet_f2.dm_in1k for 40 steps w/ input size (3, 352, 352) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 138.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 2.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 21.89 GiB is allocated by PyTorch, and 532.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model dm_nfnet_f2.dm_in1k created, param count: 193779228
Running train benchmark on dm_nfnet_f2.dm_in1k for 40 steps w/ input size (3, 352, 352) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 92.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 60.06 MiB is free. Including non-PyTorch memory, this process has 23.58 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 228.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model dm_nfnet_f2.dm_in1k created, param count: 193779228
Running train benchmark on dm_nfnet_f2.dm_in1k for 40 steps w/ input size (3, 352, 352) and batch size 48.
Train [8/40]. 121.62 samples/sec. 394.686 ms/step.
Train [16/40]. 121.59 samples/sec. 394.760 ms/step.
Train [24/40]. 121.59 samples/sec. 394.782 ms/step.
Train [32/40]. 121.58 samples/sec. 394.803 ms/step.
Train [40/40]. 121.58 samples/sec. 394.812 ms/step.
Train benchmark of dm_nfnet_f2.dm_in1k done. 120.71 samples/sec, 394.81 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model dm_nfnet_f3.dm_in1k created, param count: 254924200
Running inference benchmark on dm_nfnet_f3.dm_in1k for 40 steps w/ input size (3, 416, 416) and batch size 256.
Infer [8/40]. 180.45 samples/sec. 1418.684 ms/step.
Infer [16/40]. 180.43 samples/sec. 1418.856 ms/step.
Infer [24/40]. 180.42 samples/sec. 1418.918 ms/step.
Infer [32/40]. 180.41 samples/sec. 1418.969 ms/step.
Infer [40/40]. 180.41 samples/sec. 1419.003 ms/step.
Inference benchmark of dm_nfnet_f3.dm_in1k done. 180.40 samples/sec, 1419.00 ms/step
Model dm_nfnet_f3.dm_in1k created, param count: 254924200
Running train benchmark on dm_nfnet_f3.dm_in1k for 40 steps w/ input size (3, 416, 416) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 1.32 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.15 GiB is free. Including non-PyTorch memory, this process has 22.49 GiB memory in use. Of the allocated memory 20.15 GiB is allocated by PyTorch, and 1.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model dm_nfnet_f3.dm_in1k created, param count: 254924200
Running train benchmark on dm_nfnet_f3.dm_in1k for 40 steps w/ input size (3, 416, 416) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 508.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 356.06 MiB is free. Including non-PyTorch memory, this process has 23.29 GiB memory in use. Of the allocated memory 21.68 GiB is allocated by PyTorch, and 396.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model dm_nfnet_f3.dm_in1k created, param count: 254924200
Running train benchmark on dm_nfnet_f3.dm_in1k for 40 steps w/ input size (3, 416, 416) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 676.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 208.06 MiB is free. Including non-PyTorch memory, this process has 23.44 GiB memory in use. Of the allocated memory 21.55 GiB is allocated by PyTorch, and 680.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model dm_nfnet_f3.dm_in1k created, param count: 254924200
Running train benchmark on dm_nfnet_f3.dm_in1k for 40 steps w/ input size (3, 416, 416) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 254.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 202.06 MiB is free. Including non-PyTorch memory, this process has 23.44 GiB memory in use. Of the allocated memory 21.90 GiB is allocated by PyTorch, and 319.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model dm_nfnet_f3.dm_in1k created, param count: 254924200
Running train benchmark on dm_nfnet_f3.dm_in1k for 40 steps w/ input size (3, 416, 416) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 264.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 14.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 21.91 GiB is allocated by PyTorch, and 506.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model dm_nfnet_f3.dm_in1k created, param count: 254924200
Running train benchmark on dm_nfnet_f3.dm_in1k for 40 steps w/ input size (3, 416, 416) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 2.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 632.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model dm_nfnet_f3.dm_in1k created, param count: 254924200
Running train benchmark on dm_nfnet_f3.dm_in1k for 40 steps w/ input size (3, 416, 416) and batch size 32.
ERROR: "CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 14.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.27 GiB is allocated by PyTorch, and 129.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model dm_nfnet_f3.dm_in1k created, param count: 254924200
Running train benchmark on dm_nfnet_f3.dm_in1k for 40 steps w/ input size (3, 416, 416) and batch size 24.
Train [8/40]. 70.81 samples/sec. 338.922 ms/step.
Train [16/40]. 70.81 samples/sec. 338.931 ms/step.
Train [24/40]. 70.81 samples/sec. 338.930 ms/step.
Train [32/40]. 70.81 samples/sec. 338.938 ms/step.
Train [40/40]. 70.81 samples/sec. 338.940 ms/step.
Train benchmark of dm_nfnet_f3.dm_in1k done. 70.11 samples/sec, 338.94 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model dm_nfnet_f4.dm_in1k created, param count: 316069172
Running inference benchmark on dm_nfnet_f4.dm_in1k for 40 steps w/ input size (3, 512, 512) and batch size 256.
Infer [8/40]. 98.90 samples/sec. 2588.474 ms/step.
Infer [16/40]. 98.89 samples/sec. 2588.693 ms/step.
Infer [24/40]. 98.87 samples/sec. 2589.190 ms/step.
Infer [32/40]. 98.86 samples/sec. 2589.526 ms/step.
Infer [40/40]. 98.85 samples/sec. 2589.775 ms/step.
Inference benchmark of dm_nfnet_f4.dm_in1k done. 98.85 samples/sec, 2589.78 ms/step
Model dm_nfnet_f4.dm_in1k created, param count: 316069172
Running train benchmark on dm_nfnet_f4.dm_in1k for 40 steps w/ input size (3, 512, 512) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 282.06 MiB is free. Including non-PyTorch memory, this process has 23.37 GiB memory in use. Of the allocated memory 20.37 GiB is allocated by PyTorch, and 1.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model dm_nfnet_f4.dm_in1k created, param count: 316069172
Running train benchmark on dm_nfnet_f4.dm_in1k for 40 steps w/ input size (3, 512, 512) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 1.50 GiB. GPU 0 has a total capacty of 23.65 GiB of which 948.06 MiB is free. Including non-PyTorch memory, this process has 22.71 GiB memory in use. Of the allocated memory 19.94 GiB is allocated by PyTorch, and 1.55 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model dm_nfnet_f4.dm_in1k created, param count: 316069172
Running train benchmark on dm_nfnet_f4.dm_in1k for 40 steps w/ input size (3, 512, 512) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 512.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 376.06 MiB is free. Including non-PyTorch memory, this process has 23.27 GiB memory in use. Of the allocated memory 21.49 GiB is allocated by PyTorch, and 566.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model dm_nfnet_f4.dm_in1k created, param count: 316069172
Running train benchmark on dm_nfnet_f4.dm_in1k for 40 steps w/ input size (3, 512, 512) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 384.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 378.06 MiB is free. Including non-PyTorch memory, this process has 23.27 GiB memory in use. Of the allocated memory 21.53 GiB is allocated by PyTorch, and 532.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model dm_nfnet_f4.dm_in1k created, param count: 316069172
Running train benchmark on dm_nfnet_f4.dm_in1k for 40 steps w/ input size (3, 512, 512) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 522.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 196.06 MiB is free. Including non-PyTorch memory, this process has 23.45 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 301.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model dm_nfnet_f4.dm_in1k created, param count: 316069172
Running train benchmark on dm_nfnet_f4.dm_in1k for 40 steps w/ input size (3, 512, 512) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 48.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 21.96 GiB is allocated by PyTorch, and 419.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model dm_nfnet_f4.dm_in1k created, param count: 316069172
Running train benchmark on dm_nfnet_f4.dm_in1k for 40 steps w/ input size (3, 512, 512) and batch size 32.
ERROR: "CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 74.06 MiB is free. Including non-PyTorch memory, this process has 23.57 GiB memory in use. Of the allocated memory 22.10 GiB is allocated by PyTorch, and 244.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model dm_nfnet_f4.dm_in1k created, param count: 316069172
Running train benchmark on dm_nfnet_f4.dm_in1k for 40 steps w/ input size (3, 512, 512) and batch size 24.
ERROR: "CUDA out of memory. Tried to allocate 72.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 36.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 21.97 GiB is allocated by PyTorch, and 416.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model dm_nfnet_f4.dm_in1k created, param count: 316069172
Running train benchmark on dm_nfnet_f4.dm_in1k for 40 steps w/ input size (3, 512, 512) and batch size 16.
ERROR: "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 10.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.33 GiB is allocated by PyTorch, and 76.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 12 for retry.
Model dm_nfnet_f4.dm_in1k created, param count: 316069172
Running train benchmark on dm_nfnet_f4.dm_in1k for 40 steps w/ input size (3, 512, 512) and batch size 12.
Train [8/40]. 34.87 samples/sec. 344.139 ms/step.
Train [16/40]. 34.87 samples/sec. 344.179 ms/step.
Train [24/40]. 34.87 samples/sec. 344.156 ms/step.
Train [32/40]. 34.87 samples/sec. 344.136 ms/step.
Train [40/40]. 34.87 samples/sec. 344.149 ms/step.
Train benchmark of dm_nfnet_f4.dm_in1k done. 34.46 samples/sec, 344.15 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model dm_nfnet_f5.dm_in1k created, param count: 377214144
Running inference benchmark on dm_nfnet_f5.dm_in1k for 40 steps w/ input size (3, 544, 544) and batch size 256.
Infer [8/40]. 72.48 samples/sec. 3532.059 ms/step.
Infer [16/40]. 72.47 samples/sec. 3532.272 ms/step.
Infer [24/40]. 72.47 samples/sec. 3532.324 ms/step.
Infer [32/40]. 72.47 samples/sec. 3532.378 ms/step.
Infer [40/40]. 72.47 samples/sec. 3532.415 ms/step.
Inference benchmark of dm_nfnet_f5.dm_in1k done. 72.47 samples/sec, 3532.41 ms/step
Model dm_nfnet_f5.dm_in1k created, param count: 377214144
Running train benchmark on dm_nfnet_f5.dm_in1k for 40 steps w/ input size (3, 544, 544) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 2.26 GiB. GPU 0 has a total capacty of 23.65 GiB of which 940.06 MiB is free. Including non-PyTorch memory, this process has 22.72 GiB memory in use. Of the allocated memory 20.78 GiB is allocated by PyTorch, and 734.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model dm_nfnet_f5.dm_in1k created, param count: 377214144
Running train benchmark on dm_nfnet_f5.dm_in1k for 40 steps w/ input size (3, 544, 544) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 1.69 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.46 GiB is free. Including non-PyTorch memory, this process has 22.18 GiB memory in use. Of the allocated memory 19.16 GiB is allocated by PyTorch, and 1.80 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model dm_nfnet_f5.dm_in1k created, param count: 377214144
Running train benchmark on dm_nfnet_f5.dm_in1k for 40 steps w/ input size (3, 544, 544) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 1.13 GiB. GPU 0 has a total capacty of 23.65 GiB of which 466.06 MiB is free. Including non-PyTorch memory, this process has 23.19 GiB memory in use. Of the allocated memory 20.91 GiB is allocated by PyTorch, and 1.05 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model dm_nfnet_f5.dm_in1k created, param count: 377214144
Running train benchmark on dm_nfnet_f5.dm_in1k for 40 steps w/ input size (3, 544, 544) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 868.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 404.06 MiB is free. Including non-PyTorch memory, this process has 23.25 GiB memory in use. Of the allocated memory 20.97 GiB is allocated by PyTorch, and 1.05 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model dm_nfnet_f5.dm_in1k created, param count: 377214144
Running train benchmark on dm_nfnet_f5.dm_in1k for 40 steps w/ input size (3, 544, 544) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 578.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 128.06 MiB is free. Including non-PyTorch memory, this process has 23.52 GiB memory in use. Of the allocated memory 21.55 GiB is allocated by PyTorch, and 760.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model dm_nfnet_f5.dm_in1k created, param count: 377214144
Running train benchmark on dm_nfnet_f5.dm_in1k for 40 steps w/ input size (3, 544, 544) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 440.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 416.06 MiB is free. Including non-PyTorch memory, this process has 23.23 GiB memory in use. Of the allocated memory 21.76 GiB is allocated by PyTorch, and 254.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model dm_nfnet_f5.dm_in1k created, param count: 377214144
Running train benchmark on dm_nfnet_f5.dm_in1k for 40 steps w/ input size (3, 544, 544) and batch size 32.
ERROR: "CUDA out of memory. Tried to allocate 74.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 24.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 21.78 GiB is allocated by PyTorch, and 623.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model dm_nfnet_f5.dm_in1k created, param count: 377214144
Running train benchmark on dm_nfnet_f5.dm_in1k for 40 steps w/ input size (3, 544, 544) and batch size 24.
ERROR: "CUDA out of memory. Tried to allocate 82.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 36.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 21.83 GiB is allocated by PyTorch, and 566.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model dm_nfnet_f5.dm_in1k created, param count: 377214144
Running train benchmark on dm_nfnet_f5.dm_in1k for 40 steps w/ input size (3, 544, 544) and batch size 16.
ERROR: "CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 12.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 21.86 GiB is allocated by PyTorch, and 552.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 12 for retry.
Model dm_nfnet_f5.dm_in1k created, param count: 377214144
Running train benchmark on dm_nfnet_f5.dm_in1k for 40 steps w/ input size (3, 544, 544) and batch size 12.
ERROR: "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 4.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 21.94 GiB is allocated by PyTorch, and 481.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 8 for retry.
Model dm_nfnet_f5.dm_in1k created, param count: 377214144
Running train benchmark on dm_nfnet_f5.dm_in1k for 40 steps w/ input size (3, 544, 544) and batch size 8.
Train [8/40]. 23.19 samples/sec. 344.973 ms/step.
Train [16/40]. 23.26 samples/sec. 343.864 ms/step.
Train [24/40]. 23.29 samples/sec. 343.492 ms/step.
Train [32/40]. 23.30 samples/sec. 343.386 ms/step.
Train [40/40]. 23.30 samples/sec. 343.360 ms/step.
Train benchmark of dm_nfnet_f5.dm_in1k done. 22.99 samples/sec, 343.36 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model dm_nfnet_f6.dm_in1k created, param count: 438359116
Running inference benchmark on dm_nfnet_f6.dm_in1k for 40 steps w/ input size (3, 576, 576) and batch size 256.
Infer [8/40]. 56.30 samples/sec. 4546.924 ms/step.
Infer [16/40]. 56.29 samples/sec. 4547.665 ms/step.
Infer [24/40]. 56.29 samples/sec. 4548.061 ms/step.
Infer [32/40]. 56.28 samples/sec. 4548.335 ms/step.
Infer [40/40]. 56.28 samples/sec. 4548.487 ms/step.
Inference benchmark of dm_nfnet_f6.dm_in1k done. 56.28 samples/sec, 4548.49 ms/step
Model dm_nfnet_f6.dm_in1k created, param count: 438359116
Running train benchmark on dm_nfnet_f6.dm_in1k for 40 steps w/ input size (3, 576, 576) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 1.27 GiB. GPU 0 has a total capacty of 23.65 GiB of which 972.06 MiB is free. Including non-PyTorch memory, this process has 22.69 GiB memory in use. Of the allocated memory 20.79 GiB is allocated by PyTorch, and 693.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model dm_nfnet_f6.dm_in1k created, param count: 438359116
Running train benchmark on dm_nfnet_f6.dm_in1k for 40 steps w/ input size (3, 576, 576) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 1.90 GiB. GPU 0 has a total capacty of 23.65 GiB of which 780.06 MiB is free. Including non-PyTorch memory, this process has 22.88 GiB memory in use. Of the allocated memory 19.60 GiB is allocated by PyTorch, and 2.05 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model dm_nfnet_f6.dm_in1k created, param count: 438359116
Running train benchmark on dm_nfnet_f6.dm_in1k for 40 steps w/ input size (3, 576, 576) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 1.27 GiB. GPU 0 has a total capacty of 23.65 GiB of which 202.06 MiB is free. Including non-PyTorch memory, this process has 23.44 GiB memory in use. Of the allocated memory 20.94 GiB is allocated by PyTorch, and 1.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model dm_nfnet_f6.dm_in1k created, param count: 438359116
Running train benchmark on dm_nfnet_f6.dm_in1k for 40 steps w/ input size (3, 576, 576) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 972.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 64.06 MiB is free. Including non-PyTorch memory, this process has 23.58 GiB memory in use. Of the allocated memory 21.61 GiB is allocated by PyTorch, and 754.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model dm_nfnet_f6.dm_in1k created, param count: 438359116
Running train benchmark on dm_nfnet_f6.dm_in1k for 40 steps w/ input size (3, 576, 576) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 648.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 74.06 MiB is free. Including non-PyTorch memory, this process has 23.57 GiB memory in use. Of the allocated memory 21.65 GiB is allocated by PyTorch, and 711.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model dm_nfnet_f6.dm_in1k created, param count: 438359116
Running train benchmark on dm_nfnet_f6.dm_in1k for 40 steps w/ input size (3, 576, 576) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 486.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 48.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 21.67 GiB is allocated by PyTorch, and 710.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model dm_nfnet_f6.dm_in1k created, param count: 438359116
Running train benchmark on dm_nfnet_f6.dm_in1k for 40 steps w/ input size (3, 576, 576) and batch size 32.
ERROR: "CUDA out of memory. Tried to allocate 82.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 72.06 MiB is free. Including non-PyTorch memory, this process has 23.57 GiB memory in use. Of the allocated memory 22.02 GiB is allocated by PyTorch, and 335.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model dm_nfnet_f6.dm_in1k created, param count: 438359116
Running train benchmark on dm_nfnet_f6.dm_in1k for 40 steps w/ input size (3, 576, 576) and batch size 24.
ERROR: "CUDA out of memory. Tried to allocate 122.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 68.06 MiB is free. Including non-PyTorch memory, this process has 23.57 GiB memory in use. Of the allocated memory 22.07 GiB is allocated by PyTorch, and 280.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model dm_nfnet_f6.dm_in1k created, param count: 438359116
Running train benchmark on dm_nfnet_f6.dm_in1k for 40 steps w/ input size (3, 576, 576) and batch size 16.
ERROR: "CUDA out of memory. Tried to allocate 62.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 6.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.27 GiB is allocated by PyTorch, and 140.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 12 for retry.
Model dm_nfnet_f6.dm_in1k created, param count: 438359116
Running train benchmark on dm_nfnet_f6.dm_in1k for 40 steps w/ input size (3, 576, 576) and batch size 12.
ERROR: "CUDA out of memory. Tried to allocate 46.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 8.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 277.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 8 for retry.
Model dm_nfnet_f6.dm_in1k created, param count: 438359116
Running train benchmark on dm_nfnet_f6.dm_in1k for 40 steps w/ input size (3, 576, 576) and batch size 8.
Train [8/40]. 19.00 samples/sec. 421.010 ms/step.
Train [16/40]. 19.00 samples/sec. 421.123 ms/step.
Train [24/40]. 18.88 samples/sec. 423.803 ms/step.
Train [32/40]. 18.90 samples/sec. 423.253 ms/step.
Train [40/40]. 18.91 samples/sec. 422.948 ms/step.
Train benchmark of dm_nfnet_f6.dm_in1k done. 18.66 samples/sec, 422.95 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model dpn68.mx_in1k created, param count: 12611602
Running inference benchmark on dpn68.mx_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 4352.07 samples/sec. 58.823 ms/step.
Infer [16/40]. 4352.29 samples/sec. 58.820 ms/step.
Infer [24/40]. 4352.40 samples/sec. 58.818 ms/step.
Infer [32/40]. 4352.32 samples/sec. 58.819 ms/step.
Infer [40/40]. 4352.30 samples/sec. 58.819 ms/step.
Inference benchmark of dpn68.mx_in1k done. 4350.21 samples/sec, 58.82 ms/step
Model dpn68.mx_in1k created, param count: 12611602
Running train benchmark on dpn68.mx_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1131.58 samples/sec. 226.232 ms/step.
Train [16/40]. 1131.52 samples/sec. 226.244 ms/step.
Train [24/40]. 1131.54 samples/sec. 226.240 ms/step.
Train [32/40]. 1131.57 samples/sec. 226.235 ms/step.
Train [40/40]. 1131.55 samples/sec. 226.239 ms/step.
Train benchmark of dpn68.mx_in1k done. 1125.92 samples/sec, 226.24 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model dpn68b.mx_in1k created, param count: 12611602
Running inference benchmark on dpn68b.mx_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 4285.72 samples/sec. 59.733 ms/step.
Infer [16/40]. 4285.55 samples/sec. 59.736 ms/step.
Infer [24/40]. 4285.43 samples/sec. 59.737 ms/step.
Infer [32/40]. 4285.35 samples/sec. 59.738 ms/step.
Infer [40/40]. 4285.31 samples/sec. 59.739 ms/step.
Inference benchmark of dpn68b.mx_in1k done. 4283.31 samples/sec, 59.74 ms/step
Model dpn68b.mx_in1k created, param count: 12611602
Running train benchmark on dpn68b.mx_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1161.92 samples/sec. 220.326 ms/step.
Train [16/40]. 1161.83 samples/sec. 220.342 ms/step.
Train [24/40]. 1161.80 samples/sec. 220.347 ms/step.
Train [32/40]. 1161.79 samples/sec. 220.349 ms/step.
Train [40/40]. 1161.80 samples/sec. 220.348 ms/step.
Train benchmark of dpn68b.mx_in1k done. 1155.94 samples/sec, 220.35 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model dpn68b.ra_in1k created, param count: 12611602
Running inference benchmark on dpn68b.ra_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 2388.62 samples/sec. 107.175 ms/step.
Infer [16/40]. 2388.70 samples/sec. 107.171 ms/step.
Infer [24/40]. 2388.58 samples/sec. 107.177 ms/step.
Infer [32/40]. 2388.54 samples/sec. 107.178 ms/step.
Infer [40/40]. 2388.58 samples/sec. 107.177 ms/step.
Inference benchmark of dpn68b.ra_in1k done. 2387.90 samples/sec, 107.18 ms/step
Model dpn68b.ra_in1k created, param count: 12611602
Running train benchmark on dpn68b.ra_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 675.72 samples/sec. 378.856 ms/step.
Train [16/40]. 675.70 samples/sec. 378.869 ms/step.
Train [24/40]. 675.70 samples/sec. 378.868 ms/step.
Train [32/40]. 675.71 samples/sec. 378.863 ms/step.
Train [40/40]. 675.69 samples/sec. 378.869 ms/step.
Train benchmark of dpn68b.ra_in1k done. 673.43 samples/sec, 378.87 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model dpn92.mx_in1k created, param count: 37668392
Running inference benchmark on dpn92.mx_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 2110.05 samples/sec. 121.324 ms/step.
Infer [16/40]. 2110.21 samples/sec. 121.315 ms/step.
Infer [24/40]. 2110.15 samples/sec. 121.318 ms/step.
Infer [32/40]. 2110.12 samples/sec. 121.320 ms/step.
Infer [40/40]. 2110.10 samples/sec. 121.321 ms/step.
Inference benchmark of dpn92.mx_in1k done. 2109.56 samples/sec, 121.32 ms/step
Model dpn92.mx_in1k created, param count: 37668392
Running train benchmark on dpn92.mx_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 514.67 samples/sec. 497.403 ms/step.
Train [16/40]. 514.67 samples/sec. 497.407 ms/step.
Train [24/40]. 514.68 samples/sec. 497.400 ms/step.
Train [32/40]. 514.68 samples/sec. 497.393 ms/step.
Train [40/40]. 514.69 samples/sec. 497.389 ms/step.
Train benchmark of dpn92.mx_in1k done. 513.08 samples/sec, 497.39 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model dpn98.mx_in1k created, param count: 61570728
Running inference benchmark on dpn98.mx_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1496.45 samples/sec. 171.071 ms/step.
Infer [16/40]. 1496.41 samples/sec. 171.076 ms/step.
Infer [24/40]. 1496.40 samples/sec. 171.077 ms/step.
Infer [32/40]. 1496.37 samples/sec. 171.080 ms/step.
Infer [40/40]. 1496.33 samples/sec. 171.086 ms/step.
Inference benchmark of dpn98.mx_in1k done. 1496.01 samples/sec, 171.09 ms/step
Model dpn98.mx_in1k created, param count: 61570728
Running train benchmark on dpn98.mx_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 62.06 MiB is free. Including non-PyTorch memory, this process has 23.58 GiB memory in use. Of the allocated memory 21.17 GiB is allocated by PyTorch, and 1.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model dpn98.mx_in1k created, param count: 61570728
Running train benchmark on dpn98.mx_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
Train [8/40]. 326.73 samples/sec. 587.637 ms/step.
Train [16/40]. 357.39 samples/sec. 537.232 ms/step.
Train [24/40]. 368.86 samples/sec. 520.524 ms/step.
Train [32/40]. 374.88 samples/sec. 512.167 ms/step.
Train [40/40]. 378.60 samples/sec. 507.129 ms/step.
Train benchmark of dpn98.mx_in1k done. 377.38 samples/sec, 507.13 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model dpn107.mx_in1k created, param count: 86917800
Running inference benchmark on dpn107.mx_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1006.43 samples/sec. 254.364 ms/step.
Infer [16/40]. 1006.30 samples/sec. 254.398 ms/step.
Infer [24/40]. 1006.22 samples/sec. 254.416 ms/step.
Infer [32/40]. 1006.20 samples/sec. 254.423 ms/step.
Infer [40/40]. 1006.18 samples/sec. 254.428 ms/step.
Inference benchmark of dpn107.mx_in1k done. 1006.00 samples/sec, 254.43 ms/step
Model dpn107.mx_in1k created, param count: 86917800
Running train benchmark on dpn107.mx_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 196.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 178.06 MiB is free. Including non-PyTorch memory, this process has 23.47 GiB memory in use. Of the allocated memory 20.79 GiB is allocated by PyTorch, and 1.45 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model dpn107.mx_in1k created, param count: 86917800
Running train benchmark on dpn107.mx_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 74.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 12.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 21.97 GiB is allocated by PyTorch, and 440.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model dpn107.mx_in1k created, param count: 86917800
Running train benchmark on dpn107.mx_in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
Train [8/40]. 285.80 samples/sec. 447.869 ms/step.
Train [16/40]. 285.80 samples/sec. 447.870 ms/step.
Train [24/40]. 285.80 samples/sec. 447.859 ms/step.
Train [32/40]. 285.80 samples/sec. 447.861 ms/step.
Train [40/40]. 285.80 samples/sec. 447.860 ms/step.
Train benchmark of dpn107.mx_in1k done. 284.71 samples/sec, 447.86 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model dpn131.mx_in1k created, param count: 79254504
Running inference benchmark on dpn131.mx_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1099.68 samples/sec. 232.794 ms/step.
Infer [16/40]. 1099.51 samples/sec. 232.830 ms/step.
Infer [24/40]. 1099.41 samples/sec. 232.852 ms/step.
Infer [32/40]. 1099.35 samples/sec. 232.865 ms/step.
Infer [40/40]. 1099.34 samples/sec. 232.867 ms/step.
Inference benchmark of dpn131.mx_in1k done. 1099.13 samples/sec, 232.87 ms/step
Model dpn131.mx_in1k created, param count: 79254504
Running train benchmark on dpn131.mx_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 246.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 4.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 20.98 GiB is allocated by PyTorch, and 1.43 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model dpn131.mx_in1k created, param count: 79254504
Running train benchmark on dpn131.mx_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 114.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 64.06 MiB is free. Including non-PyTorch memory, this process has 23.58 GiB memory in use. Of the allocated memory 21.26 GiB is allocated by PyTorch, and 1.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model dpn131.mx_in1k created, param count: 79254504
Running train benchmark on dpn131.mx_in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
Train [8/40]. 308.92 samples/sec. 414.349 ms/step.
Train [16/40]. 308.93 samples/sec. 414.334 ms/step.
Train [24/40]. 308.92 samples/sec. 414.341 ms/step.
Train [32/40]. 308.92 samples/sec. 414.344 ms/step.
Train [40/40]. 308.92 samples/sec. 414.342 ms/step.
Train benchmark of dpn131.mx_in1k done. 307.45 samples/sec, 414.34 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model eca_botnext26ts_256.c1_in1k created, param count: 10593301
Running inference benchmark on eca_botnext26ts_256.c1_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
Infer [8/40]. 4579.27 samples/sec. 55.904 ms/step.
Infer [16/40]. 4579.57 samples/sec. 55.900 ms/step.
Infer [24/40]. 4579.71 samples/sec. 55.899 ms/step.
Infer [32/40]. 4579.49 samples/sec. 55.901 ms/step.
Infer [40/40]. 4579.28 samples/sec. 55.904 ms/step.
Inference benchmark of eca_botnext26ts_256.c1_in1k done. 4577.17 samples/sec, 55.90 ms/step
Model eca_botnext26ts_256.c1_in1k created, param count: 10593301
Running train benchmark on eca_botnext26ts_256.c1_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
Train [8/40]. 1218.83 samples/sec. 210.037 ms/step.
Train [16/40]. 1218.78 samples/sec. 210.046 ms/step.
Train [24/40]. 1218.78 samples/sec. 210.047 ms/step.
Train [32/40]. 1218.77 samples/sec. 210.048 ms/step.
Train [40/40]. 1218.74 samples/sec. 210.053 ms/step.
Train benchmark of eca_botnext26ts_256.c1_in1k done. 1213.98 samples/sec, 210.05 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model eca_halonext26ts.c1_in1k created, param count: 10756885
Running inference benchmark on eca_halonext26ts.c1_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
Infer [8/40]. 4530.78 samples/sec. 56.502 ms/step.
Infer [16/40]. 4530.54 samples/sec. 56.505 ms/step.
Infer [24/40]. 4530.34 samples/sec. 56.508 ms/step.
Infer [32/40]. 4530.25 samples/sec. 56.509 ms/step.
Infer [40/40]. 4530.21 samples/sec. 56.509 ms/step.
Inference benchmark of eca_halonext26ts.c1_in1k done. 4528.16 samples/sec, 56.51 ms/step
Model eca_halonext26ts.c1_in1k created, param count: 10756885
Running train benchmark on eca_halonext26ts.c1_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
Train [8/40]. 1185.48 samples/sec. 215.947 ms/step.
Train [16/40]. 1185.41 samples/sec. 215.959 ms/step.
Train [24/40]. 1185.45 samples/sec. 215.951 ms/step.
Train [32/40]. 1185.48 samples/sec. 215.946 ms/step.
Train [40/40]. 1185.48 samples/sec. 215.946 ms/step.
Train benchmark of eca_halonext26ts.c1_in1k done. 1180.59 samples/sec, 215.95 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model eca_nfnet_l0.ra2_in1k created, param count: 24143924
Running inference benchmark on eca_nfnet_l0.ra2_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 2021.06 samples/sec. 126.666 ms/step.
Infer [16/40]. 2021.27 samples/sec. 126.653 ms/step.
Infer [24/40]. 2021.33 samples/sec. 126.650 ms/step.
Infer [32/40]. 2021.34 samples/sec. 126.649 ms/step.
Infer [40/40]. 2021.35 samples/sec. 126.648 ms/step.
Inference benchmark of eca_nfnet_l0.ra2_in1k done. 2020.85 samples/sec, 126.65 ms/step
Model eca_nfnet_l0.ra2_in1k created, param count: 24143924
Running train benchmark on eca_nfnet_l0.ra2_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 730.64 samples/sec. 350.379 ms/step.
Train [16/40]. 730.50 samples/sec. 350.446 ms/step.
Train [24/40]. 730.47 samples/sec. 350.461 ms/step.
Train [32/40]. 730.46 samples/sec. 350.465 ms/step.
Train [40/40]. 730.44 samples/sec. 350.475 ms/step.
Train benchmark of eca_nfnet_l0.ra2_in1k done. 727.87 samples/sec, 350.48 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model eca_nfnet_l1.ra2_in1k created, param count: 41407728
Running inference benchmark on eca_nfnet_l1.ra2_in1k for 40 steps w/ input size (3, 320, 320) and batch size 256.
Infer [8/40]. 974.67 samples/sec. 262.653 ms/step.
Infer [16/40]. 974.69 samples/sec. 262.647 ms/step.
Infer [24/40]. 974.65 samples/sec. 262.659 ms/step.
Infer [32/40]. 974.63 samples/sec. 262.665 ms/step.
Infer [40/40]. 974.62 samples/sec. 262.666 ms/step.
Inference benchmark of eca_nfnet_l1.ra2_in1k done. 974.46 samples/sec, 262.67 ms/step
Model eca_nfnet_l1.ra2_in1k created, param count: 41407728
Running train benchmark on eca_nfnet_l1.ra2_in1k for 40 steps w/ input size (3, 320, 320) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 300.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 14.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.24 GiB is allocated by PyTorch, and 165.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model eca_nfnet_l1.ra2_in1k created, param count: 41407728
Running train benchmark on eca_nfnet_l1.ra2_in1k for 40 steps w/ input size (3, 320, 320) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 226.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 12.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.14 GiB is allocated by PyTorch, and 267.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model eca_nfnet_l1.ra2_in1k created, param count: 41407728
Running train benchmark on eca_nfnet_l1.ra2_in1k for 40 steps w/ input size (3, 320, 320) and batch size 128.
Train [8/40]. 350.96 samples/sec. 364.714 ms/step.
Train [16/40]. 350.95 samples/sec. 364.723 ms/step.
Train [24/40]. 350.95 samples/sec. 364.728 ms/step.
Train [32/40]. 350.95 samples/sec. 364.720 ms/step.
Train [40/40]. 350.95 samples/sec. 364.728 ms/step.
Train benchmark of eca_nfnet_l1.ra2_in1k done. 349.18 samples/sec, 364.73 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model eca_nfnet_l2.ra3_in1k created, param count: 56722348
Running inference benchmark on eca_nfnet_l2.ra3_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 475.89 samples/sec. 537.945 ms/step.
Infer [16/40]. 475.87 samples/sec. 537.963 ms/step.
Infer [24/40]. 475.86 samples/sec. 537.978 ms/step.
Infer [32/40]. 475.85 samples/sec. 537.983 ms/step.
Infer [40/40]. 475.85 samples/sec. 537.987 ms/step.
Inference benchmark of eca_nfnet_l2.ra3_in1k done. 475.81 samples/sec, 537.99 ms/step
Model eca_nfnet_l2.ra3_in1k created, param count: 56722348
Running train benchmark on eca_nfnet_l2.ra3_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 576.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 188.06 MiB is free. Including non-PyTorch memory, this process has 23.46 GiB memory in use. Of the allocated memory 21.99 GiB is allocated by PyTorch, and 242.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model eca_nfnet_l2.ra3_in1k created, param count: 56722348
Running train benchmark on eca_nfnet_l2.ra3_in1k for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 432.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 228.06 MiB is free. Including non-PyTorch memory, this process has 23.42 GiB memory in use. Of the allocated memory 22.01 GiB is allocated by PyTorch, and 183.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model eca_nfnet_l2.ra3_in1k created, param count: 56722348
Running train benchmark on eca_nfnet_l2.ra3_in1k for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 216.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 80.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 22.09 GiB is allocated by PyTorch, and 252.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model eca_nfnet_l2.ra3_in1k created, param count: 56722348
Running train benchmark on eca_nfnet_l2.ra3_in1k for 40 steps w/ input size (3, 384, 384) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 162.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 80.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 22.09 GiB is allocated by PyTorch, and 248.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model eca_nfnet_l2.ra3_in1k created, param count: 56722348
Running train benchmark on eca_nfnet_l2.ra3_in1k for 40 steps w/ input size (3, 384, 384) and batch size 64.
Train [8/40]. 176.25 samples/sec. 363.119 ms/step.
Train [16/40]. 176.25 samples/sec. 363.123 ms/step.
Train [24/40]. 176.25 samples/sec. 363.121 ms/step.
Train [32/40]. 176.25 samples/sec. 363.125 ms/step.
Train [40/40]. 176.25 samples/sec. 363.122 ms/step.
Train benchmark of eca_nfnet_l2.ra3_in1k done. 175.15 samples/sec, 363.12 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model eca_resnet33ts.ra2_in1k created, param count: 19676302
Running inference benchmark on eca_resnet33ts.ra2_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 3116.45 samples/sec. 82.145 ms/step.
Infer [16/40]. 3116.40 samples/sec. 82.146 ms/step.
Infer [24/40]. 3116.43 samples/sec. 82.145 ms/step.
Infer [32/40]. 3116.37 samples/sec. 82.147 ms/step.
Infer [40/40]. 3116.33 samples/sec. 82.148 ms/step.
Inference benchmark of eca_resnet33ts.ra2_in1k done. 3115.28 samples/sec, 82.15 ms/step
Model eca_resnet33ts.ra2_in1k created, param count: 19676302
Running train benchmark on eca_resnet33ts.ra2_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 841.32 samples/sec. 304.284 ms/step.
Train [16/40]. 841.36 samples/sec. 304.271 ms/step.
Train [24/40]. 841.34 samples/sec. 304.275 ms/step.
Train [32/40]. 841.34 samples/sec. 304.275 ms/step.
Train [40/40]. 841.34 samples/sec. 304.276 ms/step.
Train benchmark of eca_resnet33ts.ra2_in1k done. 838.98 samples/sec, 304.28 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model eca_resnext26ts.ch_in1k created, param count: 10297988
Running inference benchmark on eca_resnext26ts.ch_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 3781.11 samples/sec. 67.705 ms/step.
Infer [16/40]. 3780.71 samples/sec. 67.712 ms/step.
Infer [24/40]. 3780.58 samples/sec. 67.715 ms/step.
Infer [32/40]. 3780.41 samples/sec. 67.717 ms/step.
Infer [40/40]. 3780.44 samples/sec. 67.717 ms/step.
Inference benchmark of eca_resnext26ts.ch_in1k done. 3778.91 samples/sec, 67.72 ms/step
Model eca_resnext26ts.ch_in1k created, param count: 10297988
Running train benchmark on eca_resnext26ts.ch_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 979.54 samples/sec. 261.348 ms/step.
Train [16/40]. 979.57 samples/sec. 261.340 ms/step.
Train [24/40]. 979.59 samples/sec. 261.335 ms/step.
Train [32/40]. 979.58 samples/sec. 261.338 ms/step.
Train [40/40]. 979.57 samples/sec. 261.339 ms/step.
Train benchmark of eca_resnext26ts.ch_in1k done. 976.81 samples/sec, 261.34 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model ecaresnet26t.ra2_in1k created, param count: 16011916
Running inference benchmark on ecaresnet26t.ra2_in1k for 40 steps w/ input size (3, 320, 320) and batch size 256.
Infer [8/40]. 2788.33 samples/sec. 91.811 ms/step.
Infer [16/40]. 2787.47 samples/sec. 91.839 ms/step.
Infer [24/40]. 2787.25 samples/sec. 91.847 ms/step.
Infer [32/40]. 2787.13 samples/sec. 91.851 ms/step.
Infer [40/40]. 2787.01 samples/sec. 91.855 ms/step.
Inference benchmark of ecaresnet26t.ra2_in1k done. 2786.17 samples/sec, 91.86 ms/step
Model ecaresnet26t.ra2_in1k created, param count: 16011916
Running train benchmark on ecaresnet26t.ra2_in1k for 40 steps w/ input size (3, 320, 320) and batch size 256.
Train [8/40]. 732.99 samples/sec. 349.255 ms/step.
Train [16/40]. 732.99 samples/sec. 349.254 ms/step.
Train [24/40]. 733.01 samples/sec. 349.244 ms/step.
Train [32/40]. 733.01 samples/sec. 349.247 ms/step.
Train [40/40]. 733.00 samples/sec. 349.249 ms/step.
Train benchmark of ecaresnet26t.ra2_in1k done. 731.43 samples/sec, 349.25 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model ecaresnet50d.miil_in1k created, param count: 25576350
Running inference benchmark on ecaresnet50d.miil_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 2127.73 samples/sec. 120.316 ms/step.
Infer [16/40]. 2127.57 samples/sec. 120.325 ms/step.
Infer [24/40]. 2127.56 samples/sec. 120.326 ms/step.
Infer [32/40]. 2127.51 samples/sec. 120.328 ms/step.
Infer [40/40]. 2127.49 samples/sec. 120.329 ms/step.
Inference benchmark of ecaresnet50d.miil_in1k done. 2126.98 samples/sec, 120.33 ms/step
Model ecaresnet50d.miil_in1k created, param count: 25576350
Running train benchmark on ecaresnet50d.miil_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 82.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 14.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.31 GiB is allocated by PyTorch, and 89.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model ecaresnet50d.miil_in1k created, param count: 25576350
Running train benchmark on ecaresnet50d.miil_in1k for 40 steps w/ input size (3, 288, 288) and batch size 192.
Train [8/40]. 590.31 samples/sec. 325.250 ms/step.
Train [16/40]. 590.30 samples/sec. 325.261 ms/step.
Train [24/40]. 590.29 samples/sec. 325.265 ms/step.
Train [32/40]. 590.29 samples/sec. 325.264 ms/step.
Train [40/40]. 590.29 samples/sec. 325.263 ms/step.
Train benchmark of ecaresnet50d.miil_in1k done. 588.41 samples/sec, 325.26 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model ecaresnet50d_pruned.miil_in1k created, param count: 19939713
Running inference benchmark on ecaresnet50d_pruned.miil_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 3864.38 samples/sec. 66.246 ms/step.
Infer [16/40]. 3864.28 samples/sec. 66.248 ms/step.
Infer [24/40]. 3864.13 samples/sec. 66.250 ms/step.
Infer [32/40]. 3864.13 samples/sec. 66.250 ms/step.
Infer [40/40]. 3864.06 samples/sec. 66.252 ms/step.
Inference benchmark of ecaresnet50d_pruned.miil_in1k done. 3862.53 samples/sec, 66.25 ms/step
Model ecaresnet50d_pruned.miil_in1k created, param count: 19939713
Running train benchmark on ecaresnet50d_pruned.miil_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 926.97 samples/sec. 276.168 ms/step.
Train [16/40]. 926.97 samples/sec. 276.170 ms/step.
Train [24/40]. 926.96 samples/sec. 276.172 ms/step.
Train [32/40]. 926.96 samples/sec. 276.172 ms/step.
Train [40/40]. 926.96 samples/sec. 276.170 ms/step.
Train benchmark of ecaresnet50d_pruned.miil_in1k done. 923.60 samples/sec, 276.17 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model ecaresnet50t.a1_in1k created, param count: 25573814
Running inference benchmark on ecaresnet50t.a1_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 2092.17 samples/sec. 122.361 ms/step.
Infer [16/40]. 2092.09 samples/sec. 122.366 ms/step.
Infer [24/40]. 2091.50 samples/sec. 122.400 ms/step.
Infer [32/40]. 2091.14 samples/sec. 122.422 ms/step.
Infer [40/40]. 2090.91 samples/sec. 122.435 ms/step.
Inference benchmark of ecaresnet50t.a1_in1k done. 2090.40 samples/sec, 122.44 ms/step
Model ecaresnet50t.a1_in1k created, param count: 25573814
Running train benchmark on ecaresnet50t.a1_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 82.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 12.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.35 GiB is allocated by PyTorch, and 48.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model ecaresnet50t.a1_in1k created, param count: 25573814
Running train benchmark on ecaresnet50t.a1_in1k for 40 steps w/ input size (3, 288, 288) and batch size 192.
Train [8/40]. 594.58 samples/sec. 322.920 ms/step.
Train [16/40]. 594.58 samples/sec. 322.917 ms/step.
Train [24/40]. 594.58 samples/sec. 322.919 ms/step.
Train [32/40]. 594.57 samples/sec. 322.921 ms/step.
Train [40/40]. 594.57 samples/sec. 322.921 ms/step.
Train benchmark of ecaresnet50t.a1_in1k done. 592.57 samples/sec, 322.92 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model ecaresnet50t.a2_in1k created, param count: 25573814
Running inference benchmark on ecaresnet50t.a2_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 2089.97 samples/sec. 122.490 ms/step.
Infer [16/40]. 2089.98 samples/sec. 122.489 ms/step.
Infer [24/40]. 2090.01 samples/sec. 122.487 ms/step.
Infer [32/40]. 2089.98 samples/sec. 122.489 ms/step.
Infer [40/40]. 2089.97 samples/sec. 122.490 ms/step.
Inference benchmark of ecaresnet50t.a2_in1k done. 2089.45 samples/sec, 122.49 ms/step
Model ecaresnet50t.a2_in1k created, param count: 25573814
Running train benchmark on ecaresnet50t.a2_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 82.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 12.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.35 GiB is allocated by PyTorch, and 48.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model ecaresnet50t.a2_in1k created, param count: 25573814
Running train benchmark on ecaresnet50t.a2_in1k for 40 steps w/ input size (3, 288, 288) and batch size 192.
Train [8/40]. 594.57 samples/sec. 322.922 ms/step.
Train [16/40]. 594.55 samples/sec. 322.935 ms/step.
Train [24/40]. 594.55 samples/sec. 322.934 ms/step.
Train [32/40]. 594.55 samples/sec. 322.935 ms/step.
Train [40/40]. 594.55 samples/sec. 322.934 ms/step.
Train benchmark of ecaresnet50t.a2_in1k done. 592.54 samples/sec, 322.93 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model ecaresnet50t.a3_in1k created, param count: 25573814
Running inference benchmark on ecaresnet50t.a3_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3285.97 samples/sec. 77.907 ms/step.
Infer [16/40]. 3286.11 samples/sec. 77.904 ms/step.
Infer [24/40]. 3286.27 samples/sec. 77.900 ms/step.
Infer [32/40]. 3286.41 samples/sec. 77.896 ms/step.
Infer [40/40]. 3286.47 samples/sec. 77.895 ms/step.
Inference benchmark of ecaresnet50t.a3_in1k done. 3285.29 samples/sec, 77.89 ms/step
Model ecaresnet50t.a3_in1k created, param count: 25573814
Running train benchmark on ecaresnet50t.a3_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 961.71 samples/sec. 266.192 ms/step.
Train [16/40]. 961.69 samples/sec. 266.199 ms/step.
Train [24/40]. 961.66 samples/sec. 266.207 ms/step.
Train [32/40]. 961.66 samples/sec. 266.207 ms/step.
Train [40/40]. 961.66 samples/sec. 266.206 ms/step.
Train benchmark of ecaresnet50t.a3_in1k done. 958.01 samples/sec, 266.21 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model ecaresnet50t.ra2_in1k created, param count: 25573814
Running inference benchmark on ecaresnet50t.ra2_in1k for 40 steps w/ input size (3, 320, 320) and batch size 256.
Infer [8/40]. 1791.71 samples/sec. 142.881 ms/step.
Infer [16/40]. 1791.39 samples/sec. 142.906 ms/step.
Infer [24/40]. 1791.18 samples/sec. 142.923 ms/step.
Infer [32/40]. 1791.03 samples/sec. 142.934 ms/step.
Infer [40/40]. 1790.92 samples/sec. 142.943 ms/step.
Inference benchmark of ecaresnet50t.ra2_in1k done. 1790.51 samples/sec, 142.94 ms/step
Model ecaresnet50t.ra2_in1k created, param count: 25573814
Running train benchmark on ecaresnet50t.ra2_in1k for 40 steps w/ input size (3, 320, 320) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 200.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 130.06 MiB is free. Including non-PyTorch memory, this process has 23.51 GiB memory in use. Of the allocated memory 22.19 GiB is allocated by PyTorch, and 100.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model ecaresnet50t.ra2_in1k created, param count: 25573814
Running train benchmark on ecaresnet50t.ra2_in1k for 40 steps w/ input size (3, 320, 320) and batch size 192.
Train [8/40]. 479.81 samples/sec. 400.162 ms/step.
Train [16/40]. 479.82 samples/sec. 400.153 ms/step.
Train [24/40]. 479.82 samples/sec. 400.154 ms/step.
Train [32/40]. 479.81 samples/sec. 400.159 ms/step.
Train [40/40]. 479.81 samples/sec. 400.161 ms/step.
Train benchmark of ecaresnet50t.ra2_in1k done. 478.52 samples/sec, 400.16 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model ecaresnet101d.miil_in1k created, param count: 44568563
Running inference benchmark on ecaresnet101d.miil_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 1367.55 samples/sec. 187.196 ms/step.
Infer [16/40]. 1367.50 samples/sec. 187.203 ms/step.
Infer [24/40]. 1367.46 samples/sec. 187.209 ms/step.
Infer [32/40]. 1367.43 samples/sec. 187.213 ms/step.
Infer [40/40]. 1367.42 samples/sec. 187.213 ms/step.
Inference benchmark of ecaresnet101d.miil_in1k done. 1367.15 samples/sec, 187.21 ms/step
Model ecaresnet101d.miil_in1k created, param count: 44568563
Running train benchmark on ecaresnet101d.miil_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 162.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 76.06 MiB is free. Including non-PyTorch memory, this process has 23.57 GiB memory in use. Of the allocated memory 22.22 GiB is allocated by PyTorch, and 115.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model ecaresnet101d.miil_in1k created, param count: 44568563
Running train benchmark on ecaresnet101d.miil_in1k for 40 steps w/ input size (3, 288, 288) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 122.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 94.06 MiB is free. Including non-PyTorch memory, this process has 23.55 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 246.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model ecaresnet101d.miil_in1k created, param count: 44568563
Running train benchmark on ecaresnet101d.miil_in1k for 40 steps w/ input size (3, 288, 288) and batch size 128.
Train [8/40]. 394.18 samples/sec. 324.726 ms/step.
Train [16/40]. 394.19 samples/sec. 324.716 ms/step.
Train [24/40]. 394.20 samples/sec. 324.711 ms/step.
Train [32/40]. 394.20 samples/sec. 324.711 ms/step.
Train [40/40]. 394.20 samples/sec. 324.712 ms/step.
Train benchmark of ecaresnet101d.miil_in1k done. 392.32 samples/sec, 324.71 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model ecaresnet101d_pruned.miil_in1k created, param count: 24876040
Running inference benchmark on ecaresnet101d_pruned.miil_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 3386.61 samples/sec. 75.592 ms/step.
Infer [16/40]. 3386.63 samples/sec. 75.591 ms/step.
Infer [24/40]. 3386.42 samples/sec. 75.596 ms/step.
Infer [32/40]. 3386.35 samples/sec. 75.598 ms/step.
Infer [40/40]. 3386.38 samples/sec. 75.597 ms/step.
Inference benchmark of ecaresnet101d_pruned.miil_in1k done. 3385.17 samples/sec, 75.60 ms/step
Model ecaresnet101d_pruned.miil_in1k created, param count: 24876040
Running train benchmark on ecaresnet101d_pruned.miil_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 792.18 samples/sec. 323.158 ms/step.
Train [16/40]. 792.15 samples/sec. 323.173 ms/step.
Train [24/40]. 792.13 samples/sec. 323.181 ms/step.
Train [32/40]. 792.12 samples/sec. 323.184 ms/step.
Train [40/40]. 792.11 samples/sec. 323.187 ms/step.
Train benchmark of ecaresnet101d_pruned.miil_in1k done. 788.27 samples/sec, 323.19 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model ecaresnet269d.ra2_in1k created, param count: 102093077
Running inference benchmark on ecaresnet269d.ra2_in1k for 40 steps w/ input size (3, 352, 352) and batch size 256.
Infer [8/40]. 387.40 samples/sec. 660.808 ms/step.
Infer [16/40]. 387.40 samples/sec. 660.809 ms/step.
Infer [24/40]. 387.40 samples/sec. 660.813 ms/step.
Infer [32/40]. 387.40 samples/sec. 660.823 ms/step.
Infer [40/40]. 387.39 samples/sec. 660.824 ms/step.
Inference benchmark of ecaresnet269d.ra2_in1k done. 387.37 samples/sec, 660.82 ms/step
Model ecaresnet269d.ra2_in1k created, param count: 102093077
Running train benchmark on ecaresnet269d.ra2_in1k for 40 steps w/ input size (3, 352, 352) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 484.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 248.06 MiB is free. Including non-PyTorch memory, this process has 23.40 GiB memory in use. Of the allocated memory 21.89 GiB is allocated by PyTorch, and 283.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model ecaresnet269d.ra2_in1k created, param count: 102093077
Running train benchmark on ecaresnet269d.ra2_in1k for 40 steps w/ input size (3, 352, 352) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 364.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 38.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 22.17 GiB is allocated by PyTorch, and 208.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model ecaresnet269d.ra2_in1k created, param count: 102093077
Running train benchmark on ecaresnet269d.ra2_in1k for 40 steps w/ input size (3, 352, 352) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 242.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 78.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 22.18 GiB is allocated by PyTorch, and 163.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model ecaresnet269d.ra2_in1k created, param count: 102093077
Running train benchmark on ecaresnet269d.ra2_in1k for 40 steps w/ input size (3, 352, 352) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 46.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 22.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 21.64 GiB is allocated by PyTorch, and 770.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model ecaresnet269d.ra2_in1k created, param count: 102093077
Running train benchmark on ecaresnet269d.ra2_in1k for 40 steps w/ input size (3, 352, 352) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 62.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 56.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 21.84 GiB is allocated by PyTorch, and 534.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model ecaresnet269d.ra2_in1k created, param count: 102093077
Running train benchmark on ecaresnet269d.ra2_in1k for 40 steps w/ input size (3, 352, 352) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 46.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 20.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 323.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model ecaresnet269d.ra2_in1k created, param count: 102093077
Running train benchmark on ecaresnet269d.ra2_in1k for 40 steps w/ input size (3, 352, 352) and batch size 32.
Train [8/40]. 118.44 samples/sec. 270.175 ms/step.
Train [16/40]. 118.42 samples/sec. 270.224 ms/step.
Train [24/40]. 118.42 samples/sec. 270.216 ms/step.
Train [32/40]. 118.42 samples/sec. 270.232 ms/step.
Train [40/40]. 118.41 samples/sec. 270.237 ms/step.
Train benchmark of ecaresnet269d.ra2_in1k done. 117.10 samples/sec, 270.24 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model ecaresnetlight.miil_in1k created, param count: 30162046
Running inference benchmark on ecaresnetlight.miil_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 2791.25 samples/sec. 91.715 ms/step.
Infer [16/40]. 2791.10 samples/sec. 91.720 ms/step.
Infer [24/40]. 2791.02 samples/sec. 91.723 ms/step.
Infer [32/40]. 2790.80 samples/sec. 91.730 ms/step.
Infer [40/40]. 2790.83 samples/sec. 91.729 ms/step.
Inference benchmark of ecaresnetlight.miil_in1k done. 2789.96 samples/sec, 91.73 ms/step
Model ecaresnetlight.miil_in1k created, param count: 30162046
Running train benchmark on ecaresnetlight.miil_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 818.61 samples/sec. 312.727 ms/step.
Train [16/40]. 818.61 samples/sec. 312.726 ms/step.
Train [24/40]. 818.59 samples/sec. 312.732 ms/step.
Train [32/40]. 818.59 samples/sec. 312.733 ms/step.
Train [40/40]. 818.60 samples/sec. 312.728 ms/step.
Train benchmark of ecaresnetlight.miil_in1k done. 815.77 samples/sec, 312.73 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model edgenext_base.in21k_ft_in1k created, param count: 18511292
Running inference benchmark on edgenext_base.in21k_ft_in1k for 40 steps w/ input size (3, 320, 320) and batch size 256.
Infer [8/40]. 2338.05 samples/sec. 109.493 ms/step.
Infer [16/40]. 2337.55 samples/sec. 109.517 ms/step.
Infer [24/40]. 2337.56 samples/sec. 109.516 ms/step.
Infer [32/40]. 2337.63 samples/sec. 109.513 ms/step.
Infer [40/40]. 2337.68 samples/sec. 109.510 ms/step.
Inference benchmark of edgenext_base.in21k_ft_in1k done. 2337.09 samples/sec, 109.51 ms/step
Model edgenext_base.in21k_ft_in1k created, param count: 18511292
Running train benchmark on edgenext_base.in21k_ft_in1k for 40 steps w/ input size (3, 320, 320) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 58.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 32.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 22.25 GiB is allocated by PyTorch, and 137.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model edgenext_base.in21k_ft_in1k created, param count: 18511292
Running train benchmark on edgenext_base.in21k_ft_in1k for 40 steps w/ input size (3, 320, 320) and batch size 192.
Train [8/40]. 792.88 samples/sec. 242.154 ms/step.
Train [16/40]. 792.83 samples/sec. 242.170 ms/step.
Train [24/40]. 792.82 samples/sec. 242.173 ms/step.
Train [32/40]. 792.83 samples/sec. 242.170 ms/step.
Train [40/40]. 792.83 samples/sec. 242.171 ms/step.
Train benchmark of edgenext_base.in21k_ft_in1k done. 789.22 samples/sec, 242.17 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model edgenext_base.usi_in1k created, param count: 18511292
Running inference benchmark on edgenext_base.usi_in1k for 40 steps w/ input size (3, 320, 320) and batch size 256.
Infer [8/40]. 2340.40 samples/sec. 109.383 ms/step.
Infer [16/40]. 2340.15 samples/sec. 109.395 ms/step.
Infer [24/40]. 2339.50 samples/sec. 109.425 ms/step.
Infer [32/40]. 2338.95 samples/sec. 109.451 ms/step.
Infer [40/40]. 2338.46 samples/sec. 109.474 ms/step.
Inference benchmark of edgenext_base.usi_in1k done. 2337.87 samples/sec, 109.47 ms/step
Model edgenext_base.usi_in1k created, param count: 18511292
Running train benchmark on edgenext_base.usi_in1k for 40 steps w/ input size (3, 320, 320) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 58.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 32.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 22.25 GiB is allocated by PyTorch, and 137.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model edgenext_base.usi_in1k created, param count: 18511292
Running train benchmark on edgenext_base.usi_in1k for 40 steps w/ input size (3, 320, 320) and batch size 192.
Train [8/40]. 793.69 samples/sec. 241.909 ms/step.
Train [16/40]. 793.14 samples/sec. 242.075 ms/step.
Train [24/40]. 793.04 samples/sec. 242.107 ms/step.
Train [32/40]. 792.97 samples/sec. 242.129 ms/step.
Train [40/40]. 792.90 samples/sec. 242.148 ms/step.
Train benchmark of edgenext_base.usi_in1k done. 789.24 samples/sec, 242.15 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model edgenext_small.usi_in1k created, param count: 5586832
Running inference benchmark on edgenext_small.usi_in1k for 40 steps w/ input size (3, 320, 320) and batch size 256.
Infer [8/40]. 4015.37 samples/sec. 63.755 ms/step.
Infer [16/40]. 4017.11 samples/sec. 63.727 ms/step.
Infer [24/40]. 4016.43 samples/sec. 63.738 ms/step.
Infer [32/40]. 4015.90 samples/sec. 63.747 ms/step.
Infer [40/40]. 4015.93 samples/sec. 63.746 ms/step.
Inference benchmark of edgenext_small.usi_in1k done. 4014.32 samples/sec, 63.75 ms/step
Model edgenext_small.usi_in1k created, param count: 5586832
Running train benchmark on edgenext_small.usi_in1k for 40 steps w/ input size (3, 320, 320) and batch size 256.
Train [8/40]. 1320.41 samples/sec. 193.880 ms/step.
Train [16/40]. 1320.56 samples/sec. 193.856 ms/step.
Train [24/40]. 1320.68 samples/sec. 193.840 ms/step.
Train [32/40]. 1320.64 samples/sec. 193.846 ms/step.
Train [40/40]. 1320.62 samples/sec. 193.848 ms/step.
Train benchmark of edgenext_small.usi_in1k done. 1313.86 samples/sec, 193.85 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model edgenext_small_rw.sw_in1k created, param count: 7826512
Running inference benchmark on edgenext_small_rw.sw_in1k for 40 steps w/ input size (3, 320, 320) and batch size 256.
Infer [8/40]. 3638.63 samples/sec. 70.356 ms/step.
Infer [16/40]. 3638.91 samples/sec. 70.351 ms/step.
Infer [24/40]. 3638.89 samples/sec. 70.351 ms/step.
Infer [32/40]. 3638.67 samples/sec. 70.355 ms/step.
Infer [40/40]. 3638.49 samples/sec. 70.359 ms/step.
Inference benchmark of edgenext_small_rw.sw_in1k done. 3637.14 samples/sec, 70.36 ms/step
Model edgenext_small_rw.sw_in1k created, param count: 7826512
Running train benchmark on edgenext_small_rw.sw_in1k for 40 steps w/ input size (3, 320, 320) and batch size 256.
Train [8/40]. 1217.04 samples/sec. 210.346 ms/step.
Train [16/40]. 1216.92 samples/sec. 210.368 ms/step.
Train [24/40]. 1216.99 samples/sec. 210.356 ms/step.
Train [32/40]. 1216.97 samples/sec. 210.358 ms/step.
Train [40/40]. 1216.93 samples/sec. 210.366 ms/step.
Train benchmark of edgenext_small_rw.sw_in1k done. 1211.43 samples/sec, 210.37 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model edgenext_x_small.in1k created, param count: 2336804
Running inference benchmark on edgenext_x_small.in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 7171.08 samples/sec. 35.699 ms/step.
Infer [16/40]. 7170.61 samples/sec. 35.701 ms/step.
Infer [24/40]. 7167.61 samples/sec. 35.716 ms/step.
Infer [32/40]. 7167.42 samples/sec. 35.717 ms/step.
Infer [40/40]. 7166.37 samples/sec. 35.722 ms/step.
Inference benchmark of edgenext_x_small.in1k done. 7161.01 samples/sec, 35.72 ms/step
Model edgenext_x_small.in1k created, param count: 2336804
Running train benchmark on edgenext_x_small.in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 2175.31 samples/sec. 117.684 ms/step.
Train [16/40]. 2175.82 samples/sec. 117.657 ms/step.
Train [24/40]. 2175.65 samples/sec. 117.666 ms/step.
Train [32/40]. 2172.03 samples/sec. 117.862 ms/step.
Train [40/40]. 2171.77 samples/sec. 117.876 ms/step.
Train benchmark of edgenext_x_small.in1k done. 2156.55 samples/sec, 117.88 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model edgenext_xx_small.in1k created, param count: 1327216
Running inference benchmark on edgenext_xx_small.in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 11765.20 samples/sec. 21.759 ms/step.
Infer [16/40]. 11767.00 samples/sec. 21.756 ms/step.
Infer [24/40]. 11766.65 samples/sec. 21.756 ms/step.
Infer [32/40]. 11767.70 samples/sec. 21.754 ms/step.
Infer [40/40]. 11768.97 samples/sec. 21.752 ms/step.
Inference benchmark of edgenext_xx_small.in1k done. 11756.26 samples/sec, 21.75 ms/step
Model edgenext_xx_small.in1k created, param count: 1327216
Running train benchmark on edgenext_xx_small.in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 3440.87 samples/sec. 74.400 ms/step.
Train [16/40]. 3440.57 samples/sec. 74.406 ms/step.
Train [24/40]. 3439.62 samples/sec. 74.427 ms/step.
Train [32/40]. 3441.37 samples/sec. 74.389 ms/step.
Train [40/40]. 3446.47 samples/sec. 74.279 ms/step.
Train benchmark of edgenext_xx_small.in1k done. 3416.06 samples/sec, 74.28 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model efficientformer_l1.snap_dist_in1k created, param count: 12289928
Running inference benchmark on efficientformer_l1.snap_dist_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 7332.62 samples/sec. 34.912 ms/step.
Infer [16/40]. 7332.31 samples/sec. 34.914 ms/step.
Infer [24/40]. 7332.06 samples/sec. 34.915 ms/step.
Infer [32/40]. 7332.09 samples/sec. 34.915 ms/step.
Infer [40/40]. 7331.97 samples/sec. 34.916 ms/step.
Inference benchmark of efficientformer_l1.snap_dist_in1k done. 7326.69 samples/sec, 34.92 ms/step
Model efficientformer_l1.snap_dist_in1k created, param count: 12289928
Running train benchmark on efficientformer_l1.snap_dist_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 2160.38 samples/sec. 118.497 ms/step.
Train [16/40]. 2160.49 samples/sec. 118.491 ms/step.
Train [24/40]. 2160.65 samples/sec. 118.483 ms/step.
Train [32/40]. 2160.69 samples/sec. 118.481 ms/step.
Train [40/40]. 2160.71 samples/sec. 118.480 ms/step.
Train benchmark of efficientformer_l1.snap_dist_in1k done. 2147.93 samples/sec, 118.48 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model efficientformer_l3.snap_dist_in1k created, param count: 31406000
Running inference benchmark on efficientformer_l3.snap_dist_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3349.40 samples/sec. 76.432 ms/step.
Infer [16/40]. 3349.47 samples/sec. 76.430 ms/step.
Infer [24/40]. 3349.38 samples/sec. 76.432 ms/step.
Infer [32/40]. 3349.35 samples/sec. 76.433 ms/step.
Infer [40/40]. 3349.39 samples/sec. 76.432 ms/step.
Inference benchmark of efficientformer_l3.snap_dist_in1k done. 3348.21 samples/sec, 76.43 ms/step
Model efficientformer_l3.snap_dist_in1k created, param count: 31406000
Running train benchmark on efficientformer_l3.snap_dist_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1050.98 samples/sec. 243.582 ms/step.
Train [16/40]. 1050.96 samples/sec. 243.587 ms/step.
Train [24/40]. 1050.95 samples/sec. 243.588 ms/step.
Train [32/40]. 1050.93 samples/sec. 243.593 ms/step.
Train [40/40]. 1050.93 samples/sec. 243.595 ms/step.
Train benchmark of efficientformer_l3.snap_dist_in1k done. 1045.33 samples/sec, 243.59 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model efficientformer_l7.snap_dist_in1k created, param count: 82229328
Running inference benchmark on efficientformer_l7.snap_dist_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1592.05 samples/sec. 160.799 ms/step.
Infer [16/40]. 1591.99 samples/sec. 160.805 ms/step.
Infer [24/40]. 1591.62 samples/sec. 160.843 ms/step.
Infer [32/40]. 1591.37 samples/sec. 160.868 ms/step.
Infer [40/40]. 1591.24 samples/sec. 160.881 ms/step.
Inference benchmark of efficientformer_l7.snap_dist_in1k done. 1590.91 samples/sec, 160.88 ms/step
Model efficientformer_l7.snap_dist_in1k created, param count: 82229328
Running train benchmark on efficientformer_l7.snap_dist_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 294.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 238.06 MiB is free. Including non-PyTorch memory, this process has 23.41 GiB memory in use. Of the allocated memory 22.17 GiB is allocated by PyTorch, and 5.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model efficientformer_l7.snap_dist_in1k created, param count: 82229328
Running train benchmark on efficientformer_l7.snap_dist_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 16.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.10 GiB is allocated by PyTorch, and 299.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model efficientformer_l7.snap_dist_in1k created, param count: 82229328
Running train benchmark on efficientformer_l7.snap_dist_in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
Train [8/40]. 527.91 samples/sec. 242.464 ms/step.
Train [16/40]. 527.92 samples/sec. 242.463 ms/step.
Train [24/40]. 527.90 samples/sec. 242.469 ms/step.
Train [32/40]. 527.89 samples/sec. 242.473 ms/step.
Train [40/40]. 527.89 samples/sec. 242.474 ms/step.
Train benchmark of efficientformer_l7.snap_dist_in1k done. 524.21 samples/sec, 242.47 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model efficientformerv2_l.snap_dist_in1k created, param count: 26322288
Running inference benchmark on efficientformerv2_l.snap_dist_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 562.00 samples/sec. 455.513 ms/step.
Infer [16/40]. 562.08 samples/sec. 455.452 ms/step.
Infer [24/40]. 562.11 samples/sec. 455.423 ms/step.
Infer [32/40]. 562.10 samples/sec. 455.434 ms/step.
Infer [40/40]. 562.09 samples/sec. 455.440 ms/step.
Inference benchmark of efficientformerv2_l.snap_dist_in1k done. 562.04 samples/sec, 455.44 ms/step
Model efficientformerv2_l.snap_dist_in1k created, param count: 26322288
Running train benchmark on efficientformerv2_l.snap_dist_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 36.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 22.10 GiB is allocated by PyTorch, and 285.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model efficientformerv2_l.snap_dist_in1k created, param count: 26322288
Running train benchmark on efficientformerv2_l.snap_dist_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
Train [8/40]. 458.98 samples/sec. 418.319 ms/step.
Train [16/40]. 458.82 samples/sec. 418.460 ms/step.
Train [24/40]. 458.91 samples/sec. 418.387 ms/step.
Train [32/40]. 458.96 samples/sec. 418.334 ms/step.
Train [40/40]. 458.98 samples/sec. 418.319 ms/step.
Train benchmark of efficientformerv2_l.snap_dist_in1k done. 456.23 samples/sec, 418.32 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model efficientformerv2_s0.snap_dist_in1k created, param count: 3600256
Running inference benchmark on efficientformerv2_s0.snap_dist_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1758.48 samples/sec. 145.580 ms/step.
Infer [16/40]. 1757.24 samples/sec. 145.683 ms/step.
Infer [24/40]. 1756.64 samples/sec. 145.733 ms/step.
Infer [32/40]. 1756.68 samples/sec. 145.730 ms/step.
Infer [40/40]. 1756.74 samples/sec. 145.725 ms/step.
Inference benchmark of efficientformerv2_s0.snap_dist_in1k done. 1756.33 samples/sec, 145.72 ms/step
Model efficientformerv2_s0.snap_dist_in1k created, param count: 3600256
Running train benchmark on efficientformerv2_s0.snap_dist_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1236.69 samples/sec. 207.004 ms/step.
Train [16/40]. 1236.82 samples/sec. 206.983 ms/step.
Train [24/40]. 1236.85 samples/sec. 206.977 ms/step.
Train [32/40]. 1236.66 samples/sec. 207.010 ms/step.
Train [40/40]. 1236.56 samples/sec. 207.027 ms/step.
Train benchmark of efficientformerv2_s0.snap_dist_in1k done. 1229.86 samples/sec, 207.03 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model efficientformerv2_s1.snap_dist_in1k created, param count: 6185560
Running inference benchmark on efficientformerv2_s1.snap_dist_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1651.56 samples/sec. 155.005 ms/step.
Infer [16/40]. 1651.98 samples/sec. 154.966 ms/step.
Infer [24/40]. 1652.07 samples/sec. 154.957 ms/step.
Infer [32/40]. 1652.63 samples/sec. 154.905 ms/step.
Infer [40/40]. 1652.82 samples/sec. 154.886 ms/step.
Inference benchmark of efficientformerv2_s1.snap_dist_in1k done. 1652.47 samples/sec, 154.89 ms/step
Model efficientformerv2_s1.snap_dist_in1k created, param count: 6185560
Running train benchmark on efficientformerv2_s1.snap_dist_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1060.29 samples/sec. 241.443 ms/step.
Train [16/40]. 1059.83 samples/sec. 241.549 ms/step.
Train [24/40]. 1059.81 samples/sec. 241.553 ms/step.
Train [32/40]. 1059.83 samples/sec. 241.548 ms/step.
Train [40/40]. 1059.78 samples/sec. 241.560 ms/step.
Train benchmark of efficientformerv2_s1.snap_dist_in1k done. 1053.97 samples/sec, 241.56 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model efficientformerv2_s2.snap_dist_in1k created, param count: 12710112
Running inference benchmark on efficientformerv2_s2.snap_dist_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 855.88 samples/sec. 299.106 ms/step.
Infer [16/40]. 855.84 samples/sec. 299.121 ms/step.
Infer [24/40]. 855.89 samples/sec. 299.102 ms/step.
Infer [32/40]. 855.80 samples/sec. 299.136 ms/step.
Infer [40/40]. 855.79 samples/sec. 299.138 ms/step.
Inference benchmark of efficientformerv2_s2.snap_dist_in1k done. 855.66 samples/sec, 299.14 ms/step
Model efficientformerv2_s2.snap_dist_in1k created, param count: 12710112
Running train benchmark on efficientformerv2_s2.snap_dist_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 596.14 samples/sec. 429.430 ms/step.
Train [16/40]. 596.19 samples/sec. 429.395 ms/step.
Train [24/40]. 596.24 samples/sec. 429.360 ms/step.
Train [32/40]. 596.30 samples/sec. 429.311 ms/step.
Train [40/40]. 596.31 samples/sec. 429.309 ms/step.
Train benchmark of efficientformerv2_s2.snap_dist_in1k done. 593.58 samples/sec, 429.31 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model efficientnet_b0.ra_in1k created, param count: 5288548
Running inference benchmark on efficientnet_b0.ra_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 9513.76 samples/sec. 26.908 ms/step.
Infer [16/40]. 9514.07 samples/sec. 26.908 ms/step.
Infer [24/40]. 9514.27 samples/sec. 26.907 ms/step.
Infer [32/40]. 9514.31 samples/sec. 26.907 ms/step.
Infer [40/40]. 9513.40 samples/sec. 26.909 ms/step.
Inference benchmark of efficientnet_b0.ra_in1k done. 9504.81 samples/sec, 26.91 ms/step
Model efficientnet_b0.ra_in1k created, param count: 5288548
Running train benchmark on efficientnet_b0.ra_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1972.04 samples/sec. 129.815 ms/step.
Train [16/40]. 1972.12 samples/sec. 129.810 ms/step.
Train [24/40]. 1972.11 samples/sec. 129.810 ms/step.
Train [32/40]. 1972.11 samples/sec. 129.810 ms/step.
Train [40/40]. 1972.13 samples/sec. 129.809 ms/step.
Train benchmark of efficientnet_b0.ra_in1k done. 1959.89 samples/sec, 129.81 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model efficientnet_b1.ft_in1k created, param count: 7794184
Running inference benchmark on efficientnet_b1.ft_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
Infer [8/40]. 5250.62 samples/sec. 48.756 ms/step.
Infer [16/40]. 5250.58 samples/sec. 48.757 ms/step.
Infer [24/40]. 5250.55 samples/sec. 48.757 ms/step.
Infer [32/40]. 5250.55 samples/sec. 48.757 ms/step.
Infer [40/40]. 5250.54 samples/sec. 48.757 ms/step.
Inference benchmark of efficientnet_b1.ft_in1k done. 5247.83 samples/sec, 48.76 ms/step
Model efficientnet_b1.ft_in1k created, param count: 7794184
Running train benchmark on efficientnet_b1.ft_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
Train [8/40]. 1096.21 samples/sec. 233.532 ms/step.
Train [16/40]. 1096.25 samples/sec. 233.523 ms/step.
Train [24/40]. 1096.27 samples/sec. 233.519 ms/step.
Train [32/40]. 1096.29 samples/sec. 233.514 ms/step.
Train [40/40]. 1096.30 samples/sec. 233.513 ms/step.
Train benchmark of efficientnet_b1.ft_in1k done. 1090.41 samples/sec, 233.51 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model efficientnet_b1_pruned.in1k created, param count: 6331916
Running inference benchmark on efficientnet_b1_pruned.in1k for 40 steps w/ input size (3, 240, 240) and batch size 256.
Infer [8/40]. 9732.95 samples/sec. 26.302 ms/step.
Infer [16/40]. 9732.89 samples/sec. 26.303 ms/step.
Infer [24/40]. 9733.01 samples/sec. 26.302 ms/step.
Infer [32/40]. 9733.03 samples/sec. 26.302 ms/step.
Infer [40/40]. 9733.04 samples/sec. 26.302 ms/step.
Inference benchmark of efficientnet_b1_pruned.in1k done. 9724.01 samples/sec, 26.30 ms/step
Model efficientnet_b1_pruned.in1k created, param count: 6331916
Running train benchmark on efficientnet_b1_pruned.in1k for 40 steps w/ input size (3, 240, 240) and batch size 256.
Train [8/40]. 1762.29 samples/sec. 145.265 ms/step.
Train [16/40]. 1762.26 samples/sec. 145.268 ms/step.
Train [24/40]. 1762.29 samples/sec. 145.265 ms/step.
Train [32/40]. 1762.25 samples/sec. 145.269 ms/step.
Train [40/40]. 1762.22 samples/sec. 145.271 ms/step.
Train benchmark of efficientnet_b1_pruned.in1k done. 1749.44 samples/sec, 145.27 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model efficientnet_b2.ra_in1k created, param count: 9109994
Running inference benchmark on efficientnet_b2.ra_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 3717.38 samples/sec. 68.866 ms/step.
Infer [16/40]. 3717.40 samples/sec. 68.865 ms/step.
Infer [24/40]. 3717.37 samples/sec. 68.866 ms/step.
Infer [32/40]. 3717.36 samples/sec. 68.866 ms/step.
Infer [40/40]. 3717.38 samples/sec. 68.866 ms/step.
Inference benchmark of efficientnet_b2.ra_in1k done. 3715.97 samples/sec, 68.87 ms/step
Model efficientnet_b2.ra_in1k created, param count: 9109994
Running train benchmark on efficientnet_b2.ra_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 114.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 50.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 22.10 GiB is allocated by PyTorch, and 265.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model efficientnet_b2.ra_in1k created, param count: 9109994
Running train benchmark on efficientnet_b2.ra_in1k for 40 steps w/ input size (3, 288, 288) and batch size 192.
Train [8/40]. 792.95 samples/sec. 242.133 ms/step.
Train [16/40]. 792.97 samples/sec. 242.128 ms/step.
Train [24/40]. 792.99 samples/sec. 242.121 ms/step.
Train [32/40]. 792.99 samples/sec. 242.121 ms/step.
Train [40/40]. 792.99 samples/sec. 242.122 ms/step.
Train benchmark of efficientnet_b2.ra_in1k done. 788.85 samples/sec, 242.12 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model efficientnet_b2_pruned.in1k created, param count: 8309737
Running inference benchmark on efficientnet_b2_pruned.in1k for 40 steps w/ input size (3, 260, 260) and batch size 256.
Infer [8/40]. 6383.63 samples/sec. 40.103 ms/step.
Infer [16/40]. 6384.10 samples/sec. 40.100 ms/step.
Infer [24/40]. 6384.10 samples/sec. 40.100 ms/step.
Infer [32/40]. 6384.40 samples/sec. 40.098 ms/step.
Infer [40/40]. 6384.07 samples/sec. 40.100 ms/step.
Inference benchmark of efficientnet_b2_pruned.in1k done. 6380.26 samples/sec, 40.10 ms/step
Model efficientnet_b2_pruned.in1k created, param count: 8309737
Running train benchmark on efficientnet_b2_pruned.in1k for 40 steps w/ input size (3, 260, 260) and batch size 256.
Train [8/40]. 1195.95 samples/sec. 214.056 ms/step.
Train [16/40]. 1195.80 samples/sec. 214.083 ms/step.
Train [24/40]. 1195.15 samples/sec. 214.200 ms/step.
Train [32/40]. 1194.78 samples/sec. 214.266 ms/step.
Train [40/40]. 1194.60 samples/sec. 214.297 ms/step.
Train benchmark of efficientnet_b2_pruned.in1k done. 1187.83 samples/sec, 214.30 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model efficientnet_b3.ra2_in1k created, param count: 12233232
Running inference benchmark on efficientnet_b3.ra2_in1k for 40 steps w/ input size (3, 320, 320) and batch size 256.
Infer [8/40]. 2266.85 samples/sec. 112.932 ms/step.
Infer [16/40]. 2266.75 samples/sec. 112.937 ms/step.
Infer [24/40]. 2266.68 samples/sec. 112.940 ms/step.
Infer [32/40]. 2266.71 samples/sec. 112.939 ms/step.
Infer [40/40]. 2266.74 samples/sec. 112.938 ms/step.
Inference benchmark of efficientnet_b3.ra2_in1k done. 2266.18 samples/sec, 112.94 ms/step
Model efficientnet_b3.ra2_in1k created, param count: 12233232
Running train benchmark on efficientnet_b3.ra2_in1k for 40 steps w/ input size (3, 320, 320) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 600.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 598.06 MiB is free. Including non-PyTorch memory, this process has 23.06 GiB memory in use. Of the allocated memory 21.77 GiB is allocated by PyTorch, and 61.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model efficientnet_b3.ra2_in1k created, param count: 12233232
Running train benchmark on efficientnet_b3.ra2_in1k for 40 steps w/ input size (3, 320, 320) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 86.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 20.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.21 GiB is allocated by PyTorch, and 189.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model efficientnet_b3.ra2_in1k created, param count: 12233232
Running train benchmark on efficientnet_b3.ra2_in1k for 40 steps w/ input size (3, 320, 320) and batch size 128.
Train [8/40]. 529.29 samples/sec. 241.831 ms/step.
Train [16/40]. 529.27 samples/sec. 241.843 ms/step.
Train [24/40]. 529.29 samples/sec. 241.835 ms/step.
Train [32/40]. 529.30 samples/sec. 241.830 ms/step.
Train [40/40]. 529.30 samples/sec. 241.830 ms/step.
Train benchmark of efficientnet_b3.ra2_in1k done. 526.27 samples/sec, 241.83 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model efficientnet_b3_pruned.in1k created, param count: 9855020
Running inference benchmark on efficientnet_b3_pruned.in1k for 40 steps w/ input size (3, 300, 300) and batch size 256.
Infer [8/40]. 4966.85 samples/sec. 51.542 ms/step.
Infer [16/40]. 4966.60 samples/sec. 51.544 ms/step.
Infer [24/40]. 4966.59 samples/sec. 51.544 ms/step.
Infer [32/40]. 4966.57 samples/sec. 51.545 ms/step.
Infer [40/40]. 4966.58 samples/sec. 51.545 ms/step.
Inference benchmark of efficientnet_b3_pruned.in1k done. 4964.13 samples/sec, 51.55 ms/step
Model efficientnet_b3_pruned.in1k created, param count: 9855020
Running train benchmark on efficientnet_b3_pruned.in1k for 40 steps w/ input size (3, 300, 300) and batch size 256.
Train [8/40]. 939.90 samples/sec. 272.371 ms/step.
Train [16/40]. 939.87 samples/sec. 272.378 ms/step.
Train [24/40]. 939.87 samples/sec. 272.379 ms/step.
Train [32/40]. 939.86 samples/sec. 272.381 ms/step.
Train [40/40]. 939.86 samples/sec. 272.381 ms/step.
Train benchmark of efficientnet_b3_pruned.in1k done. 934.82 samples/sec, 272.38 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model efficientnet_b4.ra2_in1k created, param count: 19341616
Running inference benchmark on efficientnet_b4.ra2_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 1191.25 samples/sec. 214.900 ms/step.
Infer [16/40]. 1191.24 samples/sec. 214.901 ms/step.
Infer [24/40]. 1191.23 samples/sec. 214.904 ms/step.
Infer [32/40]. 1191.21 samples/sec. 214.907 ms/step.
Infer [40/40]. 1191.21 samples/sec. 214.908 ms/step.
Inference benchmark of efficientnet_b4.ra2_in1k done. 1190.99 samples/sec, 214.91 ms/step
Model efficientnet_b4.ra2_in1k created, param count: 19341616
Running train benchmark on efficientnet_b4.ra2_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 864.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 488.06 MiB is free. Including non-PyTorch memory, this process has 23.16 GiB memory in use. Of the allocated memory 21.64 GiB is allocated by PyTorch, and 303.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model efficientnet_b4.ra2_in1k created, param count: 19341616
Running train benchmark on efficientnet_b4.ra2_in1k for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 648.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 160.06 MiB is free. Including non-PyTorch memory, this process has 23.48 GiB memory in use. Of the allocated memory 22.15 GiB is allocated by PyTorch, and 110.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model efficientnet_b4.ra2_in1k created, param count: 19341616
Running train benchmark on efficientnet_b4.ra2_in1k for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 190.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 90.06 MiB is free. Including non-PyTorch memory, this process has 23.55 GiB memory in use. Of the allocated memory 22.15 GiB is allocated by PyTorch, and 182.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model efficientnet_b4.ra2_in1k created, param count: 19341616
Running train benchmark on efficientnet_b4.ra2_in1k for 40 steps w/ input size (3, 384, 384) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 72.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 30.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 22.10 GiB is allocated by PyTorch, and 286.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model efficientnet_b4.ra2_in1k created, param count: 19341616
Running train benchmark on efficientnet_b4.ra2_in1k for 40 steps w/ input size (3, 384, 384) and batch size 64.
Train [8/40]. 293.06 samples/sec. 218.388 ms/step.
Train [16/40]. 293.06 samples/sec. 218.385 ms/step.
Train [24/40]. 293.06 samples/sec. 218.388 ms/step.
Train [32/40]. 293.06 samples/sec. 218.387 ms/step.
Train [40/40]. 293.05 samples/sec. 218.391 ms/step.
Train benchmark of efficientnet_b4.ra2_in1k done. 291.01 samples/sec, 218.39 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model efficientnet_b5.sw_in12k created, param count: 52562013
Running inference benchmark on efficientnet_b5.sw_in12k for 40 steps w/ input size (3, 416, 416) and batch size 256.
Infer [8/40]. 722.77 samples/sec. 354.191 ms/step.
Infer [16/40]. 722.76 samples/sec. 354.196 ms/step.
Infer [24/40]. 722.75 samples/sec. 354.202 ms/step.
Infer [32/40]. 722.74 samples/sec. 354.208 ms/step.
Infer [40/40]. 722.73 samples/sec. 354.214 ms/step.
Inference benchmark of efficientnet_b5.sw_in12k done. 722.63 samples/sec, 354.21 ms/step
Model efficientnet_b5.sw_in12k created, param count: 52562013
Running train benchmark on efficientnet_b5.sw_in12k for 40 steps w/ input size (3, 416, 416) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 2.97 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.76 GiB is free. Including non-PyTorch memory, this process has 21.88 GiB memory in use. Of the allocated memory 20.18 GiB is allocated by PyTorch, and 483.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model efficientnet_b5.sw_in12k created, param count: 52562013
Running train benchmark on efficientnet_b5.sw_in12k for 40 steps w/ input size (3, 416, 416) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 952.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 430.06 MiB is free. Including non-PyTorch memory, this process has 23.22 GiB memory in use. Of the allocated memory 21.78 GiB is allocated by PyTorch, and 219.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model efficientnet_b5.sw_in12k created, param count: 52562013
Running train benchmark on efficientnet_b5.sw_in12k for 40 steps w/ input size (3, 416, 416) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 106.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 46.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 22.21 GiB is allocated by PyTorch, and 160.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model efficientnet_b5.sw_in12k created, param count: 52562013
Running train benchmark on efficientnet_b5.sw_in12k for 40 steps w/ input size (3, 416, 416) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 476.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 88.06 MiB is free. Including non-PyTorch memory, this process has 23.55 GiB memory in use. Of the allocated memory 22.11 GiB is allocated by PyTorch, and 221.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model efficientnet_b5.sw_in12k created, param count: 52562013
Running train benchmark on efficientnet_b5.sw_in12k for 40 steps w/ input size (3, 416, 416) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 4.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 22.21 GiB is allocated by PyTorch, and 207.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model efficientnet_b5.sw_in12k created, param count: 52562013
Running train benchmark on efficientnet_b5.sw_in12k for 40 steps w/ input size (3, 416, 416) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 30.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 14.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.25 GiB is allocated by PyTorch, and 156.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model efficientnet_b5.sw_in12k created, param count: 52562013
Running train benchmark on efficientnet_b5.sw_in12k for 40 steps w/ input size (3, 416, 416) and batch size 32.
Train [8/40]. 183.08 samples/sec. 174.791 ms/step.
Train [16/40]. 183.07 samples/sec. 174.796 ms/step.
Train [24/40]. 183.07 samples/sec. 174.800 ms/step.
Train [32/40]. 183.07 samples/sec. 174.800 ms/step.
Train [40/40]. 183.06 samples/sec. 174.804 ms/step.
Train benchmark of efficientnet_b5.sw_in12k done. 181.37 samples/sec, 174.80 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model efficientnet_b5.sw_in12k_ft_in1k created, param count: 30389784
Running inference benchmark on efficientnet_b5.sw_in12k_ft_in1k for 40 steps w/ input size (3, 448, 448) and batch size 256.
Infer [8/40]. 637.82 samples/sec. 401.367 ms/step.
Infer [16/40]. 637.82 samples/sec. 401.370 ms/step.
Infer [24/40]. 637.81 samples/sec. 401.371 ms/step.
Infer [32/40]. 637.81 samples/sec. 401.376 ms/step.
Infer [40/40]. 637.80 samples/sec. 401.381 ms/step.
Inference benchmark of efficientnet_b5.sw_in12k_ft_in1k done. 637.72 samples/sec, 401.38 ms/step
Model efficientnet_b5.sw_in12k_ft_in1k created, param count: 30389784
Running train benchmark on efficientnet_b5.sw_in12k_ft_in1k for 40 steps w/ input size (3, 448, 448) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 3.45 GiB. GPU 0 has a total capacty of 23.65 GiB of which 2.05 GiB is free. Including non-PyTorch memory, this process has 21.59 GiB memory in use. Of the allocated memory 19.89 GiB is allocated by PyTorch, and 485.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model efficientnet_b5.sw_in12k_ft_in1k created, param count: 30389784
Running train benchmark on efficientnet_b5.sw_in12k_ft_in1k for 40 steps w/ input size (3, 448, 448) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 662.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 244.06 MiB is free. Including non-PyTorch memory, this process has 23.40 GiB memory in use. Of the allocated memory 22.05 GiB is allocated by PyTorch, and 124.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model efficientnet_b5.sw_in12k_ft_in1k created, param count: 30389784
Running train benchmark on efficientnet_b5.sw_in12k_ft_in1k for 40 steps w/ input size (3, 448, 448) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 736.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 260.06 MiB is free. Including non-PyTorch memory, this process has 23.39 GiB memory in use. Of the allocated memory 22.10 GiB is allocated by PyTorch, and 60.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model efficientnet_b5.sw_in12k_ft_in1k created, param count: 30389784
Running train benchmark on efficientnet_b5.sw_in12k_ft_in1k for 40 steps w/ input size (3, 448, 448) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 552.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 38.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 22.17 GiB is allocated by PyTorch, and 211.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model efficientnet_b5.sw_in12k_ft_in1k created, param count: 30389784
Running train benchmark on efficientnet_b5.sw_in12k_ft_in1k for 40 steps w/ input size (3, 448, 448) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 148.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 146.06 MiB is free. Including non-PyTorch memory, this process has 23.50 GiB memory in use. Of the allocated memory 22.18 GiB is allocated by PyTorch, and 86.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model efficientnet_b5.sw_in12k_ft_in1k created, param count: 30389784
Running train benchmark on efficientnet_b5.sw_in12k_ft_in1k for 40 steps w/ input size (3, 448, 448) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 76.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 22.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.10 GiB is allocated by PyTorch, and 296.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model efficientnet_b5.sw_in12k_ft_in1k created, param count: 30389784
Running train benchmark on efficientnet_b5.sw_in12k_ft_in1k for 40 steps w/ input size (3, 448, 448) and batch size 32.
Train [8/40]. 159.27 samples/sec. 200.913 ms/step.
Train [16/40]. 159.27 samples/sec. 200.914 ms/step.
Train [24/40]. 159.27 samples/sec. 200.915 ms/step.
Train [32/40]. 159.25 samples/sec. 200.943 ms/step.
Train [40/40]. 159.23 samples/sec. 200.963 ms/step.
Train benchmark of efficientnet_b5.sw_in12k_ft_in1k done. 157.88 samples/sec, 200.96 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model efficientnet_el.ra_in1k created, param count: 10589712
Running inference benchmark on efficientnet_el.ra_in1k for 40 steps w/ input size (3, 300, 300) and batch size 256.
Infer [8/40]. 1820.07 samples/sec. 140.654 ms/step.
Infer [16/40]. 1820.06 samples/sec. 140.654 ms/step.
Infer [24/40]. 1820.06 samples/sec. 140.655 ms/step.
Infer [32/40]. 1820.03 samples/sec. 140.657 ms/step.
Infer [40/40]. 1820.01 samples/sec. 140.659 ms/step.
Inference benchmark of efficientnet_el.ra_in1k done. 1819.59 samples/sec, 140.66 ms/step
Model efficientnet_el.ra_in1k created, param count: 10589712
Running train benchmark on efficientnet_el.ra_in1k for 40 steps w/ input size (3, 300, 300) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 158.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 90.06 MiB is free. Including non-PyTorch memory, this process has 23.55 GiB memory in use. Of the allocated memory 22.23 GiB is allocated by PyTorch, and 91.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model efficientnet_el.ra_in1k created, param count: 10589712
Running train benchmark on efficientnet_el.ra_in1k for 40 steps w/ input size (3, 300, 300) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 58.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 44.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 22.17 GiB is allocated by PyTorch, and 201.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model efficientnet_el.ra_in1k created, param count: 10589712
Running train benchmark on efficientnet_el.ra_in1k for 40 steps w/ input size (3, 300, 300) and batch size 128.
Train [8/40]. 492.64 samples/sec. 259.822 ms/step.
Train [16/40]. 492.63 samples/sec. 259.832 ms/step.
Train [24/40]. 492.59 samples/sec. 259.850 ms/step.
Train [32/40]. 492.60 samples/sec. 259.845 ms/step.
Train [40/40]. 492.60 samples/sec. 259.844 ms/step.
Train benchmark of efficientnet_el.ra_in1k done. 490.55 samples/sec, 259.84 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model efficientnet_el_pruned.in1k created, param count: 10589712
Running inference benchmark on efficientnet_el_pruned.in1k for 40 steps w/ input size (3, 300, 300) and batch size 256.
Infer [8/40]. 1821.76 samples/sec. 140.523 ms/step.
Infer [16/40]. 1821.35 samples/sec. 140.555 ms/step.
Infer [24/40]. 1821.32 samples/sec. 140.558 ms/step.
Infer [32/40]. 1821.26 samples/sec. 140.562 ms/step.
Infer [40/40]. 1821.25 samples/sec. 140.562 ms/step.
Inference benchmark of efficientnet_el_pruned.in1k done. 1820.83 samples/sec, 140.56 ms/step
Model efficientnet_el_pruned.in1k created, param count: 10589712
Running train benchmark on efficientnet_el_pruned.in1k for 40 steps w/ input size (3, 300, 300) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 158.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 90.06 MiB is free. Including non-PyTorch memory, this process has 23.55 GiB memory in use. Of the allocated memory 22.23 GiB is allocated by PyTorch, and 91.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model efficientnet_el_pruned.in1k created, param count: 10589712
Running train benchmark on efficientnet_el_pruned.in1k for 40 steps w/ input size (3, 300, 300) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 58.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 2.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 22.17 GiB is allocated by PyTorch, and 246.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model efficientnet_el_pruned.in1k created, param count: 10589712
Running train benchmark on efficientnet_el_pruned.in1k for 40 steps w/ input size (3, 300, 300) and batch size 128.
Train [8/40]. 492.66 samples/sec. 259.813 ms/step.
Train [16/40]. 492.66 samples/sec. 259.814 ms/step.
Train [24/40]. 492.69 samples/sec. 259.799 ms/step.
Train [32/40]. 492.68 samples/sec. 259.804 ms/step.
Train [40/40]. 492.69 samples/sec. 259.797 ms/step.
Train benchmark of efficientnet_el_pruned.in1k done. 490.65 samples/sec, 259.80 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model efficientnet_em.ra2_in1k created, param count: 6899496
Running inference benchmark on efficientnet_em.ra2_in1k for 40 steps w/ input size (3, 240, 240) and batch size 256.
Infer [8/40]. 4127.25 samples/sec. 62.027 ms/step.
Infer [16/40]. 4126.92 samples/sec. 62.032 ms/step.
Infer [24/40]. 4126.84 samples/sec. 62.033 ms/step.
Infer [32/40]. 4126.59 samples/sec. 62.037 ms/step.
Infer [40/40]. 4126.37 samples/sec. 62.040 ms/step.
Inference benchmark of efficientnet_em.ra2_in1k done. 4124.68 samples/sec, 62.04 ms/step
Model efficientnet_em.ra2_in1k created, param count: 6899496
Running train benchmark on efficientnet_em.ra2_in1k for 40 steps w/ input size (3, 240, 240) and batch size 256.
Train [8/40]. 1050.40 samples/sec. 243.717 ms/step.
Train [16/40]. 1050.38 samples/sec. 243.720 ms/step.
Train [24/40]. 1050.37 samples/sec. 243.725 ms/step.
Train [32/40]. 1050.27 samples/sec. 243.747 ms/step.
Train [40/40]. 1050.12 samples/sec. 243.783 ms/step.
Train benchmark of efficientnet_em.ra2_in1k done. 1046.04 samples/sec, 243.78 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model efficientnet_es.ra_in1k created, param count: 5438392
Running inference benchmark on efficientnet_es.ra_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 6852.26 samples/sec. 37.360 ms/step.
Infer [16/40]. 6852.21 samples/sec. 37.360 ms/step.
Infer [24/40]. 6852.29 samples/sec. 37.360 ms/step.
Infer [32/40]. 6852.20 samples/sec. 37.360 ms/step.
Infer [40/40]. 6852.03 samples/sec. 37.361 ms/step.
Inference benchmark of efficientnet_es.ra_in1k done. 6847.48 samples/sec, 37.36 ms/step
Model efficientnet_es.ra_in1k created, param count: 5438392
Running train benchmark on efficientnet_es.ra_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1764.90 samples/sec. 145.050 ms/step.
Train [16/40]. 1764.96 samples/sec. 145.046 ms/step.
Train [24/40]. 1764.60 samples/sec. 145.076 ms/step.
Train [32/40]. 1764.35 samples/sec. 145.096 ms/step.
Train [40/40]. 1764.17 samples/sec. 145.110 ms/step.
Train benchmark of efficientnet_es.ra_in1k done. 1756.31 samples/sec, 145.11 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model efficientnet_es_pruned.in1k created, param count: 5438392
Running inference benchmark on efficientnet_es_pruned.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 6854.48 samples/sec. 37.348 ms/step.
Infer [16/40]. 6854.53 samples/sec. 37.348 ms/step.
Infer [24/40]. 6854.47 samples/sec. 37.348 ms/step.
Infer [32/40]. 6854.41 samples/sec. 37.348 ms/step.
Infer [40/40]. 6854.17 samples/sec. 37.350 ms/step.
Inference benchmark of efficientnet_es_pruned.in1k done. 6849.66 samples/sec, 37.35 ms/step
Model efficientnet_es_pruned.in1k created, param count: 5438392
Running train benchmark on efficientnet_es_pruned.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1766.33 samples/sec. 144.933 ms/step.
Train [16/40]. 1765.60 samples/sec. 144.994 ms/step.
Train [24/40]. 1765.08 samples/sec. 145.036 ms/step.
Train [32/40]. 1764.87 samples/sec. 145.053 ms/step.
Train [40/40]. 1764.69 samples/sec. 145.068 ms/step.
Train benchmark of efficientnet_es_pruned.in1k done. 1756.59 samples/sec, 145.07 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model efficientnet_lite0.ra_in1k created, param count: 4652008
Running inference benchmark on efficientnet_lite0.ra_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 11019.00 samples/sec. 23.233 ms/step.
Infer [16/40]. 11018.76 samples/sec. 23.233 ms/step.
Infer [24/40]. 11018.43 samples/sec. 23.234 ms/step.
Infer [32/40]. 11018.62 samples/sec. 23.233 ms/step.
Infer [40/40]. 11018.62 samples/sec. 23.233 ms/step.
Inference benchmark of efficientnet_lite0.ra_in1k done. 11007.37 samples/sec, 23.23 ms/step
Model efficientnet_lite0.ra_in1k created, param count: 4652008
Running train benchmark on efficientnet_lite0.ra_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 2257.10 samples/sec. 113.420 ms/step.
Train [16/40]. 2257.16 samples/sec. 113.417 ms/step.
Train [24/40]. 2257.12 samples/sec. 113.419 ms/step.
Train [32/40]. 2257.17 samples/sec. 113.416 ms/step.
Train [40/40]. 2257.08 samples/sec. 113.421 ms/step.
Train benchmark of efficientnet_lite0.ra_in1k done. 2245.61 samples/sec, 113.42 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model efficientnetv2_rw_m.agc_in1k created, param count: 53236442
Running inference benchmark on efficientnetv2_rw_m.agc_in1k for 40 steps w/ input size (3, 416, 416) and batch size 256.
Infer [8/40]. 653.42 samples/sec. 391.786 ms/step.
Infer [16/40]. 653.30 samples/sec. 391.859 ms/step.
Infer [24/40]. 653.24 samples/sec. 391.895 ms/step.
Infer [32/40]. 653.21 samples/sec. 391.913 ms/step.
Infer [40/40]. 653.19 samples/sec. 391.924 ms/step.
Inference benchmark of efficientnetv2_rw_m.agc_in1k done. 653.12 samples/sec, 391.92 ms/step
Model efficientnetv2_rw_m.agc_in1k created, param count: 53236442
Running train benchmark on efficientnetv2_rw_m.agc_in1k for 40 steps w/ input size (3, 416, 416) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacty of 23.65 GiB of which 766.06 MiB is free. Including non-PyTorch memory, this process has 22.89 GiB memory in use. Of the allocated memory 21.16 GiB is allocated by PyTorch, and 513.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model efficientnetv2_rw_m.agc_in1k created, param count: 53236442
Running train benchmark on efficientnetv2_rw_m.agc_in1k for 40 steps w/ input size (3, 416, 416) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 888.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 748.06 MiB is free. Including non-PyTorch memory, this process has 22.91 GiB memory in use. Of the allocated memory 21.12 GiB is allocated by PyTorch, and 577.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model efficientnetv2_rw_m.agc_in1k created, param count: 53236442
Running train benchmark on efficientnetv2_rw_m.agc_in1k for 40 steps w/ input size (3, 416, 416) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 102.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 16.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.27 GiB is allocated by PyTorch, and 131.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model efficientnetv2_rw_m.agc_in1k created, param count: 53236442
Running train benchmark on efficientnetv2_rw_m.agc_in1k for 40 steps w/ input size (3, 416, 416) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 144.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 48.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 21.96 GiB is allocated by PyTorch, and 419.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model efficientnetv2_rw_m.agc_in1k created, param count: 53236442
Running train benchmark on efficientnetv2_rw_m.agc_in1k for 40 steps w/ input size (3, 416, 416) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 2.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 21.82 GiB is allocated by PyTorch, and 605.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model efficientnetv2_rw_m.agc_in1k created, param count: 53236442
Running train benchmark on efficientnetv2_rw_m.agc_in1k for 40 steps w/ input size (3, 416, 416) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 20.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.04 GiB is allocated by PyTorch, and 364.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model efficientnetv2_rw_m.agc_in1k created, param count: 53236442
Running train benchmark on efficientnetv2_rw_m.agc_in1k for 40 steps w/ input size (3, 416, 416) and batch size 32.
Train [8/40]. 172.49 samples/sec. 185.517 ms/step.
Train [16/40]. 172.54 samples/sec. 185.467 ms/step.
Train [24/40]. 172.58 samples/sec. 185.420 ms/step.
Train [32/40]. 172.59 samples/sec. 185.406 ms/step.
Train [40/40]. 172.59 samples/sec. 185.414 ms/step.
Train benchmark of efficientnetv2_rw_m.agc_in1k done. 170.73 samples/sec, 185.41 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model efficientnetv2_rw_s.ra2_in1k created, param count: 23941296
Running inference benchmark on efficientnetv2_rw_s.ra2_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 1429.81 samples/sec. 179.045 ms/step.
Infer [16/40]. 1429.75 samples/sec. 179.053 ms/step.
Infer [24/40]. 1429.74 samples/sec. 179.054 ms/step.
Infer [32/40]. 1429.73 samples/sec. 179.054 ms/step.
Infer [40/40]. 1429.70 samples/sec. 179.058 ms/step.
Inference benchmark of efficientnetv2_rw_s.ra2_in1k done. 1429.43 samples/sec, 179.06 ms/step
Model efficientnetv2_rw_s.ra2_in1k created, param count: 23941296
Running train benchmark on efficientnetv2_rw_s.ra2_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 144.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 14.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.28 GiB is allocated by PyTorch, and 120.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model efficientnetv2_rw_s.ra2_in1k created, param count: 23941296
Running train benchmark on efficientnetv2_rw_s.ra2_in1k for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 204.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 78.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 22.26 GiB is allocated by PyTorch, and 73.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model efficientnetv2_rw_s.ra2_in1k created, param count: 23941296
Running train benchmark on efficientnetv2_rw_s.ra2_in1k for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 136.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 80.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 22.12 GiB is allocated by PyTorch, and 216.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model efficientnetv2_rw_s.ra2_in1k created, param count: 23941296
Running train benchmark on efficientnetv2_rw_s.ra2_in1k for 40 steps w/ input size (3, 384, 384) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 2.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 21.29 GiB is allocated by PyTorch, and 1.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model efficientnetv2_rw_s.ra2_in1k created, param count: 23941296
Running train benchmark on efficientnetv2_rw_s.ra2_in1k for 40 steps w/ input size (3, 384, 384) and batch size 64.
Train [8/40]. 364.98 samples/sec. 175.352 ms/step.
Train [16/40]. 364.98 samples/sec. 175.354 ms/step.
Train [24/40]. 364.98 samples/sec. 175.352 ms/step.
Train [32/40]. 364.97 samples/sec. 175.356 ms/step.
Train [40/40]. 364.98 samples/sec. 175.354 ms/step.
Train benchmark of efficientnetv2_rw_s.ra2_in1k done. 361.93 samples/sec, 175.35 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model efficientnetv2_rw_t.ra2_in1k created, param count: 13649388
Running inference benchmark on efficientnetv2_rw_t.ra2_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 3223.39 samples/sec. 79.420 ms/step.
Infer [16/40]. 3223.32 samples/sec. 79.421 ms/step.
Infer [24/40]. 3223.33 samples/sec. 79.421 ms/step.
Infer [32/40]. 3223.23 samples/sec. 79.423 ms/step.
Infer [40/40]. 3223.19 samples/sec. 79.424 ms/step.
Inference benchmark of efficientnetv2_rw_t.ra2_in1k done. 3222.11 samples/sec, 79.42 ms/step
Model efficientnetv2_rw_t.ra2_in1k created, param count: 13649388
Running train benchmark on efficientnetv2_rw_t.ra2_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 26.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 21.98 GiB is allocated by PyTorch, and 415.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model efficientnetv2_rw_t.ra2_in1k created, param count: 13649388
Running train benchmark on efficientnetv2_rw_t.ra2_in1k for 40 steps w/ input size (3, 288, 288) and batch size 192.
Train [8/40]. 789.06 samples/sec. 243.329 ms/step.
Train [16/40]. 789.03 samples/sec. 243.336 ms/step.
Train [24/40]. 788.98 samples/sec. 243.353 ms/step.
Train [32/40]. 788.96 samples/sec. 243.358 ms/step.
Train [40/40]. 788.95 samples/sec. 243.360 ms/step.
Train benchmark of efficientnetv2_rw_t.ra2_in1k done. 783.73 samples/sec, 243.36 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model ese_vovnet19b_dw.ra_in1k created, param count: 6543080
Running inference benchmark on ese_vovnet19b_dw.ra_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 5135.49 samples/sec. 49.849 ms/step.
Infer [16/40]. 5135.07 samples/sec. 49.853 ms/step.
Infer [24/40]. 5134.61 samples/sec. 49.858 ms/step.
Infer [32/40]. 5133.90 samples/sec. 49.865 ms/step.
Infer [40/40]. 5133.54 samples/sec. 49.868 ms/step.
Inference benchmark of ese_vovnet19b_dw.ra_in1k done. 5130.83 samples/sec, 49.87 ms/step
Model ese_vovnet19b_dw.ra_in1k created, param count: 6543080
Running train benchmark on ese_vovnet19b_dw.ra_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 1367.45 samples/sec. 187.210 ms/step.
Train [16/40]. 1367.43 samples/sec. 187.213 ms/step.
Train [24/40]. 1367.39 samples/sec. 187.218 ms/step.
Train [32/40]. 1367.40 samples/sec. 187.216 ms/step.
Train [40/40]. 1367.39 samples/sec. 187.218 ms/step.
Train benchmark of ese_vovnet19b_dw.ra_in1k done. 1362.89 samples/sec, 187.22 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model ese_vovnet39b.ra_in1k created, param count: 24568936
Running inference benchmark on ese_vovnet39b.ra_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 2690.83 samples/sec. 95.138 ms/step.
Infer [16/40]. 2690.43 samples/sec. 95.152 ms/step.
Infer [24/40]. 2689.86 samples/sec. 95.172 ms/step.
Infer [32/40]. 2689.55 samples/sec. 95.183 ms/step.
Infer [40/40]. 2689.30 samples/sec. 95.192 ms/step.
Inference benchmark of ese_vovnet39b.ra_in1k done. 2688.49 samples/sec, 95.19 ms/step
Model ese_vovnet39b.ra_in1k created, param count: 24568936
Running train benchmark on ese_vovnet39b.ra_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 756.10 samples/sec. 338.579 ms/step.
Train [16/40]. 756.04 samples/sec. 338.608 ms/step.
Train [24/40]. 756.02 samples/sec. 338.618 ms/step.
Train [32/40]. 756.01 samples/sec. 338.621 ms/step.
Train [40/40]. 755.99 samples/sec. 338.627 ms/step.
Train benchmark of ese_vovnet39b.ra_in1k done. 754.10 samples/sec, 338.63 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model eva02_base_patch14_224.mim_in22k created, param count: 85758720
Running inference benchmark on eva02_base_patch14_224.mim_in22k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1503.53 samples/sec. 170.266 ms/step.
Infer [16/40]. 1503.74 samples/sec. 170.242 ms/step.
Infer [24/40]. 1503.90 samples/sec. 170.224 ms/step.
Infer [32/40]. 1503.95 samples/sec. 170.218 ms/step.
Infer [40/40]. 1503.97 samples/sec. 170.216 ms/step.
Inference benchmark of eva02_base_patch14_224.mim_in22k done. 1503.66 samples/sec, 170.22 ms/step
Model eva02_base_patch14_224.mim_in22k created, param count: 85758720
Running train benchmark on eva02_base_patch14_224.mim_in22k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 258.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 240.06 MiB is free. Including non-PyTorch memory, this process has 23.41 GiB memory in use. Of the allocated memory 21.94 GiB is allocated by PyTorch, and 247.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model eva02_base_patch14_224.mim_in22k created, param count: 85758720
Running train benchmark on eva02_base_patch14_224.mim_in22k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model eva02_base_patch14_224.mim_in22k created, param count: 85758720
Running train benchmark on eva02_base_patch14_224.mim_in22k for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model eva02_base_patch14_224.mim_in22k created, param count: 85758720
Running train benchmark on eva02_base_patch14_224.mim_in22k for 40 steps w/ input size (3, 224, 224) and batch size 96.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model eva02_base_patch14_224.mim_in22k created, param count: 85758720
Running train benchmark on eva02_base_patch14_224.mim_in22k for 40 steps w/ input size (3, 224, 224) and batch size 64.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model eva02_base_patch14_224.mim_in22k created, param count: 85758720
Running train benchmark on eva02_base_patch14_224.mim_in22k for 40 steps w/ input size (3, 224, 224) and batch size 48.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model eva02_base_patch14_224.mim_in22k created, param count: 85758720
Running train benchmark on eva02_base_patch14_224.mim_in22k for 40 steps w/ input size (3, 224, 224) and batch size 32.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model eva02_base_patch14_224.mim_in22k created, param count: 85758720
Running train benchmark on eva02_base_patch14_224.mim_in22k for 40 steps w/ input size (3, 224, 224) and batch size 24.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model eva02_base_patch14_224.mim_in22k created, param count: 85758720
Running train benchmark on eva02_base_patch14_224.mim_in22k for 40 steps w/ input size (3, 224, 224) and batch size 16.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 12 for retry.
Model eva02_base_patch14_224.mim_in22k created, param count: 85758720
Running train benchmark on eva02_base_patch14_224.mim_in22k for 40 steps w/ input size (3, 224, 224) and batch size 12.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 8 for retry.
Model eva02_base_patch14_224.mim_in22k created, param count: 85758720
Running train benchmark on eva02_base_patch14_224.mim_in22k for 40 steps w/ input size (3, 224, 224) and batch size 8.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 6 for retry.
Model eva02_base_patch14_224.mim_in22k created, param count: 85758720
Running train benchmark on eva02_base_patch14_224.mim_in22k for 40 steps w/ input size (3, 224, 224) and batch size 6.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 4 for retry.
Model eva02_base_patch14_224.mim_in22k created, param count: 85758720
Running train benchmark on eva02_base_patch14_224.mim_in22k for 40 steps w/ input size (3, 224, 224) and batch size 4.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 3 for retry.
Model eva02_base_patch14_224.mim_in22k created, param count: 85758720
Running train benchmark on eva02_base_patch14_224.mim_in22k for 40 steps w/ input size (3, 224, 224) and batch size 3.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 2 for retry.
Model eva02_base_patch14_224.mim_in22k created, param count: 85758720
Running train benchmark on eva02_base_patch14_224.mim_in22k for 40 steps w/ input size (3, 224, 224) and batch size 2.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 1 for retry.
Model eva02_base_patch14_224.mim_in22k created, param count: 85758720
Running train benchmark on eva02_base_patch14_224.mim_in22k for 40 steps w/ input size (3, 224, 224) and batch size 1.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 0 for retry.
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model eva02_base_patch14_448.mim_in22k_ft_in1k created, param count: 87117544
Running inference benchmark on eva02_base_patch14_448.mim_in22k_ft_in1k for 40 steps w/ input size (3, 448, 448) and batch size 256.
Infer [8/40]. 335.10 samples/sec. 763.954 ms/step.
Infer [16/40]. 335.07 samples/sec. 764.011 ms/step.
Infer [24/40]. 335.00 samples/sec. 764.182 ms/step.
Infer [32/40]. 335.01 samples/sec. 764.166 ms/step.
Infer [40/40]. 335.03 samples/sec. 764.116 ms/step.
Inference benchmark of eva02_base_patch14_448.mim_in22k_ft_in1k done. 335.01 samples/sec, 764.12 ms/step
Model eva02_base_patch14_448.mim_in22k_ft_in1k created, param count: 87117544
Running train benchmark on eva02_base_patch14_448.mim_in22k_ft_in1k for 40 steps w/ input size (3, 448, 448) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 1.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 538.06 MiB is free. Including non-PyTorch memory, this process has 23.12 GiB memory in use. Of the allocated memory 21.56 GiB is allocated by PyTorch, and 333.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model eva02_base_patch14_448.mim_in22k_ft_in1k created, param count: 87117544
Running train benchmark on eva02_base_patch14_448.mim_in22k_ft_in1k for 40 steps w/ input size (3, 448, 448) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 770.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 492.06 MiB is free. Including non-PyTorch memory, this process has 23.16 GiB memory in use. Of the allocated memory 21.47 GiB is allocated by PyTorch, and 476.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model eva02_base_patch14_448.mim_in22k_ft_in1k created, param count: 87117544
Running train benchmark on eva02_base_patch14_448.mim_in22k_ft_in1k for 40 steps w/ input size (3, 448, 448) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 514.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 318.06 MiB is free. Including non-PyTorch memory, this process has 23.33 GiB memory in use. Of the allocated memory 21.90 GiB is allocated by PyTorch, and 209.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model eva02_base_patch14_448.mim_in22k_ft_in1k created, param count: 87117544
Running train benchmark on eva02_base_patch14_448.mim_in22k_ft_in1k for 40 steps w/ input size (3, 448, 448) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 146.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 118.06 MiB is free. Including non-PyTorch memory, this process has 23.53 GiB memory in use. Of the allocated memory 21.92 GiB is allocated by PyTorch, and 388.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model eva02_base_patch14_448.mim_in22k_ft_in1k created, param count: 87117544
Running train benchmark on eva02_base_patch14_448.mim_in22k_ft_in1k for 40 steps w/ input size (3, 448, 448) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 258.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 30.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 22.07 GiB is allocated by PyTorch, and 317.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model eva02_base_patch14_448.mim_in22k_ft_in1k created, param count: 87117544
Running train benchmark on eva02_base_patch14_448.mim_in22k_ft_in1k for 40 steps w/ input size (3, 448, 448) and batch size 48.
Train [8/40]. 104.01 samples/sec. 461.486 ms/step.
Train [16/40]. 104.01 samples/sec. 461.482 ms/step.
Train [24/40]. 104.02 samples/sec. 461.460 ms/step.
Train [32/40]. 104.01 samples/sec. 461.478 ms/step.
Train [40/40]. 104.01 samples/sec. 461.494 ms/step.
Train benchmark of eva02_base_patch14_448.mim_in22k_ft_in1k done. 103.68 samples/sec, 461.49 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model eva02_base_patch14_448.mim_in22k_ft_in22k created, param count: 103144273
Running inference benchmark on eva02_base_patch14_448.mim_in22k_ft_in22k for 40 steps w/ input size (3, 448, 448) and batch size 256.
Infer [8/40]. 334.80 samples/sec. 764.636 ms/step.
Infer [16/40]. 334.75 samples/sec. 764.753 ms/step.
Infer [24/40]. 334.70 samples/sec. 764.856 ms/step.
Infer [32/40]. 334.58 samples/sec. 765.141 ms/step.
Infer [40/40]. 334.55 samples/sec. 765.213 ms/step.
Inference benchmark of eva02_base_patch14_448.mim_in22k_ft_in22k done. 334.53 samples/sec, 765.21 ms/step
Model eva02_base_patch14_448.mim_in22k_ft_in22k created, param count: 103144273
Running train benchmark on eva02_base_patch14_448.mim_in22k_ft_in22k for 40 steps w/ input size (3, 448, 448) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 1.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 538.06 MiB is free. Including non-PyTorch memory, this process has 23.12 GiB memory in use. Of the allocated memory 21.59 GiB is allocated by PyTorch, and 303.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model eva02_base_patch14_448.mim_in22k_ft_in22k created, param count: 103144273
Running train benchmark on eva02_base_patch14_448.mim_in22k_ft_in22k for 40 steps w/ input size (3, 448, 448) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 770.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 492.06 MiB is free. Including non-PyTorch memory, this process has 23.16 GiB memory in use. Of the allocated memory 21.50 GiB is allocated by PyTorch, and 445.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model eva02_base_patch14_448.mim_in22k_ft_in22k created, param count: 103144273
Running train benchmark on eva02_base_patch14_448.mim_in22k_ft_in22k for 40 steps w/ input size (3, 448, 448) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 514.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 124.06 MiB is free. Including non-PyTorch memory, this process has 23.52 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 372.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model eva02_base_patch14_448.mim_in22k_ft_in22k created, param count: 103144273
Running train benchmark on eva02_base_patch14_448.mim_in22k_ft_in22k for 40 steps w/ input size (3, 448, 448) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 146.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 118.06 MiB is free. Including non-PyTorch memory, this process has 23.53 GiB memory in use. Of the allocated memory 21.95 GiB is allocated by PyTorch, and 357.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model eva02_base_patch14_448.mim_in22k_ft_in22k created, param count: 103144273
Running train benchmark on eva02_base_patch14_448.mim_in22k_ft_in22k for 40 steps w/ input size (3, 448, 448) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 258.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 30.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 22.10 GiB is allocated by PyTorch, and 286.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model eva02_base_patch14_448.mim_in22k_ft_in22k created, param count: 103144273
Running train benchmark on eva02_base_patch14_448.mim_in22k_ft_in22k for 40 steps w/ input size (3, 448, 448) and batch size 48.
Train [8/40]. 103.89 samples/sec. 462.029 ms/step.
Train [16/40]. 103.89 samples/sec. 462.048 ms/step.
Train [24/40]. 103.89 samples/sec. 462.044 ms/step.
Train [32/40]. 103.89 samples/sec. 462.034 ms/step.
Train [40/40]. 103.89 samples/sec. 462.034 ms/step.
Train benchmark of eva02_base_patch14_448.mim_in22k_ft_in22k done. 103.56 samples/sec, 462.03 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model eva02_base_patch14_448.mim_in22k_ft_in22k_in1k created, param count: 87117544
Running inference benchmark on eva02_base_patch14_448.mim_in22k_ft_in22k_in1k for 40 steps w/ input size (3, 448, 448) and batch size 256.
Infer [8/40]. 334.76 samples/sec. 764.724 ms/step.
Infer [16/40]. 334.70 samples/sec. 764.864 ms/step.
Infer [24/40]. 334.66 samples/sec. 764.951 ms/step.
Infer [32/40]. 334.72 samples/sec. 764.825 ms/step.
Infer [40/40]. 334.71 samples/sec. 764.834 ms/step.
Inference benchmark of eva02_base_patch14_448.mim_in22k_ft_in22k_in1k done. 334.69 samples/sec, 764.83 ms/step
Model eva02_base_patch14_448.mim_in22k_ft_in22k_in1k created, param count: 87117544
Running train benchmark on eva02_base_patch14_448.mim_in22k_ft_in22k_in1k for 40 steps w/ input size (3, 448, 448) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 1.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 538.06 MiB is free. Including non-PyTorch memory, this process has 23.12 GiB memory in use. Of the allocated memory 21.56 GiB is allocated by PyTorch, and 333.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model eva02_base_patch14_448.mim_in22k_ft_in22k_in1k created, param count: 87117544
Running train benchmark on eva02_base_patch14_448.mim_in22k_ft_in22k_in1k for 40 steps w/ input size (3, 448, 448) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 770.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 492.06 MiB is free. Including non-PyTorch memory, this process has 23.16 GiB memory in use. Of the allocated memory 21.47 GiB is allocated by PyTorch, and 476.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model eva02_base_patch14_448.mim_in22k_ft_in22k_in1k created, param count: 87117544
Running train benchmark on eva02_base_patch14_448.mim_in22k_ft_in22k_in1k for 40 steps w/ input size (3, 448, 448) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 514.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 318.06 MiB is free. Including non-PyTorch memory, this process has 23.33 GiB memory in use. Of the allocated memory 21.90 GiB is allocated by PyTorch, and 209.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model eva02_base_patch14_448.mim_in22k_ft_in22k_in1k created, param count: 87117544
Running train benchmark on eva02_base_patch14_448.mim_in22k_ft_in22k_in1k for 40 steps w/ input size (3, 448, 448) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 146.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 118.06 MiB is free. Including non-PyTorch memory, this process has 23.53 GiB memory in use. Of the allocated memory 21.92 GiB is allocated by PyTorch, and 388.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model eva02_base_patch14_448.mim_in22k_ft_in22k_in1k created, param count: 87117544
Running train benchmark on eva02_base_patch14_448.mim_in22k_ft_in22k_in1k for 40 steps w/ input size (3, 448, 448) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 258.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 30.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 22.07 GiB is allocated by PyTorch, and 317.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model eva02_base_patch14_448.mim_in22k_ft_in22k_in1k created, param count: 87117544
Running train benchmark on eva02_base_patch14_448.mim_in22k_ft_in22k_in1k for 40 steps w/ input size (3, 448, 448) and batch size 48.
Train [8/40]. 103.99 samples/sec. 461.580 ms/step.
Train [16/40]. 103.99 samples/sec. 461.591 ms/step.
Train [24/40]. 103.99 samples/sec. 461.601 ms/step.
Train [32/40]. 103.98 samples/sec. 461.626 ms/step.
Train [40/40]. 103.98 samples/sec. 461.606 ms/step.
Train benchmark of eva02_base_patch14_448.mim_in22k_ft_in22k_in1k done. 103.65 samples/sec, 461.61 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model eva02_base_patch16_clip_224.merged2b created, param count: 86263040
Running inference benchmark on eva02_base_patch16_clip_224.merged2b for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1994.79 samples/sec. 128.334 ms/step.
Infer [16/40]. 1994.95 samples/sec. 128.324 ms/step.
Infer [24/40]. 1994.93 samples/sec. 128.325 ms/step.
Infer [32/40]. 1994.93 samples/sec. 128.325 ms/step.
Infer [40/40]. 1994.94 samples/sec. 128.324 ms/step.
Inference benchmark of eva02_base_patch16_clip_224.merged2b done. 1994.48 samples/sec, 128.32 ms/step
Model eva02_base_patch16_clip_224.merged2b created, param count: 86263040
Running train benchmark on eva02_base_patch16_clip_224.merged2b for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 611.47 samples/sec. 418.664 ms/step.
Train [16/40]. 611.46 samples/sec. 418.670 ms/step.
Train [24/40]. 611.45 samples/sec. 418.678 ms/step.
Train [32/40]. 611.44 samples/sec. 418.681 ms/step.
Train [40/40]. 611.45 samples/sec. 418.678 ms/step.
Train benchmark of eva02_base_patch16_clip_224.merged2b done. 609.26 samples/sec, 418.68 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model eva02_enormous_patch14_clip_224.laion2b created, param count: 4350556928
Running inference benchmark on eva02_enormous_patch14_clip_224.laion2b for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 64.09 samples/sec. 3994.089 ms/step.
Infer [16/40]. 64.05 samples/sec. 3996.660 ms/step.
Infer [24/40]. 64.03 samples/sec. 3997.931 ms/step.
Infer [32/40]. 64.02 samples/sec. 3998.906 ms/step.
Infer [40/40]. 64.00 samples/sec. 3999.868 ms/step.
Inference benchmark of eva02_enormous_patch14_clip_224.laion2b done. 64.00 samples/sec, 3999.87 ms/step
Model eva02_enormous_patch14_clip_224.laion2b created, param count: 4350556928
Running train benchmark on eva02_enormous_patch14_clip_224.laion2b for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 1.88 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.10 GiB is free. Including non-PyTorch memory, this process has 22.54 GiB memory in use. Of the allocated memory 20.79 GiB is allocated by PyTorch, and 537.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model eva02_enormous_patch14_clip_224.laion2b created, param count: 4350556928
Running train benchmark on eva02_enormous_patch14_clip_224.laion2b for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 170.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 152.06 MiB is free. Including non-PyTorch memory, this process has 23.49 GiB memory in use. Of the allocated memory 21.60 GiB is allocated by PyTorch, and 674.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model eva02_enormous_patch14_clip_224.laion2b created, param count: 4350556928
Running train benchmark on eva02_enormous_patch14_clip_224.laion2b for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 114.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 16.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 21.98 GiB is allocated by PyTorch, and 425.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model eva02_enormous_patch14_clip_224.laion2b created, param count: 4350556928
Running train benchmark on eva02_enormous_patch14_clip_224.laion2b for 40 steps w/ input size (3, 224, 224) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 724.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 614.06 MiB is free. Including non-PyTorch memory, this process has 23.04 GiB memory in use. Of the allocated memory 21.16 GiB is allocated by PyTorch, and 665.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model eva02_enormous_patch14_clip_224.laion2b created, param count: 4350556928
Running train benchmark on eva02_enormous_patch14_clip_224.laion2b for 40 steps w/ input size (3, 224, 224) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 482.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 468.06 MiB is free. Including non-PyTorch memory, this process has 23.18 GiB memory in use. Of the allocated memory 21.43 GiB is allocated by PyTorch, and 533.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model eva02_enormous_patch14_clip_224.laion2b created, param count: 4350556928
Running train benchmark on eva02_enormous_patch14_clip_224.laion2b for 40 steps w/ input size (3, 224, 224) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 44.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 12.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 21.87 GiB is allocated by PyTorch, and 544.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model eva02_enormous_patch14_clip_224.laion2b created, param count: 4350556928
Running train benchmark on eva02_enormous_patch14_clip_224.laion2b for 40 steps w/ input size (3, 224, 224) and batch size 32.
ERROR: "CUDA out of memory. Tried to allocate 242.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 66.06 MiB is free. Including non-PyTorch memory, this process has 23.58 GiB memory in use. Of the allocated memory 21.68 GiB is allocated by PyTorch, and 679.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model eva02_enormous_patch14_clip_224.laion2b created, param count: 4350556928
Running train benchmark on eva02_enormous_patch14_clip_224.laion2b for 40 steps w/ input size (3, 224, 224) and batch size 24.
ERROR: "CUDA out of memory. Tried to allocate 182.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 70.06 MiB is free. Including non-PyTorch memory, this process has 23.57 GiB memory in use. Of the allocated memory 21.87 GiB is allocated by PyTorch, and 482.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model eva02_enormous_patch14_clip_224.laion2b created, param count: 4350556928
Running train benchmark on eva02_enormous_patch14_clip_224.laion2b for 40 steps w/ input size (3, 224, 224) and batch size 16.
ERROR: "CUDA out of memory. Tried to allocate 122.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 36.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 21.46 GiB is allocated by PyTorch, and 934.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 12 for retry.
Model eva02_enormous_patch14_clip_224.laion2b created, param count: 4350556928
Running train benchmark on eva02_enormous_patch14_clip_224.laion2b for 40 steps w/ input size (3, 224, 224) and batch size 12.
ERROR: "CUDA out of memory. Tried to allocate 92.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 46.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 21.52 GiB is allocated by PyTorch, and 871.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 8 for retry.
Model eva02_enormous_patch14_clip_224.laion2b created, param count: 4350556928
Running train benchmark on eva02_enormous_patch14_clip_224.laion2b for 40 steps w/ input size (3, 224, 224) and batch size 8.
ERROR: "CUDA out of memory. Tried to allocate 54.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 4.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 21.24 GiB is allocated by PyTorch, and 1.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 6 for retry.
ERROR: "CUDA out of memory. Tried to allocate 54.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 12.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 21.24 GiB is allocated by PyTorch, and 1.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 4 for retry.
ERROR: "CUDA out of memory. Tried to allocate 54.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 12.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 21.24 GiB is allocated by PyTorch, and 1.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 3 for retry.
ERROR: "CUDA out of memory. Tried to allocate 54.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 12.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 21.24 GiB is allocated by PyTorch, and 1.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 2 for retry.
ERROR: "CUDA out of memory. Tried to allocate 54.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 12.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 21.24 GiB is allocated by PyTorch, and 1.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 1 for retry.
ERROR: "CUDA out of memory. Tried to allocate 54.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 12.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 21.24 GiB is allocated by PyTorch, and 1.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 0 for retry.
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model eva02_enormous_patch14_clip_224.laion2b_plus created, param count: 4350556928
Running inference benchmark on eva02_enormous_patch14_clip_224.laion2b_plus for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 64.29 samples/sec. 3982.266 ms/step.
Infer [16/40]. 64.23 samples/sec. 3985.854 ms/step.
Infer [24/40]. 64.18 samples/sec. 3988.836 ms/step.
Infer [32/40]. 64.15 samples/sec. 3990.755 ms/step.
Infer [40/40]. 64.13 samples/sec. 3992.186 ms/step.
Inference benchmark of eva02_enormous_patch14_clip_224.laion2b_plus done. 64.12 samples/sec, 3992.19 ms/step
Model eva02_enormous_patch14_clip_224.laion2b_plus created, param count: 4350556928
Running train benchmark on eva02_enormous_patch14_clip_224.laion2b_plus for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 1.88 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.09 GiB is free. Including non-PyTorch memory, this process has 22.55 GiB memory in use. Of the allocated memory 20.79 GiB is allocated by PyTorch, and 537.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model eva02_enormous_patch14_clip_224.laion2b_plus created, param count: 4350556928
Running train benchmark on eva02_enormous_patch14_clip_224.laion2b_plus for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 170.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 150.06 MiB is free. Including non-PyTorch memory, this process has 23.49 GiB memory in use. Of the allocated memory 21.60 GiB is allocated by PyTorch, and 674.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model eva02_enormous_patch14_clip_224.laion2b_plus created, param count: 4350556928
Running train benchmark on eva02_enormous_patch14_clip_224.laion2b_plus for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 114.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 14.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 21.98 GiB is allocated by PyTorch, and 425.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model eva02_enormous_patch14_clip_224.laion2b_plus created, param count: 4350556928
Running train benchmark on eva02_enormous_patch14_clip_224.laion2b_plus for 40 steps w/ input size (3, 224, 224) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 724.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 612.06 MiB is free. Including non-PyTorch memory, this process has 23.04 GiB memory in use. Of the allocated memory 21.16 GiB is allocated by PyTorch, and 665.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model eva02_enormous_patch14_clip_224.laion2b_plus created, param count: 4350556928
Running train benchmark on eva02_enormous_patch14_clip_224.laion2b_plus for 40 steps w/ input size (3, 224, 224) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 482.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 466.06 MiB is free. Including non-PyTorch memory, this process has 23.19 GiB memory in use. Of the allocated memory 21.43 GiB is allocated by PyTorch, and 533.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model eva02_enormous_patch14_clip_224.laion2b_plus created, param count: 4350556928
Running train benchmark on eva02_enormous_patch14_clip_224.laion2b_plus for 40 steps w/ input size (3, 224, 224) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 362.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 288.06 MiB is free. Including non-PyTorch memory, this process has 23.36 GiB memory in use. Of the allocated memory 21.23 GiB is allocated by PyTorch, and 925.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model eva02_enormous_patch14_clip_224.laion2b_plus created, param count: 4350556928
Running train benchmark on eva02_enormous_patch14_clip_224.laion2b_plus for 40 steps w/ input size (3, 224, 224) and batch size 32.
ERROR: "CUDA out of memory. Tried to allocate 86.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 52.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 20.59 GiB is allocated by PyTorch, and 1.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model eva02_enormous_patch14_clip_224.laion2b_plus created, param count: 4350556928
Running train benchmark on eva02_enormous_patch14_clip_224.laion2b_plus for 40 steps w/ input size (3, 224, 224) and batch size 24.
ERROR: "CUDA out of memory. Tried to allocate 182.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 70.06 MiB is free. Including non-PyTorch memory, this process has 23.57 GiB memory in use. Of the allocated memory 21.47 GiB is allocated by PyTorch, and 894.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model eva02_enormous_patch14_clip_224.laion2b_plus created, param count: 4350556928
Running train benchmark on eva02_enormous_patch14_clip_224.laion2b_plus for 40 steps w/ input size (3, 224, 224) and batch size 16.
ERROR: "CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 8.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 20.91 GiB is allocated by PyTorch, and 1.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 12 for retry.
Model eva02_enormous_patch14_clip_224.laion2b_plus created, param count: 4350556928
Running train benchmark on eva02_enormous_patch14_clip_224.laion2b_plus for 40 steps w/ input size (3, 224, 224) and batch size 12.
ERROR: "CUDA out of memory. Tried to allocate 92.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 68.06 MiB is free. Including non-PyTorch memory, this process has 23.57 GiB memory in use. Of the allocated memory 20.99 GiB is allocated by PyTorch, and 1.36 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 8 for retry.
Model eva02_enormous_patch14_clip_224.laion2b_plus created, param count: 4350556928
Running train benchmark on eva02_enormous_patch14_clip_224.laion2b_plus for 40 steps w/ input size (3, 224, 224) and batch size 8.
ERROR: "CUDA out of memory. Tried to allocate 54.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 18.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 21.19 GiB is allocated by PyTorch, and 1.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 6 for retry.
ERROR: "CUDA out of memory. Tried to allocate 54.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 4.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 21.44 GiB is allocated by PyTorch, and 986.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 4 for retry.
ERROR: "CUDA out of memory. Tried to allocate 54.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 4.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 21.44 GiB is allocated by PyTorch, and 986.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 3 for retry.
ERROR: "CUDA out of memory. Tried to allocate 54.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 4.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 21.44 GiB is allocated by PyTorch, and 986.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 2 for retry.
ERROR: "CUDA out of memory. Tried to allocate 54.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 4.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 21.44 GiB is allocated by PyTorch, and 986.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 1 for retry.
ERROR: "CUDA out of memory. Tried to allocate 54.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 4.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 21.44 GiB is allocated by PyTorch, and 986.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 0 for retry.
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model eva02_large_patch14_224.mim_in22k created, param count: 303268800
Running inference benchmark on eva02_large_patch14_224.mim_in22k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 485.26 samples/sec. 527.551 ms/step.
Infer [16/40]. 485.16 samples/sec. 527.657 ms/step.
Infer [24/40]. 485.12 samples/sec. 527.706 ms/step.
Infer [32/40]. 485.09 samples/sec. 527.738 ms/step.
Infer [40/40]. 485.07 samples/sec. 527.757 ms/step.
Inference benchmark of eva02_large_patch14_224.mim_in22k done. 485.03 samples/sec, 527.76 ms/step
Model eva02_large_patch14_224.mim_in22k created, param count: 303268800
Running train benchmark on eva02_large_patch14_224.mim_in22k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 130.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 18.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.24 GiB is allocated by PyTorch, and 157.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model eva02_large_patch14_224.mim_in22k created, param count: 303268800
Running train benchmark on eva02_large_patch14_224.mim_in22k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 258.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 10.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.01 GiB is allocated by PyTorch, and 395.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model eva02_large_patch14_224.mim_in22k created, param count: 303268800
Running train benchmark on eva02_large_patch14_224.mim_in22k for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 172.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 134.06 MiB is free. Including non-PyTorch memory, this process has 23.51 GiB memory in use. Of the allocated memory 21.97 GiB is allocated by PyTorch, and 320.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model eva02_large_patch14_224.mim_in22k created, param count: 303268800
Running train benchmark on eva02_large_patch14_224.mim_in22k for 40 steps w/ input size (3, 224, 224) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 50.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 21.83 GiB is allocated by PyTorch, and 542.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model eva02_large_patch14_224.mim_in22k created, param count: 303268800
Running train benchmark on eva02_large_patch14_224.mim_in22k for 40 steps w/ input size (3, 224, 224) and batch size 64.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model eva02_large_patch14_224.mim_in22k created, param count: 303268800
Running train benchmark on eva02_large_patch14_224.mim_in22k for 40 steps w/ input size (3, 224, 224) and batch size 48.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model eva02_large_patch14_224.mim_in22k created, param count: 303268800
Running train benchmark on eva02_large_patch14_224.mim_in22k for 40 steps w/ input size (3, 224, 224) and batch size 32.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model eva02_large_patch14_224.mim_in22k created, param count: 303268800
Running train benchmark on eva02_large_patch14_224.mim_in22k for 40 steps w/ input size (3, 224, 224) and batch size 24.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model eva02_large_patch14_224.mim_in22k created, param count: 303268800
Running train benchmark on eva02_large_patch14_224.mim_in22k for 40 steps w/ input size (3, 224, 224) and batch size 16.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 12 for retry.
Model eva02_large_patch14_224.mim_in22k created, param count: 303268800
Running train benchmark on eva02_large_patch14_224.mim_in22k for 40 steps w/ input size (3, 224, 224) and batch size 12.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 8 for retry.
Model eva02_large_patch14_224.mim_in22k created, param count: 303268800
Running train benchmark on eva02_large_patch14_224.mim_in22k for 40 steps w/ input size (3, 224, 224) and batch size 8.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 6 for retry.
Model eva02_large_patch14_224.mim_in22k created, param count: 303268800
Running train benchmark on eva02_large_patch14_224.mim_in22k for 40 steps w/ input size (3, 224, 224) and batch size 6.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 4 for retry.
Model eva02_large_patch14_224.mim_in22k created, param count: 303268800
Running train benchmark on eva02_large_patch14_224.mim_in22k for 40 steps w/ input size (3, 224, 224) and batch size 4.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 3 for retry.
Model eva02_large_patch14_224.mim_in22k created, param count: 303268800
Running train benchmark on eva02_large_patch14_224.mim_in22k for 40 steps w/ input size (3, 224, 224) and batch size 3.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 2 for retry.
Model eva02_large_patch14_224.mim_in22k created, param count: 303268800
Running train benchmark on eva02_large_patch14_224.mim_in22k for 40 steps w/ input size (3, 224, 224) and batch size 2.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 1 for retry.
Model eva02_large_patch14_224.mim_in22k created, param count: 303268800
Running train benchmark on eva02_large_patch14_224.mim_in22k for 40 steps w/ input size (3, 224, 224) and batch size 1.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 0 for retry.
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model eva02_large_patch14_224.mim_m38m created, param count: 303268800
Running inference benchmark on eva02_large_patch14_224.mim_m38m for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 484.97 samples/sec. 527.868 ms/step.
Infer [16/40]. 484.93 samples/sec. 527.911 ms/step.
Infer [24/40]. 484.88 samples/sec. 527.960 ms/step.
Infer [32/40]. 484.88 samples/sec. 527.971 ms/step.
Infer [40/40]. 484.87 samples/sec. 527.982 ms/step.
Inference benchmark of eva02_large_patch14_224.mim_m38m done. 484.83 samples/sec, 527.98 ms/step
Model eva02_large_patch14_224.mim_m38m created, param count: 303268800
Running train benchmark on eva02_large_patch14_224.mim_m38m for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 130.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 18.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.24 GiB is allocated by PyTorch, and 157.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model eva02_large_patch14_224.mim_m38m created, param count: 303268800
Running train benchmark on eva02_large_patch14_224.mim_m38m for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 258.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 10.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.01 GiB is allocated by PyTorch, and 395.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model eva02_large_patch14_224.mim_m38m created, param count: 303268800
Running train benchmark on eva02_large_patch14_224.mim_m38m for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 172.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 134.06 MiB is free. Including non-PyTorch memory, this process has 23.51 GiB memory in use. Of the allocated memory 21.97 GiB is allocated by PyTorch, and 320.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model eva02_large_patch14_224.mim_m38m created, param count: 303268800
Running train benchmark on eva02_large_patch14_224.mim_m38m for 40 steps w/ input size (3, 224, 224) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 50.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 21.83 GiB is allocated by PyTorch, and 542.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model eva02_large_patch14_224.mim_m38m created, param count: 303268800
Running train benchmark on eva02_large_patch14_224.mim_m38m for 40 steps w/ input size (3, 224, 224) and batch size 64.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model eva02_large_patch14_224.mim_m38m created, param count: 303268800
Running train benchmark on eva02_large_patch14_224.mim_m38m for 40 steps w/ input size (3, 224, 224) and batch size 48.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model eva02_large_patch14_224.mim_m38m created, param count: 303268800
Running train benchmark on eva02_large_patch14_224.mim_m38m for 40 steps w/ input size (3, 224, 224) and batch size 32.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model eva02_large_patch14_224.mim_m38m created, param count: 303268800
Running train benchmark on eva02_large_patch14_224.mim_m38m for 40 steps w/ input size (3, 224, 224) and batch size 24.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model eva02_large_patch14_224.mim_m38m created, param count: 303268800
Running train benchmark on eva02_large_patch14_224.mim_m38m for 40 steps w/ input size (3, 224, 224) and batch size 16.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 12 for retry.
Model eva02_large_patch14_224.mim_m38m created, param count: 303268800
Running train benchmark on eva02_large_patch14_224.mim_m38m for 40 steps w/ input size (3, 224, 224) and batch size 12.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 8 for retry.
Model eva02_large_patch14_224.mim_m38m created, param count: 303268800
Running train benchmark on eva02_large_patch14_224.mim_m38m for 40 steps w/ input size (3, 224, 224) and batch size 8.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 6 for retry.
Model eva02_large_patch14_224.mim_m38m created, param count: 303268800
Running train benchmark on eva02_large_patch14_224.mim_m38m for 40 steps w/ input size (3, 224, 224) and batch size 6.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 4 for retry.
Model eva02_large_patch14_224.mim_m38m created, param count: 303268800
Running train benchmark on eva02_large_patch14_224.mim_m38m for 40 steps w/ input size (3, 224, 224) and batch size 4.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 3 for retry.
Model eva02_large_patch14_224.mim_m38m created, param count: 303268800
Running train benchmark on eva02_large_patch14_224.mim_m38m for 40 steps w/ input size (3, 224, 224) and batch size 3.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 2 for retry.
Model eva02_large_patch14_224.mim_m38m created, param count: 303268800
Running train benchmark on eva02_large_patch14_224.mim_m38m for 40 steps w/ input size (3, 224, 224) and batch size 2.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 1 for retry.
Model eva02_large_patch14_224.mim_m38m created, param count: 303268800
Running train benchmark on eva02_large_patch14_224.mim_m38m for 40 steps w/ input size (3, 224, 224) and batch size 1.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 0 for retry.
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model eva02_large_patch14_448.mim_in22k_ft_in1k created, param count: 305080232
Running inference benchmark on eva02_large_patch14_448.mim_in22k_ft_in1k for 40 steps w/ input size (3, 448, 448) and batch size 256.
Infer [8/40]. 109.85 samples/sec. 2330.378 ms/step.
Infer [16/40]. 109.85 samples/sec. 2330.369 ms/step.
Infer [24/40]. 109.83 samples/sec. 2330.919 ms/step.
Infer [32/40]. 109.80 samples/sec. 2331.563 ms/step.
Infer [40/40]. 109.78 samples/sec. 2332.020 ms/step.
Inference benchmark of eva02_large_patch14_448.mim_in22k_ft_in1k done. 109.77 samples/sec, 2332.02 ms/step
Model eva02_large_patch14_448.mim_in22k_ft_in1k created, param count: 305080232
Running train benchmark on eva02_large_patch14_448.mim_in22k_ft_in1k for 40 steps w/ input size (3, 448, 448) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 1.34 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.25 GiB is free. Including non-PyTorch memory, this process has 22.39 GiB memory in use. Of the allocated memory 20.94 GiB is allocated by PyTorch, and 227.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model eva02_large_patch14_448.mim_in22k_ft_in1k created, param count: 305080232
Running train benchmark on eva02_large_patch14_448.mim_in22k_ft_in1k for 40 steps w/ input size (3, 448, 448) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 1.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 252.06 MiB is free. Including non-PyTorch memory, this process has 23.39 GiB memory in use. Of the allocated memory 21.87 GiB is allocated by PyTorch, and 305.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model eva02_large_patch14_448.mim_in22k_ft_in1k created, param count: 305080232
Running train benchmark on eva02_large_patch14_448.mim_in22k_ft_in1k for 40 steps w/ input size (3, 448, 448) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 258.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 104.06 MiB is free. Including non-PyTorch memory, this process has 23.54 GiB memory in use. Of the allocated memory 22.14 GiB is allocated by PyTorch, and 173.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model eva02_large_patch14_448.mim_in22k_ft_in1k created, param count: 305080232
Running train benchmark on eva02_large_patch14_448.mim_in22k_ft_in1k for 40 steps w/ input size (3, 448, 448) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 106.06 MiB is free. Including non-PyTorch memory, this process has 23.54 GiB memory in use. Of the allocated memory 21.97 GiB is allocated by PyTorch, and 344.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model eva02_large_patch14_448.mim_in22k_ft_in1k created, param count: 305080232
Running train benchmark on eva02_large_patch14_448.mim_in22k_ft_in1k for 40 steps w/ input size (3, 448, 448) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 130.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 24.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.07 GiB is allocated by PyTorch, and 326.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model eva02_large_patch14_448.mim_in22k_ft_in1k created, param count: 305080232
Running train benchmark on eva02_large_patch14_448.mim_in22k_ft_in1k for 40 steps w/ input size (3, 448, 448) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 258.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 178.06 MiB is free. Including non-PyTorch memory, this process has 23.47 GiB memory in use. Of the allocated memory 21.70 GiB is allocated by PyTorch, and 545.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model eva02_large_patch14_448.mim_in22k_ft_in1k created, param count: 305080232
Running train benchmark on eva02_large_patch14_448.mim_in22k_ft_in1k for 40 steps w/ input size (3, 448, 448) and batch size 32.
ERROR: "CUDA out of memory. Tried to allocate 172.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 106.06 MiB is free. Including non-PyTorch memory, this process has 23.54 GiB memory in use. Of the allocated memory 21.85 GiB is allocated by PyTorch, and 467.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model eva02_large_patch14_448.mim_in22k_ft_in1k created, param count: 305080232
Running train benchmark on eva02_large_patch14_448.mim_in22k_ft_in1k for 40 steps w/ input size (3, 448, 448) and batch size 24.
ERROR: "CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 6.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 21.73 GiB is allocated by PyTorch, and 692.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model eva02_large_patch14_448.mim_in22k_ft_in1k created, param count: 305080232
Running train benchmark on eva02_large_patch14_448.mim_in22k_ft_in1k for 40 steps w/ input size (3, 448, 448) and batch size 16.
Train [8/40]. 35.47 samples/sec. 451.023 ms/step.
Train [16/40]. 35.47 samples/sec. 451.036 ms/step.
Train [24/40]. 35.47 samples/sec. 451.049 ms/step.
Train [32/40]. 35.47 samples/sec. 451.057 ms/step.
Train [40/40]. 35.47 samples/sec. 451.050 ms/step.
Train benchmark of eva02_large_patch14_448.mim_in22k_ft_in1k done. 35.28 samples/sec, 451.05 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model eva02_large_patch14_448.mim_in22k_ft_in22k created, param count: 326442257
Running inference benchmark on eva02_large_patch14_448.mim_in22k_ft_in22k for 40 steps w/ input size (3, 448, 448) and batch size 256.
Infer [8/40]. 109.88 samples/sec. 2329.839 ms/step.
Infer [16/40]. 109.86 samples/sec. 2330.210 ms/step.
Infer [24/40]. 109.83 samples/sec. 2330.965 ms/step.
Infer [32/40]. 109.81 samples/sec. 2331.363 ms/step.
Infer [40/40]. 109.78 samples/sec. 2331.843 ms/step.
Inference benchmark of eva02_large_patch14_448.mim_in22k_ft_in22k done. 109.78 samples/sec, 2331.84 ms/step
Model eva02_large_patch14_448.mim_in22k_ft_in22k created, param count: 326442257
Running train benchmark on eva02_large_patch14_448.mim_in22k_ft_in22k for 40 steps w/ input size (3, 448, 448) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 1.34 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.25 GiB is free. Including non-PyTorch memory, this process has 22.39 GiB memory in use. Of the allocated memory 20.98 GiB is allocated by PyTorch, and 187.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model eva02_large_patch14_448.mim_in22k_ft_in22k created, param count: 326442257
Running train benchmark on eva02_large_patch14_448.mim_in22k_ft_in22k for 40 steps w/ input size (3, 448, 448) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 1.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 168.06 MiB is free. Including non-PyTorch memory, this process has 23.48 GiB memory in use. Of the allocated memory 21.91 GiB is allocated by PyTorch, and 348.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model eva02_large_patch14_448.mim_in22k_ft_in22k created, param count: 326442257
Running train benchmark on eva02_large_patch14_448.mim_in22k_ft_in22k for 40 steps w/ input size (3, 448, 448) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 258.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 84.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 22.18 GiB is allocated by PyTorch, and 152.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model eva02_large_patch14_448.mim_in22k_ft_in22k created, param count: 326442257
Running train benchmark on eva02_large_patch14_448.mim_in22k_ft_in22k for 40 steps w/ input size (3, 448, 448) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 118.06 MiB is free. Including non-PyTorch memory, this process has 23.53 GiB memory in use. Of the allocated memory 22.01 GiB is allocated by PyTorch, and 291.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model eva02_large_patch14_448.mim_in22k_ft_in22k created, param count: 326442257
Running train benchmark on eva02_large_patch14_448.mim_in22k_ft_in22k for 40 steps w/ input size (3, 448, 448) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 130.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 4.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 22.11 GiB is allocated by PyTorch, and 306.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model eva02_large_patch14_448.mim_in22k_ft_in22k created, param count: 326442257
Running train benchmark on eva02_large_patch14_448.mim_in22k_ft_in22k for 40 steps w/ input size (3, 448, 448) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 258.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 94.06 MiB is free. Including non-PyTorch memory, this process has 23.55 GiB memory in use. Of the allocated memory 21.74 GiB is allocated by PyTorch, and 588.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model eva02_large_patch14_448.mim_in22k_ft_in22k created, param count: 326442257
Running train benchmark on eva02_large_patch14_448.mim_in22k_ft_in22k for 40 steps w/ input size (3, 448, 448) and batch size 32.
ERROR: "CUDA out of memory. Tried to allocate 172.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 126.06 MiB is free. Including non-PyTorch memory, this process has 23.52 GiB memory in use. Of the allocated memory 21.89 GiB is allocated by PyTorch, and 406.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model eva02_large_patch14_448.mim_in22k_ft_in22k created, param count: 326442257
Running train benchmark on eva02_large_patch14_448.mim_in22k_ft_in22k for 40 steps w/ input size (3, 448, 448) and batch size 24.
ERROR: "CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 2.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 21.81 GiB is allocated by PyTorch, and 608.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model eva02_large_patch14_448.mim_in22k_ft_in22k created, param count: 326442257
Running train benchmark on eva02_large_patch14_448.mim_in22k_ft_in22k for 40 steps w/ input size (3, 448, 448) and batch size 16.
Train [8/40]. 35.39 samples/sec. 452.105 ms/step.
Train [16/40]. 35.39 samples/sec. 452.159 ms/step.
Train [24/40]. 35.38 samples/sec. 452.175 ms/step.
Train [32/40]. 35.38 samples/sec. 452.184 ms/step.
Train [40/40]. 35.38 samples/sec. 452.190 ms/step.
Train benchmark of eva02_large_patch14_448.mim_in22k_ft_in22k done. 35.20 samples/sec, 452.19 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model eva02_large_patch14_448.mim_in22k_ft_in22k_in1k created, param count: 305080232
Running inference benchmark on eva02_large_patch14_448.mim_in22k_ft_in22k_in1k for 40 steps w/ input size (3, 448, 448) and batch size 256.
Infer [8/40]. 109.77 samples/sec. 2332.253 ms/step.
Infer [16/40]. 109.73 samples/sec. 2333.091 ms/step.
Infer [24/40]. 109.73 samples/sec. 2333.088 ms/step.
Infer [32/40]. 109.73 samples/sec. 2333.073 ms/step.
Infer [40/40]. 109.72 samples/sec. 2333.308 ms/step.
Inference benchmark of eva02_large_patch14_448.mim_in22k_ft_in22k_in1k done. 109.71 samples/sec, 2333.31 ms/step
Model eva02_large_patch14_448.mim_in22k_ft_in22k_in1k created, param count: 305080232
Running train benchmark on eva02_large_patch14_448.mim_in22k_ft_in22k_in1k for 40 steps w/ input size (3, 448, 448) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 1.34 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.25 GiB is free. Including non-PyTorch memory, this process has 22.39 GiB memory in use. Of the allocated memory 20.94 GiB is allocated by PyTorch, and 227.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model eva02_large_patch14_448.mim_in22k_ft_in22k_in1k created, param count: 305080232
Running train benchmark on eva02_large_patch14_448.mim_in22k_ft_in22k_in1k for 40 steps w/ input size (3, 448, 448) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 1.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 252.06 MiB is free. Including non-PyTorch memory, this process has 23.39 GiB memory in use. Of the allocated memory 21.87 GiB is allocated by PyTorch, and 305.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model eva02_large_patch14_448.mim_in22k_ft_in22k_in1k created, param count: 305080232
Running train benchmark on eva02_large_patch14_448.mim_in22k_ft_in22k_in1k for 40 steps w/ input size (3, 448, 448) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 258.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 84.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 22.14 GiB is allocated by PyTorch, and 193.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model eva02_large_patch14_448.mim_in22k_ft_in22k_in1k created, param count: 305080232
Running train benchmark on eva02_large_patch14_448.mim_in22k_ft_in22k_in1k for 40 steps w/ input size (3, 448, 448) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 106.06 MiB is free. Including non-PyTorch memory, this process has 23.54 GiB memory in use. Of the allocated memory 21.97 GiB is allocated by PyTorch, and 344.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model eva02_large_patch14_448.mim_in22k_ft_in22k_in1k created, param count: 305080232
Running train benchmark on eva02_large_patch14_448.mim_in22k_ft_in22k_in1k for 40 steps w/ input size (3, 448, 448) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 130.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 4.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 22.07 GiB is allocated by PyTorch, and 346.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model eva02_large_patch14_448.mim_in22k_ft_in22k_in1k created, param count: 305080232
Running train benchmark on eva02_large_patch14_448.mim_in22k_ft_in22k_in1k for 40 steps w/ input size (3, 448, 448) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 258.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 178.06 MiB is free. Including non-PyTorch memory, this process has 23.47 GiB memory in use. Of the allocated memory 21.70 GiB is allocated by PyTorch, and 545.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model eva02_large_patch14_448.mim_in22k_ft_in22k_in1k created, param count: 305080232
Running train benchmark on eva02_large_patch14_448.mim_in22k_ft_in22k_in1k for 40 steps w/ input size (3, 448, 448) and batch size 32.
ERROR: "CUDA out of memory. Tried to allocate 172.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 106.06 MiB is free. Including non-PyTorch memory, this process has 23.54 GiB memory in use. Of the allocated memory 21.85 GiB is allocated by PyTorch, and 467.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model eva02_large_patch14_448.mim_in22k_ft_in22k_in1k created, param count: 305080232
Running train benchmark on eva02_large_patch14_448.mim_in22k_ft_in22k_in1k for 40 steps w/ input size (3, 448, 448) and batch size 24.
ERROR: "CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 6.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 21.73 GiB is allocated by PyTorch, and 692.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model eva02_large_patch14_448.mim_in22k_ft_in22k_in1k created, param count: 305080232
Running train benchmark on eva02_large_patch14_448.mim_in22k_ft_in22k_in1k for 40 steps w/ input size (3, 448, 448) and batch size 16.
Train [8/40]. 35.46 samples/sec. 451.246 ms/step.
Train [16/40]. 35.45 samples/sec. 451.296 ms/step.
Train [24/40]. 35.45 samples/sec. 451.296 ms/step.
Train [32/40]. 35.45 samples/sec. 451.313 ms/step.
Train [40/40]. 35.45 samples/sec. 451.331 ms/step.
Train benchmark of eva02_large_patch14_448.mim_in22k_ft_in22k_in1k done. 35.26 samples/sec, 451.33 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model eva02_large_patch14_448.mim_m38m_ft_in1k created, param count: 305080232
Running inference benchmark on eva02_large_patch14_448.mim_m38m_ft_in1k for 40 steps w/ input size (3, 448, 448) and batch size 256.
Infer [8/40]. 109.82 samples/sec. 2330.987 ms/step.
Infer [16/40]. 109.82 samples/sec. 2331.000 ms/step.
Infer [24/40]. 109.78 samples/sec. 2331.984 ms/step.
Infer [32/40]. 109.74 samples/sec. 2332.830 ms/step.
Infer [40/40]. 109.70 samples/sec. 2333.653 ms/step.
Inference benchmark of eva02_large_patch14_448.mim_m38m_ft_in1k done. 109.70 samples/sec, 2333.65 ms/step
Model eva02_large_patch14_448.mim_m38m_ft_in1k created, param count: 305080232
Running train benchmark on eva02_large_patch14_448.mim_m38m_ft_in1k for 40 steps w/ input size (3, 448, 448) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 1.34 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.25 GiB is free. Including non-PyTorch memory, this process has 22.39 GiB memory in use. Of the allocated memory 20.94 GiB is allocated by PyTorch, and 227.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model eva02_large_patch14_448.mim_m38m_ft_in1k created, param count: 305080232
Running train benchmark on eva02_large_patch14_448.mim_m38m_ft_in1k for 40 steps w/ input size (3, 448, 448) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 1.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 252.06 MiB is free. Including non-PyTorch memory, this process has 23.39 GiB memory in use. Of the allocated memory 21.87 GiB is allocated by PyTorch, and 305.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model eva02_large_patch14_448.mim_m38m_ft_in1k created, param count: 305080232
Running train benchmark on eva02_large_patch14_448.mim_m38m_ft_in1k for 40 steps w/ input size (3, 448, 448) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 258.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 84.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 22.14 GiB is allocated by PyTorch, and 193.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model eva02_large_patch14_448.mim_m38m_ft_in1k created, param count: 305080232
Running train benchmark on eva02_large_patch14_448.mim_m38m_ft_in1k for 40 steps w/ input size (3, 448, 448) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 106.06 MiB is free. Including non-PyTorch memory, this process has 23.54 GiB memory in use. Of the allocated memory 21.97 GiB is allocated by PyTorch, and 344.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model eva02_large_patch14_448.mim_m38m_ft_in1k created, param count: 305080232
Running train benchmark on eva02_large_patch14_448.mim_m38m_ft_in1k for 40 steps w/ input size (3, 448, 448) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 130.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 4.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 22.07 GiB is allocated by PyTorch, and 346.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model eva02_large_patch14_448.mim_m38m_ft_in1k created, param count: 305080232
Running train benchmark on eva02_large_patch14_448.mim_m38m_ft_in1k for 40 steps w/ input size (3, 448, 448) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 258.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 178.06 MiB is free. Including non-PyTorch memory, this process has 23.47 GiB memory in use. Of the allocated memory 21.70 GiB is allocated by PyTorch, and 545.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model eva02_large_patch14_448.mim_m38m_ft_in1k created, param count: 305080232
Running train benchmark on eva02_large_patch14_448.mim_m38m_ft_in1k for 40 steps w/ input size (3, 448, 448) and batch size 32.
ERROR: "CUDA out of memory. Tried to allocate 172.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 106.06 MiB is free. Including non-PyTorch memory, this process has 23.54 GiB memory in use. Of the allocated memory 21.85 GiB is allocated by PyTorch, and 467.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model eva02_large_patch14_448.mim_m38m_ft_in1k created, param count: 305080232
Running train benchmark on eva02_large_patch14_448.mim_m38m_ft_in1k for 40 steps w/ input size (3, 448, 448) and batch size 24.
ERROR: "CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 6.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 21.73 GiB is allocated by PyTorch, and 692.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model eva02_large_patch14_448.mim_m38m_ft_in1k created, param count: 305080232
Running train benchmark on eva02_large_patch14_448.mim_m38m_ft_in1k for 40 steps w/ input size (3, 448, 448) and batch size 16.
Train [8/40]. 35.46 samples/sec. 451.245 ms/step.
Train [16/40]. 35.45 samples/sec. 451.279 ms/step.
Train [24/40]. 35.45 samples/sec. 451.297 ms/step.
Train [32/40]. 35.45 samples/sec. 451.299 ms/step.
Train [40/40]. 35.45 samples/sec. 451.307 ms/step.
Train benchmark of eva02_large_patch14_448.mim_m38m_ft_in1k done. 35.26 samples/sec, 451.31 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model eva02_large_patch14_448.mim_m38m_ft_in22k created, param count: 326442257
Running inference benchmark on eva02_large_patch14_448.mim_m38m_ft_in22k for 40 steps w/ input size (3, 448, 448) and batch size 256.
Infer [8/40]. 109.77 samples/sec. 2332.153 ms/step.
Infer [16/40]. 109.75 samples/sec. 2332.614 ms/step.
Infer [24/40]. 109.75 samples/sec. 2332.570 ms/step.
Infer [32/40]. 109.74 samples/sec. 2332.766 ms/step.
Infer [40/40]. 109.74 samples/sec. 2332.890 ms/step.
Inference benchmark of eva02_large_patch14_448.mim_m38m_ft_in22k done. 109.73 samples/sec, 2332.89 ms/step
Model eva02_large_patch14_448.mim_m38m_ft_in22k created, param count: 326442257
Running train benchmark on eva02_large_patch14_448.mim_m38m_ft_in22k for 40 steps w/ input size (3, 448, 448) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 1.34 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.25 GiB is free. Including non-PyTorch memory, this process has 22.39 GiB memory in use. Of the allocated memory 20.98 GiB is allocated by PyTorch, and 187.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model eva02_large_patch14_448.mim_m38m_ft_in22k created, param count: 326442257
Running train benchmark on eva02_large_patch14_448.mim_m38m_ft_in22k for 40 steps w/ input size (3, 448, 448) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 1.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 168.06 MiB is free. Including non-PyTorch memory, this process has 23.48 GiB memory in use. Of the allocated memory 21.91 GiB is allocated by PyTorch, and 348.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model eva02_large_patch14_448.mim_m38m_ft_in22k created, param count: 326442257
Running train benchmark on eva02_large_patch14_448.mim_m38m_ft_in22k for 40 steps w/ input size (3, 448, 448) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 258.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 84.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 22.18 GiB is allocated by PyTorch, and 152.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model eva02_large_patch14_448.mim_m38m_ft_in22k created, param count: 326442257
Running train benchmark on eva02_large_patch14_448.mim_m38m_ft_in22k for 40 steps w/ input size (3, 448, 448) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 118.06 MiB is free. Including non-PyTorch memory, this process has 23.53 GiB memory in use. Of the allocated memory 22.01 GiB is allocated by PyTorch, and 291.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model eva02_large_patch14_448.mim_m38m_ft_in22k created, param count: 326442257
Running train benchmark on eva02_large_patch14_448.mim_m38m_ft_in22k for 40 steps w/ input size (3, 448, 448) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 130.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 4.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 22.11 GiB is allocated by PyTorch, and 306.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model eva02_large_patch14_448.mim_m38m_ft_in22k created, param count: 326442257
Running train benchmark on eva02_large_patch14_448.mim_m38m_ft_in22k for 40 steps w/ input size (3, 448, 448) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 258.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 94.06 MiB is free. Including non-PyTorch memory, this process has 23.55 GiB memory in use. Of the allocated memory 21.74 GiB is allocated by PyTorch, and 588.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model eva02_large_patch14_448.mim_m38m_ft_in22k created, param count: 326442257
Running train benchmark on eva02_large_patch14_448.mim_m38m_ft_in22k for 40 steps w/ input size (3, 448, 448) and batch size 32.
ERROR: "CUDA out of memory. Tried to allocate 172.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 126.06 MiB is free. Including non-PyTorch memory, this process has 23.52 GiB memory in use. Of the allocated memory 21.89 GiB is allocated by PyTorch, and 406.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model eva02_large_patch14_448.mim_m38m_ft_in22k created, param count: 326442257
Running train benchmark on eva02_large_patch14_448.mim_m38m_ft_in22k for 40 steps w/ input size (3, 448, 448) and batch size 24.
ERROR: "CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 2.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 21.81 GiB is allocated by PyTorch, and 608.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model eva02_large_patch14_448.mim_m38m_ft_in22k created, param count: 326442257
Running train benchmark on eva02_large_patch14_448.mim_m38m_ft_in22k for 40 steps w/ input size (3, 448, 448) and batch size 16.
Train [8/40]. 35.39 samples/sec. 452.125 ms/step.
Train [16/40]. 35.39 samples/sec. 452.146 ms/step.
Train [24/40]. 35.39 samples/sec. 452.154 ms/step.
Train [32/40]. 35.39 samples/sec. 452.162 ms/step.
Train [40/40]. 35.39 samples/sec. 452.160 ms/step.
Train benchmark of eva02_large_patch14_448.mim_m38m_ft_in22k done. 35.20 samples/sec, 452.16 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model eva02_large_patch14_448.mim_m38m_ft_in22k_in1k created, param count: 305080232
Running inference benchmark on eva02_large_patch14_448.mim_m38m_ft_in22k_in1k for 40 steps w/ input size (3, 448, 448) and batch size 256.
Infer [8/40]. 109.75 samples/sec. 2332.609 ms/step.
Infer [16/40]. 109.74 samples/sec. 2332.839 ms/step.
Infer [24/40]. 109.73 samples/sec. 2333.024 ms/step.
Infer [32/40]. 109.72 samples/sec. 2333.258 ms/step.
Infer [40/40]. 109.71 samples/sec. 2333.420 ms/step.
Inference benchmark of eva02_large_patch14_448.mim_m38m_ft_in22k_in1k done. 109.71 samples/sec, 2333.42 ms/step
Model eva02_large_patch14_448.mim_m38m_ft_in22k_in1k created, param count: 305080232
Running train benchmark on eva02_large_patch14_448.mim_m38m_ft_in22k_in1k for 40 steps w/ input size (3, 448, 448) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 1.34 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.25 GiB is free. Including non-PyTorch memory, this process has 22.39 GiB memory in use. Of the allocated memory 20.94 GiB is allocated by PyTorch, and 227.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model eva02_large_patch14_448.mim_m38m_ft_in22k_in1k created, param count: 305080232
Running train benchmark on eva02_large_patch14_448.mim_m38m_ft_in22k_in1k for 40 steps w/ input size (3, 448, 448) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 1.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 252.06 MiB is free. Including non-PyTorch memory, this process has 23.39 GiB memory in use. Of the allocated memory 21.87 GiB is allocated by PyTorch, and 305.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model eva02_large_patch14_448.mim_m38m_ft_in22k_in1k created, param count: 305080232
Running train benchmark on eva02_large_patch14_448.mim_m38m_ft_in22k_in1k for 40 steps w/ input size (3, 448, 448) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 258.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 84.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 22.14 GiB is allocated by PyTorch, and 193.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model eva02_large_patch14_448.mim_m38m_ft_in22k_in1k created, param count: 305080232
Running train benchmark on eva02_large_patch14_448.mim_m38m_ft_in22k_in1k for 40 steps w/ input size (3, 448, 448) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 106.06 MiB is free. Including non-PyTorch memory, this process has 23.54 GiB memory in use. Of the allocated memory 21.97 GiB is allocated by PyTorch, and 344.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model eva02_large_patch14_448.mim_m38m_ft_in22k_in1k created, param count: 305080232
Running train benchmark on eva02_large_patch14_448.mim_m38m_ft_in22k_in1k for 40 steps w/ input size (3, 448, 448) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 130.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 4.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 22.07 GiB is allocated by PyTorch, and 346.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model eva02_large_patch14_448.mim_m38m_ft_in22k_in1k created, param count: 305080232
Running train benchmark on eva02_large_patch14_448.mim_m38m_ft_in22k_in1k for 40 steps w/ input size (3, 448, 448) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 258.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 178.06 MiB is free. Including non-PyTorch memory, this process has 23.47 GiB memory in use. Of the allocated memory 21.70 GiB is allocated by PyTorch, and 545.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model eva02_large_patch14_448.mim_m38m_ft_in22k_in1k created, param count: 305080232
Running train benchmark on eva02_large_patch14_448.mim_m38m_ft_in22k_in1k for 40 steps w/ input size (3, 448, 448) and batch size 32.
ERROR: "CUDA out of memory. Tried to allocate 172.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 106.06 MiB is free. Including non-PyTorch memory, this process has 23.54 GiB memory in use. Of the allocated memory 21.85 GiB is allocated by PyTorch, and 467.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model eva02_large_patch14_448.mim_m38m_ft_in22k_in1k created, param count: 305080232
Running train benchmark on eva02_large_patch14_448.mim_m38m_ft_in22k_in1k for 40 steps w/ input size (3, 448, 448) and batch size 24.
ERROR: "CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 6.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 21.73 GiB is allocated by PyTorch, and 692.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model eva02_large_patch14_448.mim_m38m_ft_in22k_in1k created, param count: 305080232
Running train benchmark on eva02_large_patch14_448.mim_m38m_ft_in22k_in1k for 40 steps w/ input size (3, 448, 448) and batch size 16.
Train [8/40]. 35.45 samples/sec. 451.391 ms/step.
Train [16/40]. 35.45 samples/sec. 451.396 ms/step.
Train [24/40]. 35.45 samples/sec. 451.398 ms/step.
Train [32/40]. 35.44 samples/sec. 451.418 ms/step.
Train [40/40]. 35.44 samples/sec. 451.433 ms/step.
Train benchmark of eva02_large_patch14_448.mim_m38m_ft_in22k_in1k done. 35.26 samples/sec, 451.43 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model eva02_large_patch14_clip_224.merged2b created, param count: 304105152
Running inference benchmark on eva02_large_patch14_clip_224.merged2b for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 478.44 samples/sec. 535.075 ms/step.
Infer [16/40]. 478.42 samples/sec. 535.093 ms/step.
Infer [24/40]. 478.42 samples/sec. 535.096 ms/step.
Infer [32/40]. 478.40 samples/sec. 535.111 ms/step.
Infer [40/40]. 478.41 samples/sec. 535.108 ms/step.
Inference benchmark of eva02_large_patch14_clip_224.merged2b done. 478.37 samples/sec, 535.11 ms/step
Model eva02_large_patch14_clip_224.merged2b created, param count: 304105152
Running train benchmark on eva02_large_patch14_clip_224.merged2b for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 344.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 136.06 MiB is free. Including non-PyTorch memory, this process has 23.51 GiB memory in use. Of the allocated memory 22.12 GiB is allocated by PyTorch, and 161.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model eva02_large_patch14_clip_224.merged2b created, param count: 304105152
Running train benchmark on eva02_large_patch14_clip_224.merged2b for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 34.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 21.98 GiB is allocated by PyTorch, and 402.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model eva02_large_patch14_clip_224.merged2b created, param count: 304105152
Running train benchmark on eva02_large_patch14_clip_224.merged2b for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 48.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 22.05 GiB is allocated by PyTorch, and 320.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model eva02_large_patch14_clip_224.merged2b created, param count: 304105152
Running train benchmark on eva02_large_patch14_clip_224.merged2b for 40 steps w/ input size (3, 224, 224) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 46.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 21.82 GiB is allocated by PyTorch, and 560.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model eva02_large_patch14_clip_224.merged2b created, param count: 304105152
Running train benchmark on eva02_large_patch14_clip_224.merged2b for 40 steps w/ input size (3, 224, 224) and batch size 64.
Train [8/40]. 155.40 samples/sec. 411.837 ms/step.
Train [16/40]. 155.41 samples/sec. 411.824 ms/step.
Train [24/40]. 155.41 samples/sec. 411.818 ms/step.
Train [32/40]. 155.41 samples/sec. 411.810 ms/step.
Train [40/40]. 155.41 samples/sec. 411.822 ms/step.
Train benchmark of eva02_large_patch14_clip_224.merged2b done. 154.49 samples/sec, 411.82 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model eva02_large_patch14_clip_336.merged2b created, param count: 304432832
Running inference benchmark on eva02_large_patch14_clip_336.merged2b for 40 steps w/ input size (3, 336, 336) and batch size 256.
Infer [8/40]. 210.04 samples/sec. 1218.800 ms/step.
Infer [16/40]. 209.95 samples/sec. 1219.310 ms/step.
Infer [24/40]. 209.93 samples/sec. 1219.430 ms/step.
Infer [32/40]. 209.92 samples/sec. 1219.486 ms/step.
Infer [40/40]. 209.91 samples/sec. 1219.588 ms/step.
Inference benchmark of eva02_large_patch14_clip_336.merged2b done. 209.90 samples/sec, 1219.59 ms/step
Model eva02_large_patch14_clip_336.merged2b created, param count: 304432832
Running train benchmark on eva02_large_patch14_clip_336.merged2b for 40 steps w/ input size (3, 336, 336) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 770.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 102.06 MiB is free. Including non-PyTorch memory, this process has 23.54 GiB memory in use. Of the allocated memory 22.22 GiB is allocated by PyTorch, and 91.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model eva02_large_patch14_clip_336.merged2b created, param count: 304432832
Running train benchmark on eva02_large_patch14_clip_336.merged2b for 40 steps w/ input size (3, 336, 336) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 578.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 42.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 22.09 GiB is allocated by PyTorch, and 281.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model eva02_large_patch14_clip_336.merged2b created, param count: 304432832
Running train benchmark on eva02_large_patch14_clip_336.merged2b for 40 steps w/ input size (3, 336, 336) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 386.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 100.06 MiB is free. Including non-PyTorch memory, this process has 23.54 GiB memory in use. Of the allocated memory 21.98 GiB is allocated by PyTorch, and 339.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model eva02_large_patch14_clip_336.merged2b created, param count: 304432832
Running train benchmark on eva02_large_patch14_clip_336.merged2b for 40 steps w/ input size (3, 336, 336) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 110.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 26.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 21.92 GiB is allocated by PyTorch, and 471.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model eva02_large_patch14_clip_336.merged2b created, param count: 304432832
Running train benchmark on eva02_large_patch14_clip_336.merged2b for 40 steps w/ input size (3, 336, 336) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 194.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 56.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 21.94 GiB is allocated by PyTorch, and 424.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model eva02_large_patch14_clip_336.merged2b created, param count: 304432832
Running train benchmark on eva02_large_patch14_clip_336.merged2b for 40 steps w/ input size (3, 336, 336) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 146.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 78.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 21.75 GiB is allocated by PyTorch, and 595.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model eva02_large_patch14_clip_336.merged2b created, param count: 304432832
Running train benchmark on eva02_large_patch14_clip_336.merged2b for 40 steps w/ input size (3, 336, 336) and batch size 32.
Train [8/40]. 66.53 samples/sec. 481.019 ms/step.
Train [16/40]. 66.53 samples/sec. 481.007 ms/step.
Train [24/40]. 66.53 samples/sec. 481.005 ms/step.
Train [32/40]. 66.53 samples/sec. 481.015 ms/step.
Train [40/40]. 66.53 samples/sec. 481.011 ms/step.
Train benchmark of eva02_large_patch14_clip_336.merged2b done. 66.18 samples/sec, 481.01 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model eva02_small_patch14_224.mim_in22k created, param count: 21621120
Running inference benchmark on eva02_small_patch14_224.mim_in22k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3885.84 samples/sec. 65.880 ms/step.
Infer [16/40]. 3885.68 samples/sec. 65.883 ms/step.
Infer [24/40]. 3885.36 samples/sec. 65.888 ms/step.
Infer [32/40]. 3885.41 samples/sec. 65.888 ms/step.
Infer [40/40]. 3885.49 samples/sec. 65.886 ms/step.
Inference benchmark of eva02_small_patch14_224.mim_in22k done. 3883.99 samples/sec, 65.89 ms/step
Model eva02_small_patch14_224.mim_in22k created, param count: 21621120
Running train benchmark on eva02_small_patch14_224.mim_in22k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model eva02_small_patch14_224.mim_in22k created, param count: 21621120
Running train benchmark on eva02_small_patch14_224.mim_in22k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model eva02_small_patch14_224.mim_in22k created, param count: 21621120
Running train benchmark on eva02_small_patch14_224.mim_in22k for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model eva02_small_patch14_224.mim_in22k created, param count: 21621120
Running train benchmark on eva02_small_patch14_224.mim_in22k for 40 steps w/ input size (3, 224, 224) and batch size 96.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model eva02_small_patch14_224.mim_in22k created, param count: 21621120
Running train benchmark on eva02_small_patch14_224.mim_in22k for 40 steps w/ input size (3, 224, 224) and batch size 64.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model eva02_small_patch14_224.mim_in22k created, param count: 21621120
Running train benchmark on eva02_small_patch14_224.mim_in22k for 40 steps w/ input size (3, 224, 224) and batch size 48.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model eva02_small_patch14_224.mim_in22k created, param count: 21621120
Running train benchmark on eva02_small_patch14_224.mim_in22k for 40 steps w/ input size (3, 224, 224) and batch size 32.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model eva02_small_patch14_224.mim_in22k created, param count: 21621120
Running train benchmark on eva02_small_patch14_224.mim_in22k for 40 steps w/ input size (3, 224, 224) and batch size 24.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model eva02_small_patch14_224.mim_in22k created, param count: 21621120
Running train benchmark on eva02_small_patch14_224.mim_in22k for 40 steps w/ input size (3, 224, 224) and batch size 16.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 12 for retry.
Model eva02_small_patch14_224.mim_in22k created, param count: 21621120
Running train benchmark on eva02_small_patch14_224.mim_in22k for 40 steps w/ input size (3, 224, 224) and batch size 12.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 8 for retry.
Model eva02_small_patch14_224.mim_in22k created, param count: 21621120
Running train benchmark on eva02_small_patch14_224.mim_in22k for 40 steps w/ input size (3, 224, 224) and batch size 8.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 6 for retry.
Model eva02_small_patch14_224.mim_in22k created, param count: 21621120
Running train benchmark on eva02_small_patch14_224.mim_in22k for 40 steps w/ input size (3, 224, 224) and batch size 6.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 4 for retry.
Model eva02_small_patch14_224.mim_in22k created, param count: 21621120
Running train benchmark on eva02_small_patch14_224.mim_in22k for 40 steps w/ input size (3, 224, 224) and batch size 4.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 3 for retry.
Model eva02_small_patch14_224.mim_in22k created, param count: 21621120
Running train benchmark on eva02_small_patch14_224.mim_in22k for 40 steps w/ input size (3, 224, 224) and batch size 3.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 2 for retry.
Model eva02_small_patch14_224.mim_in22k created, param count: 21621120
Running train benchmark on eva02_small_patch14_224.mim_in22k for 40 steps w/ input size (3, 224, 224) and batch size 2.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 1 for retry.
Model eva02_small_patch14_224.mim_in22k created, param count: 21621120
Running train benchmark on eva02_small_patch14_224.mim_in22k for 40 steps w/ input size (3, 224, 224) and batch size 1.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 0 for retry.
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model eva02_small_patch14_336.mim_in22k_ft_in1k created, param count: 22129000
Running inference benchmark on eva02_small_patch14_336.mim_in22k_ft_in1k for 40 steps w/ input size (3, 336, 336) and batch size 256.
Infer [8/40]. 1687.54 samples/sec. 151.700 ms/step.
Infer [16/40]. 1687.40 samples/sec. 151.713 ms/step.
Infer [24/40]. 1687.34 samples/sec. 151.718 ms/step.
Infer [32/40]. 1687.33 samples/sec. 151.719 ms/step.
Infer [40/40]. 1687.27 samples/sec. 151.724 ms/step.
Inference benchmark of eva02_small_patch14_336.mim_in22k_ft_in1k done. 1686.92 samples/sec, 151.72 ms/step
Model eva02_small_patch14_336.mim_in22k_ft_in1k created, param count: 22129000
Running train benchmark on eva02_small_patch14_336.mim_in22k_ft_in1k for 40 steps w/ input size (3, 336, 336) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 290.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 140.06 MiB is free. Including non-PyTorch memory, this process has 23.50 GiB memory in use. Of the allocated memory 21.84 GiB is allocated by PyTorch, and 443.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model eva02_small_patch14_336.mim_in22k_ft_in1k created, param count: 22129000
Running train benchmark on eva02_small_patch14_336.mim_in22k_ft_in1k for 40 steps w/ input size (3, 336, 336) and batch size 192.
Train [8/40]. 488.07 samples/sec. 393.389 ms/step.
Train [16/40]. 488.06 samples/sec. 393.394 ms/step.
Train [24/40]. 488.06 samples/sec. 393.393 ms/step.
Train [32/40]. 488.06 samples/sec. 393.394 ms/step.
Train [40/40]. 488.06 samples/sec. 393.392 ms/step.
Train benchmark of eva02_small_patch14_336.mim_in22k_ft_in1k done. 486.56 samples/sec, 393.39 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model eva02_tiny_patch14_224.mim_in22k created, param count: 5502144
Running inference benchmark on eva02_tiny_patch14_224.mim_in22k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 10420.05 samples/sec. 24.568 ms/step.
Infer [16/40]. 10418.17 samples/sec. 24.572 ms/step.
Infer [24/40]. 10415.52 samples/sec. 24.579 ms/step.
Infer [32/40]. 10414.52 samples/sec. 24.581 ms/step.
Infer [40/40]. 10413.73 samples/sec. 24.583 ms/step.
Inference benchmark of eva02_tiny_patch14_224.mim_in22k done. 10403.71 samples/sec, 24.58 ms/step
Model eva02_tiny_patch14_224.mim_in22k created, param count: 5502144
Running train benchmark on eva02_tiny_patch14_224.mim_in22k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model eva02_tiny_patch14_224.mim_in22k created, param count: 5502144
Running train benchmark on eva02_tiny_patch14_224.mim_in22k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model eva02_tiny_patch14_224.mim_in22k created, param count: 5502144
Running train benchmark on eva02_tiny_patch14_224.mim_in22k for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model eva02_tiny_patch14_224.mim_in22k created, param count: 5502144
Running train benchmark on eva02_tiny_patch14_224.mim_in22k for 40 steps w/ input size (3, 224, 224) and batch size 96.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model eva02_tiny_patch14_224.mim_in22k created, param count: 5502144
Running train benchmark on eva02_tiny_patch14_224.mim_in22k for 40 steps w/ input size (3, 224, 224) and batch size 64.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model eva02_tiny_patch14_224.mim_in22k created, param count: 5502144
Running train benchmark on eva02_tiny_patch14_224.mim_in22k for 40 steps w/ input size (3, 224, 224) and batch size 48.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model eva02_tiny_patch14_224.mim_in22k created, param count: 5502144
Running train benchmark on eva02_tiny_patch14_224.mim_in22k for 40 steps w/ input size (3, 224, 224) and batch size 32.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model eva02_tiny_patch14_224.mim_in22k created, param count: 5502144
Running train benchmark on eva02_tiny_patch14_224.mim_in22k for 40 steps w/ input size (3, 224, 224) and batch size 24.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model eva02_tiny_patch14_224.mim_in22k created, param count: 5502144
Running train benchmark on eva02_tiny_patch14_224.mim_in22k for 40 steps w/ input size (3, 224, 224) and batch size 16.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 12 for retry.
Model eva02_tiny_patch14_224.mim_in22k created, param count: 5502144
Running train benchmark on eva02_tiny_patch14_224.mim_in22k for 40 steps w/ input size (3, 224, 224) and batch size 12.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 8 for retry.
Model eva02_tiny_patch14_224.mim_in22k created, param count: 5502144
Running train benchmark on eva02_tiny_patch14_224.mim_in22k for 40 steps w/ input size (3, 224, 224) and batch size 8.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 6 for retry.
Model eva02_tiny_patch14_224.mim_in22k created, param count: 5502144
Running train benchmark on eva02_tiny_patch14_224.mim_in22k for 40 steps w/ input size (3, 224, 224) and batch size 6.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 4 for retry.
Model eva02_tiny_patch14_224.mim_in22k created, param count: 5502144
Running train benchmark on eva02_tiny_patch14_224.mim_in22k for 40 steps w/ input size (3, 224, 224) and batch size 4.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 3 for retry.
Model eva02_tiny_patch14_224.mim_in22k created, param count: 5502144
Running train benchmark on eva02_tiny_patch14_224.mim_in22k for 40 steps w/ input size (3, 224, 224) and batch size 3.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 2 for retry.
Model eva02_tiny_patch14_224.mim_in22k created, param count: 5502144
Running train benchmark on eva02_tiny_patch14_224.mim_in22k for 40 steps w/ input size (3, 224, 224) and batch size 2.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 1 for retry.
Model eva02_tiny_patch14_224.mim_in22k created, param count: 5502144
Running train benchmark on eva02_tiny_patch14_224.mim_in22k for 40 steps w/ input size (3, 224, 224) and batch size 1.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 0 for retry.
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model eva02_tiny_patch14_336.mim_in22k_ft_in1k created, param count: 5756584
Running inference benchmark on eva02_tiny_patch14_336.mim_in22k_ft_in1k for 40 steps w/ input size (3, 336, 336) and batch size 256.
Infer [8/40]. 3803.27 samples/sec. 67.310 ms/step.
Infer [16/40]. 3803.17 samples/sec. 67.312 ms/step.
Infer [24/40]. 3803.22 samples/sec. 67.311 ms/step.
Infer [32/40]. 3803.10 samples/sec. 67.314 ms/step.
Infer [40/40]. 3802.97 samples/sec. 67.316 ms/step.
Inference benchmark of eva02_tiny_patch14_336.mim_in22k_ft_in1k done. 3801.46 samples/sec, 67.32 ms/step
Model eva02_tiny_patch14_336.mim_in22k_ft_in1k created, param count: 5756584
Running train benchmark on eva02_tiny_patch14_336.mim_in22k_ft_in1k for 40 steps w/ input size (3, 336, 336) and batch size 256.
Train [8/40]. 1047.28 samples/sec. 244.443 ms/step.
Train [16/40]. 1047.26 samples/sec. 244.447 ms/step.
Train [24/40]. 1047.23 samples/sec. 244.453 ms/step.
Train [32/40]. 1047.22 samples/sec. 244.457 ms/step.
Train [40/40]. 1047.22 samples/sec. 244.457 ms/step.
Train benchmark of eva02_tiny_patch14_336.mim_in22k_ft_in1k done. 1042.44 samples/sec, 244.46 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model eva_giant_patch14_224.clip_ft_in1k created, param count: 1012555112
Running inference benchmark on eva_giant_patch14_224.clip_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 239.10 samples/sec. 1070.669 ms/step.
Infer [16/40]. 238.78 samples/sec. 1072.117 ms/step.
Infer [24/40]. 238.59 samples/sec. 1072.984 ms/step.
Infer [32/40]. 238.62 samples/sec. 1072.829 ms/step.
Infer [40/40]. 238.57 samples/sec. 1073.064 ms/step.
Inference benchmark of eva_giant_patch14_224.clip_ft_in1k done. 238.56 samples/sec, 1073.06 ms/step
Model eva_giant_patch14_224.clip_ft_in1k created, param count: 1012555112
Running train benchmark on eva_giant_patch14_224.clip_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 772.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 644.06 MiB is free. Including non-PyTorch memory, this process has 23.01 GiB memory in use. Of the allocated memory 21.48 GiB is allocated by PyTorch, and 305.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model eva_giant_patch14_224.clip_ft_in1k created, param count: 1012555112
Running train benchmark on eva_giant_patch14_224.clip_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 398.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 138.06 MiB is free. Including non-PyTorch memory, this process has 23.51 GiB memory in use. Of the allocated memory 21.75 GiB is allocated by PyTorch, and 536.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model eva_giant_patch14_224.clip_ft_in1k created, param count: 1012555112
Running train benchmark on eva_giant_patch14_224.clip_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 386.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 364.06 MiB is free. Including non-PyTorch memory, this process has 23.29 GiB memory in use. Of the allocated memory 21.84 GiB is allocated by PyTorch, and 222.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model eva_giant_patch14_224.clip_ft_in1k created, param count: 1012555112
Running train benchmark on eva_giant_patch14_224.clip_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 68.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 32.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 21.88 GiB is allocated by PyTorch, and 508.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model eva_giant_patch14_224.clip_ft_in1k created, param count: 1012555112
Running train benchmark on eva_giant_patch14_224.clip_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 194.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 130.06 MiB is free. Including non-PyTorch memory, this process has 23.51 GiB memory in use. Of the allocated memory 21.80 GiB is allocated by PyTorch, and 498.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model eva_giant_patch14_224.clip_ft_in1k created, param count: 1012555112
Running train benchmark on eva_giant_patch14_224.clip_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 146.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 44.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 22.02 GiB is allocated by PyTorch, and 357.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model eva_giant_patch14_224.clip_ft_in1k created, param count: 1012555112
Running train benchmark on eva_giant_patch14_224.clip_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 32.
Train [8/40]. 69.65 samples/sec. 459.462 ms/step.
Train [16/40]. 69.64 samples/sec. 459.531 ms/step.
Train [24/40]. 69.63 samples/sec. 459.557 ms/step.
Train [32/40]. 69.63 samples/sec. 459.578 ms/step.
Train [40/40]. 69.63 samples/sec. 459.593 ms/step.
Train benchmark of eva_giant_patch14_224.clip_ft_in1k done. 69.26 samples/sec, 459.59 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model eva_giant_patch14_336.clip_ft_in1k created, param count: 1013005672
Running inference benchmark on eva_giant_patch14_336.clip_ft_in1k for 40 steps w/ input size (3, 336, 336) and batch size 256.
Infer [8/40]. 94.19 samples/sec. 2717.921 ms/step.
Infer [16/40]. 94.09 samples/sec. 2720.715 ms/step.
Infer [24/40]. 94.06 samples/sec. 2721.738 ms/step.
Infer [32/40]. 94.03 samples/sec. 2722.485 ms/step.
Infer [40/40]. 94.02 samples/sec. 2722.954 ms/step.
Inference benchmark of eva_giant_patch14_336.clip_ft_in1k done. 94.01 samples/sec, 2722.95 ms/step
Model eva_giant_patch14_336.clip_ft_in1k created, param count: 1013005672
Running train benchmark on eva_giant_patch14_336.clip_ft_in1k for 40 steps w/ input size (3, 336, 336) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 398.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 386.06 MiB is free. Including non-PyTorch memory, this process has 23.26 GiB memory in use. Of the allocated memory 21.55 GiB is allocated by PyTorch, and 490.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model eva_giant_patch14_336.clip_ft_in1k created, param count: 1013005672
Running train benchmark on eva_giant_patch14_336.clip_ft_in1k for 40 steps w/ input size (3, 336, 336) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 298.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 192.06 MiB is free. Including non-PyTorch memory, this process has 23.45 GiB memory in use. Of the allocated memory 21.81 GiB is allocated by PyTorch, and 421.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model eva_giant_patch14_336.clip_ft_in1k created, param count: 1013005672
Running train benchmark on eva_giant_patch14_336.clip_ft_in1k for 40 steps w/ input size (3, 336, 336) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 596.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 374.06 MiB is free. Including non-PyTorch memory, this process has 23.28 GiB memory in use. Of the allocated memory 21.87 GiB is allocated by PyTorch, and 182.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model eva_giant_patch14_336.clip_ft_in1k created, param count: 1013005672
Running train benchmark on eva_giant_patch14_336.clip_ft_in1k for 40 steps w/ input size (3, 336, 336) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 448.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 170.06 MiB is free. Including non-PyTorch memory, this process has 23.47 GiB memory in use. Of the allocated memory 21.76 GiB is allocated by PyTorch, and 497.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model eva_giant_patch14_336.clip_ft_in1k created, param count: 1013005672
Running train benchmark on eva_giant_patch14_336.clip_ft_in1k for 40 steps w/ input size (3, 336, 336) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 50.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 22.17 GiB is allocated by PyTorch, and 198.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model eva_giant_patch14_336.clip_ft_in1k created, param count: 1013005672
Running train benchmark on eva_giant_patch14_336.clip_ft_in1k for 40 steps w/ input size (3, 336, 336) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 326.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 8.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.02 GiB is allocated by PyTorch, and 391.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model eva_giant_patch14_336.clip_ft_in1k created, param count: 1013005672
Running train benchmark on eva_giant_patch14_336.clip_ft_in1k for 40 steps w/ input size (3, 336, 336) and batch size 32.
ERROR: "CUDA out of memory. Tried to allocate 218.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 118.06 MiB is free. Including non-PyTorch memory, this process has 23.53 GiB memory in use. Of the allocated memory 22.06 GiB is allocated by PyTorch, and 242.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model eva_giant_patch14_336.clip_ft_in1k created, param count: 1013005672
Running train benchmark on eva_giant_patch14_336.clip_ft_in1k for 40 steps w/ input size (3, 336, 336) and batch size 24.
ERROR: "CUDA out of memory. Tried to allocate 164.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 32.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 22.00 GiB is allocated by PyTorch, and 392.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model eva_giant_patch14_336.clip_ft_in1k created, param count: 1013005672
Running train benchmark on eva_giant_patch14_336.clip_ft_in1k for 40 steps w/ input size (3, 336, 336) and batch size 16.
Train [8/40]. 29.19 samples/sec. 548.173 ms/step.
Train [16/40]. 29.19 samples/sec. 548.198 ms/step.
Train [24/40]. 29.19 samples/sec. 548.211 ms/step.
Train [32/40]. 29.19 samples/sec. 548.205 ms/step.
Train [40/40]. 29.19 samples/sec. 548.212 ms/step.
Train benchmark of eva_giant_patch14_336.clip_ft_in1k done. 29.05 samples/sec, 548.21 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model eva_giant_patch14_336.m30m_ft_in22k_in1k created, param count: 1013005672
Running inference benchmark on eva_giant_patch14_336.m30m_ft_in22k_in1k for 40 steps w/ input size (3, 336, 336) and batch size 256.
Infer [8/40]. 94.11 samples/sec. 2720.292 ms/step.
Infer [16/40]. 94.06 samples/sec. 2721.554 ms/step.
Infer [24/40]. 94.04 samples/sec. 2722.144 ms/step.
Infer [32/40]. 94.03 samples/sec. 2722.591 ms/step.
Infer [40/40]. 94.01 samples/sec. 2723.005 ms/step.
Inference benchmark of eva_giant_patch14_336.m30m_ft_in22k_in1k done. 94.01 samples/sec, 2723.01 ms/step
Model eva_giant_patch14_336.m30m_ft_in22k_in1k created, param count: 1013005672
Running train benchmark on eva_giant_patch14_336.m30m_ft_in22k_in1k for 40 steps w/ input size (3, 336, 336) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 398.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 386.06 MiB is free. Including non-PyTorch memory, this process has 23.26 GiB memory in use. Of the allocated memory 21.55 GiB is allocated by PyTorch, and 490.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model eva_giant_patch14_336.m30m_ft_in22k_in1k created, param count: 1013005672
Running train benchmark on eva_giant_patch14_336.m30m_ft_in22k_in1k for 40 steps w/ input size (3, 336, 336) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 298.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 144.06 MiB is free. Including non-PyTorch memory, this process has 23.50 GiB memory in use. Of the allocated memory 21.81 GiB is allocated by PyTorch, and 469.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model eva_giant_patch14_336.m30m_ft_in22k_in1k created, param count: 1013005672
Running train benchmark on eva_giant_patch14_336.m30m_ft_in22k_in1k for 40 steps w/ input size (3, 336, 336) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 596.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 338.06 MiB is free. Including non-PyTorch memory, this process has 23.31 GiB memory in use. Of the allocated memory 21.87 GiB is allocated by PyTorch, and 218.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model eva_giant_patch14_336.m30m_ft_in22k_in1k created, param count: 1013005672
Running train benchmark on eva_giant_patch14_336.m30m_ft_in22k_in1k for 40 steps w/ input size (3, 336, 336) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 448.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 158.06 MiB is free. Including non-PyTorch memory, this process has 23.49 GiB memory in use. Of the allocated memory 21.76 GiB is allocated by PyTorch, and 509.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model eva_giant_patch14_336.m30m_ft_in22k_in1k created, param count: 1013005672
Running train benchmark on eva_giant_patch14_336.m30m_ft_in22k_in1k for 40 steps w/ input size (3, 336, 336) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 26.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.17 GiB is allocated by PyTorch, and 222.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model eva_giant_patch14_336.m30m_ft_in22k_in1k created, param count: 1013005672
Running train benchmark on eva_giant_patch14_336.m30m_ft_in22k_in1k for 40 steps w/ input size (3, 336, 336) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 326.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 8.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.02 GiB is allocated by PyTorch, and 391.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model eva_giant_patch14_336.m30m_ft_in22k_in1k created, param count: 1013005672
Running train benchmark on eva_giant_patch14_336.m30m_ft_in22k_in1k for 40 steps w/ input size (3, 336, 336) and batch size 32.
ERROR: "CUDA out of memory. Tried to allocate 218.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 118.06 MiB is free. Including non-PyTorch memory, this process has 23.53 GiB memory in use. Of the allocated memory 22.06 GiB is allocated by PyTorch, and 242.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model eva_giant_patch14_336.m30m_ft_in22k_in1k created, param count: 1013005672
Running train benchmark on eva_giant_patch14_336.m30m_ft_in22k_in1k for 40 steps w/ input size (3, 336, 336) and batch size 24.
ERROR: "CUDA out of memory. Tried to allocate 164.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 32.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 22.00 GiB is allocated by PyTorch, and 392.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model eva_giant_patch14_336.m30m_ft_in22k_in1k created, param count: 1013005672
Running train benchmark on eva_giant_patch14_336.m30m_ft_in22k_in1k for 40 steps w/ input size (3, 336, 336) and batch size 16.
Train [8/40]. 29.19 samples/sec. 548.132 ms/step.
Train [16/40]. 29.19 samples/sec. 548.187 ms/step.
Train [24/40]. 29.19 samples/sec. 548.199 ms/step.
Train [32/40]. 29.19 samples/sec. 548.207 ms/step.
Train [40/40]. 29.19 samples/sec. 548.223 ms/step.
Train benchmark of eva_giant_patch14_336.m30m_ft_in22k_in1k done. 29.05 samples/sec, 548.22 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model eva_giant_patch14_560.m30m_ft_in22k_in1k created, param count: 1014447464
Running inference benchmark on eva_giant_patch14_560.m30m_ft_in22k_in1k for 40 steps w/ input size (3, 560, 560) and batch size 256.
Infer [8/40]. 25.03 samples/sec. 10228.367 ms/step.
Infer [16/40]. 25.03 samples/sec. 10229.144 ms/step.
Infer [24/40]. 25.03 samples/sec. 10227.107 ms/step.
Infer [32/40]. 25.03 samples/sec. 10227.838 ms/step.
Infer [40/40]. 25.03 samples/sec. 10228.124 ms/step.
Inference benchmark of eva_giant_patch14_560.m30m_ft_in22k_in1k done. 25.03 samples/sec, 10228.12 ms/step
Model eva_giant_patch14_560.m30m_ft_in22k_in1k created, param count: 1014447464
Running train benchmark on eva_giant_patch14_560.m30m_ft_in22k_in1k for 40 steps w/ input size (3, 560, 560) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 1.08 GiB. GPU 0 has a total capacty of 23.65 GiB of which 912.06 MiB is free. Including non-PyTorch memory, this process has 22.75 GiB memory in use. Of the allocated memory 20.38 GiB is allocated by PyTorch, and 1.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model eva_giant_patch14_560.m30m_ft_in22k_in1k created, param count: 1014447464
Running train benchmark on eva_giant_patch14_560.m30m_ft_in22k_in1k for 40 steps w/ input size (3, 560, 560) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 826.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 766.06 MiB is free. Including non-PyTorch memory, this process has 22.89 GiB memory in use. Of the allocated memory 20.63 GiB is allocated by PyTorch, and 1.03 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model eva_giant_patch14_560.m30m_ft_in22k_in1k created, param count: 1014447464
Running train benchmark on eva_giant_patch14_560.m30m_ft_in22k_in1k for 40 steps w/ input size (3, 560, 560) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 1.61 GiB. GPU 0 has a total capacty of 23.65 GiB of which 990.06 MiB is free. Including non-PyTorch memory, this process has 22.67 GiB memory in use. Of the allocated memory 21.23 GiB is allocated by PyTorch, and 220.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model eva_giant_patch14_560.m30m_ft_in22k_in1k created, param count: 1014447464
Running train benchmark on eva_giant_patch14_560.m30m_ft_in22k_in1k for 40 steps w/ input size (3, 560, 560) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 1.76 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.09 GiB is free. Including non-PyTorch memory, this process has 22.55 GiB memory in use. Of the allocated memory 20.59 GiB is allocated by PyTorch, and 747.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model eva_giant_patch14_560.m30m_ft_in22k_in1k created, param count: 1014447464
Running train benchmark on eva_giant_patch14_560.m30m_ft_in22k_in1k for 40 steps w/ input size (3, 560, 560) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 276.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 198.06 MiB is free. Including non-PyTorch memory, this process has 23.45 GiB memory in use. Of the allocated memory 21.95 GiB is allocated by PyTorch, and 275.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model eva_giant_patch14_560.m30m_ft_in22k_in1k created, param count: 1014447464
Running train benchmark on eva_giant_patch14_560.m30m_ft_in22k_in1k for 40 steps w/ input size (3, 560, 560) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 902.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 214.06 MiB is free. Including non-PyTorch memory, this process has 23.43 GiB memory in use. Of the allocated memory 21.39 GiB is allocated by PyTorch, and 828.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model eva_giant_patch14_560.m30m_ft_in22k_in1k created, param count: 1014447464
Running train benchmark on eva_giant_patch14_560.m30m_ft_in22k_in1k for 40 steps w/ input size (3, 560, 560) and batch size 32.
ERROR: "CUDA out of memory. Tried to allocate 602.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 406.06 MiB is free. Including non-PyTorch memory, this process has 23.24 GiB memory in use. Of the allocated memory 21.67 GiB is allocated by PyTorch, and 355.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model eva_giant_patch14_560.m30m_ft_in22k_in1k created, param count: 1014447464
Running train benchmark on eva_giant_patch14_560.m30m_ft_in22k_in1k for 40 steps w/ input size (3, 560, 560) and batch size 24.
ERROR: "CUDA out of memory. Tried to allocate 452.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 266.06 MiB is free. Including non-PyTorch memory, this process has 23.38 GiB memory in use. Of the allocated memory 21.83 GiB is allocated by PyTorch, and 323.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model eva_giant_patch14_560.m30m_ft_in22k_in1k created, param count: 1014447464
Running train benchmark on eva_giant_patch14_560.m30m_ft_in22k_in1k for 40 steps w/ input size (3, 560, 560) and batch size 16.
ERROR: "CUDA out of memory. Tried to allocate 302.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 104.06 MiB is free. Including non-PyTorch memory, this process has 23.54 GiB memory in use. Of the allocated memory 21.91 GiB is allocated by PyTorch, and 405.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 12 for retry.
Model eva_giant_patch14_560.m30m_ft_in22k_in1k created, param count: 1014447464
Running train benchmark on eva_giant_patch14_560.m30m_ft_in22k_in1k for 40 steps w/ input size (3, 560, 560) and batch size 12.
ERROR: "CUDA out of memory. Tried to allocate 226.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 218.06 MiB is free. Including non-PyTorch memory, this process has 23.43 GiB memory in use. Of the allocated memory 21.85 GiB is allocated by PyTorch, and 358.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 8 for retry.
Model eva_giant_patch14_560.m30m_ft_in22k_in1k created, param count: 1014447464
Running train benchmark on eva_giant_patch14_560.m30m_ft_in22k_in1k for 40 steps w/ input size (3, 560, 560) and batch size 8.
ERROR: "CUDA out of memory. Tried to allocate 104.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 64.06 MiB is free. Including non-PyTorch memory, this process has 23.58 GiB memory in use. Of the allocated memory 21.73 GiB is allocated by PyTorch, and 636.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 6 for retry.
Model eva_giant_patch14_560.m30m_ft_in22k_in1k created, param count: 1014447464
Running train benchmark on eva_giant_patch14_560.m30m_ft_in22k_in1k for 40 steps w/ input size (3, 560, 560) and batch size 6.
Train [8/40]. 8.41 samples/sec. 713.160 ms/step.
Train [16/40]. 8.41 samples/sec. 713.154 ms/step.
Train [24/40]. 8.41 samples/sec. 713.150 ms/step.
Train [32/40]. 8.41 samples/sec. 713.149 ms/step.
Train [40/40]. 8.41 samples/sec. 713.154 ms/step.
Train benchmark of eva_giant_patch14_560.m30m_ft_in22k_in1k done. 8.38 samples/sec, 713.15 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model eva_giant_patch14_clip_224.laion400m created, param count: 1012588928
Running inference benchmark on eva_giant_patch14_clip_224.laion400m for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 235.48 samples/sec. 1087.153 ms/step.
Infer [16/40]. 236.91 samples/sec. 1080.591 ms/step.
Infer [24/40]. 237.07 samples/sec. 1079.869 ms/step.
Infer [32/40]. 237.36 samples/sec. 1078.534 ms/step.
Infer [40/40]. 237.51 samples/sec. 1077.845 ms/step.
Inference benchmark of eva_giant_patch14_clip_224.laion400m done. 237.50 samples/sec, 1077.85 ms/step
Model eva_giant_patch14_clip_224.laion400m created, param count: 1012588928
Running train benchmark on eva_giant_patch14_clip_224.laion400m for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 772.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 644.06 MiB is free. Including non-PyTorch memory, this process has 23.01 GiB memory in use. Of the allocated memory 21.48 GiB is allocated by PyTorch, and 305.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model eva_giant_patch14_clip_224.laion400m created, param count: 1012588928
Running train benchmark on eva_giant_patch14_clip_224.laion400m for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 398.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 102.06 MiB is free. Including non-PyTorch memory, this process has 23.54 GiB memory in use. Of the allocated memory 21.75 GiB is allocated by PyTorch, and 572.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model eva_giant_patch14_clip_224.laion400m created, param count: 1012588928
Running train benchmark on eva_giant_patch14_clip_224.laion400m for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 386.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 340.06 MiB is free. Including non-PyTorch memory, this process has 23.31 GiB memory in use. Of the allocated memory 21.84 GiB is allocated by PyTorch, and 244.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model eva_giant_patch14_clip_224.laion400m created, param count: 1012588928
Running train benchmark on eva_giant_patch14_clip_224.laion400m for 40 steps w/ input size (3, 224, 224) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 68.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 32.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 21.88 GiB is allocated by PyTorch, and 508.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model eva_giant_patch14_clip_224.laion400m created, param count: 1012588928
Running train benchmark on eva_giant_patch14_clip_224.laion400m for 40 steps w/ input size (3, 224, 224) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 194.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 130.06 MiB is free. Including non-PyTorch memory, this process has 23.51 GiB memory in use. Of the allocated memory 21.80 GiB is allocated by PyTorch, and 498.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model eva_giant_patch14_clip_224.laion400m created, param count: 1012588928
Running train benchmark on eva_giant_patch14_clip_224.laion400m for 40 steps w/ input size (3, 224, 224) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 146.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 44.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 22.02 GiB is allocated by PyTorch, and 357.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model eva_giant_patch14_clip_224.laion400m created, param count: 1012588928
Running train benchmark on eva_giant_patch14_clip_224.laion400m for 40 steps w/ input size (3, 224, 224) and batch size 32.
Train [8/40]. 69.63 samples/sec. 459.599 ms/step.
Train [16/40]. 69.62 samples/sec. 459.631 ms/step.
Train [24/40]. 69.62 samples/sec. 459.657 ms/step.
Train [32/40]. 69.62 samples/sec. 459.671 ms/step.
Train [40/40]. 69.61 samples/sec. 459.674 ms/step.
Train benchmark of eva_giant_patch14_clip_224.laion400m done. 69.23 samples/sec, 459.67 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model eva_giant_patch14_clip_224.merged2b created, param count: 1012588928
Running inference benchmark on eva_giant_patch14_clip_224.merged2b for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 239.26 samples/sec. 1069.951 ms/step.
Infer [16/40]. 237.76 samples/sec. 1076.732 ms/step.
Infer [24/40]. 238.06 samples/sec. 1075.367 ms/step.
Infer [32/40]. 238.17 samples/sec. 1074.846 ms/step.
Infer [40/40]. 238.26 samples/sec. 1074.468 ms/step.
Inference benchmark of eva_giant_patch14_clip_224.merged2b done. 238.25 samples/sec, 1074.47 ms/step
Model eva_giant_patch14_clip_224.merged2b created, param count: 1012588928
Running train benchmark on eva_giant_patch14_clip_224.merged2b for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 772.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 644.06 MiB is free. Including non-PyTorch memory, this process has 23.01 GiB memory in use. Of the allocated memory 21.48 GiB is allocated by PyTorch, and 305.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model eva_giant_patch14_clip_224.merged2b created, param count: 1012588928
Running train benchmark on eva_giant_patch14_clip_224.merged2b for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 398.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 102.06 MiB is free. Including non-PyTorch memory, this process has 23.54 GiB memory in use. Of the allocated memory 21.75 GiB is allocated by PyTorch, and 572.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model eva_giant_patch14_clip_224.merged2b created, param count: 1012588928
Running train benchmark on eva_giant_patch14_clip_224.merged2b for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 386.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 340.06 MiB is free. Including non-PyTorch memory, this process has 23.31 GiB memory in use. Of the allocated memory 21.84 GiB is allocated by PyTorch, and 244.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model eva_giant_patch14_clip_224.merged2b created, param count: 1012588928
Running train benchmark on eva_giant_patch14_clip_224.merged2b for 40 steps w/ input size (3, 224, 224) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 68.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 32.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 21.88 GiB is allocated by PyTorch, and 508.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model eva_giant_patch14_clip_224.merged2b created, param count: 1012588928
Running train benchmark on eva_giant_patch14_clip_224.merged2b for 40 steps w/ input size (3, 224, 224) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 194.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 130.06 MiB is free. Including non-PyTorch memory, this process has 23.51 GiB memory in use. Of the allocated memory 21.80 GiB is allocated by PyTorch, and 498.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model eva_giant_patch14_clip_224.merged2b created, param count: 1012588928
Running train benchmark on eva_giant_patch14_clip_224.merged2b for 40 steps w/ input size (3, 224, 224) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 146.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 44.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 22.02 GiB is allocated by PyTorch, and 357.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model eva_giant_patch14_clip_224.merged2b created, param count: 1012588928
Running train benchmark on eva_giant_patch14_clip_224.merged2b for 40 steps w/ input size (3, 224, 224) and batch size 32.
Train [8/40]. 69.66 samples/sec. 459.351 ms/step.
Train [16/40]. 69.66 samples/sec. 459.370 ms/step.
Train [24/40]. 69.66 samples/sec. 459.375 ms/step.
Train [32/40]. 69.66 samples/sec. 459.378 ms/step.
Train [40/40]. 69.66 samples/sec. 459.377 ms/step.
Train benchmark of eva_giant_patch14_clip_224.merged2b done. 69.28 samples/sec, 459.38 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model eva_large_patch14_196.in22k_ft_in1k created, param count: 304142312
Running inference benchmark on eva_large_patch14_196.in22k_ft_in1k for 40 steps w/ input size (3, 196, 196) and batch size 256.
Infer [8/40]. 1030.65 samples/sec. 248.387 ms/step.
Infer [16/40]. 1030.65 samples/sec. 248.387 ms/step.
Infer [24/40]. 1029.96 samples/sec. 248.553 ms/step.
Infer [32/40]. 1029.54 samples/sec. 248.654 ms/step.
Infer [40/40]. 1029.30 samples/sec. 248.712 ms/step.
Inference benchmark of eva_large_patch14_196.in22k_ft_in1k done. 1029.13 samples/sec, 248.71 ms/step
Model eva_large_patch14_196.in22k_ft_in1k created, param count: 304142312
Running train benchmark on eva_large_patch14_196.in22k_ft_in1k for 40 steps w/ input size (3, 196, 196) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 24.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.25 GiB is allocated by PyTorch, and 139.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model eva_large_patch14_196.in22k_ft_in1k created, param count: 304142312
Running train benchmark on eva_large_patch14_196.in22k_ft_in1k for 40 steps w/ input size (3, 196, 196) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 296.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 220.06 MiB is free. Including non-PyTorch memory, this process has 23.43 GiB memory in use. Of the allocated memory 22.07 GiB is allocated by PyTorch, and 125.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model eva_large_patch14_196.in22k_ft_in1k created, param count: 304142312
Running train benchmark on eva_large_patch14_196.in22k_ft_in1k for 40 steps w/ input size (3, 196, 196) and batch size 128.
Train [8/40]. 313.74 samples/sec. 407.978 ms/step.
Train [16/40]. 313.76 samples/sec. 407.960 ms/step.
Train [24/40]. 313.74 samples/sec. 407.976 ms/step.
Train [32/40]. 313.73 samples/sec. 407.993 ms/step.
Train [40/40]. 313.72 samples/sec. 408.003 ms/step.
Train benchmark of eva_large_patch14_196.in22k_ft_in1k done. 312.49 samples/sec, 408.00 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model eva_large_patch14_196.in22k_ft_in22k_in1k created, param count: 304142312
Running inference benchmark on eva_large_patch14_196.in22k_ft_in22k_in1k for 40 steps w/ input size (3, 196, 196) and batch size 256.
Infer [8/40]. 1029.20 samples/sec. 248.737 ms/step.
Infer [16/40]. 1028.91 samples/sec. 248.808 ms/step.
Infer [24/40]. 1028.81 samples/sec. 248.830 ms/step.
Infer [32/40]. 1028.84 samples/sec. 248.825 ms/step.
Infer [40/40]. 1028.77 samples/sec. 248.840 ms/step.
Inference benchmark of eva_large_patch14_196.in22k_ft_in22k_in1k done. 1028.61 samples/sec, 248.84 ms/step
Model eva_large_patch14_196.in22k_ft_in22k_in1k created, param count: 304142312
Running train benchmark on eva_large_patch14_196.in22k_ft_in22k_in1k for 40 steps w/ input size (3, 196, 196) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 24.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.25 GiB is allocated by PyTorch, and 139.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model eva_large_patch14_196.in22k_ft_in22k_in1k created, param count: 304142312
Running train benchmark on eva_large_patch14_196.in22k_ft_in22k_in1k for 40 steps w/ input size (3, 196, 196) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 296.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 220.06 MiB is free. Including non-PyTorch memory, this process has 23.43 GiB memory in use. Of the allocated memory 22.07 GiB is allocated by PyTorch, and 125.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model eva_large_patch14_196.in22k_ft_in22k_in1k created, param count: 304142312
Running train benchmark on eva_large_patch14_196.in22k_ft_in22k_in1k for 40 steps w/ input size (3, 196, 196) and batch size 128.
Train [8/40]. 314.01 samples/sec. 407.628 ms/step.
Train [16/40]. 314.00 samples/sec. 407.642 ms/step.
Train [24/40]. 314.00 samples/sec. 407.645 ms/step.
Train [32/40]. 313.98 samples/sec. 407.669 ms/step.
Train [40/40]. 313.96 samples/sec. 407.689 ms/step.
Train benchmark of eva_large_patch14_196.in22k_ft_in22k_in1k done. 312.73 samples/sec, 407.69 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model eva_large_patch14_336.in22k_ft_in1k created, param count: 304531432
Running inference benchmark on eva_large_patch14_336.in22k_ft_in1k for 40 steps w/ input size (3, 336, 336) and batch size 256.
Infer [8/40]. 324.01 samples/sec. 790.088 ms/step.
Infer [16/40]. 323.65 samples/sec. 790.976 ms/step.
Infer [24/40]. 323.56 samples/sec. 791.188 ms/step.
Infer [32/40]. 323.60 samples/sec. 791.100 ms/step.
Infer [40/40]. 323.60 samples/sec. 791.095 ms/step.
Inference benchmark of eva_large_patch14_336.in22k_ft_in1k done. 323.58 samples/sec, 791.10 ms/step
Model eva_large_patch14_336.in22k_ft_in1k created, param count: 304531432
Running train benchmark on eva_large_patch14_336.in22k_ft_in1k for 40 steps w/ input size (3, 336, 336) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 1.13 GiB. GPU 0 has a total capacty of 23.65 GiB of which 134.06 MiB is free. Including non-PyTorch memory, this process has 23.51 GiB memory in use. Of the allocated memory 22.22 GiB is allocated by PyTorch, and 63.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model eva_large_patch14_336.in22k_ft_in1k created, param count: 304531432
Running train benchmark on eva_large_patch14_336.in22k_ft_in1k for 40 steps w/ input size (3, 336, 336) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 218.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 78.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 22.11 GiB is allocated by PyTorch, and 228.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model eva_large_patch14_336.in22k_ft_in1k created, param count: 304531432
Running train benchmark on eva_large_patch14_336.in22k_ft_in1k for 40 steps w/ input size (3, 336, 336) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 146.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 84.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 22.01 GiB is allocated by PyTorch, and 325.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model eva_large_patch14_336.in22k_ft_in1k created, param count: 304531432
Running train benchmark on eva_large_patch14_336.in22k_ft_in1k for 40 steps w/ input size (3, 336, 336) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 434.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 166.06 MiB is free. Including non-PyTorch memory, this process has 23.48 GiB memory in use. Of the allocated memory 21.83 GiB is allocated by PyTorch, and 423.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model eva_large_patch14_336.in22k_ft_in1k created, param count: 304531432
Running train benchmark on eva_large_patch14_336.in22k_ft_in1k for 40 steps w/ input size (3, 336, 336) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 290.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 128.06 MiB is free. Including non-PyTorch memory, this process has 23.52 GiB memory in use. Of the allocated memory 21.81 GiB is allocated by PyTorch, and 484.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model eva_large_patch14_336.in22k_ft_in1k created, param count: 304531432
Running train benchmark on eva_large_patch14_336.in22k_ft_in1k for 40 steps w/ input size (3, 336, 336) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 74.06 MiB is free. Including non-PyTorch memory, this process has 23.57 GiB memory in use. Of the allocated memory 21.80 GiB is allocated by PyTorch, and 551.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model eva_large_patch14_336.in22k_ft_in1k created, param count: 304531432
Running train benchmark on eva_large_patch14_336.in22k_ft_in1k for 40 steps w/ input size (3, 336, 336) and batch size 32.
Train [8/40]. 94.80 samples/sec. 337.546 ms/step.
Train [16/40]. 94.80 samples/sec. 337.553 ms/step.
Train [24/40]. 94.80 samples/sec. 337.558 ms/step.
Train [32/40]. 94.80 samples/sec. 337.564 ms/step.
Train [40/40]. 94.79 samples/sec. 337.582 ms/step.
Train benchmark of eva_large_patch14_336.in22k_ft_in1k done. 94.35 samples/sec, 337.58 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model eva_large_patch14_336.in22k_ft_in22k_in1k created, param count: 304531432
Running inference benchmark on eva_large_patch14_336.in22k_ft_in22k_in1k for 40 steps w/ input size (3, 336, 336) and batch size 256.
Infer [8/40]. 324.70 samples/sec. 788.417 ms/step.
Infer [16/40]. 324.39 samples/sec. 789.162 ms/step.
Infer [24/40]. 324.25 samples/sec. 789.522 ms/step.
Infer [32/40]. 324.14 samples/sec. 789.789 ms/step.
Infer [40/40]. 324.02 samples/sec. 790.066 ms/step.
Inference benchmark of eva_large_patch14_336.in22k_ft_in22k_in1k done. 324.01 samples/sec, 790.07 ms/step
Model eva_large_patch14_336.in22k_ft_in22k_in1k created, param count: 304531432
Running train benchmark on eva_large_patch14_336.in22k_ft_in22k_in1k for 40 steps w/ input size (3, 336, 336) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 1.13 GiB. GPU 0 has a total capacty of 23.65 GiB of which 134.06 MiB is free. Including non-PyTorch memory, this process has 23.51 GiB memory in use. Of the allocated memory 22.22 GiB is allocated by PyTorch, and 63.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model eva_large_patch14_336.in22k_ft_in22k_in1k created, param count: 304531432
Running train benchmark on eva_large_patch14_336.in22k_ft_in22k_in1k for 40 steps w/ input size (3, 336, 336) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 218.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 78.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 22.11 GiB is allocated by PyTorch, and 228.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model eva_large_patch14_336.in22k_ft_in22k_in1k created, param count: 304531432
Running train benchmark on eva_large_patch14_336.in22k_ft_in22k_in1k for 40 steps w/ input size (3, 336, 336) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 146.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 84.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 22.01 GiB is allocated by PyTorch, and 325.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model eva_large_patch14_336.in22k_ft_in22k_in1k created, param count: 304531432
Running train benchmark on eva_large_patch14_336.in22k_ft_in22k_in1k for 40 steps w/ input size (3, 336, 336) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 434.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 166.06 MiB is free. Including non-PyTorch memory, this process has 23.48 GiB memory in use. Of the allocated memory 21.83 GiB is allocated by PyTorch, and 423.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model eva_large_patch14_336.in22k_ft_in22k_in1k created, param count: 304531432
Running train benchmark on eva_large_patch14_336.in22k_ft_in22k_in1k for 40 steps w/ input size (3, 336, 336) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 290.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 128.06 MiB is free. Including non-PyTorch memory, this process has 23.52 GiB memory in use. Of the allocated memory 21.81 GiB is allocated by PyTorch, and 484.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model eva_large_patch14_336.in22k_ft_in22k_in1k created, param count: 304531432
Running train benchmark on eva_large_patch14_336.in22k_ft_in22k_in1k for 40 steps w/ input size (3, 336, 336) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 218.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 10.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 21.58 GiB is allocated by PyTorch, and 840.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model eva_large_patch14_336.in22k_ft_in22k_in1k created, param count: 304531432
Running train benchmark on eva_large_patch14_336.in22k_ft_in22k_in1k for 40 steps w/ input size (3, 336, 336) and batch size 32.
Train [8/40]. 94.80 samples/sec. 337.564 ms/step.
Train [16/40]. 94.80 samples/sec. 337.559 ms/step.
Train [24/40]. 94.80 samples/sec. 337.565 ms/step.
Train [32/40]. 94.80 samples/sec. 337.563 ms/step.
Train [40/40]. 94.79 samples/sec. 337.574 ms/step.
Train benchmark of eva_large_patch14_336.in22k_ft_in22k_in1k done. 94.34 samples/sec, 337.57 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model fbnetc_100.rmsp_in1k created, param count: 5572200
Running inference benchmark on fbnetc_100.rmsp_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 11631.45 samples/sec. 22.009 ms/step.
Infer [16/40]. 11631.23 samples/sec. 22.010 ms/step.
Infer [24/40]. 11631.80 samples/sec. 22.009 ms/step.
Infer [32/40]. 11631.06 samples/sec. 22.010 ms/step.
Infer [40/40]. 11631.02 samples/sec. 22.010 ms/step.
Inference benchmark of fbnetc_100.rmsp_in1k done. 11618.13 samples/sec, 22.01 ms/step
Model fbnetc_100.rmsp_in1k created, param count: 5572200
Running train benchmark on fbnetc_100.rmsp_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 2190.86 samples/sec. 116.849 ms/step.
Train [16/40]. 2190.67 samples/sec. 116.859 ms/step.
Train [24/40]. 2190.84 samples/sec. 116.850 ms/step.
Train [32/40]. 2190.91 samples/sec. 116.847 ms/step.
Train [40/40]. 2190.89 samples/sec. 116.847 ms/step.
Train benchmark of fbnetc_100.rmsp_in1k done. 2177.77 samples/sec, 116.85 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model fbnetv3_b.ra2_in1k created, param count: 8598464
Running inference benchmark on fbnetv3_b.ra2_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
Infer [8/40]. 7873.85 samples/sec. 32.513 ms/step.
Infer [16/40]. 7873.11 samples/sec. 32.516 ms/step.
Infer [24/40]. 7868.32 samples/sec. 32.536 ms/step.
Infer [32/40]. 7866.49 samples/sec. 32.543 ms/step.
Infer [40/40]. 7865.26 samples/sec. 32.548 ms/step.
Inference benchmark of fbnetv3_b.ra2_in1k done. 7859.36 samples/sec, 32.55 ms/step
Model fbnetv3_b.ra2_in1k created, param count: 8598464
Running train benchmark on fbnetv3_b.ra2_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
Train [8/40]. 1372.02 samples/sec. 186.586 ms/step.
Train [16/40]. 1372.08 samples/sec. 186.578 ms/step.
Train [24/40]. 1372.11 samples/sec. 186.574 ms/step.
Train [32/40]. 1372.12 samples/sec. 186.572 ms/step.
Train [40/40]. 1372.11 samples/sec. 186.575 ms/step.
Train benchmark of fbnetv3_b.ra2_in1k done. 1363.05 samples/sec, 186.57 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model fbnetv3_d.ra2_in1k created, param count: 10306272
Running inference benchmark on fbnetv3_d.ra2_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
Infer [8/40]. 6681.30 samples/sec. 38.316 ms/step.
Infer [16/40]. 6680.82 samples/sec. 38.319 ms/step.
Infer [24/40]. 6681.13 samples/sec. 38.317 ms/step.
Infer [32/40]. 6681.06 samples/sec. 38.317 ms/step.
Infer [40/40]. 6680.91 samples/sec. 38.318 ms/step.
Inference benchmark of fbnetv3_d.ra2_in1k done. 6676.55 samples/sec, 38.32 ms/step
Model fbnetv3_d.ra2_in1k created, param count: 10306272
Running train benchmark on fbnetv3_d.ra2_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
Train [8/40]. 1214.06 samples/sec. 210.863 ms/step.
Train [16/40]. 1214.12 samples/sec. 210.852 ms/step.
Train [24/40]. 1214.11 samples/sec. 210.855 ms/step.
Train [32/40]. 1214.09 samples/sec. 210.858 ms/step.
Train [40/40]. 1214.09 samples/sec. 210.857 ms/step.
Train benchmark of fbnetv3_d.ra2_in1k done. 1206.16 samples/sec, 210.86 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model fbnetv3_g.ra2_in1k created, param count: 16623888
Running inference benchmark on fbnetv3_g.ra2_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 3039.22 samples/sec. 84.232 ms/step.
Infer [16/40]. 3037.46 samples/sec. 84.281 ms/step.
Infer [24/40]. 3036.71 samples/sec. 84.302 ms/step.
Infer [32/40]. 3036.38 samples/sec. 84.311 ms/step.
Infer [40/40]. 3036.17 samples/sec. 84.317 ms/step.
Inference benchmark of fbnetv3_g.ra2_in1k done. 3035.17 samples/sec, 84.32 ms/step
Model fbnetv3_g.ra2_in1k created, param count: 16623888
Running train benchmark on fbnetv3_g.ra2_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 76.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 76.06 MiB is free. Including non-PyTorch memory, this process has 23.57 GiB memory in use. Of the allocated memory 22.12 GiB is allocated by PyTorch, and 224.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model fbnetv3_g.ra2_in1k created, param count: 16623888
Running train benchmark on fbnetv3_g.ra2_in1k for 40 steps w/ input size (3, 288, 288) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 36.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 22.02 GiB is allocated by PyTorch, and 368.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model fbnetv3_g.ra2_in1k created, param count: 16623888
Running train benchmark on fbnetv3_g.ra2_in1k for 40 steps w/ input size (3, 288, 288) and batch size 128.
Train [8/40]. 654.85 samples/sec. 195.465 ms/step.
Train [16/40]. 654.62 samples/sec. 195.533 ms/step.
Train [24/40]. 654.18 samples/sec. 195.663 ms/step.
Train [32/40]. 654.00 samples/sec. 195.720 ms/step.
Train [40/40]. 653.88 samples/sec. 195.755 ms/step.
Train benchmark of fbnetv3_g.ra2_in1k done. 648.98 samples/sec, 195.75 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model flexivit_base.300ep_in1k created, param count: 86589160
Running inference benchmark on flexivit_base.300ep_in1k for 40 steps w/ input size (3, 240, 240) and batch size 256.
Infer [8/40]. 2852.28 samples/sec. 89.753 ms/step.
Infer [16/40]. 2852.10 samples/sec. 89.758 ms/step.
Infer [24/40]. 2851.54 samples/sec. 89.776 ms/step.
Infer [32/40]. 2851.13 samples/sec. 89.789 ms/step.
Infer [40/40]. 2850.69 samples/sec. 89.803 ms/step.
Inference benchmark of flexivit_base.300ep_in1k done. 2849.86 samples/sec, 89.80 ms/step
Model flexivit_base.300ep_in1k created, param count: 86589160
Running train benchmark on flexivit_base.300ep_in1k for 40 steps w/ input size (3, 240, 240) and batch size 256.
Train [8/40]. 862.74 samples/sec. 296.729 ms/step.
Train [16/40]. 862.74 samples/sec. 296.729 ms/step.
Train [24/40]. 862.71 samples/sec. 296.741 ms/step.
Train [32/40]. 862.70 samples/sec. 296.744 ms/step.
Train [40/40]. 862.69 samples/sec. 296.745 ms/step.
Train benchmark of flexivit_base.300ep_in1k done. 859.71 samples/sec, 296.75 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model flexivit_base.600ep_in1k created, param count: 86589160
Running inference benchmark on flexivit_base.600ep_in1k for 40 steps w/ input size (3, 240, 240) and batch size 256.
Infer [8/40]. 2850.82 samples/sec. 89.799 ms/step.
Infer [16/40]. 2851.15 samples/sec. 89.788 ms/step.
Infer [24/40]. 2850.75 samples/sec. 89.801 ms/step.
Infer [32/40]. 2850.40 samples/sec. 89.812 ms/step.
Infer [40/40]. 2850.08 samples/sec. 89.822 ms/step.
Inference benchmark of flexivit_base.600ep_in1k done. 2849.26 samples/sec, 89.82 ms/step
Model flexivit_base.600ep_in1k created, param count: 86589160
Running train benchmark on flexivit_base.600ep_in1k for 40 steps w/ input size (3, 240, 240) and batch size 256.
Train [8/40]. 862.33 samples/sec. 296.869 ms/step.
Train [16/40]. 862.27 samples/sec. 296.890 ms/step.
Train [24/40]. 862.25 samples/sec. 296.899 ms/step.
Train [32/40]. 862.25 samples/sec. 296.896 ms/step.
Train [40/40]. 862.27 samples/sec. 296.892 ms/step.
Train benchmark of flexivit_base.600ep_in1k done. 859.31 samples/sec, 296.89 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model flexivit_base.1200ep_in1k created, param count: 86589160
Running inference benchmark on flexivit_base.1200ep_in1k for 40 steps w/ input size (3, 240, 240) and batch size 256.
Infer [8/40]. 2849.23 samples/sec. 89.849 ms/step.
Infer [16/40]. 2848.88 samples/sec. 89.860 ms/step.
Infer [24/40]. 2848.08 samples/sec. 89.885 ms/step.
Infer [32/40]. 2847.97 samples/sec. 89.888 ms/step.
Infer [40/40]. 2847.31 samples/sec. 89.909 ms/step.
Inference benchmark of flexivit_base.1200ep_in1k done. 2846.46 samples/sec, 89.91 ms/step
Model flexivit_base.1200ep_in1k created, param count: 86589160
Running train benchmark on flexivit_base.1200ep_in1k for 40 steps w/ input size (3, 240, 240) and batch size 256.
Train [8/40]. 862.54 samples/sec. 296.797 ms/step.
Train [16/40]. 862.40 samples/sec. 296.845 ms/step.
Train [24/40]. 862.39 samples/sec. 296.851 ms/step.
Train [32/40]. 862.32 samples/sec. 296.872 ms/step.
Train [40/40]. 862.29 samples/sec. 296.885 ms/step.
Train benchmark of flexivit_base.1200ep_in1k done. 859.33 samples/sec, 296.88 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model flexivit_large.300ep_in1k created, param count: 304355304
Running inference benchmark on flexivit_large.300ep_in1k for 40 steps w/ input size (3, 240, 240) and batch size 256.
Infer [8/40]. 891.52 samples/sec. 287.149 ms/step.
Infer [16/40]. 891.35 samples/sec. 287.206 ms/step.
Infer [24/40]. 891.19 samples/sec. 287.257 ms/step.
Infer [32/40]. 891.10 samples/sec. 287.285 ms/step.
Infer [40/40]. 891.01 samples/sec. 287.314 ms/step.
Inference benchmark of flexivit_large.300ep_in1k done. 890.88 samples/sec, 287.31 ms/step
Model flexivit_large.300ep_in1k created, param count: 304355304
Running train benchmark on flexivit_large.300ep_in1k for 40 steps w/ input size (3, 240, 240) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 198.06 MiB is free. Including non-PyTorch memory, this process has 23.45 GiB memory in use. Of the allocated memory 22.20 GiB is allocated by PyTorch, and 13.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model flexivit_large.300ep_in1k created, param count: 304355304
Running train benchmark on flexivit_large.300ep_in1k for 40 steps w/ input size (3, 240, 240) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 40.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 22.09 GiB is allocated by PyTorch, and 285.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model flexivit_large.300ep_in1k created, param count: 304355304
Running train benchmark on flexivit_large.300ep_in1k for 40 steps w/ input size (3, 240, 240) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 226.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 150.06 MiB is free. Including non-PyTorch memory, this process has 23.49 GiB memory in use. Of the allocated memory 21.94 GiB is allocated by PyTorch, and 333.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model flexivit_large.300ep_in1k created, param count: 304355304
Running train benchmark on flexivit_large.300ep_in1k for 40 steps w/ input size (3, 240, 240) and batch size 96.
Train [8/40]. 275.11 samples/sec. 348.947 ms/step.
Train [16/40]. 275.06 samples/sec. 349.009 ms/step.
Train [24/40]. 275.09 samples/sec. 348.982 ms/step.
Train [32/40]. 275.06 samples/sec. 349.009 ms/step.
Train [40/40]. 275.06 samples/sec. 349.017 ms/step.
Train benchmark of flexivit_large.300ep_in1k done. 273.77 samples/sec, 349.02 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model flexivit_large.600ep_in1k created, param count: 304355304
Running inference benchmark on flexivit_large.600ep_in1k for 40 steps w/ input size (3, 240, 240) and batch size 256.
Infer [8/40]. 893.03 samples/sec. 286.666 ms/step.
Infer [16/40]. 892.93 samples/sec. 286.697 ms/step.
Infer [24/40]. 892.71 samples/sec. 286.767 ms/step.
Infer [32/40]. 892.55 samples/sec. 286.820 ms/step.
Infer [40/40]. 892.47 samples/sec. 286.843 ms/step.
Inference benchmark of flexivit_large.600ep_in1k done. 892.35 samples/sec, 286.84 ms/step
Model flexivit_large.600ep_in1k created, param count: 304355304
Running train benchmark on flexivit_large.600ep_in1k for 40 steps w/ input size (3, 240, 240) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 198.06 MiB is free. Including non-PyTorch memory, this process has 23.45 GiB memory in use. Of the allocated memory 22.20 GiB is allocated by PyTorch, and 13.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model flexivit_large.600ep_in1k created, param count: 304355304
Running train benchmark on flexivit_large.600ep_in1k for 40 steps w/ input size (3, 240, 240) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 40.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 22.09 GiB is allocated by PyTorch, and 285.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model flexivit_large.600ep_in1k created, param count: 304355304
Running train benchmark on flexivit_large.600ep_in1k for 40 steps w/ input size (3, 240, 240) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 226.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 150.06 MiB is free. Including non-PyTorch memory, this process has 23.49 GiB memory in use. Of the allocated memory 21.94 GiB is allocated by PyTorch, and 333.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model flexivit_large.600ep_in1k created, param count: 304355304
Running train benchmark on flexivit_large.600ep_in1k for 40 steps w/ input size (3, 240, 240) and batch size 96.
Train [8/40]. 275.24 samples/sec. 348.786 ms/step.
Train [16/40]. 275.18 samples/sec. 348.865 ms/step.
Train [24/40]. 275.15 samples/sec. 348.906 ms/step.
Train [32/40]. 275.14 samples/sec. 348.909 ms/step.
Train [40/40]. 275.11 samples/sec. 348.952 ms/step.
Train benchmark of flexivit_large.600ep_in1k done. 273.84 samples/sec, 348.95 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model flexivit_large.1200ep_in1k created, param count: 304355304
Running inference benchmark on flexivit_large.1200ep_in1k for 40 steps w/ input size (3, 240, 240) and batch size 256.
Infer [8/40]. 891.54 samples/sec. 287.145 ms/step.
Infer [16/40]. 891.41 samples/sec. 287.186 ms/step.
Infer [24/40]. 891.28 samples/sec. 287.228 ms/step.
Infer [32/40]. 891.15 samples/sec. 287.269 ms/step.
Infer [40/40]. 891.05 samples/sec. 287.302 ms/step.
Inference benchmark of flexivit_large.1200ep_in1k done. 890.92 samples/sec, 287.30 ms/step
Model flexivit_large.1200ep_in1k created, param count: 304355304
Running train benchmark on flexivit_large.1200ep_in1k for 40 steps w/ input size (3, 240, 240) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 198.06 MiB is free. Including non-PyTorch memory, this process has 23.45 GiB memory in use. Of the allocated memory 22.20 GiB is allocated by PyTorch, and 13.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model flexivit_large.1200ep_in1k created, param count: 304355304
Running train benchmark on flexivit_large.1200ep_in1k for 40 steps w/ input size (3, 240, 240) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 40.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 22.09 GiB is allocated by PyTorch, and 285.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model flexivit_large.1200ep_in1k created, param count: 304355304
Running train benchmark on flexivit_large.1200ep_in1k for 40 steps w/ input size (3, 240, 240) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 226.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 150.06 MiB is free. Including non-PyTorch memory, this process has 23.49 GiB memory in use. Of the allocated memory 21.94 GiB is allocated by PyTorch, and 333.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model flexivit_large.1200ep_in1k created, param count: 304355304
Running train benchmark on flexivit_large.1200ep_in1k for 40 steps w/ input size (3, 240, 240) and batch size 96.
Train [8/40]. 275.12 samples/sec. 348.935 ms/step.
Train [16/40]. 275.12 samples/sec. 348.943 ms/step.
Train [24/40]. 275.12 samples/sec. 348.937 ms/step.
Train [32/40]. 275.12 samples/sec. 348.935 ms/step.
Train [40/40]. 275.11 samples/sec. 348.952 ms/step.
Train benchmark of flexivit_large.1200ep_in1k done. 273.78 samples/sec, 348.95 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model flexivit_small.300ep_in1k created, param count: 22061416
Running inference benchmark on flexivit_small.300ep_in1k for 40 steps w/ input size (3, 240, 240) and batch size 256.
Infer [8/40]. 8046.38 samples/sec. 31.816 ms/step.
Infer [16/40]. 8047.59 samples/sec. 31.811 ms/step.
Infer [24/40]. 8046.31 samples/sec. 31.816 ms/step.
Infer [32/40]. 8046.19 samples/sec. 31.816 ms/step.
Infer [40/40]. 8046.08 samples/sec. 31.817 ms/step.
Inference benchmark of flexivit_small.300ep_in1k done. 8039.92 samples/sec, 31.82 ms/step
Model flexivit_small.300ep_in1k created, param count: 22061416
Running train benchmark on flexivit_small.300ep_in1k for 40 steps w/ input size (3, 240, 240) and batch size 256.
Train [8/40]. 2328.68 samples/sec. 109.933 ms/step.
Train [16/40]. 2328.80 samples/sec. 109.928 ms/step.
Train [24/40]. 2328.79 samples/sec. 109.928 ms/step.
Train [32/40]. 2328.95 samples/sec. 109.921 ms/step.
Train [40/40]. 2328.98 samples/sec. 109.919 ms/step.
Train benchmark of flexivit_small.300ep_in1k done. 2314.12 samples/sec, 109.92 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model flexivit_small.600ep_in1k created, param count: 22061416
Running inference benchmark on flexivit_small.600ep_in1k for 40 steps w/ input size (3, 240, 240) and batch size 256.
Infer [8/40]. 8045.47 samples/sec. 31.819 ms/step.
Infer [16/40]. 8045.35 samples/sec. 31.820 ms/step.
Infer [24/40]. 8045.42 samples/sec. 31.819 ms/step.
Infer [32/40]. 8045.25 samples/sec. 31.820 ms/step.
Infer [40/40]. 8045.05 samples/sec. 31.821 ms/step.
Inference benchmark of flexivit_small.600ep_in1k done. 8038.69 samples/sec, 31.82 ms/step
Model flexivit_small.600ep_in1k created, param count: 22061416
Running train benchmark on flexivit_small.600ep_in1k for 40 steps w/ input size (3, 240, 240) and batch size 256.
Train [8/40]. 2329.77 samples/sec. 109.882 ms/step.
Train [16/40]. 2329.69 samples/sec. 109.886 ms/step.
Train [24/40]. 2329.65 samples/sec. 109.888 ms/step.
Train [32/40]. 2329.62 samples/sec. 109.889 ms/step.
Train [40/40]. 2329.68 samples/sec. 109.886 ms/step.
Train benchmark of flexivit_small.600ep_in1k done. 2314.45 samples/sec, 109.89 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model flexivit_small.1200ep_in1k created, param count: 22061416
Running inference benchmark on flexivit_small.1200ep_in1k for 40 steps w/ input size (3, 240, 240) and batch size 256.
Infer [8/40]. 8056.26 samples/sec. 31.777 ms/step.
Infer [16/40]. 8056.98 samples/sec. 31.774 ms/step.
Infer [24/40]. 8057.26 samples/sec. 31.773 ms/step.
Infer [32/40]. 8056.98 samples/sec. 31.774 ms/step.
Infer [40/40]. 8056.76 samples/sec. 31.775 ms/step.
Inference benchmark of flexivit_small.1200ep_in1k done. 8050.65 samples/sec, 31.77 ms/step
Model flexivit_small.1200ep_in1k created, param count: 22061416
Running train benchmark on flexivit_small.1200ep_in1k for 40 steps w/ input size (3, 240, 240) and batch size 256.
Train [8/40]. 2328.95 samples/sec. 109.921 ms/step.
Train [16/40]. 2328.92 samples/sec. 109.922 ms/step.
Train [24/40]. 2328.95 samples/sec. 109.921 ms/step.
Train [32/40]. 2329.03 samples/sec. 109.917 ms/step.
Train [40/40]. 2329.05 samples/sec. 109.916 ms/step.
Train benchmark of flexivit_small.1200ep_in1k done. 2313.84 samples/sec, 109.92 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model focalnet_base_lrf.ms_in1k created, param count: 88749768
Running inference benchmark on focalnet_base_lrf.ms_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1352.24 samples/sec. 189.315 ms/step.
Infer [16/40]. 1352.12 samples/sec. 189.332 ms/step.
Infer [24/40]. 1351.94 samples/sec. 189.357 ms/step.
Infer [32/40]. 1351.62 samples/sec. 189.402 ms/step.
Infer [40/40]. 1351.45 samples/sec. 189.426 ms/step.
Inference benchmark of focalnet_base_lrf.ms_in1k done. 1351.20 samples/sec, 189.43 ms/step
Model focalnet_base_lrf.ms_in1k created, param count: 88749768
Running train benchmark on focalnet_base_lrf.ms_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 18.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.12 GiB is allocated by PyTorch, and 276.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model focalnet_base_lrf.ms_in1k created, param count: 88749768
Running train benchmark on focalnet_base_lrf.ms_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 24.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 469.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model focalnet_base_lrf.ms_in1k created, param count: 88749768
Running train benchmark on focalnet_base_lrf.ms_in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
Train [8/40]. 475.02 samples/sec. 269.465 ms/step.
Train [16/40]. 474.99 samples/sec. 269.482 ms/step.
Train [24/40]. 474.97 samples/sec. 269.489 ms/step.
Train [32/40]. 474.98 samples/sec. 269.486 ms/step.
Train [40/40]. 474.97 samples/sec. 269.488 ms/step.
Train benchmark of focalnet_base_lrf.ms_in1k done. 471.87 samples/sec, 269.49 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model focalnet_base_srf.ms_in1k created, param count: 88148144
Running inference benchmark on focalnet_base_srf.ms_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1436.99 samples/sec. 178.151 ms/step.
Infer [16/40]. 1436.91 samples/sec. 178.160 ms/step.
Infer [24/40]. 1436.93 samples/sec. 178.157 ms/step.
Infer [32/40]. 1436.86 samples/sec. 178.167 ms/step.
Infer [40/40]. 1436.83 samples/sec. 178.170 ms/step.
Inference benchmark of focalnet_base_srf.ms_in1k done. 1436.53 samples/sec, 178.17 ms/step
Model focalnet_base_srf.ms_in1k created, param count: 88148144
Running train benchmark on focalnet_base_srf.ms_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 30.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 22.33 GiB is allocated by PyTorch, and 47.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model focalnet_base_srf.ms_in1k created, param count: 88148144
Running train benchmark on focalnet_base_srf.ms_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 26.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.10 GiB is allocated by PyTorch, and 286.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model focalnet_base_srf.ms_in1k created, param count: 88148144
Running train benchmark on focalnet_base_srf.ms_in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
Train [8/40]. 514.58 samples/sec. 248.745 ms/step.
Train [16/40]. 514.34 samples/sec. 248.861 ms/step.
Train [24/40]. 514.28 samples/sec. 248.892 ms/step.
Train [32/40]. 514.24 samples/sec. 248.911 ms/step.
Train [40/40]. 514.22 samples/sec. 248.920 ms/step.
Train benchmark of focalnet_base_srf.ms_in1k done. 510.89 samples/sec, 248.92 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model focalnet_huge_fl3.ms_in22k created, param count: 745276338
Running inference benchmark on focalnet_huge_fl3.ms_in22k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 333.84 samples/sec. 766.837 ms/step.
Infer [16/40]. 333.82 samples/sec. 766.870 ms/step.
Infer [24/40]. 333.82 samples/sec. 766.888 ms/step.
Infer [32/40]. 333.81 samples/sec. 766.906 ms/step.
Infer [40/40]. 333.81 samples/sec. 766.908 ms/step.
Inference benchmark of focalnet_huge_fl3.ms_in22k done. 333.79 samples/sec, 766.91 ms/step
Model focalnet_huge_fl3.ms_in22k created, param count: 745276338
Running train benchmark on focalnet_huge_fl3.ms_in22k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 540.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 322.06 MiB is free. Including non-PyTorch memory, this process has 23.33 GiB memory in use. Of the allocated memory 22.07 GiB is allocated by PyTorch, and 22.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model focalnet_huge_fl3.ms_in22k created, param count: 745276338
Running train benchmark on focalnet_huge_fl3.ms_in22k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 406.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 182.06 MiB is free. Including non-PyTorch memory, this process has 23.46 GiB memory in use. Of the allocated memory 21.67 GiB is allocated by PyTorch, and 578.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model focalnet_huge_fl3.ms_in22k created, param count: 745276338
Running train benchmark on focalnet_huge_fl3.ms_in22k for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 540.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 372.06 MiB is free. Including non-PyTorch memory, this process has 23.28 GiB memory in use. Of the allocated memory 21.90 GiB is allocated by PyTorch, and 153.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model focalnet_huge_fl3.ms_in22k created, param count: 745276338
Running train benchmark on focalnet_huge_fl3.ms_in22k for 40 steps w/ input size (3, 224, 224) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 52.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 10.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 21.96 GiB is allocated by PyTorch, and 452.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model focalnet_huge_fl3.ms_in22k created, param count: 745276338
Running train benchmark on focalnet_huge_fl3.ms_in22k for 40 steps w/ input size (3, 224, 224) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 34.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 32.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 22.17 GiB is allocated by PyTorch, and 210.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model focalnet_huge_fl3.ms_in22k created, param count: 745276338
Running train benchmark on focalnet_huge_fl3.ms_in22k for 40 steps w/ input size (3, 224, 224) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 52.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 32.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 304.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model focalnet_huge_fl3.ms_in22k created, param count: 745276338
Running train benchmark on focalnet_huge_fl3.ms_in22k for 40 steps w/ input size (3, 224, 224) and batch size 32.
Train [8/40]. 112.21 samples/sec. 285.176 ms/step.
Train [16/40]. 112.20 samples/sec. 285.209 ms/step.
Train [24/40]. 112.20 samples/sec. 285.208 ms/step.
Train [32/40]. 112.19 samples/sec. 285.219 ms/step.
Train [40/40]. 112.19 samples/sec. 285.239 ms/step.
Train benchmark of focalnet_huge_fl3.ms_in22k done. 111.38 samples/sec, 285.24 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model focalnet_huge_fl4.ms_in22k created, param count: 686460664
Running inference benchmark on focalnet_huge_fl4.ms_in22k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 219.93 samples/sec. 1164.012 ms/step.
Infer [16/40]. 219.92 samples/sec. 1164.036 ms/step.
Infer [24/40]. 219.92 samples/sec. 1164.043 ms/step.
Infer [32/40]. 219.92 samples/sec. 1164.039 ms/step.
Infer [40/40]. 219.92 samples/sec. 1164.048 ms/step.
Inference benchmark of focalnet_huge_fl4.ms_in22k done. 219.91 samples/sec, 1164.05 ms/step
Model focalnet_huge_fl4.ms_in22k created, param count: 686460664
Running train benchmark on focalnet_huge_fl4.ms_in22k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 540.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 436.06 MiB is free. Including non-PyTorch memory, this process has 23.21 GiB memory in use. Of the allocated memory 21.97 GiB is allocated by PyTorch, and 17.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model focalnet_huge_fl4.ms_in22k created, param count: 686460664
Running train benchmark on focalnet_huge_fl4.ms_in22k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 1.58 GiB. GPU 0 has a total capacty of 23.65 GiB of which 276.06 MiB is free. Including non-PyTorch memory, this process has 23.37 GiB memory in use. Of the allocated memory 21.56 GiB is allocated by PyTorch, and 598.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model focalnet_huge_fl4.ms_in22k created, param count: 686460664
Running train benchmark on focalnet_huge_fl4.ms_in22k for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 136.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 84.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 22.18 GiB is allocated by PyTorch, and 148.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model focalnet_huge_fl4.ms_in22k created, param count: 686460664
Running train benchmark on focalnet_huge_fl4.ms_in22k for 40 steps w/ input size (3, 224, 224) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 52.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 22.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.03 GiB is allocated by PyTorch, and 362.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model focalnet_huge_fl4.ms_in22k created, param count: 686460664
Running train benchmark on focalnet_huge_fl4.ms_in22k for 40 steps w/ input size (3, 224, 224) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 136.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 118.06 MiB is free. Including non-PyTorch memory, this process has 23.53 GiB memory in use. Of the allocated memory 21.89 GiB is allocated by PyTorch, and 417.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model focalnet_huge_fl4.ms_in22k created, param count: 686460664
Running train benchmark on focalnet_huge_fl4.ms_in22k for 40 steps w/ input size (3, 224, 224) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 52.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 44.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 21.90 GiB is allocated by PyTorch, and 481.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model focalnet_huge_fl4.ms_in22k created, param count: 686460664
Running train benchmark on focalnet_huge_fl4.ms_in22k for 40 steps w/ input size (3, 224, 224) and batch size 32.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model focalnet_huge_fl4.ms_in22k created, param count: 686460664
Running train benchmark on focalnet_huge_fl4.ms_in22k for 40 steps w/ input size (3, 224, 224) and batch size 24.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model focalnet_huge_fl4.ms_in22k created, param count: 686460664
Running train benchmark on focalnet_huge_fl4.ms_in22k for 40 steps w/ input size (3, 224, 224) and batch size 16.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 12 for retry.
Model focalnet_huge_fl4.ms_in22k created, param count: 686460664
Running train benchmark on focalnet_huge_fl4.ms_in22k for 40 steps w/ input size (3, 224, 224) and batch size 12.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 8 for retry.
Model focalnet_huge_fl4.ms_in22k created, param count: 686460664
Running train benchmark on focalnet_huge_fl4.ms_in22k for 40 steps w/ input size (3, 224, 224) and batch size 8.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 6 for retry.
Model focalnet_huge_fl4.ms_in22k created, param count: 686460664
Running train benchmark on focalnet_huge_fl4.ms_in22k for 40 steps w/ input size (3, 224, 224) and batch size 6.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 4 for retry.
Model focalnet_huge_fl4.ms_in22k created, param count: 686460664
Running train benchmark on focalnet_huge_fl4.ms_in22k for 40 steps w/ input size (3, 224, 224) and batch size 4.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 3 for retry.
Model focalnet_huge_fl4.ms_in22k created, param count: 686460664
Running train benchmark on focalnet_huge_fl4.ms_in22k for 40 steps w/ input size (3, 224, 224) and batch size 3.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 2 for retry.
Model focalnet_huge_fl4.ms_in22k created, param count: 686460664
Running train benchmark on focalnet_huge_fl4.ms_in22k for 40 steps w/ input size (3, 224, 224) and batch size 2.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 1 for retry.
Model focalnet_huge_fl4.ms_in22k created, param count: 686460664
Running train benchmark on focalnet_huge_fl4.ms_in22k for 40 steps w/ input size (3, 224, 224) and batch size 1.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 0 for retry.
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model focalnet_large_fl3.ms_in22k created, param count: 239134898
Running inference benchmark on focalnet_large_fl3.ms_in22k for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 161.22 samples/sec. 1587.918 ms/step.
Infer [16/40]. 161.22 samples/sec. 1587.875 ms/step.
Infer [24/40]. 161.21 samples/sec. 1587.969 ms/step.
Infer [32/40]. 161.21 samples/sec. 1587.984 ms/step.
Infer [40/40]. 161.21 samples/sec. 1588.030 ms/step.
Inference benchmark of focalnet_large_fl3.ms_in22k done. 161.20 samples/sec, 1588.03 ms/step
Model focalnet_large_fl3.ms_in22k created, param count: 239134898
Running train benchmark on focalnet_large_fl3.ms_in22k for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 3.38 GiB. GPU 0 has a total capacty of 23.65 GiB of which 2.12 GiB is free. Including non-PyTorch memory, this process has 21.52 GiB memory in use. Of the allocated memory 17.60 GiB is allocated by PyTorch, and 2.69 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model focalnet_large_fl3.ms_in22k created, param count: 239134898
Running train benchmark on focalnet_large_fl3.ms_in22k for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 648.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 308.06 MiB is free. Including non-PyTorch memory, this process has 23.34 GiB memory in use. Of the allocated memory 21.57 GiB is allocated by PyTorch, and 547.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model focalnet_large_fl3.ms_in22k created, param count: 239134898
Running train benchmark on focalnet_large_fl3.ms_in22k for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 1.69 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.19 GiB is free. Including non-PyTorch memory, this process has 22.45 GiB memory in use. Of the allocated memory 19.61 GiB is allocated by PyTorch, and 1.61 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model focalnet_large_fl3.ms_in22k created, param count: 239134898
Running train benchmark on focalnet_large_fl3.ms_in22k for 40 steps w/ input size (3, 384, 384) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 162.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 130.06 MiB is free. Including non-PyTorch memory, this process has 23.51 GiB memory in use. Of the allocated memory 22.12 GiB is allocated by PyTorch, and 171.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model focalnet_large_fl3.ms_in22k created, param count: 239134898
Running train benchmark on focalnet_large_fl3.ms_in22k for 40 steps w/ input size (3, 384, 384) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 216.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 62.06 MiB is free. Including non-PyTorch memory, this process has 23.58 GiB memory in use. Of the allocated memory 22.03 GiB is allocated by PyTorch, and 330.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model focalnet_large_fl3.ms_in22k created, param count: 239134898
Running train benchmark on focalnet_large_fl3.ms_in22k for 40 steps w/ input size (3, 384, 384) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 38.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 22.16 GiB is allocated by PyTorch, and 218.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model focalnet_large_fl3.ms_in22k created, param count: 239134898
Running train benchmark on focalnet_large_fl3.ms_in22k for 40 steps w/ input size (3, 384, 384) and batch size 32.
ERROR: "CUDA out of memory. Tried to allocate 54.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 54.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 22.04 GiB is allocated by PyTorch, and 326.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model focalnet_large_fl3.ms_in22k created, param count: 239134898
Running train benchmark on focalnet_large_fl3.ms_in22k for 40 steps w/ input size (3, 384, 384) and batch size 24.
Train [8/40]. 52.62 samples/sec. 456.123 ms/step.
Train [16/40]. 52.62 samples/sec. 456.113 ms/step.
Train [24/40]. 52.62 samples/sec. 456.122 ms/step.
Train [32/40]. 52.62 samples/sec. 456.116 ms/step.
Train [40/40]. 52.62 samples/sec. 456.119 ms/step.
Train benchmark of focalnet_large_fl3.ms_in22k done. 52.37 samples/sec, 456.12 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model focalnet_large_fl4.ms_in22k created, param count: 239315402
Running inference benchmark on focalnet_large_fl4.ms_in22k for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 153.03 samples/sec. 1672.840 ms/step.
Infer [16/40]. 153.02 samples/sec. 1672.942 ms/step.
Infer [24/40]. 153.02 samples/sec. 1672.980 ms/step.
Infer [32/40]. 153.02 samples/sec. 1673.013 ms/step.
Infer [40/40]. 153.01 samples/sec. 1673.071 ms/step.
Inference benchmark of focalnet_large_fl4.ms_in22k done. 153.01 samples/sec, 1673.07 ms/step
Model focalnet_large_fl4.ms_in22k created, param count: 239315402
Running train benchmark on focalnet_large_fl4.ms_in22k for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 3.38 GiB. GPU 0 has a total capacty of 23.65 GiB of which 438.06 MiB is free. Including non-PyTorch memory, this process has 23.21 GiB memory in use. Of the allocated memory 19.29 GiB is allocated by PyTorch, and 2.69 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model focalnet_large_fl4.ms_in22k created, param count: 239315402
Running train benchmark on focalnet_large_fl4.ms_in22k for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 648.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 324.06 MiB is free. Including non-PyTorch memory, this process has 23.32 GiB memory in use. Of the allocated memory 21.58 GiB is allocated by PyTorch, and 526.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model focalnet_large_fl4.ms_in22k created, param count: 239315402
Running train benchmark on focalnet_large_fl4.ms_in22k for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 1.69 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.16 GiB is free. Including non-PyTorch memory, this process has 22.48 GiB memory in use. Of the allocated memory 19.61 GiB is allocated by PyTorch, and 1.64 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model focalnet_large_fl4.ms_in22k created, param count: 239315402
Running train benchmark on focalnet_large_fl4.ms_in22k for 40 steps w/ input size (3, 384, 384) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 648.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 610.06 MiB is free. Including non-PyTorch memory, this process has 23.04 GiB memory in use. Of the allocated memory 21.17 GiB is allocated by PyTorch, and 663.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model focalnet_large_fl4.ms_in22k created, param count: 239315402
Running train benchmark on focalnet_large_fl4.ms_in22k for 40 steps w/ input size (3, 384, 384) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 216.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 10.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 325.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model focalnet_large_fl4.ms_in22k created, param count: 239315402
Running train benchmark on focalnet_large_fl4.ms_in22k for 40 steps w/ input size (3, 384, 384) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 162.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 70.06 MiB is free. Including non-PyTorch memory, this process has 23.57 GiB memory in use. Of the allocated memory 22.04 GiB is allocated by PyTorch, and 304.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model focalnet_large_fl4.ms_in22k created, param count: 239315402
Running train benchmark on focalnet_large_fl4.ms_in22k for 40 steps w/ input size (3, 384, 384) and batch size 32.
ERROR: "CUDA out of memory. Tried to allocate 108.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 6.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.12 GiB is allocated by PyTorch, and 292.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model focalnet_large_fl4.ms_in22k created, param count: 239315402
Running train benchmark on focalnet_large_fl4.ms_in22k for 40 steps w/ input size (3, 384, 384) and batch size 24.
Train [8/40]. 50.84 samples/sec. 472.061 ms/step.
Train [16/40]. 50.84 samples/sec. 472.062 ms/step.
Train [24/40]. 50.84 samples/sec. 472.056 ms/step.
Train [32/40]. 50.84 samples/sec. 472.058 ms/step.
Train [40/40]. 50.84 samples/sec. 472.055 ms/step.
Train benchmark of focalnet_large_fl4.ms_in22k done. 50.59 samples/sec, 472.06 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model focalnet_small_lrf.ms_in1k created, param count: 50342440
Running inference benchmark on focalnet_small_lrf.ms_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1977.82 samples/sec. 129.435 ms/step.
Infer [16/40]. 1977.32 samples/sec. 129.468 ms/step.
Infer [24/40]. 1976.97 samples/sec. 129.491 ms/step.
Infer [32/40]. 1977.00 samples/sec. 129.489 ms/step.
Infer [40/40]. 1976.89 samples/sec. 129.496 ms/step.
Inference benchmark of focalnet_small_lrf.ms_in1k done. 1976.30 samples/sec, 129.50 ms/step
Model focalnet_small_lrf.ms_in1k created, param count: 50342440
Running train benchmark on focalnet_small_lrf.ms_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 12.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 21.96 GiB is allocated by PyTorch, and 446.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model focalnet_small_lrf.ms_in1k created, param count: 50342440
Running train benchmark on focalnet_small_lrf.ms_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
Train [8/40]. 648.09 samples/sec. 296.257 ms/step.
Train [16/40]. 648.08 samples/sec. 296.260 ms/step.
Train [24/40]. 648.06 samples/sec. 296.267 ms/step.
Train [32/40]. 648.07 samples/sec. 296.263 ms/step.
Train [40/40]. 648.07 samples/sec. 296.266 ms/step.
Train benchmark of focalnet_small_lrf.ms_in1k done. 643.92 samples/sec, 296.27 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model focalnet_small_srf.ms_in1k created, param count: 49891216
Running inference benchmark on focalnet_small_srf.ms_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 2113.49 samples/sec. 121.127 ms/step.
Infer [16/40]. 2112.85 samples/sec. 121.164 ms/step.
Infer [24/40]. 2112.62 samples/sec. 121.177 ms/step.
Infer [32/40]. 2112.45 samples/sec. 121.186 ms/step.
Infer [40/40]. 2112.36 samples/sec. 121.191 ms/step.
Inference benchmark of focalnet_small_srf.ms_in1k done. 2111.81 samples/sec, 121.19 ms/step
Model focalnet_small_srf.ms_in1k created, param count: 49891216
Running train benchmark on focalnet_small_srf.ms_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 4.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 21.92 GiB is allocated by PyTorch, and 496.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model focalnet_small_srf.ms_in1k created, param count: 49891216
Running train benchmark on focalnet_small_srf.ms_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
Train [8/40]. 700.61 samples/sec. 274.046 ms/step.
Train [16/40]. 700.56 samples/sec. 274.067 ms/step.
Train [24/40]. 700.55 samples/sec. 274.071 ms/step.
Train [32/40]. 700.54 samples/sec. 274.075 ms/step.
Train [40/40]. 700.53 samples/sec. 274.078 ms/step.
Train benchmark of focalnet_small_srf.ms_in1k done. 696.32 samples/sec, 274.08 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model focalnet_tiny_lrf.ms_in1k created, param count: 28647928
Running inference benchmark on focalnet_tiny_lrf.ms_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3110.88 samples/sec. 82.292 ms/step.
Infer [16/40]. 3110.93 samples/sec. 82.290 ms/step.
Infer [24/40]. 3110.94 samples/sec. 82.290 ms/step.
Infer [32/40]. 3110.97 samples/sec. 82.289 ms/step.
Infer [40/40]. 3110.98 samples/sec. 82.289 ms/step.
Inference benchmark of focalnet_tiny_lrf.ms_in1k done. 3109.92 samples/sec, 82.29 ms/step
Model focalnet_tiny_lrf.ms_in1k created, param count: 28647928
Running train benchmark on focalnet_tiny_lrf.ms_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 995.40 samples/sec. 257.183 ms/step.
Train [16/40]. 995.44 samples/sec. 257.173 ms/step.
Train [24/40]. 995.46 samples/sec. 257.167 ms/step.
Train [32/40]. 995.47 samples/sec. 257.166 ms/step.
Train [40/40]. 995.46 samples/sec. 257.168 ms/step.
Train benchmark of focalnet_tiny_lrf.ms_in1k done. 990.89 samples/sec, 257.17 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model focalnet_tiny_srf.ms_in1k created, param count: 28427116
Running inference benchmark on focalnet_tiny_srf.ms_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3305.49 samples/sec. 77.447 ms/step.
Infer [16/40]. 3305.05 samples/sec. 77.457 ms/step.
Infer [24/40]. 3304.86 samples/sec. 77.462 ms/step.
Infer [32/40]. 3304.65 samples/sec. 77.467 ms/step.
Infer [40/40]. 3304.56 samples/sec. 77.469 ms/step.
Inference benchmark of focalnet_tiny_srf.ms_in1k done. 3303.29 samples/sec, 77.47 ms/step
Model focalnet_tiny_srf.ms_in1k created, param count: 28427116
Running train benchmark on focalnet_tiny_srf.ms_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1078.44 samples/sec. 237.381 ms/step.
Train [16/40]. 1078.44 samples/sec. 237.381 ms/step.
Train [24/40]. 1078.45 samples/sec. 237.378 ms/step.
Train [32/40]. 1078.43 samples/sec. 237.381 ms/step.
Train [40/40]. 1078.44 samples/sec. 237.381 ms/step.
Train benchmark of focalnet_tiny_srf.ms_in1k done. 1073.33 samples/sec, 237.38 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model focalnet_xlarge_fl3.ms_in22k created, param count: 408787378
Running inference benchmark on focalnet_xlarge_fl3.ms_in22k for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 111.52 samples/sec. 2295.480 ms/step.
Infer [16/40]. 111.52 samples/sec. 2295.630 ms/step.
Infer [24/40]. 111.51 samples/sec. 2295.798 ms/step.
Infer [32/40]. 111.50 samples/sec. 2295.895 ms/step.
Infer [40/40]. 111.50 samples/sec. 2296.017 ms/step.
Inference benchmark of focalnet_xlarge_fl3.ms_in22k done. 111.49 samples/sec, 2296.02 ms/step
Model focalnet_xlarge_fl3.ms_in22k created, param count: 408787378
Running train benchmark on focalnet_xlarge_fl3.ms_in22k for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 4.50 GiB. GPU 0 has a total capacty of 23.65 GiB of which 3.35 GiB is free. Including non-PyTorch memory, this process has 20.29 GiB memory in use. Of the allocated memory 19.04 GiB is allocated by PyTorch, and 15.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model focalnet_xlarge_fl3.ms_in22k created, param count: 408787378
Running train benchmark on focalnet_xlarge_fl3.ms_in22k for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 3.38 GiB. GPU 0 has a total capacty of 23.65 GiB of which 2.04 GiB is free. Including non-PyTorch memory, this process has 21.61 GiB memory in use. Of the allocated memory 17.85 GiB is allocated by PyTorch, and 2.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model focalnet_xlarge_fl3.ms_in22k created, param count: 408787378
Running train benchmark on focalnet_xlarge_fl3.ms_in22k for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 576.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 442.06 MiB is free. Including non-PyTorch memory, this process has 23.21 GiB memory in use. Of the allocated memory 21.74 GiB is allocated by PyTorch, and 240.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model focalnet_xlarge_fl3.ms_in22k created, param count: 408787378
Running train benchmark on focalnet_xlarge_fl3.ms_in22k for 40 steps w/ input size (3, 384, 384) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 1.69 GiB. GPU 0 has a total capacty of 23.65 GiB of which 902.06 MiB is free. Including non-PyTorch memory, this process has 22.76 GiB memory in use. Of the allocated memory 19.89 GiB is allocated by PyTorch, and 1.64 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model focalnet_xlarge_fl3.ms_in22k created, param count: 408787378
Running train benchmark on focalnet_xlarge_fl3.ms_in22k for 40 steps w/ input size (3, 384, 384) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 576.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 500.06 MiB is free. Including non-PyTorch memory, this process has 23.15 GiB memory in use. Of the allocated memory 21.26 GiB is allocated by PyTorch, and 674.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model focalnet_xlarge_fl3.ms_in22k created, param count: 408787378
Running train benchmark on focalnet_xlarge_fl3.ms_in22k for 40 steps w/ input size (3, 384, 384) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 216.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 52.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 22.11 GiB is allocated by PyTorch, and 257.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model focalnet_xlarge_fl3.ms_in22k created, param count: 408787378
Running train benchmark on focalnet_xlarge_fl3.ms_in22k for 40 steps w/ input size (3, 384, 384) and batch size 32.
ERROR: "CUDA out of memory. Tried to allocate 144.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 72.06 MiB is free. Including non-PyTorch memory, this process has 23.57 GiB memory in use. Of the allocated memory 22.04 GiB is allocated by PyTorch, and 309.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model focalnet_xlarge_fl3.ms_in22k created, param count: 408787378
Running train benchmark on focalnet_xlarge_fl3.ms_in22k for 40 steps w/ input size (3, 384, 384) and batch size 24.
ERROR: "CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 10.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 21.41 GiB is allocated by PyTorch, and 1009.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model focalnet_xlarge_fl3.ms_in22k created, param count: 408787378
Running train benchmark on focalnet_xlarge_fl3.ms_in22k for 40 steps w/ input size (3, 384, 384) and batch size 16.
Train [8/40]. 37.27 samples/sec. 429.295 ms/step.
Train [16/40]. 37.27 samples/sec. 429.286 ms/step.
Train [24/40]. 37.27 samples/sec. 429.278 ms/step.
Train [32/40]. 37.27 samples/sec. 429.287 ms/step.
Train [40/40]. 37.27 samples/sec. 429.282 ms/step.
Train benchmark of focalnet_xlarge_fl3.ms_in22k done. 37.09 samples/sec, 429.28 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model focalnet_xlarge_fl4.ms_in22k created, param count: 409028042
Running inference benchmark on focalnet_xlarge_fl4.ms_in22k for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 106.09 samples/sec. 2413.087 ms/step.
Infer [16/40]. 106.08 samples/sec. 2413.277 ms/step.
Infer [24/40]. 106.06 samples/sec. 2413.655 ms/step.
Infer [32/40]. 106.05 samples/sec. 2413.921 ms/step.
Infer [40/40]. 106.04 samples/sec. 2414.079 ms/step.
Inference benchmark of focalnet_xlarge_fl4.ms_in22k done. 106.04 samples/sec, 2414.08 ms/step
Model focalnet_xlarge_fl4.ms_in22k created, param count: 409028042
Running train benchmark on focalnet_xlarge_fl4.ms_in22k for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 1.12 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.10 GiB is free. Including non-PyTorch memory, this process has 22.54 GiB memory in use. Of the allocated memory 21.30 GiB is allocated by PyTorch, and 16.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model focalnet_xlarge_fl4.ms_in22k created, param count: 409028042
Running train benchmark on focalnet_xlarge_fl4.ms_in22k for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 3.38 GiB. GPU 0 has a total capacty of 23.65 GiB of which 2.87 GiB is free. Including non-PyTorch memory, this process has 20.77 GiB memory in use. Of the allocated memory 16.17 GiB is allocated by PyTorch, and 3.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model focalnet_xlarge_fl4.ms_in22k created, param count: 409028042
Running train benchmark on focalnet_xlarge_fl4.ms_in22k for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 576.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 438.06 MiB is free. Including non-PyTorch memory, this process has 23.21 GiB memory in use. Of the allocated memory 21.75 GiB is allocated by PyTorch, and 239.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model focalnet_xlarge_fl4.ms_in22k created, param count: 409028042
Running train benchmark on focalnet_xlarge_fl4.ms_in22k for 40 steps w/ input size (3, 384, 384) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 1.69 GiB. GPU 0 has a total capacty of 23.65 GiB of which 896.06 MiB is free. Including non-PyTorch memory, this process has 22.77 GiB memory in use. Of the allocated memory 19.89 GiB is allocated by PyTorch, and 1.64 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model focalnet_xlarge_fl4.ms_in22k created, param count: 409028042
Running train benchmark on focalnet_xlarge_fl4.ms_in22k for 40 steps w/ input size (3, 384, 384) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 144.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 68.06 MiB is free. Including non-PyTorch memory, this process has 23.57 GiB memory in use. Of the allocated memory 22.11 GiB is allocated by PyTorch, and 240.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model focalnet_xlarge_fl4.ms_in22k created, param count: 409028042
Running train benchmark on focalnet_xlarge_fl4.ms_in22k for 40 steps w/ input size (3, 384, 384) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 54.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 40.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 22.22 GiB is allocated by PyTorch, and 159.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model focalnet_xlarge_fl4.ms_in22k created, param count: 409028042
Running train benchmark on focalnet_xlarge_fl4.ms_in22k for 40 steps w/ input size (3, 384, 384) and batch size 32.
ERROR: "CUDA out of memory. Tried to allocate 144.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 130.06 MiB is free. Including non-PyTorch memory, this process has 23.51 GiB memory in use. Of the allocated memory 21.97 GiB is allocated by PyTorch, and 323.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model focalnet_xlarge_fl4.ms_in22k created, param count: 409028042
Running train benchmark on focalnet_xlarge_fl4.ms_in22k for 40 steps w/ input size (3, 384, 384) and batch size 24.
ERROR: "CUDA out of memory. Tried to allocate 108.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 2.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 21.87 GiB is allocated by PyTorch, and 554.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model focalnet_xlarge_fl4.ms_in22k created, param count: 409028042
Running train benchmark on focalnet_xlarge_fl4.ms_in22k for 40 steps w/ input size (3, 384, 384) and batch size 16.
Train [8/40]. 35.96 samples/sec. 444.907 ms/step.
Train [16/40]. 35.96 samples/sec. 444.930 ms/step.
Train [24/40]. 35.96 samples/sec. 444.927 ms/step.
Train [32/40]. 35.96 samples/sec. 444.922 ms/step.
Train [40/40]. 35.96 samples/sec. 444.926 ms/step.
Train benchmark of focalnet_xlarge_fl4.ms_in22k done. 35.78 samples/sec, 444.93 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model gc_efficientnetv2_rw_t.agc_in1k created, param count: 13677713
Running inference benchmark on gc_efficientnetv2_rw_t.agc_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 3155.74 samples/sec. 81.122 ms/step.
Infer [16/40]. 3154.26 samples/sec. 81.160 ms/step.
Infer [24/40]. 3153.78 samples/sec. 81.172 ms/step.
Infer [32/40]. 3153.63 samples/sec. 81.176 ms/step.
Infer [40/40]. 3153.58 samples/sec. 81.178 ms/step.
Inference benchmark of gc_efficientnetv2_rw_t.agc_in1k done. 3152.56 samples/sec, 81.18 ms/step
Model gc_efficientnetv2_rw_t.agc_in1k created, param count: 13677713
Running train benchmark on gc_efficientnetv2_rw_t.agc_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 24.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 21.94 GiB is allocated by PyTorch, and 462.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model gc_efficientnetv2_rw_t.agc_in1k created, param count: 13677713
Running train benchmark on gc_efficientnetv2_rw_t.agc_in1k for 40 steps w/ input size (3, 288, 288) and batch size 192.
Train [8/40]. 747.98 samples/sec. 256.691 ms/step.
Train [16/40]. 747.93 samples/sec. 256.707 ms/step.
Train [24/40]. 747.96 samples/sec. 256.699 ms/step.
Train [32/40]. 747.98 samples/sec. 256.693 ms/step.
Train [40/40]. 747.98 samples/sec. 256.690 ms/step.
Train benchmark of gc_efficientnetv2_rw_t.agc_in1k done. 741.67 samples/sec, 256.69 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model gcresnet33ts.ra2_in1k created, param count: 19880698
Running inference benchmark on gcresnet33ts.ra2_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 3093.57 samples/sec. 82.752 ms/step.
Infer [16/40]. 3093.41 samples/sec. 82.757 ms/step.
Infer [24/40]. 3092.36 samples/sec. 82.785 ms/step.
Infer [32/40]. 3091.89 samples/sec. 82.797 ms/step.
Infer [40/40]. 3091.55 samples/sec. 82.806 ms/step.
Inference benchmark of gcresnet33ts.ra2_in1k done. 3090.54 samples/sec, 82.81 ms/step
Model gcresnet33ts.ra2_in1k created, param count: 19880698
Running train benchmark on gcresnet33ts.ra2_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 836.94 samples/sec. 305.876 ms/step.
Train [16/40]. 836.93 samples/sec. 305.880 ms/step.
Train [24/40]. 836.82 samples/sec. 305.918 ms/step.
Train [32/40]. 836.58 samples/sec. 306.009 ms/step.
Train [40/40]. 836.44 samples/sec. 306.060 ms/step.
Train benchmark of gcresnet33ts.ra2_in1k done. 833.28 samples/sec, 306.06 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model gcresnet50t.ra2_in1k created, param count: 25897080
Running inference benchmark on gcresnet50t.ra2_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 2427.45 samples/sec. 105.460 ms/step.
Infer [16/40]. 2426.29 samples/sec. 105.511 ms/step.
Infer [24/40]. 2425.80 samples/sec. 105.532 ms/step.
Infer [32/40]. 2425.55 samples/sec. 105.543 ms/step.
Infer [40/40]. 2425.37 samples/sec. 105.551 ms/step.
Inference benchmark of gcresnet50t.ra2_in1k done. 2424.72 samples/sec, 105.55 ms/step
Model gcresnet50t.ra2_in1k created, param count: 25897080
Running train benchmark on gcresnet50t.ra2_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 702.51 samples/sec. 364.407 ms/step.
Train [16/40]. 702.50 samples/sec. 364.411 ms/step.
Train [24/40]. 702.49 samples/sec. 364.416 ms/step.
Train [32/40]. 702.50 samples/sec. 364.414 ms/step.
Train [40/40]. 702.50 samples/sec. 364.414 ms/step.
Train benchmark of gcresnet50t.ra2_in1k done. 699.60 samples/sec, 364.41 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model gcresnext26ts.ch_in1k created, param count: 10476600
Running inference benchmark on gcresnext26ts.ch_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 3745.34 samples/sec. 68.352 ms/step.
Infer [16/40]. 3744.66 samples/sec. 68.364 ms/step.
Infer [24/40]. 3744.23 samples/sec. 68.372 ms/step.
Infer [32/40]. 3744.05 samples/sec. 68.375 ms/step.
Infer [40/40]. 3743.82 samples/sec. 68.379 ms/step.
Inference benchmark of gcresnext26ts.ch_in1k done. 3742.33 samples/sec, 68.38 ms/step
Model gcresnext26ts.ch_in1k created, param count: 10476600
Running train benchmark on gcresnext26ts.ch_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 973.84 samples/sec. 262.878 ms/step.
Train [16/40]. 973.83 samples/sec. 262.880 ms/step.
Train [24/40]. 973.79 samples/sec. 262.891 ms/step.
Train [32/40]. 973.78 samples/sec. 262.893 ms/step.
Train [40/40]. 973.77 samples/sec. 262.894 ms/step.
Train benchmark of gcresnext26ts.ch_in1k done. 970.01 samples/sec, 262.89 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model gcresnext50ts.ch_in1k created, param count: 15667320
Running inference benchmark on gcresnext50ts.ch_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 2497.59 samples/sec. 102.499 ms/step.
Infer [16/40]. 2497.59 samples/sec. 102.499 ms/step.
Infer [24/40]. 2497.49 samples/sec. 102.503 ms/step.
Infer [32/40]. 2497.44 samples/sec. 102.505 ms/step.
Infer [40/40]. 2497.31 samples/sec. 102.510 ms/step.
Inference benchmark of gcresnext50ts.ch_in1k done. 2496.61 samples/sec, 102.51 ms/step
Model gcresnext50ts.ch_in1k created, param count: 15667320
Running train benchmark on gcresnext50ts.ch_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 162.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 78.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 22.18 GiB is allocated by PyTorch, and 160.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model gcresnext50ts.ch_in1k created, param count: 15667320
Running train benchmark on gcresnext50ts.ch_in1k for 40 steps w/ input size (3, 288, 288) and batch size 192.
Train [8/40]. 668.24 samples/sec. 287.323 ms/step.
Train [16/40]. 668.24 samples/sec. 287.322 ms/step.
Train [24/40]. 668.25 samples/sec. 287.316 ms/step.
Train [32/40]. 668.23 samples/sec. 287.325 ms/step.
Train [40/40]. 668.23 samples/sec. 287.326 ms/step.
Train benchmark of gcresnext50ts.ch_in1k done. 664.85 samples/sec, 287.33 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model gcvit_base.in1k created, param count: 90320836
Running inference benchmark on gcvit_base.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1083.62 samples/sec. 236.245 ms/step.
Infer [16/40]. 1083.57 samples/sec. 236.256 ms/step.
Infer [24/40]. 1083.55 samples/sec. 236.261 ms/step.
Infer [32/40]. 1083.55 samples/sec. 236.260 ms/step.
Infer [40/40]. 1083.56 samples/sec. 236.257 ms/step.
Inference benchmark of gcvit_base.in1k done. 1083.38 samples/sec, 236.26 ms/step
Model gcvit_base.in1k created, param count: 90320836
Running train benchmark on gcvit_base.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 196.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 134.06 MiB is free. Including non-PyTorch memory, this process has 23.51 GiB memory in use. Of the allocated memory 21.99 GiB is allocated by PyTorch, and 298.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model gcvit_base.in1k created, param count: 90320836
Running train benchmark on gcvit_base.in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 226.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 22.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.20 GiB is allocated by PyTorch, and 190.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model gcvit_base.in1k created, param count: 90320836
Running train benchmark on gcvit_base.in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 12.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.00 GiB is allocated by PyTorch, and 411.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model gcvit_base.in1k created, param count: 90320836
Running train benchmark on gcvit_base.in1k for 40 steps w/ input size (3, 224, 224) and batch size 96.
Train [8/40]. 380.09 samples/sec. 252.572 ms/step.
Train [16/40]. 380.04 samples/sec. 252.602 ms/step.
Train [24/40]. 379.94 samples/sec. 252.671 ms/step.
Train [32/40]. 379.90 samples/sec. 252.700 ms/step.
Train [40/40]. 379.87 samples/sec. 252.718 ms/step.
Train benchmark of gcvit_base.in1k done. 376.15 samples/sec, 252.72 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model gcvit_small.in1k created, param count: 51092173
Running inference benchmark on gcvit_small.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1495.76 samples/sec. 171.151 ms/step.
Infer [16/40]. 1495.70 samples/sec. 171.158 ms/step.
Infer [24/40]. 1495.72 samples/sec. 171.155 ms/step.
Infer [32/40]. 1495.71 samples/sec. 171.156 ms/step.
Infer [40/40]. 1495.69 samples/sec. 171.158 ms/step.
Inference benchmark of gcvit_small.in1k done. 1495.38 samples/sec, 171.16 ms/step
Model gcvit_small.in1k created, param count: 51092173
Running train benchmark on gcvit_small.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 226.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 32.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 22.17 GiB is allocated by PyTorch, and 215.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model gcvit_small.in1k created, param count: 51092173
Running train benchmark on gcvit_small.in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 170.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 24.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.11 GiB is allocated by PyTorch, and 283.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model gcvit_small.in1k created, param count: 51092173
Running train benchmark on gcvit_small.in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
Train [8/40]. 510.77 samples/sec. 250.603 ms/step.
Train [16/40]. 510.77 samples/sec. 250.604 ms/step.
Train [24/40]. 510.77 samples/sec. 250.603 ms/step.
Train [32/40]. 510.77 samples/sec. 250.600 ms/step.
Train [40/40]. 510.73 samples/sec. 250.622 ms/step.
Train benchmark of gcvit_small.in1k done. 505.63 samples/sec, 250.62 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model gcvit_tiny.in1k created, param count: 28221974
Running inference benchmark on gcvit_tiny.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 2228.20 samples/sec. 114.891 ms/step.
Infer [16/40]. 2228.23 samples/sec. 114.889 ms/step.
Infer [24/40]. 2228.20 samples/sec. 114.891 ms/step.
Infer [32/40]. 2228.17 samples/sec. 114.892 ms/step.
Infer [40/40]. 2228.21 samples/sec. 114.890 ms/step.
Inference benchmark of gcvit_tiny.in1k done. 2227.67 samples/sec, 114.89 ms/step
Model gcvit_tiny.in1k created, param count: 28221974
Running train benchmark on gcvit_tiny.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 6.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.03 GiB is allocated by PyTorch, and 380.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model gcvit_tiny.in1k created, param count: 28221974
Running train benchmark on gcvit_tiny.in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
Train [8/40]. 747.17 samples/sec. 256.968 ms/step.
Train [16/40]. 747.18 samples/sec. 256.966 ms/step.
Train [24/40]. 747.18 samples/sec. 256.965 ms/step.
Train [32/40]. 747.17 samples/sec. 256.971 ms/step.
Train [40/40]. 747.16 samples/sec. 256.973 ms/step.
Train benchmark of gcvit_tiny.in1k done. 740.36 samples/sec, 256.97 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model gcvit_xtiny.in1k created, param count: 19981294
Running inference benchmark on gcvit_xtiny.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3141.25 samples/sec. 81.496 ms/step.
Infer [16/40]. 3140.47 samples/sec. 81.517 ms/step.
Infer [24/40]. 3140.20 samples/sec. 81.523 ms/step.
Infer [32/40]. 3140.16 samples/sec. 81.525 ms/step.
Infer [40/40]. 3140.07 samples/sec. 81.527 ms/step.
Inference benchmark of gcvit_xtiny.in1k done. 3139.03 samples/sec, 81.53 ms/step
Model gcvit_xtiny.in1k created, param count: 19981294
Running train benchmark on gcvit_xtiny.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1010.45 samples/sec. 253.353 ms/step.
Train [16/40]. 1010.44 samples/sec. 253.356 ms/step.
Train [24/40]. 1010.44 samples/sec. 253.354 ms/step.
Train [32/40]. 1010.44 samples/sec. 253.354 ms/step.
Train [40/40]. 1010.44 samples/sec. 253.356 ms/step.
Train benchmark of gcvit_xtiny.in1k done. 1003.61 samples/sec, 253.36 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model gcvit_xxtiny.in1k created, param count: 11995428
Running inference benchmark on gcvit_xxtiny.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 4080.69 samples/sec. 62.735 ms/step.
Infer [16/40]. 4080.69 samples/sec. 62.735 ms/step.
Infer [24/40]. 4080.81 samples/sec. 62.733 ms/step.
Infer [32/40]. 4080.66 samples/sec. 62.735 ms/step.
Infer [40/40]. 4080.68 samples/sec. 62.735 ms/step.
Inference benchmark of gcvit_xxtiny.in1k done. 4079.01 samples/sec, 62.73 ms/step
Model gcvit_xxtiny.in1k created, param count: 11995428
Running train benchmark on gcvit_xxtiny.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1293.66 samples/sec. 197.888 ms/step.
Train [16/40]. 1293.68 samples/sec. 197.885 ms/step.
Train [24/40]. 1293.66 samples/sec. 197.889 ms/step.
Train [32/40]. 1293.65 samples/sec. 197.890 ms/step.
Train [40/40]. 1293.64 samples/sec. 197.892 ms/step.
Train benchmark of gcvit_xxtiny.in1k done. 1285.18 samples/sec, 197.89 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model gernet_l.idstcv_in1k created, param count: 31078280
Running inference benchmark on gernet_l.idstcv_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
Infer [8/40]. 6076.47 samples/sec. 42.130 ms/step.
Infer [16/40]. 6076.18 samples/sec. 42.132 ms/step.
Infer [24/40]. 6075.74 samples/sec. 42.135 ms/step.
Infer [32/40]. 6075.53 samples/sec. 42.136 ms/step.
Infer [40/40]. 6075.33 samples/sec. 42.138 ms/step.
Inference benchmark of gernet_l.idstcv_in1k done. 6071.69 samples/sec, 42.14 ms/step
Model gernet_l.idstcv_in1k created, param count: 31078280
Running train benchmark on gernet_l.idstcv_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
Train [8/40]. 1645.30 samples/sec. 155.594 ms/step.
Train [16/40]. 1645.29 samples/sec. 155.596 ms/step.
Train [24/40]. 1645.28 samples/sec. 155.597 ms/step.
Train [32/40]. 1645.29 samples/sec. 155.596 ms/step.
Train [40/40]. 1645.25 samples/sec. 155.600 ms/step.
Train benchmark of gernet_l.idstcv_in1k done. 1637.32 samples/sec, 155.60 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model gernet_m.idstcv_in1k created, param count: 21142920
Running inference benchmark on gernet_m.idstcv_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 8486.10 samples/sec. 30.167 ms/step.
Infer [16/40]. 8485.90 samples/sec. 30.168 ms/step.
Infer [24/40]. 8485.60 samples/sec. 30.169 ms/step.
Infer [32/40]. 8485.49 samples/sec. 30.169 ms/step.
Infer [40/40]. 8485.58 samples/sec. 30.169 ms/step.
Inference benchmark of gernet_m.idstcv_in1k done. 8478.82 samples/sec, 30.17 ms/step
Model gernet_m.idstcv_in1k created, param count: 21142920
Running train benchmark on gernet_m.idstcv_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 2312.92 samples/sec. 110.683 ms/step.
Train [16/40]. 2312.84 samples/sec. 110.687 ms/step.
Train [24/40]. 2312.71 samples/sec. 110.693 ms/step.
Train [32/40]. 2312.68 samples/sec. 110.694 ms/step.
Train [40/40]. 2312.65 samples/sec. 110.696 ms/step.
Train benchmark of gernet_m.idstcv_in1k done. 2300.80 samples/sec, 110.70 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model gernet_s.idstcv_in1k created, param count: 8173761
Running inference benchmark on gernet_s.idstcv_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 20666.69 samples/sec. 12.387 ms/step.
Infer [16/40]. 20661.11 samples/sec. 12.390 ms/step.
Infer [24/40]. 20657.70 samples/sec. 12.392 ms/step.
Infer [32/40]. 20655.37 samples/sec. 12.394 ms/step.
Infer [40/40]. 20653.77 samples/sec. 12.395 ms/step.
Inference benchmark of gernet_s.idstcv_in1k done. 20617.71 samples/sec, 12.39 ms/step
Model gernet_s.idstcv_in1k created, param count: 8173761
Running train benchmark on gernet_s.idstcv_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 4660.66 samples/sec. 54.928 ms/step.
Train [16/40]. 4660.66 samples/sec. 54.928 ms/step.
Train [24/40]. 4660.83 samples/sec. 54.926 ms/step.
Train [32/40]. 4660.79 samples/sec. 54.926 ms/step.
Train [40/40]. 4660.80 samples/sec. 54.926 ms/step.
Train benchmark of gernet_s.idstcv_in1k done. 4618.13 samples/sec, 54.93 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model ghostnet_100.in1k created, param count: 5182508
Running inference benchmark on ghostnet_100.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 13731.01 samples/sec. 18.644 ms/step.
Infer [16/40]. 13728.90 samples/sec. 18.647 ms/step.
Infer [24/40]. 13731.44 samples/sec. 18.643 ms/step.
Infer [32/40]. 13736.10 samples/sec. 18.637 ms/step.
Infer [40/40]. 13738.23 samples/sec. 18.634 ms/step.
Inference benchmark of ghostnet_100.in1k done. 13721.60 samples/sec, 18.63 ms/step
Model ghostnet_100.in1k created, param count: 5182508
Running train benchmark on ghostnet_100.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 2156.33 samples/sec. 118.720 ms/step.
Train [16/40]. 2156.28 samples/sec. 118.723 ms/step.
Train [24/40]. 2156.15 samples/sec. 118.730 ms/step.
Train [32/40]. 2156.13 samples/sec. 118.731 ms/step.
Train [40/40]. 2156.07 samples/sec. 118.735 ms/step.
Train benchmark of ghostnet_100.in1k done. 2139.21 samples/sec, 118.73 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model gmixer_24_224.ra3_in1k created, param count: 24721096
Running inference benchmark on gmixer_24_224.ra3_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3769.19 samples/sec. 67.919 ms/step.
Infer [16/40]. 3769.42 samples/sec. 67.915 ms/step.
Infer [24/40]. 3769.57 samples/sec. 67.912 ms/step.
Infer [32/40]. 3769.66 samples/sec. 67.911 ms/step.
Infer [40/40]. 3769.71 samples/sec. 67.910 ms/step.
Inference benchmark of gmixer_24_224.ra3_in1k done. 3768.22 samples/sec, 67.91 ms/step
Model gmixer_24_224.ra3_in1k created, param count: 24721096
Running train benchmark on gmixer_24_224.ra3_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1276.66 samples/sec. 200.524 ms/step.
Train [16/40]. 1276.65 samples/sec. 200.525 ms/step.
Train [24/40]. 1276.66 samples/sec. 200.524 ms/step.
Train [32/40]. 1276.65 samples/sec. 200.525 ms/step.
Train [40/40]. 1276.67 samples/sec. 200.522 ms/step.
Train benchmark of gmixer_24_224.ra3_in1k done. 1267.38 samples/sec, 200.52 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model gmlp_s16_224.ra3_in1k created, param count: 19422656
Running inference benchmark on gmlp_s16_224.ra3_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3097.32 samples/sec. 82.652 ms/step.
Infer [16/40]. 3097.27 samples/sec. 82.653 ms/step.
Infer [24/40]. 3097.38 samples/sec. 82.650 ms/step.
Infer [32/40]. 3097.40 samples/sec. 82.650 ms/step.
Infer [40/40]. 3097.32 samples/sec. 82.652 ms/step.
Inference benchmark of gmlp_s16_224.ra3_in1k done. 3096.31 samples/sec, 82.65 ms/step
Model gmlp_s16_224.ra3_in1k created, param count: 19422656
Running train benchmark on gmlp_s16_224.ra3_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1062.89 samples/sec. 240.854 ms/step.
Train [16/40]. 1062.86 samples/sec. 240.860 ms/step.
Train [24/40]. 1062.86 samples/sec. 240.860 ms/step.
Train [32/40]. 1062.86 samples/sec. 240.859 ms/step.
Train [40/40]. 1062.85 samples/sec. 240.861 ms/step.
Train benchmark of gmlp_s16_224.ra3_in1k done. 1056.68 samples/sec, 240.86 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model halo2botnet50ts_256.a1h_in1k created, param count: 22635360
Running inference benchmark on halo2botnet50ts_256.a1h_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
Infer [8/40]. 2133.84 samples/sec. 119.972 ms/step.
Infer [16/40]. 2133.83 samples/sec. 119.972 ms/step.
Infer [24/40]. 2133.81 samples/sec. 119.973 ms/step.
Infer [32/40]. 2133.77 samples/sec. 119.975 ms/step.
Infer [40/40]. 2133.79 samples/sec. 119.975 ms/step.
Inference benchmark of halo2botnet50ts_256.a1h_in1k done. 2133.26 samples/sec, 119.97 ms/step
Model halo2botnet50ts_256.a1h_in1k created, param count: 22635360
Running train benchmark on halo2botnet50ts_256.a1h_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 196.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 12.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.00 GiB is allocated by PyTorch, and 405.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model halo2botnet50ts_256.a1h_in1k created, param count: 22635360
Running train benchmark on halo2botnet50ts_256.a1h_in1k for 40 steps w/ input size (3, 256, 256) and batch size 192.
Train [8/40]. 662.31 samples/sec. 289.895 ms/step.
Train [16/40]. 662.32 samples/sec. 289.889 ms/step.
Train [24/40]. 662.35 samples/sec. 289.877 ms/step.
Train [32/40]. 662.33 samples/sec. 289.886 ms/step.
Train [40/40]. 662.33 samples/sec. 289.884 ms/step.
Train benchmark of halo2botnet50ts_256.a1h_in1k done. 659.79 samples/sec, 289.88 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model halonet26t.a1h_in1k created, param count: 12480288
Running inference benchmark on halonet26t.a1h_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
Infer [8/40]. 4345.59 samples/sec. 58.910 ms/step.
Infer [16/40]. 4345.36 samples/sec. 58.913 ms/step.
Infer [24/40]. 4345.29 samples/sec. 58.914 ms/step.
Infer [32/40]. 4345.30 samples/sec. 58.914 ms/step.
Infer [40/40]. 4345.27 samples/sec. 58.915 ms/step.
Inference benchmark of halonet26t.a1h_in1k done. 4343.40 samples/sec, 58.91 ms/step
Model halonet26t.a1h_in1k created, param count: 12480288
Running train benchmark on halonet26t.a1h_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
Train [8/40]. 1215.21 samples/sec. 210.664 ms/step.
Train [16/40]. 1215.17 samples/sec. 210.669 ms/step.
Train [24/40]. 1215.08 samples/sec. 210.687 ms/step.
Train [32/40]. 1214.97 samples/sec. 210.705 ms/step.
Train [40/40]. 1214.87 samples/sec. 210.722 ms/step.
Train benchmark of halonet26t.a1h_in1k done. 1210.54 samples/sec, 210.72 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model halonet50ts.a1h_in1k created, param count: 22733280
Running inference benchmark on halonet50ts.a1h_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
Infer [8/40]. 2485.66 samples/sec. 102.991 ms/step.
Infer [16/40]. 2485.52 samples/sec. 102.997 ms/step.
Infer [24/40]. 2485.54 samples/sec. 102.996 ms/step.
Infer [32/40]. 2485.54 samples/sec. 102.996 ms/step.
Infer [40/40]. 2485.55 samples/sec. 102.995 ms/step.
Inference benchmark of halonet50ts.a1h_in1k done. 2484.87 samples/sec, 103.00 ms/step
Model halonet50ts.a1h_in1k created, param count: 22733280
Running train benchmark on halonet50ts.a1h_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 196.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 30.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 22.03 GiB is allocated by PyTorch, and 354.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model halonet50ts.a1h_in1k created, param count: 22733280
Running train benchmark on halonet50ts.a1h_in1k for 40 steps w/ input size (3, 256, 256) and batch size 192.
Train [8/40]. 721.92 samples/sec. 265.959 ms/step.
Train [16/40]. 721.90 samples/sec. 265.965 ms/step.
Train [24/40]. 721.86 samples/sec. 265.979 ms/step.
Train [32/40]. 721.87 samples/sec. 265.974 ms/step.
Train [40/40]. 721.88 samples/sec. 265.972 ms/step.
Train benchmark of halonet50ts.a1h_in1k done. 718.94 samples/sec, 265.97 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model haloregnetz_b.ra3_in1k created, param count: 11680072
Running inference benchmark on haloregnetz_b.ra3_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 4074.61 samples/sec. 62.828 ms/step.
Infer [16/40]. 4074.31 samples/sec. 62.833 ms/step.
Infer [24/40]. 4074.25 samples/sec. 62.834 ms/step.
Infer [32/40]. 4074.09 samples/sec. 62.836 ms/step.
Infer [40/40]. 4074.06 samples/sec. 62.837 ms/step.
Inference benchmark of haloregnetz_b.ra3_in1k done. 4072.36 samples/sec, 62.84 ms/step
Model haloregnetz_b.ra3_in1k created, param count: 11680072
Running train benchmark on haloregnetz_b.ra3_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1030.27 samples/sec. 248.478 ms/step.
Train [16/40]. 1030.34 samples/sec. 248.461 ms/step.
Train [24/40]. 1030.30 samples/sec. 248.471 ms/step.
Train [32/40]. 1030.31 samples/sec. 248.470 ms/step.
Train [40/40]. 1030.30 samples/sec. 248.470 ms/step.
Train benchmark of haloregnetz_b.ra3_in1k done. 1024.61 samples/sec, 248.47 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model hardcorenas_a.miil_green_in1k created, param count: 5260232
Running inference benchmark on hardcorenas_a.miil_green_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 15412.88 samples/sec. 16.609 ms/step.
Infer [16/40]. 15410.65 samples/sec. 16.612 ms/step.
Infer [24/40]. 15410.67 samples/sec. 16.612 ms/step.
Infer [32/40]. 15405.81 samples/sec. 16.617 ms/step.
Infer [40/40]. 15402.82 samples/sec. 16.620 ms/step.
Inference benchmark of hardcorenas_a.miil_green_in1k done. 15381.91 samples/sec, 16.62 ms/step
Model hardcorenas_a.miil_green_in1k created, param count: 5260232
Running train benchmark on hardcorenas_a.miil_green_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 2792.17 samples/sec. 91.685 ms/step.
Train [16/40]. 2792.36 samples/sec. 91.679 ms/step.
Train [24/40]. 2792.47 samples/sec. 91.675 ms/step.
Train [32/40]. 2792.52 samples/sec. 91.673 ms/step.
Train [40/40]. 2792.02 samples/sec. 91.690 ms/step.
Train benchmark of hardcorenas_a.miil_green_in1k done. 2775.51 samples/sec, 91.69 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model hardcorenas_b.miil_green_in1k created, param count: 5176544
Running inference benchmark on hardcorenas_b.miil_green_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 15076.19 samples/sec. 16.980 ms/step.
Infer [16/40]. 15072.00 samples/sec. 16.985 ms/step.
Infer [24/40]. 15071.40 samples/sec. 16.986 ms/step.
Infer [32/40]. 15071.10 samples/sec. 16.986 ms/step.
Infer [40/40]. 15070.81 samples/sec. 16.986 ms/step.
Inference benchmark of hardcorenas_b.miil_green_in1k done. 15050.19 samples/sec, 16.99 ms/step
Model hardcorenas_b.miil_green_in1k created, param count: 5176544
Running train benchmark on hardcorenas_b.miil_green_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 2648.47 samples/sec. 96.660 ms/step.
Train [16/40]. 2648.25 samples/sec. 96.668 ms/step.
Train [24/40]. 2648.38 samples/sec. 96.663 ms/step.
Train [32/40]. 2648.31 samples/sec. 96.665 ms/step.
Train [40/40]. 2648.38 samples/sec. 96.663 ms/step.
Train benchmark of hardcorenas_b.miil_green_in1k done. 2630.63 samples/sec, 96.66 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model hardcorenas_c.miil_green_in1k created, param count: 5521224
Running inference benchmark on hardcorenas_c.miil_green_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 15272.06 samples/sec. 16.763 ms/step.
Infer [16/40]. 15270.36 samples/sec. 16.765 ms/step.
Infer [24/40]. 15269.30 samples/sec. 16.766 ms/step.
Infer [32/40]. 15268.29 samples/sec. 16.767 ms/step.
Infer [40/40]. 15268.10 samples/sec. 16.767 ms/step.
Inference benchmark of hardcorenas_c.miil_green_in1k done. 15246.61 samples/sec, 16.77 ms/step
Model hardcorenas_c.miil_green_in1k created, param count: 5521224
Running train benchmark on hardcorenas_c.miil_green_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 2673.85 samples/sec. 95.742 ms/step.
Train [16/40]. 2673.96 samples/sec. 95.738 ms/step.
Train [24/40]. 2674.02 samples/sec. 95.736 ms/step.
Train [32/40]. 2673.97 samples/sec. 95.738 ms/step.
Train [40/40]. 2674.00 samples/sec. 95.737 ms/step.
Train benchmark of hardcorenas_c.miil_green_in1k done. 2655.50 samples/sec, 95.74 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model hardcorenas_d.miil_green_in1k created, param count: 7500208
Running inference benchmark on hardcorenas_d.miil_green_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 14302.90 samples/sec. 17.898 ms/step.
Infer [16/40]. 14301.17 samples/sec. 17.901 ms/step.
Infer [24/40]. 14300.79 samples/sec. 17.901 ms/step.
Infer [32/40]. 14300.60 samples/sec. 17.901 ms/step.
Infer [40/40]. 14300.59 samples/sec. 17.901 ms/step.
Inference benchmark of hardcorenas_d.miil_green_in1k done. 14281.95 samples/sec, 17.90 ms/step
Model hardcorenas_d.miil_green_in1k created, param count: 7500208
Running train benchmark on hardcorenas_d.miil_green_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 2589.81 samples/sec. 98.849 ms/step.
Train [16/40]. 2589.97 samples/sec. 98.843 ms/step.
Train [24/40]. 2590.07 samples/sec. 98.839 ms/step.
Train [32/40]. 2590.01 samples/sec. 98.841 ms/step.
Train [40/40]. 2589.97 samples/sec. 98.843 ms/step.
Train benchmark of hardcorenas_d.miil_green_in1k done. 2569.27 samples/sec, 98.84 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model hardcorenas_e.miil_green_in1k created, param count: 8070992
Running inference benchmark on hardcorenas_e.miil_green_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 11881.56 samples/sec. 21.546 ms/step.
Infer [16/40]. 11880.94 samples/sec. 21.547 ms/step.
Infer [24/40]. 11880.76 samples/sec. 21.547 ms/step.
Infer [32/40]. 11880.42 samples/sec. 21.548 ms/step.
Infer [40/40]. 11880.77 samples/sec. 21.547 ms/step.
Inference benchmark of hardcorenas_e.miil_green_in1k done. 11867.61 samples/sec, 21.55 ms/step
Model hardcorenas_e.miil_green_in1k created, param count: 8070992
Running train benchmark on hardcorenas_e.miil_green_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 2254.59 samples/sec. 113.546 ms/step.
Train [16/40]. 2254.58 samples/sec. 113.547 ms/step.
Train [24/40]. 2254.60 samples/sec. 113.546 ms/step.
Train [32/40]. 2254.55 samples/sec. 113.548 ms/step.
Train [40/40]. 2254.62 samples/sec. 113.545 ms/step.
Train benchmark of hardcorenas_e.miil_green_in1k done. 2239.37 samples/sec, 113.55 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model hardcorenas_f.miil_green_in1k created, param count: 8199688
Running inference benchmark on hardcorenas_f.miil_green_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 11918.02 samples/sec. 21.480 ms/step.
Infer [16/40]. 11917.30 samples/sec. 21.481 ms/step.
Infer [24/40]. 11917.44 samples/sec. 21.481 ms/step.
Infer [32/40]. 11917.26 samples/sec. 21.481 ms/step.
Infer [40/40]. 11917.40 samples/sec. 21.481 ms/step.
Inference benchmark of hardcorenas_f.miil_green_in1k done. 11904.36 samples/sec, 21.48 ms/step
Model hardcorenas_f.miil_green_in1k created, param count: 8199688
Running train benchmark on hardcorenas_f.miil_green_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 2298.94 samples/sec. 111.356 ms/step.
Train [16/40]. 2298.85 samples/sec. 111.360 ms/step.
Train [24/40]. 2298.84 samples/sec. 111.360 ms/step.
Train [32/40]. 2298.78 samples/sec. 111.363 ms/step.
Train [40/40]. 2298.67 samples/sec. 111.369 ms/step.
Train benchmark of hardcorenas_f.miil_green_in1k done. 2282.84 samples/sec, 111.37 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model hrnet_w18.ms_aug_in1k created, param count: 21299004
Running inference benchmark on hrnet_w18.ms_aug_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3092.89 samples/sec. 82.771 ms/step.
Infer [16/40]. 3092.65 samples/sec. 82.777 ms/step.
Infer [24/40]. 3092.58 samples/sec. 82.779 ms/step.
Infer [32/40]. 3092.57 samples/sec. 82.779 ms/step.
Infer [40/40]. 3092.59 samples/sec. 82.779 ms/step.
Inference benchmark of hrnet_w18.ms_aug_in1k done. 3091.65 samples/sec, 82.78 ms/step
Model hrnet_w18.ms_aug_in1k created, param count: 21299004
Running train benchmark on hrnet_w18.ms_aug_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 601.90 samples/sec. 425.321 ms/step.
Train [16/40]. 601.90 samples/sec. 425.322 ms/step.
Train [24/40]. 601.90 samples/sec. 425.321 ms/step.
Train [32/40]. 601.90 samples/sec. 425.322 ms/step.
Train [40/40]. 601.90 samples/sec. 425.320 ms/step.
Train benchmark of hrnet_w18.ms_aug_in1k done. 597.73 samples/sec, 425.32 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model hrnet_w18.ms_in1k created, param count: 21299004
Running inference benchmark on hrnet_w18.ms_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3087.86 samples/sec. 82.905 ms/step.
Infer [16/40]. 3087.97 samples/sec. 82.902 ms/step.
Infer [24/40]. 3087.95 samples/sec. 82.903 ms/step.
Infer [32/40]. 3087.90 samples/sec. 82.904 ms/step.
Infer [40/40]. 3087.85 samples/sec. 82.906 ms/step.
Inference benchmark of hrnet_w18.ms_in1k done. 3086.93 samples/sec, 82.91 ms/step
Model hrnet_w18.ms_in1k created, param count: 21299004
Running train benchmark on hrnet_w18.ms_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 600.67 samples/sec. 426.190 ms/step.
Train [16/40]. 600.67 samples/sec. 426.194 ms/step.
Train [24/40]. 600.66 samples/sec. 426.196 ms/step.
Train [32/40]. 600.66 samples/sec. 426.198 ms/step.
Train [40/40]. 600.66 samples/sec. 426.196 ms/step.
Train benchmark of hrnet_w18.ms_in1k done. 596.48 samples/sec, 426.20 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model hrnet_w18_small.ms_in1k created, param count: 13187464
Running inference benchmark on hrnet_w18_small.ms_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 9380.20 samples/sec. 27.292 ms/step.
Infer [16/40]. 9378.08 samples/sec. 27.298 ms/step.
Infer [24/40]. 9377.82 samples/sec. 27.298 ms/step.
Infer [32/40]. 9377.57 samples/sec. 27.299 ms/step.
Infer [40/40]. 9377.28 samples/sec. 27.300 ms/step.
Inference benchmark of hrnet_w18_small.ms_in1k done. 9368.90 samples/sec, 27.30 ms/step
Model hrnet_w18_small.ms_in1k created, param count: 13187464
Running train benchmark on hrnet_w18_small.ms_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 2039.00 samples/sec. 125.552 ms/step.
Train [16/40]. 2039.03 samples/sec. 125.550 ms/step.
Train [24/40]. 2039.10 samples/sec. 125.546 ms/step.
Train [32/40]. 2039.12 samples/sec. 125.545 ms/step.
Train [40/40]. 2039.12 samples/sec. 125.545 ms/step.
Train benchmark of hrnet_w18_small.ms_in1k done. 2024.54 samples/sec, 125.55 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model hrnet_w18_small_v2.ms_in1k created, param count: 15597464
Running inference benchmark on hrnet_w18_small_v2.ms_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 5125.50 samples/sec. 49.946 ms/step.
Infer [16/40]. 5125.42 samples/sec. 49.947 ms/step.
Infer [24/40]. 5125.31 samples/sec. 49.948 ms/step.
Infer [32/40]. 5125.12 samples/sec. 49.950 ms/step.
Infer [40/40]. 5124.97 samples/sec. 49.951 ms/step.
Inference benchmark of hrnet_w18_small_v2.ms_in1k done. 5122.37 samples/sec, 49.95 ms/step
Model hrnet_w18_small_v2.ms_in1k created, param count: 15597464
Running train benchmark on hrnet_w18_small_v2.ms_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1105.22 samples/sec. 231.627 ms/step.
Train [16/40]. 1105.21 samples/sec. 231.629 ms/step.
Train [24/40]. 1105.22 samples/sec. 231.628 ms/step.
Train [32/40]. 1105.22 samples/sec. 231.628 ms/step.
Train [40/40]. 1105.22 samples/sec. 231.628 ms/step.
Train benchmark of hrnet_w18_small_v2.ms_in1k done. 1097.74 samples/sec, 231.63 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model hrnet_w18_ssld.paddle_in1k created, param count: 21295164
Running inference benchmark on hrnet_w18_ssld.paddle_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 1729.12 samples/sec. 148.052 ms/step.
Infer [16/40]. 1729.03 samples/sec. 148.060 ms/step.
Infer [24/40]. 1728.99 samples/sec. 148.063 ms/step.
Infer [32/40]. 1728.96 samples/sec. 148.066 ms/step.
Infer [40/40]. 1728.94 samples/sec. 148.068 ms/step.
Inference benchmark of hrnet_w18_ssld.paddle_in1k done. 1728.61 samples/sec, 148.07 ms/step
Model hrnet_w18_ssld.paddle_in1k created, param count: 21295164
Running train benchmark on hrnet_w18_ssld.paddle_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 324.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 108.06 MiB is free. Including non-PyTorch memory, this process has 23.54 GiB memory in use. Of the allocated memory 22.07 GiB is allocated by PyTorch, and 240.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model hrnet_w18_ssld.paddle_in1k created, param count: 21295164
Running train benchmark on hrnet_w18_ssld.paddle_in1k for 40 steps w/ input size (3, 288, 288) and batch size 192.
Train [8/40]. 363.35 samples/sec. 528.411 ms/step.
Train [16/40]. 363.36 samples/sec. 528.400 ms/step.
Train [24/40]. 363.37 samples/sec. 528.390 ms/step.
Train [32/40]. 363.37 samples/sec. 528.387 ms/step.
Train [40/40]. 363.37 samples/sec. 528.393 ms/step.
Train benchmark of hrnet_w18_ssld.paddle_in1k done. 361.12 samples/sec, 528.39 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model hrnet_w30.ms_in1k created, param count: 37712220
Running inference benchmark on hrnet_w30.ms_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 2025.49 samples/sec. 126.389 ms/step.
Infer [16/40]. 2025.47 samples/sec. 126.390 ms/step.
Infer [24/40]. 2025.47 samples/sec. 126.391 ms/step.
Infer [32/40]. 2025.43 samples/sec. 126.393 ms/step.
Infer [40/40]. 2025.40 samples/sec. 126.395 ms/step.
Inference benchmark of hrnet_w30.ms_in1k done. 2024.97 samples/sec, 126.39 ms/step
Model hrnet_w30.ms_in1k created, param count: 37712220
Running train benchmark on hrnet_w30.ms_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 470.36 samples/sec. 544.265 ms/step.
Train [16/40]. 470.36 samples/sec. 544.260 ms/step.
Train [24/40]. 470.36 samples/sec. 544.262 ms/step.
Train [32/40]. 470.35 samples/sec. 544.270 ms/step.
Train [40/40]. 470.35 samples/sec. 544.274 ms/step.
Train benchmark of hrnet_w30.ms_in1k done. 467.63 samples/sec, 544.27 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model hrnet_w32.ms_in1k created, param count: 41232680
Running inference benchmark on hrnet_w32.ms_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 2130.03 samples/sec. 120.186 ms/step.
Infer [16/40]. 2130.01 samples/sec. 120.187 ms/step.
Infer [24/40]. 2130.00 samples/sec. 120.188 ms/step.
Infer [32/40]. 2129.97 samples/sec. 120.190 ms/step.
Infer [40/40]. 2129.96 samples/sec. 120.190 ms/step.
Inference benchmark of hrnet_w32.ms_in1k done. 2129.48 samples/sec, 120.19 ms/step
Model hrnet_w32.ms_in1k created, param count: 41232680
Running train benchmark on hrnet_w32.ms_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 478.49 samples/sec. 535.021 ms/step.
Train [16/40]. 478.48 samples/sec. 535.031 ms/step.
Train [24/40]. 478.48 samples/sec. 535.027 ms/step.
Train [32/40]. 478.48 samples/sec. 535.028 ms/step.
Train [40/40]. 478.48 samples/sec. 535.025 ms/step.
Train benchmark of hrnet_w32.ms_in1k done. 475.56 samples/sec, 535.02 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model hrnet_w40.ms_in1k created, param count: 57557160
Running inference benchmark on hrnet_w40.ms_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1543.27 samples/sec. 165.881 ms/step.
Infer [16/40]. 1543.24 samples/sec. 165.885 ms/step.
Infer [24/40]. 1543.26 samples/sec. 165.883 ms/step.
Infer [32/40]. 1543.26 samples/sec. 165.883 ms/step.
Infer [40/40]. 1543.26 samples/sec. 165.883 ms/step.
Inference benchmark of hrnet_w40.ms_in1k done. 1542.99 samples/sec, 165.88 ms/step
Model hrnet_w40.ms_in1k created, param count: 57557160
Running train benchmark on hrnet_w40.ms_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 196.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 22.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 319.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model hrnet_w40.ms_in1k created, param count: 57557160
Running train benchmark on hrnet_w40.ms_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
Train [8/40]. 391.41 samples/sec. 490.536 ms/step.
Train [16/40]. 391.40 samples/sec. 490.546 ms/step.
Train [24/40]. 391.40 samples/sec. 490.548 ms/step.
Train [32/40]. 391.40 samples/sec. 490.552 ms/step.
Train [40/40]. 391.40 samples/sec. 490.548 ms/step.
Train benchmark of hrnet_w40.ms_in1k done. 388.78 samples/sec, 490.55 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model hrnet_w44.ms_in1k created, param count: 67064984
Running inference benchmark on hrnet_w44.ms_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1367.27 samples/sec. 187.234 ms/step.
Infer [16/40]. 1367.26 samples/sec. 187.235 ms/step.
Infer [24/40]. 1367.23 samples/sec. 187.240 ms/step.
Infer [32/40]. 1367.21 samples/sec. 187.243 ms/step.
Infer [40/40]. 1367.19 samples/sec. 187.245 ms/step.
Inference benchmark of hrnet_w44.ms_in1k done. 1366.99 samples/sec, 187.25 ms/step
Model hrnet_w44.ms_in1k created, param count: 67064984
Running train benchmark on hrnet_w44.ms_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 68.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 2.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 22.12 GiB is allocated by PyTorch, and 294.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model hrnet_w44.ms_in1k created, param count: 67064984
Running train benchmark on hrnet_w44.ms_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
Train [8/40]. 351.07 samples/sec. 546.898 ms/step.
Train [16/40]. 351.08 samples/sec. 546.890 ms/step.
Train [24/40]. 351.08 samples/sec. 546.889 ms/step.
Train [32/40]. 351.08 samples/sec. 546.891 ms/step.
Train [40/40]. 351.07 samples/sec. 546.896 ms/step.
Train benchmark of hrnet_w44.ms_in1k done. 348.96 samples/sec, 546.90 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model hrnet_w48.ms_in1k created, param count: 77469864
Running inference benchmark on hrnet_w48.ms_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1310.44 samples/sec. 195.354 ms/step.
Infer [16/40]. 1310.45 samples/sec. 195.352 ms/step.
Infer [24/40]. 1310.44 samples/sec. 195.355 ms/step.
Infer [32/40]. 1310.40 samples/sec. 195.360 ms/step.
Infer [40/40]. 1310.41 samples/sec. 195.359 ms/step.
Inference benchmark of hrnet_w48.ms_in1k done. 1310.22 samples/sec, 195.36 ms/step
Model hrnet_w48.ms_in1k created, param count: 77469864
Running train benchmark on hrnet_w48.ms_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 74.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 52.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 22.22 GiB is allocated by PyTorch, and 145.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model hrnet_w48.ms_in1k created, param count: 77469864
Running train benchmark on hrnet_w48.ms_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
Train [8/40]. 343.54 samples/sec. 558.885 ms/step.
Train [16/40]. 343.55 samples/sec. 558.877 ms/step.
Train [24/40]. 343.55 samples/sec. 558.871 ms/step.
Train [32/40]. 343.55 samples/sec. 558.874 ms/step.
Train [40/40]. 343.54 samples/sec. 558.879 ms/step.
Train benchmark of hrnet_w48.ms_in1k done. 341.46 samples/sec, 558.88 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model hrnet_w48_ssld.paddle_in1k created, param count: 77466024
Running inference benchmark on hrnet_w48_ssld.paddle_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 746.49 samples/sec. 342.939 ms/step.
Infer [16/40]. 746.48 samples/sec. 342.943 ms/step.
Infer [24/40]. 746.47 samples/sec. 342.946 ms/step.
Infer [32/40]. 746.47 samples/sec. 342.949 ms/step.
Infer [40/40]. 746.46 samples/sec. 342.950 ms/step.
Inference benchmark of hrnet_w48_ssld.paddle_in1k done. 746.40 samples/sec, 342.95 ms/step
Model hrnet_w48_ssld.paddle_in1k created, param count: 77466024
Running train benchmark on hrnet_w48_ssld.paddle_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 122.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 84.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 22.24 GiB is allocated by PyTorch, and 85.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model hrnet_w48_ssld.paddle_in1k created, param count: 77466024
Running train benchmark on hrnet_w48_ssld.paddle_in1k for 40 steps w/ input size (3, 288, 288) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 92.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 84.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 22.14 GiB is allocated by PyTorch, and 196.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model hrnet_w48_ssld.paddle_in1k created, param count: 77466024
Running train benchmark on hrnet_w48_ssld.paddle_in1k for 40 steps w/ input size (3, 288, 288) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 28.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 314.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model hrnet_w48_ssld.paddle_in1k created, param count: 77466024
Running train benchmark on hrnet_w48_ssld.paddle_in1k for 40 steps w/ input size (3, 288, 288) and batch size 96.
Train [8/40]. 213.25 samples/sec. 450.184 ms/step.
Train [16/40]. 213.24 samples/sec. 450.188 ms/step.
Train [24/40]. 213.24 samples/sec. 450.192 ms/step.
Train [32/40]. 213.24 samples/sec. 450.196 ms/step.
Train [40/40]. 213.24 samples/sec. 450.198 ms/step.
Train benchmark of hrnet_w48_ssld.paddle_in1k done. 211.69 samples/sec, 450.20 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model hrnet_w64.ms_in1k created, param count: 128059944
Running inference benchmark on hrnet_w64.ms_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 715.67 samples/sec. 357.708 ms/step.
Infer [16/40]. 715.65 samples/sec. 357.718 ms/step.
Infer [24/40]. 715.66 samples/sec. 357.712 ms/step.
Infer [32/40]. 715.48 samples/sec. 357.804 ms/step.
Infer [40/40]. 715.43 samples/sec. 357.829 ms/step.
Inference benchmark of hrnet_w64.ms_in1k done. 715.36 samples/sec, 357.83 ms/step
Model hrnet_w64.ms_in1k created, param count: 128059944
Running train benchmark on hrnet_w64.ms_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 48.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 22.03 GiB is allocated by PyTorch, and 337.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model hrnet_w64.ms_in1k created, param count: 128059944
Running train benchmark on hrnet_w64.ms_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 74.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 56.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 22.17 GiB is allocated by PyTorch, and 194.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model hrnet_w64.ms_in1k created, param count: 128059944
Running train benchmark on hrnet_w64.ms_in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
Train [8/40]. 285.08 samples/sec. 448.995 ms/step.
Train [16/40]. 285.09 samples/sec. 448.980 ms/step.
Train [24/40]. 285.08 samples/sec. 448.993 ms/step.
Train [32/40]. 285.08 samples/sec. 449.002 ms/step.
Train [40/40]. 285.07 samples/sec. 449.010 ms/step.
Train benchmark of hrnet_w64.ms_in1k done. 282.97 samples/sec, 449.01 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model inception_resnet_v2.tf_ens_adv_in1k created, param count: 55843464
Running inference benchmark on inception_resnet_v2.tf_ens_adv_in1k for 40 steps w/ input size (3, 299, 299) and batch size 256.
Infer [8/40]. 1438.30 samples/sec. 177.988 ms/step.
Infer [16/40]. 1438.27 samples/sec. 177.991 ms/step.
Infer [24/40]. 1438.27 samples/sec. 177.991 ms/step.
Infer [32/40]. 1438.28 samples/sec. 177.990 ms/step.
Infer [40/40]. 1438.28 samples/sec. 177.990 ms/step.
Inference benchmark of inception_resnet_v2.tf_ens_adv_in1k done. 1438.02 samples/sec, 177.99 ms/step
Model inception_resnet_v2.tf_ens_adv_in1k created, param count: 55843464
Running train benchmark on inception_resnet_v2.tf_ens_adv_in1k for 40 steps w/ input size (3, 299, 299) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 66.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 6.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.20 GiB is allocated by PyTorch, and 206.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model inception_resnet_v2.tf_ens_adv_in1k created, param count: 55843464
Running train benchmark on inception_resnet_v2.tf_ens_adv_in1k for 40 steps w/ input size (3, 299, 299) and batch size 192.
Train [8/40]. 431.44 samples/sec. 445.021 ms/step.
Train [16/40]. 431.43 samples/sec. 445.037 ms/step.
Train [24/40]. 431.42 samples/sec. 445.044 ms/step.
Train [32/40]. 431.42 samples/sec. 445.039 ms/step.
Train [40/40]. 431.43 samples/sec. 445.036 ms/step.
Train benchmark of inception_resnet_v2.tf_ens_adv_in1k done. 428.87 samples/sec, 445.04 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model inception_resnet_v2.tf_in1k created, param count: 55843464
Running inference benchmark on inception_resnet_v2.tf_in1k for 40 steps w/ input size (3, 299, 299) and batch size 256.
Infer [8/40]. 1438.42 samples/sec. 177.973 ms/step.
Infer [16/40]. 1438.42 samples/sec. 177.973 ms/step.
Infer [24/40]. 1438.42 samples/sec. 177.972 ms/step.
Infer [32/40]. 1438.38 samples/sec. 177.977 ms/step.
Infer [40/40]. 1438.39 samples/sec. 177.977 ms/step.
Inference benchmark of inception_resnet_v2.tf_in1k done. 1438.13 samples/sec, 177.98 ms/step
Model inception_resnet_v2.tf_in1k created, param count: 55843464
Running train benchmark on inception_resnet_v2.tf_in1k for 40 steps w/ input size (3, 299, 299) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 66.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 6.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.20 GiB is allocated by PyTorch, and 206.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model inception_resnet_v2.tf_in1k created, param count: 55843464
Running train benchmark on inception_resnet_v2.tf_in1k for 40 steps w/ input size (3, 299, 299) and batch size 192.
Train [8/40]. 431.09 samples/sec. 445.386 ms/step.
Train [16/40]. 431.09 samples/sec. 445.382 ms/step.
Train [24/40]. 431.09 samples/sec. 445.380 ms/step.
Train [32/40]. 431.10 samples/sec. 445.377 ms/step.
Train [40/40]. 431.09 samples/sec. 445.384 ms/step.
Train benchmark of inception_resnet_v2.tf_in1k done. 428.49 samples/sec, 445.38 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model inception_v3.gluon_in1k created, param count: 23834568
Running inference benchmark on inception_v3.gluon_in1k for 40 steps w/ input size (3, 299, 299) and batch size 256.
Infer [8/40]. 3904.79 samples/sec. 65.561 ms/step.
Infer [16/40]. 3905.17 samples/sec. 65.554 ms/step.
Infer [24/40]. 3905.28 samples/sec. 65.552 ms/step.
Infer [32/40]. 3905.23 samples/sec. 65.553 ms/step.
Infer [40/40]. 3905.14 samples/sec. 65.555 ms/step.
Inference benchmark of inception_v3.gluon_in1k done. 3903.50 samples/sec, 65.56 ms/step
Model inception_v3.gluon_in1k created, param count: 23834568
Running train benchmark on inception_v3.gluon_in1k for 40 steps w/ input size (3, 299, 299) and batch size 256.
Train [8/40]. 949.99 samples/sec. 269.477 ms/step.
Train [16/40]. 949.97 samples/sec. 269.484 ms/step.
Train [24/40]. 949.95 samples/sec. 269.488 ms/step.
Train [32/40]. 949.94 samples/sec. 269.490 ms/step.
Train [40/40]. 949.95 samples/sec. 269.487 ms/step.
Train benchmark of inception_v3.gluon_in1k done. 945.23 samples/sec, 269.49 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model inception_v3.tf_adv_in1k created, param count: 23834568
Running inference benchmark on inception_v3.tf_adv_in1k for 40 steps w/ input size (3, 299, 299) and batch size 256.
Infer [8/40]. 3909.82 samples/sec. 65.476 ms/step.
Infer [16/40]. 3908.16 samples/sec. 65.504 ms/step.
Infer [24/40]. 3907.01 samples/sec. 65.523 ms/step.
Infer [32/40]. 3906.42 samples/sec. 65.533 ms/step.
Infer [40/40]. 3906.08 samples/sec. 65.539 ms/step.
Inference benchmark of inception_v3.tf_adv_in1k done. 3904.51 samples/sec, 65.54 ms/step
Model inception_v3.tf_adv_in1k created, param count: 23834568
Running train benchmark on inception_v3.tf_adv_in1k for 40 steps w/ input size (3, 299, 299) and batch size 256.
Train [8/40]. 949.88 samples/sec. 269.506 ms/step.
Train [16/40]. 949.77 samples/sec. 269.539 ms/step.
Train [24/40]. 949.70 samples/sec. 269.558 ms/step.
Train [32/40]. 949.72 samples/sec. 269.554 ms/step.
Train [40/40]. 949.73 samples/sec. 269.550 ms/step.
Train benchmark of inception_v3.tf_adv_in1k done. 944.91 samples/sec, 269.55 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model inception_v3.tf_in1k created, param count: 23834568
Running inference benchmark on inception_v3.tf_in1k for 40 steps w/ input size (3, 299, 299) and batch size 256.
Infer [8/40]. 3904.46 samples/sec. 65.566 ms/step.
Infer [16/40]. 3904.57 samples/sec. 65.564 ms/step.
Infer [24/40]. 3904.66 samples/sec. 65.563 ms/step.
Infer [32/40]. 3904.69 samples/sec. 65.562 ms/step.
Infer [40/40]. 3904.65 samples/sec. 65.563 ms/step.
Inference benchmark of inception_v3.tf_in1k done. 3903.09 samples/sec, 65.56 ms/step
Model inception_v3.tf_in1k created, param count: 23834568
Running train benchmark on inception_v3.tf_in1k for 40 steps w/ input size (3, 299, 299) and batch size 256.
Train [8/40]. 949.89 samples/sec. 269.506 ms/step.
Train [16/40]. 949.86 samples/sec. 269.514 ms/step.
Train [24/40]. 949.85 samples/sec. 269.517 ms/step.
Train [32/40]. 949.86 samples/sec. 269.512 ms/step.
Train [40/40]. 949.84 samples/sec. 269.520 ms/step.
Train benchmark of inception_v3.tf_in1k done. 945.20 samples/sec, 269.52 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model inception_v3.tv_in1k created, param count: 23834568
Running inference benchmark on inception_v3.tv_in1k for 40 steps w/ input size (3, 299, 299) and batch size 256.
Infer [8/40]. 3905.71 samples/sec. 65.545 ms/step.
Infer [16/40]. 3905.44 samples/sec. 65.550 ms/step.
Infer [24/40]. 3905.40 samples/sec. 65.550 ms/step.
Infer [32/40]. 3905.34 samples/sec. 65.551 ms/step.
Infer [40/40]. 3905.29 samples/sec. 65.552 ms/step.
Inference benchmark of inception_v3.tv_in1k done. 3903.76 samples/sec, 65.55 ms/step
Model inception_v3.tv_in1k created, param count: 23834568
Running train benchmark on inception_v3.tv_in1k for 40 steps w/ input size (3, 299, 299) and batch size 256.
Train [8/40]. 949.67 samples/sec. 269.568 ms/step.
Train [16/40]. 949.61 samples/sec. 269.584 ms/step.
Train [24/40]. 949.64 samples/sec. 269.576 ms/step.
Train [32/40]. 949.61 samples/sec. 269.583 ms/step.
Train [40/40]. 949.63 samples/sec. 269.578 ms/step.
Train benchmark of inception_v3.tv_in1k done. 944.79 samples/sec, 269.58 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model inception_v4.tf_in1k created, param count: 42679816
Running inference benchmark on inception_v4.tf_in1k for 40 steps w/ input size (3, 299, 299) and batch size 256.
Infer [8/40]. 2007.26 samples/sec. 127.537 ms/step.
Infer [16/40]. 2007.28 samples/sec. 127.536 ms/step.
Infer [24/40]. 2007.25 samples/sec. 127.538 ms/step.
Infer [32/40]. 2007.24 samples/sec. 127.538 ms/step.
Infer [40/40]. 2007.27 samples/sec. 127.537 ms/step.
Inference benchmark of inception_v4.tf_in1k done. 2006.77 samples/sec, 127.54 ms/step
Model inception_v4.tf_in1k created, param count: 42679816
Running train benchmark on inception_v4.tf_in1k for 40 steps w/ input size (3, 299, 299) and batch size 256.
Train [8/40]. 520.11 samples/sec. 492.205 ms/step.
Train [16/40]. 520.09 samples/sec. 492.220 ms/step.
Train [24/40]. 520.08 samples/sec. 492.235 ms/step.
Train [32/40]. 520.05 samples/sec. 492.256 ms/step.
Train [40/40]. 520.01 samples/sec. 492.300 ms/step.
Train benchmark of inception_v4.tf_in1k done. 517.89 samples/sec, 492.30 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model lambda_resnet26rpt_256.c1_in1k created, param count: 10988688
Running inference benchmark on lambda_resnet26rpt_256.c1_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
Infer [8/40]. 3255.62 samples/sec. 78.633 ms/step.
Infer [16/40]. 3255.58 samples/sec. 78.634 ms/step.
Infer [24/40]. 3255.63 samples/sec. 78.633 ms/step.
Infer [32/40]. 3255.73 samples/sec. 78.631 ms/step.
Infer [40/40]. 3255.80 samples/sec. 78.629 ms/step.
Inference benchmark of lambda_resnet26rpt_256.c1_in1k done. 3254.70 samples/sec, 78.63 ms/step
Model lambda_resnet26rpt_256.c1_in1k created, param count: 10988688
Running train benchmark on lambda_resnet26rpt_256.c1_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 3.91 GiB is free. Including non-PyTorch memory, this process has 19.73 GiB memory in use. Of the allocated memory 17.50 GiB is allocated by PyTorch, and 1015.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model lambda_resnet26rpt_256.c1_in1k created, param count: 10988688
Running train benchmark on lambda_resnet26rpt_256.c1_in1k for 40 steps w/ input size (3, 256, 256) and batch size 192.
Train [8/40]. 1036.35 samples/sec. 185.265 ms/step.
Train [16/40]. 1036.31 samples/sec. 185.273 ms/step.
Train [24/40]. 1036.31 samples/sec. 185.273 ms/step.
Train [32/40]. 1036.27 samples/sec. 185.279 ms/step.
Train [40/40]. 1036.28 samples/sec. 185.277 ms/step.
Train benchmark of lambda_resnet26rpt_256.c1_in1k done. 1031.88 samples/sec, 185.28 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model lambda_resnet26t.c1_in1k created, param count: 10958272
Running inference benchmark on lambda_resnet26t.c1_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
Infer [8/40]. 4265.11 samples/sec. 60.022 ms/step.
Infer [16/40]. 4265.07 samples/sec. 60.023 ms/step.
Infer [24/40]. 4264.95 samples/sec. 60.024 ms/step.
Infer [32/40]. 4264.84 samples/sec. 60.026 ms/step.
Infer [40/40]. 4264.70 samples/sec. 60.028 ms/step.
Inference benchmark of lambda_resnet26t.c1_in1k done. 4262.81 samples/sec, 60.03 ms/step
Model lambda_resnet26t.c1_in1k created, param count: 10958272
Running train benchmark on lambda_resnet26t.c1_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
Train [8/40]. 1140.65 samples/sec. 224.434 ms/step.
Train [16/40]. 1140.72 samples/sec. 224.419 ms/step.
Train [24/40]. 1140.73 samples/sec. 224.417 ms/step.
Train [32/40]. 1140.74 samples/sec. 224.417 ms/step.
Train [40/40]. 1140.74 samples/sec. 224.416 ms/step.
Train benchmark of lambda_resnet26t.c1_in1k done. 1136.63 samples/sec, 224.42 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model lambda_resnet50ts.a1h_in1k created, param count: 21536832
Running inference benchmark on lambda_resnet50ts.a1h_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
Infer [8/40]. 2833.11 samples/sec. 90.360 ms/step.
Infer [16/40]. 2833.04 samples/sec. 90.362 ms/step.
Infer [24/40]. 2833.00 samples/sec. 90.364 ms/step.
Infer [32/40]. 2833.01 samples/sec. 90.363 ms/step.
Infer [40/40]. 2833.03 samples/sec. 90.363 ms/step.
Inference benchmark of lambda_resnet50ts.a1h_in1k done. 2832.13 samples/sec, 90.36 ms/step
Model lambda_resnet50ts.a1h_in1k created, param count: 21536832
Running train benchmark on lambda_resnet50ts.a1h_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 512.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 478.06 MiB is free. Including non-PyTorch memory, this process has 23.17 GiB memory in use. Of the allocated memory 21.90 GiB is allocated by PyTorch, and 48.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model lambda_resnet50ts.a1h_in1k created, param count: 21536832
Running train benchmark on lambda_resnet50ts.a1h_in1k for 40 steps w/ input size (3, 256, 256) and batch size 192.
Train [8/40]. 747.37 samples/sec. 256.900 ms/step.
Train [16/40]. 747.34 samples/sec. 256.911 ms/step.
Train [24/40]. 747.36 samples/sec. 256.903 ms/step.
Train [32/40]. 747.37 samples/sec. 256.900 ms/step.
Train [40/40]. 747.38 samples/sec. 256.898 ms/step.
Train benchmark of lambda_resnet50ts.a1h_in1k done. 744.30 samples/sec, 256.90 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model lamhalobotnet50ts_256.a1h_in1k created, param count: 22569824
Running inference benchmark on lamhalobotnet50ts_256.a1h_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
Infer [8/40]. 2617.23 samples/sec. 97.813 ms/step.
Infer [16/40]. 2617.13 samples/sec. 97.817 ms/step.
Infer [24/40]. 2617.09 samples/sec. 97.819 ms/step.
Infer [32/40]. 2617.09 samples/sec. 97.818 ms/step.
Infer [40/40]. 2617.08 samples/sec. 97.819 ms/step.
Inference benchmark of lamhalobotnet50ts_256.a1h_in1k done. 2616.35 samples/sec, 97.82 ms/step
Model lamhalobotnet50ts_256.a1h_in1k created, param count: 22569824
Running train benchmark on lamhalobotnet50ts_256.a1h_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 6.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.26 GiB is allocated by PyTorch, and 151.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model lamhalobotnet50ts_256.a1h_in1k created, param count: 22569824
Running train benchmark on lamhalobotnet50ts_256.a1h_in1k for 40 steps w/ input size (3, 256, 256) and batch size 192.
Train [8/40]. 691.69 samples/sec. 277.582 ms/step.
Train [16/40]. 691.62 samples/sec. 277.610 ms/step.
Train [24/40]. 691.59 samples/sec. 277.622 ms/step.
Train [32/40]. 691.57 samples/sec. 277.630 ms/step.
Train [40/40]. 691.49 samples/sec. 277.662 ms/step.
Train benchmark of lamhalobotnet50ts_256.a1h_in1k done. 688.68 samples/sec, 277.66 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model lcnet_050.ra2_in1k created, param count: 1880856
Running inference benchmark on lcnet_050.ra2_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 42709.79 samples/sec. 5.994 ms/step.
Infer [16/40]. 42669.69 samples/sec. 6.000 ms/step.
Infer [24/40]. 42652.29 samples/sec. 6.002 ms/step.
Infer [32/40]. 42631.01 samples/sec. 6.005 ms/step.
Infer [40/40]. 42624.89 samples/sec. 6.006 ms/step.
Inference benchmark of lcnet_050.ra2_in1k done. 42492.89 samples/sec, 6.01 ms/step
Model lcnet_050.ra2_in1k created, param count: 1880856
Running train benchmark on lcnet_050.ra2_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 6948.93 samples/sec. 36.840 ms/step.
Train [16/40]. 6949.21 samples/sec. 36.839 ms/step.
Train [24/40]. 6949.26 samples/sec. 36.838 ms/step.
Train [32/40]. 6948.63 samples/sec. 36.842 ms/step.
Train [40/40]. 6948.91 samples/sec. 36.840 ms/step.
Train benchmark of lcnet_050.ra2_in1k done. 6873.61 samples/sec, 36.84 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model lcnet_075.ra2_in1k created, param count: 2358288
Running inference benchmark on lcnet_075.ra2_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 37781.45 samples/sec. 6.776 ms/step.
Infer [16/40]. 37775.25 samples/sec. 6.777 ms/step.
Infer [24/40]. 37864.13 samples/sec. 6.761 ms/step.
Infer [32/40]. 38163.66 samples/sec. 6.708 ms/step.
Infer [40/40]. 38348.11 samples/sec. 6.676 ms/step.
Inference benchmark of lcnet_075.ra2_in1k done. 38236.35 samples/sec, 6.68 ms/step
Model lcnet_075.ra2_in1k created, param count: 2358288
Running train benchmark on lcnet_075.ra2_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 5436.77 samples/sec. 47.087 ms/step.
Train [16/40]. 5436.85 samples/sec. 47.086 ms/step.
Train [24/40]. 5437.18 samples/sec. 47.083 ms/step.
Train [32/40]. 5437.01 samples/sec. 47.085 ms/step.
Train [40/40]. 5437.02 samples/sec. 47.085 ms/step.
Train benchmark of lcnet_075.ra2_in1k done. 5390.12 samples/sec, 47.09 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model lcnet_100.ra2_in1k created, param count: 2953800
Running inference benchmark on lcnet_100.ra2_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 30934.36 samples/sec. 8.276 ms/step.
Infer [16/40]. 30929.76 samples/sec. 8.277 ms/step.
Infer [24/40]. 30925.72 samples/sec. 8.278 ms/step.
Infer [32/40]. 30924.25 samples/sec. 8.278 ms/step.
Infer [40/40]. 30921.18 samples/sec. 8.279 ms/step.
Inference benchmark of lcnet_100.ra2_in1k done. 30840.60 samples/sec, 8.28 ms/step
Model lcnet_100.ra2_in1k created, param count: 2953800
Running train benchmark on lcnet_100.ra2_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 4814.13 samples/sec. 53.177 ms/step.
Train [16/40]. 4813.96 samples/sec. 53.179 ms/step.
Train [24/40]. 4813.71 samples/sec. 53.181 ms/step.
Train [32/40]. 4813.71 samples/sec. 53.181 ms/step.
Train [40/40]. 4813.74 samples/sec. 53.181 ms/step.
Train benchmark of lcnet_100.ra2_in1k done. 4776.24 samples/sec, 53.18 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model legacy_senet154.in1k created, param count: 115088984
Running inference benchmark on legacy_senet154.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1032.55 samples/sec. 247.930 ms/step.
Infer [16/40]. 1032.61 samples/sec. 247.916 ms/step.
Infer [24/40]. 1032.59 samples/sec. 247.921 ms/step.
Infer [32/40]. 1032.51 samples/sec. 247.939 ms/step.
Infer [40/40]. 1032.38 samples/sec. 247.971 ms/step.
Inference benchmark of legacy_senet154.in1k done. 1032.25 samples/sec, 247.97 ms/step
Model legacy_senet154.in1k created, param count: 115088984
Running train benchmark on legacy_senet154.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 2.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 286.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model legacy_senet154.in1k created, param count: 115088984
Running train benchmark on legacy_senet154.in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 74.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 28.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 22.12 GiB is allocated by PyTorch, and 270.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model legacy_senet154.in1k created, param count: 115088984
Running train benchmark on legacy_senet154.in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 16.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.01 GiB is allocated by PyTorch, and 394.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model legacy_senet154.in1k created, param count: 115088984
Running train benchmark on legacy_senet154.in1k for 40 steps w/ input size (3, 224, 224) and batch size 96.
Train [8/40]. 311.92 samples/sec. 307.774 ms/step.
Train [16/40]. 311.79 samples/sec. 307.897 ms/step.
Train [24/40]. 311.76 samples/sec. 307.928 ms/step.
Train [32/40]. 311.74 samples/sec. 307.949 ms/step.
Train [40/40]. 311.74 samples/sec. 307.953 ms/step.
Train benchmark of legacy_senet154.in1k done. 309.36 samples/sec, 307.95 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model legacy_seresnet18.in1k created, param count: 11778592
Running inference benchmark on legacy_seresnet18.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 7071.67 samples/sec. 36.201 ms/step.
Infer [16/40]. 7137.42 samples/sec. 35.867 ms/step.
Infer [24/40]. 7098.43 samples/sec. 36.064 ms/step.
Infer [32/40]. 7131.71 samples/sec. 35.896 ms/step.
Infer [40/40]. 7096.33 samples/sec. 36.075 ms/step.
Inference benchmark of legacy_seresnet18.in1k done. 7091.47 samples/sec, 36.08 ms/step
Model legacy_seresnet18.in1k created, param count: 11778592
Running train benchmark on legacy_seresnet18.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 2838.37 samples/sec. 90.193 ms/step.
Train [16/40]. 2831.94 samples/sec. 90.397 ms/step.
Train [24/40]. 2827.74 samples/sec. 90.532 ms/step.
Train [32/40]. 2829.08 samples/sec. 90.489 ms/step.
Train [40/40]. 2826.59 samples/sec. 90.568 ms/step.
Train benchmark of legacy_seresnet18.in1k done. 2812.16 samples/sec, 90.57 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model legacy_seresnet34.in1k created, param count: 21958868
Running inference benchmark on legacy_seresnet34.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 4148.19 samples/sec. 61.714 ms/step.
Infer [16/40]. 4132.73 samples/sec. 61.945 ms/step.
Infer [24/40]. 4138.28 samples/sec. 61.861 ms/step.
Infer [32/40]. 4153.50 samples/sec. 61.635 ms/step.
Infer [40/40]. 4158.85 samples/sec. 61.555 ms/step.
Inference benchmark of legacy_seresnet34.in1k done. 4157.11 samples/sec, 61.55 ms/step
Model legacy_seresnet34.in1k created, param count: 21958868
Running train benchmark on legacy_seresnet34.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1651.94 samples/sec. 154.969 ms/step.
Train [16/40]. 1656.00 samples/sec. 154.589 ms/step.
Train [24/40]. 1654.46 samples/sec. 154.734 ms/step.
Train [32/40]. 1652.70 samples/sec. 154.898 ms/step.
Train [40/40]. 1652.93 samples/sec. 154.876 ms/step.
Train benchmark of legacy_seresnet34.in1k done. 1644.67 samples/sec, 154.88 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model legacy_seresnet50.in1k created, param count: 28088024
Running inference benchmark on legacy_seresnet50.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3046.26 samples/sec. 84.037 ms/step.
Infer [16/40]. 3041.06 samples/sec. 84.181 ms/step.
Infer [24/40]. 3042.94 samples/sec. 84.129 ms/step.
Infer [32/40]. 3041.00 samples/sec. 84.183 ms/step.
Infer [40/40]. 3040.72 samples/sec. 84.190 ms/step.
Inference benchmark of legacy_seresnet50.in1k done. 3039.75 samples/sec, 84.19 ms/step
Model legacy_seresnet50.in1k created, param count: 28088024
Running train benchmark on legacy_seresnet50.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1033.81 samples/sec. 247.627 ms/step.
Train [16/40]. 1034.64 samples/sec. 247.429 ms/step.
Train [24/40]. 1035.34 samples/sec. 247.262 ms/step.
Train [32/40]. 1035.73 samples/sec. 247.170 ms/step.
Train [40/40]. 1035.41 samples/sec. 247.244 ms/step.
Train benchmark of legacy_seresnet50.in1k done. 1030.89 samples/sec, 247.24 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model legacy_seresnet101.in1k created, param count: 49326872
Running inference benchmark on legacy_seresnet101.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1753.87 samples/sec. 145.963 ms/step.
Infer [16/40]. 1749.07 samples/sec. 146.364 ms/step.
Infer [24/40]. 1746.27 samples/sec. 146.598 ms/step.
Infer [32/40]. 1743.40 samples/sec. 146.840 ms/step.
Infer [40/40]. 1743.26 samples/sec. 146.851 ms/step.
Inference benchmark of legacy_seresnet101.in1k done. 1742.91 samples/sec, 146.85 ms/step
Model legacy_seresnet101.in1k created, param count: 49326872
Running train benchmark on legacy_seresnet101.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 636.78 samples/sec. 402.021 ms/step.
Train [16/40]. 636.64 samples/sec. 402.114 ms/step.
Train [24/40]. 636.36 samples/sec. 402.288 ms/step.
Train [32/40]. 636.28 samples/sec. 402.340 ms/step.
Train [40/40]. 636.31 samples/sec. 402.318 ms/step.
Train benchmark of legacy_seresnet101.in1k done. 633.51 samples/sec, 402.32 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model legacy_seresnet152.in1k created, param count: 66821848
Running inference benchmark on legacy_seresnet152.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1211.00 samples/sec. 211.396 ms/step.
Infer [16/40]. 1209.62 samples/sec. 211.638 ms/step.
Infer [24/40]. 1209.34 samples/sec. 211.686 ms/step.
Infer [32/40]. 1207.36 samples/sec. 212.033 ms/step.
Infer [40/40]. 1206.51 samples/sec. 212.182 ms/step.
Inference benchmark of legacy_seresnet152.in1k done. 1206.31 samples/sec, 212.18 ms/step
Model legacy_seresnet152.in1k created, param count: 66821848
Running train benchmark on legacy_seresnet152.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 34.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 22.16 GiB is allocated by PyTorch, and 223.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model legacy_seresnet152.in1k created, param count: 66821848
Running train benchmark on legacy_seresnet152.in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
Train [8/40]. 499.91 samples/sec. 384.068 ms/step.
Train [16/40]. 499.91 samples/sec. 384.066 ms/step.
Train [24/40]. 499.92 samples/sec. 384.063 ms/step.
Train [32/40]. 499.93 samples/sec. 384.051 ms/step.
Train [40/40]. 499.93 samples/sec. 384.051 ms/step.
Train benchmark of legacy_seresnet152.in1k done. 496.78 samples/sec, 384.05 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model legacy_seresnext26_32x4d.in1k created, param count: 16790280
Running inference benchmark on legacy_seresnext26_32x4d.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 5102.52 samples/sec. 50.171 ms/step.
Infer [16/40]. 5101.91 samples/sec. 50.177 ms/step.
Infer [24/40]. 5100.75 samples/sec. 50.189 ms/step.
Infer [32/40]. 5100.12 samples/sec. 50.195 ms/step.
Infer [40/40]. 5099.59 samples/sec. 50.200 ms/step.
Inference benchmark of legacy_seresnext26_32x4d.in1k done. 5096.94 samples/sec, 50.20 ms/step
Model legacy_seresnext26_32x4d.in1k created, param count: 16790280
Running train benchmark on legacy_seresnext26_32x4d.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1463.69 samples/sec. 174.901 ms/step.
Train [16/40]. 1463.65 samples/sec. 174.906 ms/step.
Train [24/40]. 1463.61 samples/sec. 174.910 ms/step.
Train [32/40]. 1463.64 samples/sec. 174.907 ms/step.
Train [40/40]. 1463.64 samples/sec. 174.907 ms/step.
Train benchmark of legacy_seresnext26_32x4d.in1k done. 1457.98 samples/sec, 174.91 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model legacy_seresnext50_32x4d.in1k created, param count: 27559896
Running inference benchmark on legacy_seresnext50_32x4d.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3150.22 samples/sec. 81.264 ms/step.
Infer [16/40]. 3150.23 samples/sec. 81.264 ms/step.
Infer [24/40]. 3150.30 samples/sec. 81.262 ms/step.
Infer [32/40]. 3150.23 samples/sec. 81.264 ms/step.
Infer [40/40]. 3150.23 samples/sec. 81.264 ms/step.
Inference benchmark of legacy_seresnext50_32x4d.in1k done. 3149.15 samples/sec, 81.26 ms/step
Model legacy_seresnext50_32x4d.in1k created, param count: 27559896
Running train benchmark on legacy_seresnext50_32x4d.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 913.27 samples/sec. 280.311 ms/step.
Train [16/40]. 913.25 samples/sec. 280.317 ms/step.
Train [24/40]. 913.28 samples/sec. 280.308 ms/step.
Train [32/40]. 913.26 samples/sec. 280.314 ms/step.
Train [40/40]. 913.26 samples/sec. 280.315 ms/step.
Train benchmark of legacy_seresnext50_32x4d.in1k done. 909.44 samples/sec, 280.31 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model legacy_seresnext101_32x4d.in1k created, param count: 48955416
Running inference benchmark on legacy_seresnext101_32x4d.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 2022.12 samples/sec. 126.600 ms/step.
Infer [16/40]. 2021.29 samples/sec. 126.652 ms/step.
Infer [24/40]. 2020.77 samples/sec. 126.684 ms/step.
Infer [32/40]. 2020.52 samples/sec. 126.700 ms/step.
Infer [40/40]. 2020.40 samples/sec. 126.708 ms/step.
Inference benchmark of legacy_seresnext101_32x4d.in1k done. 2019.92 samples/sec, 126.71 ms/step
Model legacy_seresnext101_32x4d.in1k created, param count: 48955416
Running train benchmark on legacy_seresnext101_32x4d.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 86.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 22.12 GiB is allocated by PyTorch, and 212.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model legacy_seresnext101_32x4d.in1k created, param count: 48955416
Running train benchmark on legacy_seresnext101_32x4d.in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
Train [8/40]. 598.99 samples/sec. 320.542 ms/step.
Train [16/40]. 598.97 samples/sec. 320.551 ms/step.
Train [24/40]. 598.96 samples/sec. 320.558 ms/step.
Train [32/40]. 598.95 samples/sec. 320.560 ms/step.
Train [40/40]. 598.95 samples/sec. 320.559 ms/step.
Train benchmark of legacy_seresnext101_32x4d.in1k done. 595.51 samples/sec, 320.56 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model legacy_xception.tf_in1k created, param count: 22855952
Running inference benchmark on legacy_xception.tf_in1k for 40 steps w/ input size (3, 299, 299) and batch size 256.
Infer [8/40]. 2071.65 samples/sec. 123.573 ms/step.
Infer [16/40]. 2071.68 samples/sec. 123.571 ms/step.
Infer [24/40]. 2071.66 samples/sec. 123.572 ms/step.
Infer [32/40]. 2071.66 samples/sec. 123.573 ms/step.
Infer [40/40]. 2071.65 samples/sec. 123.573 ms/step.
Inference benchmark of legacy_xception.tf_in1k done. 2071.12 samples/sec, 123.57 ms/step
Model legacy_xception.tf_in1k created, param count: 22855952
Running train benchmark on legacy_xception.tf_in1k for 40 steps w/ input size (3, 299, 299) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 130.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 90.06 MiB is free. Including non-PyTorch memory, this process has 23.55 GiB memory in use. Of the allocated memory 22.06 GiB is allocated by PyTorch, and 272.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model legacy_xception.tf_in1k created, param count: 22855952
Running train benchmark on legacy_xception.tf_in1k for 40 steps w/ input size (3, 299, 299) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 28.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 22.01 GiB is allocated by PyTorch, and 384.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model legacy_xception.tf_in1k created, param count: 22855952
Running train benchmark on legacy_xception.tf_in1k for 40 steps w/ input size (3, 299, 299) and batch size 128.
Train [8/40]. 590.35 samples/sec. 216.819 ms/step.
Train [16/40]. 590.41 samples/sec. 216.799 ms/step.
Train [24/40]. 590.33 samples/sec. 216.828 ms/step.
Train [32/40]. 590.21 samples/sec. 216.871 ms/step.
Train [40/40]. 590.16 samples/sec. 216.890 ms/step.
Train benchmark of legacy_xception.tf_in1k done. 587.86 samples/sec, 216.89 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model levit_128.fb_dist_in1k created, param count: 9213936
Running inference benchmark on levit_128.fb_dist_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 14160.53 samples/sec. 18.078 ms/step.
Infer [16/40]. 14163.16 samples/sec. 18.075 ms/step.
Infer [24/40]. 14158.10 samples/sec. 18.082 ms/step.
Infer [32/40]. 14159.29 samples/sec. 18.080 ms/step.
Infer [40/40]. 14159.34 samples/sec. 18.080 ms/step.
Inference benchmark of levit_128.fb_dist_in1k done. 14141.71 samples/sec, 18.08 ms/step
Model levit_128.fb_dist_in1k created, param count: 9213936
Running train benchmark on levit_128.fb_dist_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 4197.80 samples/sec. 60.984 ms/step.
Train [16/40]. 4198.11 samples/sec. 60.980 ms/step.
Train [24/40]. 4197.21 samples/sec. 60.993 ms/step.
Train [32/40]. 4195.41 samples/sec. 61.019 ms/step.
Train [40/40]. 4195.67 samples/sec. 61.015 ms/step.
Train benchmark of levit_128.fb_dist_in1k done. 4134.16 samples/sec, 61.02 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model levit_128s.fb_dist_in1k created, param count: 7777058
Running inference benchmark on levit_128s.fb_dist_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 17081.78 samples/sec. 14.987 ms/step.
Infer [16/40]. 17068.79 samples/sec. 14.998 ms/step.
Infer [24/40]. 17071.65 samples/sec. 14.996 ms/step.
Infer [32/40]. 17067.78 samples/sec. 14.999 ms/step.
Infer [40/40]. 17069.28 samples/sec. 14.998 ms/step.
Inference benchmark of levit_128s.fb_dist_in1k done. 17043.20 samples/sec, 15.00 ms/step
Model levit_128s.fb_dist_in1k created, param count: 7777058
Running train benchmark on levit_128s.fb_dist_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 5121.05 samples/sec. 49.990 ms/step.
Train [16/40]. 5121.58 samples/sec. 49.985 ms/step.
Train [24/40]. 5122.34 samples/sec. 49.977 ms/step.
Train [32/40]. 5067.12 samples/sec. 50.522 ms/step.
Train [40/40]. 5078.33 samples/sec. 50.410 ms/step.
Train benchmark of levit_128s.fb_dist_in1k done. 5001.18 samples/sec, 50.41 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model levit_192.fb_dist_in1k created, param count: 10947069
Running inference benchmark on levit_192.fb_dist_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 14382.40 samples/sec. 17.800 ms/step.
Infer [16/40]. 14384.03 samples/sec. 17.798 ms/step.
Infer [24/40]. 14389.49 samples/sec. 17.791 ms/step.
Infer [32/40]. 14395.75 samples/sec. 17.783 ms/step.
Infer [40/40]. 14396.70 samples/sec. 17.782 ms/step.
Inference benchmark of levit_192.fb_dist_in1k done. 14378.83 samples/sec, 17.78 ms/step
Model levit_192.fb_dist_in1k created, param count: 10947069
Running train benchmark on levit_192.fb_dist_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 3996.82 samples/sec. 64.051 ms/step.
Train [16/40]. 3982.28 samples/sec. 64.285 ms/step.
Train [24/40]. 3987.38 samples/sec. 64.203 ms/step.
Train [32/40]. 3989.75 samples/sec. 64.164 ms/step.
Train [40/40]. 3991.75 samples/sec. 64.132 ms/step.
Train benchmark of levit_192.fb_dist_in1k done. 3936.71 samples/sec, 64.13 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model levit_256.fb_dist_in1k created, param count: 18893876
Running inference benchmark on levit_256.fb_dist_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 14137.80 samples/sec. 18.107 ms/step.
Infer [16/40]. 14202.43 samples/sec. 18.025 ms/step.
Infer [24/40]. 14221.73 samples/sec. 18.001 ms/step.
Infer [32/40]. 14230.71 samples/sec. 17.989 ms/step.
Infer [40/40]. 14237.01 samples/sec. 17.981 ms/step.
Inference benchmark of levit_256.fb_dist_in1k done. 14219.49 samples/sec, 17.98 ms/step
Model levit_256.fb_dist_in1k created, param count: 18893876
Running train benchmark on levit_256.fb_dist_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 3645.13 samples/sec. 70.231 ms/step.
Train [16/40]. 3643.52 samples/sec. 70.262 ms/step.
Train [24/40]. 3642.99 samples/sec. 70.272 ms/step.
Train [32/40]. 3643.69 samples/sec. 70.258 ms/step.
Train [40/40]. 3643.58 samples/sec. 70.261 ms/step.
Train benchmark of levit_256.fb_dist_in1k done. 3596.96 samples/sec, 70.26 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model levit_384.fb_dist_in1k created, param count: 39128836
Running inference benchmark on levit_384.fb_dist_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 8562.58 samples/sec. 29.898 ms/step.
Infer [16/40]. 8561.83 samples/sec. 29.900 ms/step.
Infer [24/40]. 8561.75 samples/sec. 29.900 ms/step.
Infer [32/40]. 8561.89 samples/sec. 29.900 ms/step.
Infer [40/40]. 8561.94 samples/sec. 29.900 ms/step.
Inference benchmark of levit_384.fb_dist_in1k done. 8555.06 samples/sec, 29.90 ms/step
Model levit_384.fb_dist_in1k created, param count: 39128836
Running train benchmark on levit_384.fb_dist_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 2759.84 samples/sec. 92.759 ms/step.
Train [16/40]. 2759.73 samples/sec. 92.763 ms/step.
Train [24/40]. 2759.57 samples/sec. 92.768 ms/step.
Train [32/40]. 2759.60 samples/sec. 92.767 ms/step.
Train [40/40]. 2759.64 samples/sec. 92.766 ms/step.
Train benchmark of levit_384.fb_dist_in1k done. 2731.25 samples/sec, 92.77 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model levit_conv_128.fb_dist_in1k created, param count: 9213936
Running inference benchmark on levit_conv_128.fb_dist_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 14829.15 samples/sec. 17.263 ms/step.
Infer [16/40]. 14814.58 samples/sec. 17.280 ms/step.
Infer [24/40]. 14804.12 samples/sec. 17.292 ms/step.
Infer [32/40]. 14809.82 samples/sec. 17.286 ms/step.
Infer [40/40]. 14811.00 samples/sec. 17.284 ms/step.
Inference benchmark of levit_conv_128.fb_dist_in1k done. 14792.16 samples/sec, 17.28 ms/step
Model levit_conv_128.fb_dist_in1k created, param count: 9213936
Running train benchmark on levit_conv_128.fb_dist_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 4221.39 samples/sec. 60.643 ms/step.
Train [16/40]. 4221.31 samples/sec. 60.645 ms/step.
Train [24/40]. 4221.37 samples/sec. 60.644 ms/step.
Train [32/40]. 4221.75 samples/sec. 60.638 ms/step.
Train [40/40]. 4221.80 samples/sec. 60.638 ms/step.
Train benchmark of levit_conv_128.fb_dist_in1k done. 4169.18 samples/sec, 60.64 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model levit_conv_128s.fb_dist_in1k created, param count: 7777058
Running inference benchmark on levit_conv_128s.fb_dist_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 18138.56 samples/sec. 14.114 ms/step.
Infer [16/40]. 18108.51 samples/sec. 14.137 ms/step.
Infer [24/40]. 18102.75 samples/sec. 14.141 ms/step.
Infer [32/40]. 18107.26 samples/sec. 14.138 ms/step.
Infer [40/40]. 18100.29 samples/sec. 14.143 ms/step.
Inference benchmark of levit_conv_128s.fb_dist_in1k done. 18072.09 samples/sec, 14.14 ms/step
Model levit_conv_128s.fb_dist_in1k created, param count: 7777058
Running train benchmark on levit_conv_128s.fb_dist_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 5278.95 samples/sec. 48.494 ms/step.
Train [16/40]. 5278.38 samples/sec. 48.500 ms/step.
Train [24/40]. 5277.96 samples/sec. 48.504 ms/step.
Train [32/40]. 5278.12 samples/sec. 48.502 ms/step.
Train [40/40]. 5277.46 samples/sec. 48.508 ms/step.
Train benchmark of levit_conv_128s.fb_dist_in1k done. 5206.70 samples/sec, 48.51 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model levit_conv_192.fb_dist_in1k created, param count: 10947069
Running inference benchmark on levit_conv_192.fb_dist_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 14708.73 samples/sec. 17.405 ms/step.
Infer [16/40]. 14705.85 samples/sec. 17.408 ms/step.
Infer [24/40]. 14703.95 samples/sec. 17.410 ms/step.
Infer [32/40]. 14710.70 samples/sec. 17.402 ms/step.
Infer [40/40]. 14708.31 samples/sec. 17.405 ms/step.
Inference benchmark of levit_conv_192.fb_dist_in1k done. 14689.17 samples/sec, 17.41 ms/step
Model levit_conv_192.fb_dist_in1k created, param count: 10947069
Running train benchmark on levit_conv_192.fb_dist_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 3843.40 samples/sec. 66.608 ms/step.
Train [16/40]. 3841.19 samples/sec. 66.646 ms/step.
Train [24/40]. 3840.91 samples/sec. 66.651 ms/step.
Train [32/40]. 3842.10 samples/sec. 66.630 ms/step.
Train [40/40]. 3842.85 samples/sec. 66.617 ms/step.
Train benchmark of levit_conv_192.fb_dist_in1k done. 3796.43 samples/sec, 66.62 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model levit_conv_256.fb_dist_in1k created, param count: 18893876
Running inference benchmark on levit_conv_256.fb_dist_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 13856.12 samples/sec. 18.476 ms/step.
Infer [16/40]. 13851.22 samples/sec. 18.482 ms/step.
Infer [24/40]. 13867.75 samples/sec. 18.460 ms/step.
Infer [32/40]. 13864.50 samples/sec. 18.464 ms/step.
Infer [40/40]. 13867.46 samples/sec. 18.460 ms/step.
Inference benchmark of levit_conv_256.fb_dist_in1k done. 13850.27 samples/sec, 18.46 ms/step
Model levit_conv_256.fb_dist_in1k created, param count: 18893876
Running train benchmark on levit_conv_256.fb_dist_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 3438.81 samples/sec. 74.444 ms/step.
Train [16/40]. 3440.41 samples/sec. 74.410 ms/step.
Train [24/40]. 3440.80 samples/sec. 74.401 ms/step.
Train [32/40]. 3440.27 samples/sec. 74.413 ms/step.
Train [40/40]. 3440.38 samples/sec. 74.410 ms/step.
Train benchmark of levit_conv_256.fb_dist_in1k done. 3402.56 samples/sec, 74.41 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model levit_conv_384.fb_dist_in1k created, param count: 39128836
Running inference benchmark on levit_conv_384.fb_dist_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 8846.87 samples/sec. 28.937 ms/step.
Infer [16/40]. 8847.22 samples/sec. 28.936 ms/step.
Infer [24/40]. 8847.18 samples/sec. 28.936 ms/step.
Infer [32/40]. 8847.23 samples/sec. 28.936 ms/step.
Infer [40/40]. 8847.13 samples/sec. 28.936 ms/step.
Inference benchmark of levit_conv_384.fb_dist_in1k done. 8839.41 samples/sec, 28.94 ms/step
Model levit_conv_384.fb_dist_in1k created, param count: 39128836
Running train benchmark on levit_conv_384.fb_dist_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 2472.41 samples/sec. 103.543 ms/step.
Train [16/40]. 2472.57 samples/sec. 103.536 ms/step.
Train [24/40]. 2472.60 samples/sec. 103.535 ms/step.
Train [32/40]. 2472.56 samples/sec. 103.536 ms/step.
Train [40/40]. 2471.47 samples/sec. 103.582 ms/step.
Train benchmark of levit_conv_384.fb_dist_in1k done. 2451.53 samples/sec, 103.58 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model maxvit_base_tf_224.in1k created, param count: 119467708
Running inference benchmark on maxvit_base_tf_224.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 541.54 samples/sec. 472.727 ms/step.
Infer [16/40]. 541.27 samples/sec. 472.959 ms/step.
Infer [24/40]. 541.19 samples/sec. 473.028 ms/step.
Infer [32/40]. 541.15 samples/sec. 473.069 ms/step.
Infer [40/40]. 541.12 samples/sec. 473.096 ms/step.
Inference benchmark of maxvit_base_tf_224.in1k done. 541.08 samples/sec, 473.10 ms/step
Model maxvit_base_tf_224.in1k created, param count: 119467708
Running train benchmark on maxvit_base_tf_224.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 442.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 366.06 MiB is free. Including non-PyTorch memory, this process has 23.28 GiB memory in use. Of the allocated memory 21.82 GiB is allocated by PyTorch, and 238.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model maxvit_base_tf_224.in1k created, param count: 119467708
Running train benchmark on maxvit_base_tf_224.in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 882.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 872.06 MiB is free. Including non-PyTorch memory, this process has 22.79 GiB memory in use. Of the allocated memory 21.16 GiB is allocated by PyTorch, and 406.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model maxvit_base_tf_224.in1k created, param count: 119467708
Running train benchmark on maxvit_base_tf_224.in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 148.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 96.06 MiB is free. Including non-PyTorch memory, this process has 23.55 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 246.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model maxvit_base_tf_224.in1k created, param count: 119467708
Running train benchmark on maxvit_base_tf_224.in1k for 40 steps w/ input size (3, 224, 224) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 222.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 92.06 MiB is free. Including non-PyTorch memory, this process has 23.55 GiB memory in use. Of the allocated memory 21.84 GiB is allocated by PyTorch, and 492.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model maxvit_base_tf_224.in1k created, param count: 119467708
Running train benchmark on maxvit_base_tf_224.in1k for 40 steps w/ input size (3, 224, 224) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 4.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 21.76 GiB is allocated by PyTorch, and 663.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model maxvit_base_tf_224.in1k created, param count: 119467708
Running train benchmark on maxvit_base_tf_224.in1k for 40 steps w/ input size (3, 224, 224) and batch size 48.
Train [8/40]. 161.98 samples/sec. 296.328 ms/step.
Train [16/40]. 161.99 samples/sec. 296.309 ms/step.
Train [24/40]. 161.99 samples/sec. 296.308 ms/step.
Train [32/40]. 162.02 samples/sec. 296.264 ms/step.
Train [40/40]. 162.05 samples/sec. 296.207 ms/step.
Train benchmark of maxvit_base_tf_224.in1k done. 159.76 samples/sec, 296.21 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model maxvit_base_tf_384.in1k created, param count: 119653468
Running inference benchmark on maxvit_base_tf_384.in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 6.75 GiB. GPU 0 has a total capacty of 23.65 GiB of which 5.79 GiB is free. Including non-PyTorch memory, this process has 17.85 GiB memory in use. Of the allocated memory 15.50 GiB is allocated by PyTorch, and 1.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model maxvit_base_tf_384.in1k created, param count: 119653468
Running inference benchmark on maxvit_base_tf_384.in1k for 40 steps w/ input size (3, 384, 384) and batch size 192.
Infer [8/40]. 162.78 samples/sec. 1179.489 ms/step.
Infer [16/40]. 162.78 samples/sec. 1179.519 ms/step.
Infer [24/40]. 162.77 samples/sec. 1179.543 ms/step.
Infer [32/40]. 162.77 samples/sec. 1179.569 ms/step.
Infer [40/40]. 162.77 samples/sec. 1179.589 ms/step.
Inference benchmark of maxvit_base_tf_384.in1k done. 162.76 samples/sec, 1179.59 ms/step
Model maxvit_base_tf_384.in1k created, param count: 119653468
Running train benchmark on maxvit_base_tf_384.in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 6.75 GiB. GPU 0 has a total capacty of 23.65 GiB of which 164.06 MiB is free. Including non-PyTorch memory, this process has 23.48 GiB memory in use. Of the allocated memory 21.62 GiB is allocated by PyTorch, and 647.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model maxvit_base_tf_384.in1k created, param count: 119653468
Running train benchmark on maxvit_base_tf_384.in1k for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 23.65 GiB of which 450.06 MiB is free. Including non-PyTorch memory, this process has 23.20 GiB memory in use. Of the allocated memory 21.33 GiB is allocated by PyTorch, and 651.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model maxvit_base_tf_384.in1k created, param count: 119653468
Running train benchmark on maxvit_base_tf_384.in1k for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 864.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 844.06 MiB is free. Including non-PyTorch memory, this process has 22.82 GiB memory in use. Of the allocated memory 21.00 GiB is allocated by PyTorch, and 598.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model maxvit_base_tf_384.in1k created, param count: 119653468
Running train benchmark on maxvit_base_tf_384.in1k for 40 steps w/ input size (3, 384, 384) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 648.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 146.06 MiB is free. Including non-PyTorch memory, this process has 23.50 GiB memory in use. Of the allocated memory 21.92 GiB is allocated by PyTorch, and 357.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model maxvit_base_tf_384.in1k created, param count: 119653468
Running train benchmark on maxvit_base_tf_384.in1k for 40 steps w/ input size (3, 384, 384) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 864.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 244.06 MiB is free. Including non-PyTorch memory, this process has 23.40 GiB memory in use. Of the allocated memory 21.96 GiB is allocated by PyTorch, and 217.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model maxvit_base_tf_384.in1k created, param count: 119653468
Running train benchmark on maxvit_base_tf_384.in1k for 40 steps w/ input size (3, 384, 384) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 122.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 82.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 22.17 GiB is allocated by PyTorch, and 163.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model maxvit_base_tf_384.in1k created, param count: 119653468
Running train benchmark on maxvit_base_tf_384.in1k for 40 steps w/ input size (3, 384, 384) and batch size 32.
ERROR: "CUDA out of memory. Tried to allocate 122.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 92.06 MiB is free. Including non-PyTorch memory, this process has 23.55 GiB memory in use. Of the allocated memory 22.07 GiB is allocated by PyTorch, and 254.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model maxvit_base_tf_384.in1k created, param count: 119653468
Running train benchmark on maxvit_base_tf_384.in1k for 40 steps w/ input size (3, 384, 384) and batch size 24.
ERROR: "CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 2.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 21.95 GiB is allocated by PyTorch, and 467.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model maxvit_base_tf_384.in1k created, param count: 119653468
Running train benchmark on maxvit_base_tf_384.in1k for 40 steps w/ input size (3, 384, 384) and batch size 16.
Train [8/40]. 52.32 samples/sec. 305.803 ms/step.
Train [16/40]. 52.31 samples/sec. 305.864 ms/step.
Train [24/40]. 52.31 samples/sec. 305.874 ms/step.
Train [32/40]. 52.30 samples/sec. 305.904 ms/step.
Train [40/40]. 52.30 samples/sec. 305.900 ms/step.
Train benchmark of maxvit_base_tf_384.in1k done. 51.57 samples/sec, 305.90 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model maxvit_base_tf_384.in21k_ft_in1k created, param count: 119653468
Running inference benchmark on maxvit_base_tf_384.in21k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 6.75 GiB. GPU 0 has a total capacty of 23.65 GiB of which 5.79 GiB is free. Including non-PyTorch memory, this process has 17.85 GiB memory in use. Of the allocated memory 15.50 GiB is allocated by PyTorch, and 1.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model maxvit_base_tf_384.in21k_ft_in1k created, param count: 119653468
Running inference benchmark on maxvit_base_tf_384.in21k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 192.
Infer [8/40]. 162.79 samples/sec. 1179.441 ms/step.
Infer [16/40]. 162.79 samples/sec. 1179.465 ms/step.
Infer [24/40]. 162.78 samples/sec. 1179.487 ms/step.
Infer [32/40]. 162.78 samples/sec. 1179.508 ms/step.
Infer [40/40]. 162.78 samples/sec. 1179.527 ms/step.
Inference benchmark of maxvit_base_tf_384.in21k_ft_in1k done. 162.77 samples/sec, 1179.53 ms/step
Model maxvit_base_tf_384.in21k_ft_in1k created, param count: 119653468
Running train benchmark on maxvit_base_tf_384.in21k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 6.75 GiB. GPU 0 has a total capacty of 23.65 GiB of which 164.06 MiB is free. Including non-PyTorch memory, this process has 23.48 GiB memory in use. Of the allocated memory 21.62 GiB is allocated by PyTorch, and 647.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model maxvit_base_tf_384.in21k_ft_in1k created, param count: 119653468
Running train benchmark on maxvit_base_tf_384.in21k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 23.65 GiB of which 450.06 MiB is free. Including non-PyTorch memory, this process has 23.20 GiB memory in use. Of the allocated memory 21.33 GiB is allocated by PyTorch, and 651.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model maxvit_base_tf_384.in21k_ft_in1k created, param count: 119653468
Running train benchmark on maxvit_base_tf_384.in21k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 864.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 844.06 MiB is free. Including non-PyTorch memory, this process has 22.82 GiB memory in use. Of the allocated memory 21.00 GiB is allocated by PyTorch, and 598.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model maxvit_base_tf_384.in21k_ft_in1k created, param count: 119653468
Running train benchmark on maxvit_base_tf_384.in21k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 648.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 146.06 MiB is free. Including non-PyTorch memory, this process has 23.50 GiB memory in use. Of the allocated memory 21.92 GiB is allocated by PyTorch, and 357.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model maxvit_base_tf_384.in21k_ft_in1k created, param count: 119653468
Running train benchmark on maxvit_base_tf_384.in21k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 864.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 244.06 MiB is free. Including non-PyTorch memory, this process has 23.40 GiB memory in use. Of the allocated memory 21.96 GiB is allocated by PyTorch, and 217.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model maxvit_base_tf_384.in21k_ft_in1k created, param count: 119653468
Running train benchmark on maxvit_base_tf_384.in21k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 122.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 80.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 22.17 GiB is allocated by PyTorch, and 165.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model maxvit_base_tf_384.in21k_ft_in1k created, param count: 119653468
Running train benchmark on maxvit_base_tf_384.in21k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 32.
ERROR: "CUDA out of memory. Tried to allocate 122.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 100.06 MiB is free. Including non-PyTorch memory, this process has 23.54 GiB memory in use. Of the allocated memory 22.07 GiB is allocated by PyTorch, and 246.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model maxvit_base_tf_384.in21k_ft_in1k created, param count: 119653468
Running train benchmark on maxvit_base_tf_384.in21k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 24.
ERROR: "CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 24.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 21.91 GiB is allocated by PyTorch, and 487.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model maxvit_base_tf_384.in21k_ft_in1k created, param count: 119653468
Running train benchmark on maxvit_base_tf_384.in21k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 16.
Train [8/40]. 52.38 samples/sec. 305.484 ms/step.
Train [16/40]. 52.38 samples/sec. 305.480 ms/step.
Train [24/40]. 52.37 samples/sec. 305.516 ms/step.
Train [32/40]. 52.37 samples/sec. 305.513 ms/step.
Train [40/40]. 52.36 samples/sec. 305.557 ms/step.
Train benchmark of maxvit_base_tf_384.in21k_ft_in1k done. 51.63 samples/sec, 305.56 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model maxvit_base_tf_512.in1k created, param count: 119876380
Running inference benchmark on maxvit_base_tf_512.in1k for 40 steps w/ input size (3, 512, 512) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 12.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 5.54 GiB is free. Including non-PyTorch memory, this process has 18.11 GiB memory in use. Of the allocated memory 15.37 GiB is allocated by PyTorch, and 1.51 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model maxvit_base_tf_512.in1k created, param count: 119876380
Running inference benchmark on maxvit_base_tf_512.in1k for 40 steps w/ input size (3, 512, 512) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 9.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 548.06 MiB is free. Including non-PyTorch memory, this process has 23.11 GiB memory in use. Of the allocated memory 20.58 GiB is allocated by PyTorch, and 1.29 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model maxvit_base_tf_512.in1k created, param count: 119876380
Running inference benchmark on maxvit_base_tf_512.in1k for 40 steps w/ input size (3, 512, 512) and batch size 128.
Infer [8/40]. 82.57 samples/sec. 1550.150 ms/step.
Infer [16/40]. 82.57 samples/sec. 1550.165 ms/step.
Infer [24/40]. 82.57 samples/sec. 1550.181 ms/step.
Infer [32/40]. 82.57 samples/sec. 1550.187 ms/step.
Infer [40/40]. 82.57 samples/sec. 1550.199 ms/step.
Inference benchmark of maxvit_base_tf_512.in1k done. 82.57 samples/sec, 1550.20 ms/step
Model maxvit_base_tf_512.in1k created, param count: 119876380
Running train benchmark on maxvit_base_tf_512.in1k for 40 steps w/ input size (3, 512, 512) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 12.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 7.16 GiB is free. Including non-PyTorch memory, this process has 16.48 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 1.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model maxvit_base_tf_512.in1k created, param count: 119876380
Running train benchmark on maxvit_base_tf_512.in1k for 40 steps w/ input size (3, 512, 512) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 9.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.75 GiB is free. Including non-PyTorch memory, this process has 21.89 GiB memory in use. Of the allocated memory 19.74 GiB is allocated by PyTorch, and 939.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model maxvit_base_tf_512.in1k created, param count: 119876380
Running train benchmark on maxvit_base_tf_512.in1k for 40 steps w/ input size (3, 512, 512) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 6.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 2.53 GiB is free. Including non-PyTorch memory, this process has 21.11 GiB memory in use. Of the allocated memory 19.24 GiB is allocated by PyTorch, and 650.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model maxvit_base_tf_512.in1k created, param count: 119876380
Running train benchmark on maxvit_base_tf_512.in1k for 40 steps w/ input size (3, 512, 512) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 4.54 GiB. GPU 0 has a total capacty of 23.65 GiB of which 2.78 GiB is free. Including non-PyTorch memory, this process has 20.86 GiB memory in use. Of the allocated memory 18.99 GiB is allocated by PyTorch, and 652.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model maxvit_base_tf_512.in1k created, param count: 119876380
Running train benchmark on maxvit_base_tf_512.in1k for 40 steps w/ input size (3, 512, 512) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 1.50 GiB. GPU 0 has a total capacty of 23.65 GiB of which 324.06 MiB is free. Including non-PyTorch memory, this process has 23.32 GiB memory in use. Of the allocated memory 21.98 GiB is allocated by PyTorch, and 117.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model maxvit_base_tf_512.in1k created, param count: 119876380
Running train benchmark on maxvit_base_tf_512.in1k for 40 steps w/ input size (3, 512, 512) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 576.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 570.06 MiB is free. Including non-PyTorch memory, this process has 23.08 GiB memory in use. Of the allocated memory 21.61 GiB is allocated by PyTorch, and 246.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model maxvit_base_tf_512.in1k created, param count: 119876380
Running train benchmark on maxvit_base_tf_512.in1k for 40 steps w/ input size (3, 512, 512) and batch size 32.
ERROR: "CUDA out of memory. Tried to allocate 782.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 30.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 22.36 GiB is allocated by PyTorch, and 25.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model maxvit_base_tf_512.in1k created, param count: 119876380
Running train benchmark on maxvit_base_tf_512.in1k for 40 steps w/ input size (3, 512, 512) and batch size 24.
ERROR: "CUDA out of memory. Tried to allocate 144.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 14.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.29 GiB is allocated by PyTorch, and 104.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model maxvit_base_tf_512.in1k created, param count: 119876380
Running train benchmark on maxvit_base_tf_512.in1k for 40 steps w/ input size (3, 512, 512) and batch size 16.
ERROR: "CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 162.06 MiB is free. Including non-PyTorch memory, this process has 23.48 GiB memory in use. Of the allocated memory 22.21 GiB is allocated by PyTorch, and 42.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 12 for retry.
Model maxvit_base_tf_512.in1k created, param count: 119876380
Running train benchmark on maxvit_base_tf_512.in1k for 40 steps w/ input size (3, 512, 512) and batch size 12.
ERROR: "CUDA out of memory. Tried to allocate 72.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 8.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.25 GiB is allocated by PyTorch, and 151.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 8 for retry.
Model maxvit_base_tf_512.in1k created, param count: 119876380
Running train benchmark on maxvit_base_tf_512.in1k for 40 steps w/ input size (3, 512, 512) and batch size 8.
Train [8/40]. 26.44 samples/sec. 302.608 ms/step.
Train [16/40]. 26.43 samples/sec. 302.636 ms/step.
Train [24/40]. 26.43 samples/sec. 302.705 ms/step.
Train [32/40]. 26.43 samples/sec. 302.736 ms/step.
Train [40/40]. 26.43 samples/sec. 302.729 ms/step.
Train benchmark of maxvit_base_tf_512.in1k done. 26.05 samples/sec, 302.73 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model maxvit_base_tf_512.in21k_ft_in1k created, param count: 119876380
Running inference benchmark on maxvit_base_tf_512.in21k_ft_in1k for 40 steps w/ input size (3, 512, 512) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 12.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 5.54 GiB is free. Including non-PyTorch memory, this process has 18.11 GiB memory in use. Of the allocated memory 15.37 GiB is allocated by PyTorch, and 1.51 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model maxvit_base_tf_512.in21k_ft_in1k created, param count: 119876380
Running inference benchmark on maxvit_base_tf_512.in21k_ft_in1k for 40 steps w/ input size (3, 512, 512) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 9.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 548.06 MiB is free. Including non-PyTorch memory, this process has 23.11 GiB memory in use. Of the allocated memory 20.58 GiB is allocated by PyTorch, and 1.29 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model maxvit_base_tf_512.in21k_ft_in1k created, param count: 119876380
Running inference benchmark on maxvit_base_tf_512.in21k_ft_in1k for 40 steps w/ input size (3, 512, 512) and batch size 128.
Infer [8/40]. 82.59 samples/sec. 1549.898 ms/step.
Infer [16/40]. 82.58 samples/sec. 1549.951 ms/step.
Infer [24/40]. 82.58 samples/sec. 1549.994 ms/step.
Infer [32/40]. 82.58 samples/sec. 1550.009 ms/step.
Infer [40/40]. 82.58 samples/sec. 1550.023 ms/step.
Inference benchmark of maxvit_base_tf_512.in21k_ft_in1k done. 82.58 samples/sec, 1550.02 ms/step
Model maxvit_base_tf_512.in21k_ft_in1k created, param count: 119876380
Running train benchmark on maxvit_base_tf_512.in21k_ft_in1k for 40 steps w/ input size (3, 512, 512) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 12.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 7.16 GiB is free. Including non-PyTorch memory, this process has 16.48 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 1.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model maxvit_base_tf_512.in21k_ft_in1k created, param count: 119876380
Running train benchmark on maxvit_base_tf_512.in21k_ft_in1k for 40 steps w/ input size (3, 512, 512) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 9.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.75 GiB is free. Including non-PyTorch memory, this process has 21.89 GiB memory in use. Of the allocated memory 19.74 GiB is allocated by PyTorch, and 939.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model maxvit_base_tf_512.in21k_ft_in1k created, param count: 119876380
Running train benchmark on maxvit_base_tf_512.in21k_ft_in1k for 40 steps w/ input size (3, 512, 512) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 6.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 2.53 GiB is free. Including non-PyTorch memory, this process has 21.11 GiB memory in use. Of the allocated memory 19.24 GiB is allocated by PyTorch, and 650.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model maxvit_base_tf_512.in21k_ft_in1k created, param count: 119876380
Running train benchmark on maxvit_base_tf_512.in21k_ft_in1k for 40 steps w/ input size (3, 512, 512) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 4.54 GiB. GPU 0 has a total capacty of 23.65 GiB of which 2.78 GiB is free. Including non-PyTorch memory, this process has 20.86 GiB memory in use. Of the allocated memory 18.99 GiB is allocated by PyTorch, and 652.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model maxvit_base_tf_512.in21k_ft_in1k created, param count: 119876380
Running train benchmark on maxvit_base_tf_512.in21k_ft_in1k for 40 steps w/ input size (3, 512, 512) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 1.50 GiB. GPU 0 has a total capacty of 23.65 GiB of which 324.06 MiB is free. Including non-PyTorch memory, this process has 23.32 GiB memory in use. Of the allocated memory 21.98 GiB is allocated by PyTorch, and 117.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model maxvit_base_tf_512.in21k_ft_in1k created, param count: 119876380
Running train benchmark on maxvit_base_tf_512.in21k_ft_in1k for 40 steps w/ input size (3, 512, 512) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 576.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 570.06 MiB is free. Including non-PyTorch memory, this process has 23.08 GiB memory in use. Of the allocated memory 21.61 GiB is allocated by PyTorch, and 246.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model maxvit_base_tf_512.in21k_ft_in1k created, param count: 119876380
Running train benchmark on maxvit_base_tf_512.in21k_ft_in1k for 40 steps w/ input size (3, 512, 512) and batch size 32.
ERROR: "CUDA out of memory. Tried to allocate 768.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 766.06 MiB is free. Including non-PyTorch memory, this process has 22.89 GiB memory in use. Of the allocated memory 21.61 GiB is allocated by PyTorch, and 57.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model maxvit_base_tf_512.in21k_ft_in1k created, param count: 119876380
Running train benchmark on maxvit_base_tf_512.in21k_ft_in1k for 40 steps w/ input size (3, 512, 512) and batch size 24.
ERROR: "CUDA out of memory. Tried to allocate 144.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 138.06 MiB is free. Including non-PyTorch memory, this process has 23.51 GiB memory in use. Of the allocated memory 22.15 GiB is allocated by PyTorch, and 124.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model maxvit_base_tf_512.in21k_ft_in1k created, param count: 119876380
Running train benchmark on maxvit_base_tf_512.in21k_ft_in1k for 40 steps w/ input size (3, 512, 512) and batch size 16.
ERROR: "CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 162.06 MiB is free. Including non-PyTorch memory, this process has 23.48 GiB memory in use. Of the allocated memory 22.21 GiB is allocated by PyTorch, and 42.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 12 for retry.
Model maxvit_base_tf_512.in21k_ft_in1k created, param count: 119876380
Running train benchmark on maxvit_base_tf_512.in21k_ft_in1k for 40 steps w/ input size (3, 512, 512) and batch size 12.
ERROR: "CUDA out of memory. Tried to allocate 72.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 2.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 22.25 GiB is allocated by PyTorch, and 157.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 8 for retry.
Model maxvit_base_tf_512.in21k_ft_in1k created, param count: 119876380
Running train benchmark on maxvit_base_tf_512.in21k_ft_in1k for 40 steps w/ input size (3, 512, 512) and batch size 8.
Train [8/40]. 26.50 samples/sec. 301.890 ms/step.
Train [16/40]. 26.51 samples/sec. 301.792 ms/step.
Train [24/40]. 26.50 samples/sec. 301.849 ms/step.
Train [32/40]. 26.50 samples/sec. 301.834 ms/step.
Train [40/40]. 26.51 samples/sec. 301.817 ms/step.
Train benchmark of maxvit_base_tf_512.in21k_ft_in1k done. 26.13 samples/sec, 301.82 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model maxvit_large_tf_224.in1k created, param count: 211785560
Running inference benchmark on maxvit_large_tf_224.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 388.67 samples/sec. 658.663 ms/step.
Infer [16/40]. 388.67 samples/sec. 658.656 ms/step.
Infer [24/40]. 388.66 samples/sec. 658.677 ms/step.
Infer [32/40]. 388.65 samples/sec. 658.692 ms/step.
Infer [40/40]. 388.63 samples/sec. 658.723 ms/step.
Inference benchmark of maxvit_large_tf_224.in1k done. 388.61 samples/sec, 658.72 ms/step
Model maxvit_large_tf_224.in1k created, param count: 211785560
Running train benchmark on maxvit_large_tf_224.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 196.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 24.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.17 GiB is allocated by PyTorch, and 223.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model maxvit_large_tf_224.in1k created, param count: 211785560
Running train benchmark on maxvit_large_tf_224.in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 588.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 294.06 MiB is free. Including non-PyTorch memory, this process has 23.35 GiB memory in use. Of the allocated memory 21.99 GiB is allocated by PyTorch, and 137.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model maxvit_large_tf_224.in1k created, param count: 211785560
Running train benchmark on maxvit_large_tf_224.in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 44.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 22.33 GiB is allocated by PyTorch, and 40.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model maxvit_large_tf_224.in1k created, param count: 211785560
Running train benchmark on maxvit_large_tf_224.in1k for 40 steps w/ input size (3, 224, 224) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 30.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 22.12 GiB is allocated by PyTorch, and 270.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model maxvit_large_tf_224.in1k created, param count: 211785560
Running train benchmark on maxvit_large_tf_224.in1k for 40 steps w/ input size (3, 224, 224) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 48.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 22.18 GiB is allocated by PyTorch, and 187.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model maxvit_large_tf_224.in1k created, param count: 211785560
Running train benchmark on maxvit_large_tf_224.in1k for 40 steps w/ input size (3, 224, 224) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 16.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 21.76 GiB is allocated by PyTorch, and 649.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model maxvit_large_tf_224.in1k created, param count: 211785560
Running train benchmark on maxvit_large_tf_224.in1k for 40 steps w/ input size (3, 224, 224) and batch size 32.
Train [8/40]. 108.72 samples/sec. 294.322 ms/step.
Train [16/40]. 108.76 samples/sec. 294.226 ms/step.
Train [24/40]. 108.75 samples/sec. 294.245 ms/step.
Train [32/40]. 108.75 samples/sec. 294.266 ms/step.
Train [40/40]. 108.71 samples/sec. 294.362 ms/step.
Train benchmark of maxvit_large_tf_224.in1k done. 107.10 samples/sec, 294.36 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model maxvit_large_tf_384.in1k created, param count: 212033240
Running inference benchmark on maxvit_large_tf_384.in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 9.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 5.78 GiB is free. Including non-PyTorch memory, this process has 17.86 GiB memory in use. Of the allocated memory 12.44 GiB is allocated by PyTorch, and 4.19 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model maxvit_large_tf_384.in1k created, param count: 212033240
Running inference benchmark on maxvit_large_tf_384.in1k for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 6.75 GiB. GPU 0 has a total capacty of 23.65 GiB of which 2.87 GiB is free. Including non-PyTorch memory, this process has 20.77 GiB memory in use. Of the allocated memory 16.18 GiB is allocated by PyTorch, and 3.36 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model maxvit_large_tf_384.in1k created, param count: 212033240
Running inference benchmark on maxvit_large_tf_384.in1k for 40 steps w/ input size (3, 384, 384) and batch size 128.
Infer [8/40]. 114.13 samples/sec. 1121.490 ms/step.
Infer [16/40]. 114.13 samples/sec. 1121.533 ms/step.
Infer [24/40]. 114.13 samples/sec. 1121.524 ms/step.
Infer [32/40]. 114.13 samples/sec. 1121.541 ms/step.
Infer [40/40]. 114.13 samples/sec. 1121.555 ms/step.
Inference benchmark of maxvit_large_tf_384.in1k done. 114.12 samples/sec, 1121.56 ms/step
Model maxvit_large_tf_384.in1k created, param count: 212033240
Running train benchmark on maxvit_large_tf_384.in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 9.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 5.78 GiB is free. Including non-PyTorch memory, this process has 17.86 GiB memory in use. Of the allocated memory 14.90 GiB is allocated by PyTorch, and 1.73 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model maxvit_large_tf_384.in1k created, param count: 212033240
Running train benchmark on maxvit_large_tf_384.in1k for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 6.75 GiB. GPU 0 has a total capacty of 23.65 GiB of which 2.87 GiB is free. Including non-PyTorch memory, this process has 20.77 GiB memory in use. Of the allocated memory 18.03 GiB is allocated by PyTorch, and 1.51 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model maxvit_large_tf_384.in1k created, param count: 212033240
Running train benchmark on maxvit_large_tf_384.in1k for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 4.50 GiB. GPU 0 has a total capacty of 23.65 GiB of which 4.17 GiB is free. Including non-PyTorch memory, this process has 19.47 GiB memory in use. Of the allocated memory 16.66 GiB is allocated by PyTorch, and 1.59 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model maxvit_large_tf_384.in1k created, param count: 212033240
Running train benchmark on maxvit_large_tf_384.in1k for 40 steps w/ input size (3, 384, 384) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 972.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 816.06 MiB is free. Including non-PyTorch memory, this process has 22.84 GiB memory in use. Of the allocated memory 21.29 GiB is allocated by PyTorch, and 335.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model maxvit_large_tf_384.in1k created, param count: 212033240
Running train benchmark on maxvit_large_tf_384.in1k for 40 steps w/ input size (3, 384, 384) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 576.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 396.06 MiB is free. Including non-PyTorch memory, this process has 23.25 GiB memory in use. Of the allocated memory 21.80 GiB is allocated by PyTorch, and 231.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model maxvit_large_tf_384.in1k created, param count: 212033240
Running train benchmark on maxvit_large_tf_384.in1k for 40 steps w/ input size (3, 384, 384) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 864.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 36.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 22.03 GiB is allocated by PyTorch, and 356.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model maxvit_large_tf_384.in1k created, param count: 212033240
Running train benchmark on maxvit_large_tf_384.in1k for 40 steps w/ input size (3, 384, 384) and batch size 32.
ERROR: "CUDA out of memory. Tried to allocate 108.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 98.06 MiB is free. Including non-PyTorch memory, this process has 23.54 GiB memory in use. Of the allocated memory 22.15 GiB is allocated by PyTorch, and 164.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model maxvit_large_tf_384.in1k created, param count: 212033240
Running train benchmark on maxvit_large_tf_384.in1k for 40 steps w/ input size (3, 384, 384) and batch size 24.
ERROR: "CUDA out of memory. Tried to allocate 108.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 104.06 MiB is free. Including non-PyTorch memory, this process has 23.54 GiB memory in use. Of the allocated memory 22.00 GiB is allocated by PyTorch, and 317.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model maxvit_large_tf_384.in1k created, param count: 212033240
Running train benchmark on maxvit_large_tf_384.in1k for 40 steps w/ input size (3, 384, 384) and batch size 16.
ERROR: "CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 12.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.12 GiB is allocated by PyTorch, and 289.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 12 for retry.
Model maxvit_large_tf_384.in1k created, param count: 212033240
Running train benchmark on maxvit_large_tf_384.in1k for 40 steps w/ input size (3, 384, 384) and batch size 12.
Train [8/40]. 38.80 samples/sec. 309.253 ms/step.
Train [16/40]. 38.81 samples/sec. 309.210 ms/step.
Train [24/40]. 38.80 samples/sec. 309.299 ms/step.
Train [32/40]. 38.79 samples/sec. 309.341 ms/step.
Train [40/40]. 38.79 samples/sec. 309.352 ms/step.
Train benchmark of maxvit_large_tf_384.in1k done. 38.24 samples/sec, 309.35 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model maxvit_large_tf_384.in21k_ft_in1k created, param count: 212033240
Running inference benchmark on maxvit_large_tf_384.in21k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 9.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 8.04 GiB is free. Including non-PyTorch memory, this process has 15.61 GiB memory in use. Of the allocated memory 12.44 GiB is allocated by PyTorch, and 1.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model maxvit_large_tf_384.in21k_ft_in1k created, param count: 212033240
Running inference benchmark on maxvit_large_tf_384.in21k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 6.75 GiB. GPU 0 has a total capacty of 23.65 GiB of which 4.56 GiB is free. Including non-PyTorch memory, this process has 19.08 GiB memory in use. Of the allocated memory 16.18 GiB is allocated by PyTorch, and 1.67 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model maxvit_large_tf_384.in21k_ft_in1k created, param count: 212033240
Running inference benchmark on maxvit_large_tf_384.in21k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 128.
Infer [8/40]. 114.12 samples/sec. 1121.588 ms/step.
Infer [16/40]. 114.10 samples/sec. 1121.855 ms/step.
Infer [24/40]. 114.09 samples/sec. 1121.969 ms/step.
Infer [32/40]. 114.08 samples/sec. 1122.028 ms/step.
Infer [40/40]. 114.08 samples/sec. 1122.065 ms/step.
Inference benchmark of maxvit_large_tf_384.in21k_ft_in1k done. 114.07 samples/sec, 1122.07 ms/step
Model maxvit_large_tf_384.in21k_ft_in1k created, param count: 212033240
Running train benchmark on maxvit_large_tf_384.in21k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 9.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 5.78 GiB is free. Including non-PyTorch memory, this process has 17.86 GiB memory in use. Of the allocated memory 14.90 GiB is allocated by PyTorch, and 1.73 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model maxvit_large_tf_384.in21k_ft_in1k created, param count: 212033240
Running train benchmark on maxvit_large_tf_384.in21k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 6.75 GiB. GPU 0 has a total capacty of 23.65 GiB of which 2.87 GiB is free. Including non-PyTorch memory, this process has 20.77 GiB memory in use. Of the allocated memory 18.03 GiB is allocated by PyTorch, and 1.51 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model maxvit_large_tf_384.in21k_ft_in1k created, param count: 212033240
Running train benchmark on maxvit_large_tf_384.in21k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 4.50 GiB. GPU 0 has a total capacty of 23.65 GiB of which 4.17 GiB is free. Including non-PyTorch memory, this process has 19.47 GiB memory in use. Of the allocated memory 16.66 GiB is allocated by PyTorch, and 1.59 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model maxvit_large_tf_384.in21k_ft_in1k created, param count: 212033240
Running train benchmark on maxvit_large_tf_384.in21k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 972.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 816.06 MiB is free. Including non-PyTorch memory, this process has 22.84 GiB memory in use. Of the allocated memory 21.29 GiB is allocated by PyTorch, and 335.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model maxvit_large_tf_384.in21k_ft_in1k created, param count: 212033240
Running train benchmark on maxvit_large_tf_384.in21k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 576.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 396.06 MiB is free. Including non-PyTorch memory, this process has 23.25 GiB memory in use. Of the allocated memory 21.80 GiB is allocated by PyTorch, and 231.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model maxvit_large_tf_384.in21k_ft_in1k created, param count: 212033240
Running train benchmark on maxvit_large_tf_384.in21k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 864.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 36.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 22.03 GiB is allocated by PyTorch, and 356.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model maxvit_large_tf_384.in21k_ft_in1k created, param count: 212033240
Running train benchmark on maxvit_large_tf_384.in21k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 32.
ERROR: "CUDA out of memory. Tried to allocate 108.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 98.06 MiB is free. Including non-PyTorch memory, this process has 23.54 GiB memory in use. Of the allocated memory 22.15 GiB is allocated by PyTorch, and 164.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model maxvit_large_tf_384.in21k_ft_in1k created, param count: 212033240
Running train benchmark on maxvit_large_tf_384.in21k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 24.
ERROR: "CUDA out of memory. Tried to allocate 108.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 104.06 MiB is free. Including non-PyTorch memory, this process has 23.54 GiB memory in use. Of the allocated memory 22.00 GiB is allocated by PyTorch, and 317.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model maxvit_large_tf_384.in21k_ft_in1k created, param count: 212033240
Running train benchmark on maxvit_large_tf_384.in21k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 16.
ERROR: "CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 16.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.11 GiB is allocated by PyTorch, and 286.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 12 for retry.
Model maxvit_large_tf_384.in21k_ft_in1k created, param count: 212033240
Running train benchmark on maxvit_large_tf_384.in21k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 12.
Train [8/40]. 38.87 samples/sec. 308.738 ms/step.
Train [16/40]. 38.85 samples/sec. 308.874 ms/step.
Train [24/40]. 38.86 samples/sec. 308.828 ms/step.
Train [32/40]. 38.85 samples/sec. 308.859 ms/step.
Train [40/40]. 38.86 samples/sec. 308.814 ms/step.
Train benchmark of maxvit_large_tf_384.in21k_ft_in1k done. 38.32 samples/sec, 308.81 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model maxvit_large_tf_512.in1k created, param count: 212330456
Running inference benchmark on maxvit_large_tf_512.in1k for 40 steps w/ input size (3, 512, 512) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 5.53 GiB is free. Including non-PyTorch memory, this process has 18.11 GiB memory in use. Of the allocated memory 9.79 GiB is allocated by PyTorch, and 7.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model maxvit_large_tf_512.in1k created, param count: 212330456
Running inference benchmark on maxvit_large_tf_512.in1k for 40 steps w/ input size (3, 512, 512) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 12.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 9.26 GiB is free. Including non-PyTorch memory, this process has 14.38 GiB memory in use. Of the allocated memory 7.45 GiB is allocated by PyTorch, and 5.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model maxvit_large_tf_512.in1k created, param count: 212330456
Running inference benchmark on maxvit_large_tf_512.in1k for 40 steps w/ input size (3, 512, 512) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 7.47 GiB is free. Including non-PyTorch memory, this process has 16.17 GiB memory in use. Of the allocated memory 11.10 GiB is allocated by PyTorch, and 3.84 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model maxvit_large_tf_512.in1k created, param count: 212330456
Running inference benchmark on maxvit_large_tf_512.in1k for 40 steps w/ input size (3, 512, 512) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 6.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 4.95 GiB is free. Including non-PyTorch memory, this process has 18.69 GiB memory in use. Of the allocated memory 14.43 GiB is allocated by PyTorch, and 3.03 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model maxvit_large_tf_512.in1k created, param count: 212330456
Running inference benchmark on maxvit_large_tf_512.in1k for 40 steps w/ input size (3, 512, 512) and batch size 64.
Infer [8/40]. 58.90 samples/sec. 1086.505 ms/step.
Infer [16/40]. 58.90 samples/sec. 1086.604 ms/step.
Infer [24/40]. 58.90 samples/sec. 1086.623 ms/step.
Infer [32/40]. 58.90 samples/sec. 1086.606 ms/step.
Infer [40/40]. 58.90 samples/sec. 1086.645 ms/step.
Inference benchmark of maxvit_large_tf_512.in1k done. 58.89 samples/sec, 1086.64 ms/step
Model maxvit_large_tf_512.in1k created, param count: 212330456
Running train benchmark on maxvit_large_tf_512.in1k for 40 steps w/ input size (3, 512, 512) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 160.06 MiB is free. Including non-PyTorch memory, this process has 23.48 GiB memory in use. Of the allocated memory 22.17 GiB is allocated by PyTorch, and 90.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model maxvit_large_tf_512.in1k created, param count: 212330456
Running train benchmark on maxvit_large_tf_512.in1k for 40 steps w/ input size (3, 512, 512) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 12.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 262.06 MiB is free. Including non-PyTorch memory, this process has 23.38 GiB memory in use. Of the allocated memory 19.73 GiB is allocated by PyTorch, and 2.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model maxvit_large_tf_512.in1k created, param count: 212330456
Running train benchmark on maxvit_large_tf_512.in1k for 40 steps w/ input size (3, 512, 512) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 7.47 GiB is free. Including non-PyTorch memory, this process has 16.17 GiB memory in use. Of the allocated memory 13.29 GiB is allocated by PyTorch, and 1.65 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model maxvit_large_tf_512.in1k created, param count: 212330456
Running train benchmark on maxvit_large_tf_512.in1k for 40 steps w/ input size (3, 512, 512) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 6.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 4.95 GiB is free. Including non-PyTorch memory, this process has 18.69 GiB memory in use. Of the allocated memory 16.07 GiB is allocated by PyTorch, and 1.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model maxvit_large_tf_512.in1k created, param count: 212330456
Running train benchmark on maxvit_large_tf_512.in1k for 40 steps w/ input size (3, 512, 512) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 4.03 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.44 GiB is free. Including non-PyTorch memory, this process has 22.20 GiB memory in use. Of the allocated memory 18.85 GiB is allocated by PyTorch, and 2.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model maxvit_large_tf_512.in1k created, param count: 212330456
Running train benchmark on maxvit_large_tf_512.in1k for 40 steps w/ input size (3, 512, 512) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 576.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 44.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 22.16 GiB is allocated by PyTorch, and 209.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model maxvit_large_tf_512.in1k created, param count: 212330456
Running train benchmark on maxvit_large_tf_512.in1k for 40 steps w/ input size (3, 512, 512) and batch size 32.
ERROR: "CUDA out of memory. Tried to allocate 128.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 50.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 22.17 GiB is allocated by PyTorch, and 195.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model maxvit_large_tf_512.in1k created, param count: 212330456
Running train benchmark on maxvit_large_tf_512.in1k for 40 steps w/ input size (3, 512, 512) and batch size 24.
ERROR: "CUDA out of memory. Tried to allocate 768.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 612.06 MiB is free. Including non-PyTorch memory, this process has 23.04 GiB memory in use. Of the allocated memory 21.69 GiB is allocated by PyTorch, and 125.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model maxvit_large_tf_512.in1k created, param count: 212330456
Running train benchmark on maxvit_large_tf_512.in1k for 40 steps w/ input size (3, 512, 512) and batch size 16.
ERROR: "CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 98.06 MiB is free. Including non-PyTorch memory, this process has 23.54 GiB memory in use. Of the allocated memory 22.24 GiB is allocated by PyTorch, and 80.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 12 for retry.
Model maxvit_large_tf_512.in1k created, param count: 212330456
Running train benchmark on maxvit_large_tf_512.in1k for 40 steps w/ input size (3, 512, 512) and batch size 12.
ERROR: "CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 96.06 MiB is free. Including non-PyTorch memory, this process has 23.55 GiB memory in use. Of the allocated memory 22.11 GiB is allocated by PyTorch, and 214.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 8 for retry.
Model maxvit_large_tf_512.in1k created, param count: 212330456
Running train benchmark on maxvit_large_tf_512.in1k for 40 steps w/ input size (3, 512, 512) and batch size 8.
ERROR: "CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 58.06 MiB is free. Including non-PyTorch memory, this process has 23.58 GiB memory in use. Of the allocated memory 22.07 GiB is allocated by PyTorch, and 292.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 6 for retry.
Model maxvit_large_tf_512.in1k created, param count: 212330456
Running train benchmark on maxvit_large_tf_512.in1k for 40 steps w/ input size (3, 512, 512) and batch size 6.
Train [8/40]. 19.68 samples/sec. 304.867 ms/step.
Train [16/40]. 19.68 samples/sec. 304.917 ms/step.
Train [24/40]. 19.68 samples/sec. 304.864 ms/step.
Train [32/40]. 19.68 samples/sec. 304.848 ms/step.
Train [40/40]. 19.68 samples/sec. 304.861 ms/step.
Train benchmark of maxvit_large_tf_512.in1k done. 19.40 samples/sec, 304.86 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model maxvit_large_tf_512.in21k_ft_in1k created, param count: 212330456
Running inference benchmark on maxvit_large_tf_512.in21k_ft_in1k for 40 steps w/ input size (3, 512, 512) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 9.53 GiB is free. Including non-PyTorch memory, this process has 14.11 GiB memory in use. Of the allocated memory 9.79 GiB is allocated by PyTorch, and 3.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model maxvit_large_tf_512.in21k_ft_in1k created, param count: 212330456
Running inference benchmark on maxvit_large_tf_512.in21k_ft_in1k for 40 steps w/ input size (3, 512, 512) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 12.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 3.26 GiB is free. Including non-PyTorch memory, this process has 20.38 GiB memory in use. Of the allocated memory 16.45 GiB is allocated by PyTorch, and 2.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model maxvit_large_tf_512.in21k_ft_in1k created, param count: 212330456
Running inference benchmark on maxvit_large_tf_512.in21k_ft_in1k for 40 steps w/ input size (3, 512, 512) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.47 GiB is free. Including non-PyTorch memory, this process has 22.17 GiB memory in use. Of the allocated memory 19.10 GiB is allocated by PyTorch, and 1.84 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model maxvit_large_tf_512.in21k_ft_in1k created, param count: 212330456
Running inference benchmark on maxvit_large_tf_512.in21k_ft_in1k for 40 steps w/ input size (3, 512, 512) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 6.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 2.98 GiB is free. Including non-PyTorch memory, this process has 20.66 GiB memory in use. Of the allocated memory 14.43 GiB is allocated by PyTorch, and 5.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model maxvit_large_tf_512.in21k_ft_in1k created, param count: 212330456
Running inference benchmark on maxvit_large_tf_512.in21k_ft_in1k for 40 steps w/ input size (3, 512, 512) and batch size 64.
Infer [8/40]. 58.90 samples/sec. 1086.508 ms/step.
Infer [16/40]. 58.90 samples/sec. 1086.583 ms/step.
Infer [24/40]. 58.90 samples/sec. 1086.598 ms/step.
Infer [32/40]. 58.90 samples/sec. 1086.624 ms/step.
Infer [40/40]. 58.90 samples/sec. 1086.663 ms/step.
Inference benchmark of maxvit_large_tf_512.in21k_ft_in1k done. 58.89 samples/sec, 1086.66 ms/step
Model maxvit_large_tf_512.in21k_ft_in1k created, param count: 212330456
Running train benchmark on maxvit_large_tf_512.in21k_ft_in1k for 40 steps w/ input size (3, 512, 512) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 160.06 MiB is free. Including non-PyTorch memory, this process has 23.48 GiB memory in use. Of the allocated memory 22.17 GiB is allocated by PyTorch, and 90.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model maxvit_large_tf_512.in21k_ft_in1k created, param count: 212330456
Running train benchmark on maxvit_large_tf_512.in21k_ft_in1k for 40 steps w/ input size (3, 512, 512) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 12.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 262.06 MiB is free. Including non-PyTorch memory, this process has 23.38 GiB memory in use. Of the allocated memory 19.73 GiB is allocated by PyTorch, and 2.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model maxvit_large_tf_512.in21k_ft_in1k created, param count: 212330456
Running train benchmark on maxvit_large_tf_512.in21k_ft_in1k for 40 steps w/ input size (3, 512, 512) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 7.47 GiB is free. Including non-PyTorch memory, this process has 16.17 GiB memory in use. Of the allocated memory 13.29 GiB is allocated by PyTorch, and 1.65 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model maxvit_large_tf_512.in21k_ft_in1k created, param count: 212330456
Running train benchmark on maxvit_large_tf_512.in21k_ft_in1k for 40 steps w/ input size (3, 512, 512) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 6.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 4.95 GiB is free. Including non-PyTorch memory, this process has 18.69 GiB memory in use. Of the allocated memory 16.07 GiB is allocated by PyTorch, and 1.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model maxvit_large_tf_512.in21k_ft_in1k created, param count: 212330456
Running train benchmark on maxvit_large_tf_512.in21k_ft_in1k for 40 steps w/ input size (3, 512, 512) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 4.03 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.44 GiB is free. Including non-PyTorch memory, this process has 22.20 GiB memory in use. Of the allocated memory 18.85 GiB is allocated by PyTorch, and 2.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model maxvit_large_tf_512.in21k_ft_in1k created, param count: 212330456
Running train benchmark on maxvit_large_tf_512.in21k_ft_in1k for 40 steps w/ input size (3, 512, 512) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 576.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 44.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 22.16 GiB is allocated by PyTorch, and 209.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model maxvit_large_tf_512.in21k_ft_in1k created, param count: 212330456
Running train benchmark on maxvit_large_tf_512.in21k_ft_in1k for 40 steps w/ input size (3, 512, 512) and batch size 32.
ERROR: "CUDA out of memory. Tried to allocate 128.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 50.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 22.17 GiB is allocated by PyTorch, and 195.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model maxvit_large_tf_512.in21k_ft_in1k created, param count: 212330456
Running train benchmark on maxvit_large_tf_512.in21k_ft_in1k for 40 steps w/ input size (3, 512, 512) and batch size 24.
ERROR: "CUDA out of memory. Tried to allocate 768.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 612.06 MiB is free. Including non-PyTorch memory, this process has 23.04 GiB memory in use. Of the allocated memory 21.69 GiB is allocated by PyTorch, and 125.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model maxvit_large_tf_512.in21k_ft_in1k created, param count: 212330456
Running train benchmark on maxvit_large_tf_512.in21k_ft_in1k for 40 steps w/ input size (3, 512, 512) and batch size 16.
ERROR: "CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 98.06 MiB is free. Including non-PyTorch memory, this process has 23.54 GiB memory in use. Of the allocated memory 22.24 GiB is allocated by PyTorch, and 80.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 12 for retry.
Model maxvit_large_tf_512.in21k_ft_in1k created, param count: 212330456
Running train benchmark on maxvit_large_tf_512.in21k_ft_in1k for 40 steps w/ input size (3, 512, 512) and batch size 12.
ERROR: "CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 88.06 MiB is free. Including non-PyTorch memory, this process has 23.55 GiB memory in use. Of the allocated memory 22.11 GiB is allocated by PyTorch, and 222.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 8 for retry.
Model maxvit_large_tf_512.in21k_ft_in1k created, param count: 212330456
Running train benchmark on maxvit_large_tf_512.in21k_ft_in1k for 40 steps w/ input size (3, 512, 512) and batch size 8.
ERROR: "CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 6.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.14 GiB is allocated by PyTorch, and 274.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 6 for retry.
Model maxvit_large_tf_512.in21k_ft_in1k created, param count: 212330456
Running train benchmark on maxvit_large_tf_512.in21k_ft_in1k for 40 steps w/ input size (3, 512, 512) and batch size 6.
Train [8/40]. 19.59 samples/sec. 306.250 ms/step.
Train [16/40]. 19.60 samples/sec. 306.174 ms/step.
Train [24/40]. 19.60 samples/sec. 306.144 ms/step.
Train [32/40]. 19.60 samples/sec. 306.171 ms/step.
Train [40/40]. 19.60 samples/sec. 306.145 ms/step.
Train benchmark of maxvit_large_tf_512.in21k_ft_in1k done. 19.32 samples/sec, 306.14 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model maxvit_nano_rw_256.sw_in1k created, param count: 15451148
Running inference benchmark on maxvit_nano_rw_256.sw_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
Infer [8/40]. 1906.42 samples/sec. 134.283 ms/step.
Infer [16/40]. 1906.30 samples/sec. 134.291 ms/step.
Infer [24/40]. 1906.16 samples/sec. 134.302 ms/step.
Infer [32/40]. 1906.13 samples/sec. 134.304 ms/step.
Infer [40/40]. 1906.11 samples/sec. 134.305 ms/step.
Inference benchmark of maxvit_nano_rw_256.sw_in1k done. 1905.67 samples/sec, 134.31 ms/step
Model maxvit_nano_rw_256.sw_in1k created, param count: 15451148
Running train benchmark on maxvit_nano_rw_256.sw_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 188.06 MiB is free. Including non-PyTorch memory, this process has 23.46 GiB memory in use. Of the allocated memory 22.15 GiB is allocated by PyTorch, and 73.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model maxvit_nano_rw_256.sw_in1k created, param count: 15451148
Running train benchmark on maxvit_nano_rw_256.sw_in1k for 40 steps w/ input size (3, 256, 256) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 24.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.29 GiB is allocated by PyTorch, and 97.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model maxvit_nano_rw_256.sw_in1k created, param count: 15451148
Running train benchmark on maxvit_nano_rw_256.sw_in1k for 40 steps w/ input size (3, 256, 256) and batch size 128.
Train [8/40]. 609.07 samples/sec. 210.158 ms/step.
Train [16/40]. 609.05 samples/sec. 210.165 ms/step.
Train [24/40]. 609.05 samples/sec. 210.163 ms/step.
Train [32/40]. 609.07 samples/sec. 210.158 ms/step.
Train [40/40]. 609.06 samples/sec. 210.160 ms/step.
Train benchmark of maxvit_nano_rw_256.sw_in1k done. 604.52 samples/sec, 210.16 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model maxvit_rmlp_base_rw_224.sw_in12k created, param count: 124457137
Running inference benchmark on maxvit_rmlp_base_rw_224.sw_in12k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 594.83 samples/sec. 430.378 ms/step.
Infer [16/40]. 594.84 samples/sec. 430.371 ms/step.
Infer [24/40]. 594.83 samples/sec. 430.372 ms/step.
Infer [32/40]. 594.82 samples/sec. 430.383 ms/step.
Infer [40/40]. 594.81 samples/sec. 430.389 ms/step.
Inference benchmark of maxvit_rmlp_base_rw_224.sw_in12k done. 594.76 samples/sec, 430.39 ms/step
Model maxvit_rmlp_base_rw_224.sw_in12k created, param count: 124457137
Running train benchmark on maxvit_rmlp_base_rw_224.sw_in12k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 588.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 60.06 MiB is free. Including non-PyTorch memory, this process has 23.58 GiB memory in use. Of the allocated memory 21.76 GiB is allocated by PyTorch, and 607.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model maxvit_rmlp_base_rw_224.sw_in12k created, param count: 124457137
Running train benchmark on maxvit_rmlp_base_rw_224.sw_in12k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 222.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 146.06 MiB is free. Including non-PyTorch memory, this process has 23.50 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 195.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model maxvit_rmlp_base_rw_224.sw_in12k created, param count: 124457137
Running train benchmark on maxvit_rmlp_base_rw_224.sw_in12k for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 12.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.07 GiB is allocated by PyTorch, and 335.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model maxvit_rmlp_base_rw_224.sw_in12k created, param count: 124457137
Running train benchmark on maxvit_rmlp_base_rw_224.sw_in12k for 40 steps w/ input size (3, 224, 224) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 22.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.10 GiB is allocated by PyTorch, and 293.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model maxvit_rmlp_base_rw_224.sw_in12k created, param count: 124457137
Running train benchmark on maxvit_rmlp_base_rw_224.sw_in12k for 40 steps w/ input size (3, 224, 224) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 8.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 21.73 GiB is allocated by PyTorch, and 687.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model maxvit_rmlp_base_rw_224.sw_in12k created, param count: 124457137
Running train benchmark on maxvit_rmlp_base_rw_224.sw_in12k for 40 steps w/ input size (3, 224, 224) and batch size 48.
Train [8/40]. 161.26 samples/sec. 297.660 ms/step.
Train [16/40]. 161.15 samples/sec. 297.852 ms/step.
Train [24/40]. 161.10 samples/sec. 297.944 ms/step.
Train [32/40]. 161.07 samples/sec. 298.008 ms/step.
Train [40/40]. 161.09 samples/sec. 297.971 ms/step.
Train benchmark of maxvit_rmlp_base_rw_224.sw_in12k done. 158.69 samples/sec, 297.97 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model maxvit_rmlp_base_rw_224.sw_in12k_ft_in1k created, param count: 116135788
Running inference benchmark on maxvit_rmlp_base_rw_224.sw_in12k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 594.87 samples/sec. 430.346 ms/step.
Infer [16/40]. 594.86 samples/sec. 430.352 ms/step.
Infer [24/40]. 594.85 samples/sec. 430.363 ms/step.
Infer [32/40]. 594.85 samples/sec. 430.362 ms/step.
Infer [40/40]. 594.84 samples/sec. 430.370 ms/step.
Inference benchmark of maxvit_rmlp_base_rw_224.sw_in12k_ft_in1k done. 594.79 samples/sec, 430.37 ms/step
Model maxvit_rmlp_base_rw_224.sw_in12k_ft_in1k created, param count: 116135788
Running train benchmark on maxvit_rmlp_base_rw_224.sw_in12k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 588.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 60.06 MiB is free. Including non-PyTorch memory, this process has 23.58 GiB memory in use. Of the allocated memory 21.74 GiB is allocated by PyTorch, and 623.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model maxvit_rmlp_base_rw_224.sw_in12k_ft_in1k created, param count: 116135788
Running train benchmark on maxvit_rmlp_base_rw_224.sw_in12k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 222.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 146.06 MiB is free. Including non-PyTorch memory, this process has 23.50 GiB memory in use. Of the allocated memory 22.06 GiB is allocated by PyTorch, and 212.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model maxvit_rmlp_base_rw_224.sw_in12k_ft_in1k created, param count: 116135788
Running train benchmark on maxvit_rmlp_base_rw_224.sw_in12k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 34.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 22.06 GiB is allocated by PyTorch, and 329.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model maxvit_rmlp_base_rw_224.sw_in12k_ft_in1k created, param count: 116135788
Running train benchmark on maxvit_rmlp_base_rw_224.sw_in12k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 44.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 22.09 GiB is allocated by PyTorch, and 288.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model maxvit_rmlp_base_rw_224.sw_in12k_ft_in1k created, param count: 116135788
Running train benchmark on maxvit_rmlp_base_rw_224.sw_in12k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 6.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 21.74 GiB is allocated by PyTorch, and 678.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model maxvit_rmlp_base_rw_224.sw_in12k_ft_in1k created, param count: 116135788
Running train benchmark on maxvit_rmlp_base_rw_224.sw_in12k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 48.
Train [8/40]. 164.35 samples/sec. 292.051 ms/step.
Train [16/40]. 164.32 samples/sec. 292.120 ms/step.
Train [24/40]. 164.28 samples/sec. 292.191 ms/step.
Train [32/40]. 164.27 samples/sec. 292.197 ms/step.
Train [40/40]. 164.28 samples/sec. 292.187 ms/step.
Train benchmark of maxvit_rmlp_base_rw_224.sw_in12k_ft_in1k done. 161.85 samples/sec, 292.19 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model maxvit_rmlp_base_rw_384.sw_in12k_ft_in1k created, param count: 116135788
Running inference benchmark on maxvit_rmlp_base_rw_384.sw_in12k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 177.09 samples/sec. 1445.574 ms/step.
Infer [16/40]. 177.09 samples/sec. 1445.603 ms/step.
Infer [24/40]. 177.09 samples/sec. 1445.615 ms/step.
Infer [32/40]. 177.09 samples/sec. 1445.625 ms/step.
Infer [40/40]. 177.08 samples/sec. 1445.711 ms/step.
Inference benchmark of maxvit_rmlp_base_rw_384.sw_in12k_ft_in1k done. 177.07 samples/sec, 1445.71 ms/step
Model maxvit_rmlp_base_rw_384.sw_in12k_ft_in1k created, param count: 116135788
Running train benchmark on maxvit_rmlp_base_rw_384.sw_in12k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 1.12 GiB. GPU 0 has a total capacty of 23.65 GiB of which 156.06 MiB is free. Including non-PyTorch memory, this process has 23.49 GiB memory in use. Of the allocated memory 21.97 GiB is allocated by PyTorch, and 296.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model maxvit_rmlp_base_rw_384.sw_in12k_ft_in1k created, param count: 116135788
Running train benchmark on maxvit_rmlp_base_rw_384.sw_in12k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 1.42 GiB. GPU 0 has a total capacty of 23.65 GiB of which 700.06 MiB is free. Including non-PyTorch memory, this process has 22.96 GiB memory in use. Of the allocated memory 21.35 GiB is allocated by PyTorch, and 386.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model maxvit_rmlp_base_rw_384.sw_in12k_ft_in1k created, param count: 116135788
Running train benchmark on maxvit_rmlp_base_rw_384.sw_in12k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 864.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 642.06 MiB is free. Including non-PyTorch memory, this process has 23.01 GiB memory in use. Of the allocated memory 21.41 GiB is allocated by PyTorch, and 384.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model maxvit_rmlp_base_rw_384.sw_in12k_ft_in1k created, param count: 116135788
Running train benchmark on maxvit_rmlp_base_rw_384.sw_in12k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 730.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 570.06 MiB is free. Including non-PyTorch memory, this process has 23.08 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 355.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model maxvit_rmlp_base_rw_384.sw_in12k_ft_in1k created, param count: 116135788
Running train benchmark on maxvit_rmlp_base_rw_384.sw_in12k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 216.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 126.06 MiB is free. Including non-PyTorch memory, this process has 23.52 GiB memory in use. Of the allocated memory 22.11 GiB is allocated by PyTorch, and 176.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model maxvit_rmlp_base_rw_384.sw_in12k_ft_in1k created, param count: 116135788
Running train benchmark on maxvit_rmlp_base_rw_384.sw_in12k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 162.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 138.06 MiB is free. Including non-PyTorch memory, this process has 23.51 GiB memory in use. Of the allocated memory 21.91 GiB is allocated by PyTorch, and 377.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model maxvit_rmlp_base_rw_384.sw_in12k_ft_in1k created, param count: 116135788
Running train benchmark on maxvit_rmlp_base_rw_384.sw_in12k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 32.
ERROR: "CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 6.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.31 GiB is allocated by PyTorch, and 96.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model maxvit_rmlp_base_rw_384.sw_in12k_ft_in1k created, param count: 116135788
Running train benchmark on maxvit_rmlp_base_rw_384.sw_in12k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 24.
ERROR: "CUDA out of memory. Tried to allocate 46.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 20.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 21.78 GiB is allocated by PyTorch, and 621.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model maxvit_rmlp_base_rw_384.sw_in12k_ft_in1k created, param count: 116135788
Running train benchmark on maxvit_rmlp_base_rw_384.sw_in12k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 16.
Train [8/40]. 51.96 samples/sec. 307.927 ms/step.
Train [16/40]. 51.94 samples/sec. 308.051 ms/step.
Train [24/40]. 51.94 samples/sec. 308.075 ms/step.
Train [32/40]. 51.94 samples/sec. 308.070 ms/step.
Train [40/40]. 51.93 samples/sec. 308.083 ms/step.
Train benchmark of maxvit_rmlp_base_rw_384.sw_in12k_ft_in1k done. 51.16 samples/sec, 308.08 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model maxvit_rmlp_nano_rw_256.sw_in1k created, param count: 15501452
Running inference benchmark on maxvit_rmlp_nano_rw_256.sw_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
Infer [8/40]. 1905.84 samples/sec. 134.324 ms/step.
Infer [16/40]. 1905.96 samples/sec. 134.315 ms/step.
Infer [24/40]. 1905.95 samples/sec. 134.317 ms/step.
Infer [32/40]. 1905.96 samples/sec. 134.315 ms/step.
Infer [40/40]. 1905.90 samples/sec. 134.320 ms/step.
Inference benchmark of maxvit_rmlp_nano_rw_256.sw_in1k done. 1905.47 samples/sec, 134.32 ms/step
Model maxvit_rmlp_nano_rw_256.sw_in1k created, param count: 15501452
Running train benchmark on maxvit_rmlp_nano_rw_256.sw_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 186.06 MiB is free. Including non-PyTorch memory, this process has 23.46 GiB memory in use. Of the allocated memory 22.16 GiB is allocated by PyTorch, and 72.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model maxvit_rmlp_nano_rw_256.sw_in1k created, param count: 15501452
Running train benchmark on maxvit_rmlp_nano_rw_256.sw_in1k for 40 steps w/ input size (3, 256, 256) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 34.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 22.30 GiB is allocated by PyTorch, and 80.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model maxvit_rmlp_nano_rw_256.sw_in1k created, param count: 15501452
Running train benchmark on maxvit_rmlp_nano_rw_256.sw_in1k for 40 steps w/ input size (3, 256, 256) and batch size 128.
Train [8/40]. 608.48 samples/sec. 210.360 ms/step.
Train [16/40]. 608.43 samples/sec. 210.378 ms/step.
Train [24/40]. 608.40 samples/sec. 210.390 ms/step.
Train [32/40]. 608.40 samples/sec. 210.388 ms/step.
Train [40/40]. 608.40 samples/sec. 210.387 ms/step.
Train benchmark of maxvit_rmlp_nano_rw_256.sw_in1k done. 603.15 samples/sec, 210.39 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model maxvit_rmlp_pico_rw_256.sw_in1k created, param count: 7515980
Running inference benchmark on maxvit_rmlp_pico_rw_256.sw_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
Infer [8/40]. 2721.15 samples/sec. 94.078 ms/step.
Infer [16/40]. 2721.00 samples/sec. 94.083 ms/step.
Infer [24/40]. 2721.07 samples/sec. 94.080 ms/step.
Infer [32/40]. 2721.11 samples/sec. 94.079 ms/step.
Infer [40/40]. 2721.14 samples/sec. 94.078 ms/step.
Inference benchmark of maxvit_rmlp_pico_rw_256.sw_in1k done. 2720.36 samples/sec, 94.08 ms/step
Model maxvit_rmlp_pico_rw_256.sw_in1k created, param count: 7515980
Running train benchmark on maxvit_rmlp_pico_rw_256.sw_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 6.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.36 GiB is allocated by PyTorch, and 41.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model maxvit_rmlp_pico_rw_256.sw_in1k created, param count: 7515980
Running train benchmark on maxvit_rmlp_pico_rw_256.sw_in1k for 40 steps w/ input size (3, 256, 256) and batch size 192.
Train [8/40]. 776.32 samples/sec. 247.322 ms/step.
Train [16/40]. 776.26 samples/sec. 247.338 ms/step.
Train [24/40]. 776.25 samples/sec. 247.343 ms/step.
Train [32/40]. 776.25 samples/sec. 247.345 ms/step.
Train [40/40]. 776.27 samples/sec. 247.337 ms/step.
Train benchmark of maxvit_rmlp_pico_rw_256.sw_in1k done. 768.32 samples/sec, 247.34 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model maxvit_rmlp_small_rw_224.sw_in1k created, param count: 64895044
Running inference benchmark on maxvit_rmlp_small_rw_224.sw_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1111.88 samples/sec. 230.241 ms/step.
Infer [16/40]. 1111.84 samples/sec. 230.249 ms/step.
Infer [24/40]. 1111.81 samples/sec. 230.254 ms/step.
Infer [32/40]. 1111.83 samples/sec. 230.251 ms/step.
Infer [40/40]. 1111.83 samples/sec. 230.251 ms/step.
Inference benchmark of maxvit_rmlp_small_rw_224.sw_in1k done. 1111.63 samples/sec, 230.25 ms/step
Model maxvit_rmlp_small_rw_224.sw_in1k created, param count: 64895044
Running train benchmark on maxvit_rmlp_small_rw_224.sw_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 226.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 122.06 MiB is free. Including non-PyTorch memory, this process has 23.52 GiB memory in use. Of the allocated memory 22.00 GiB is allocated by PyTorch, and 299.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model maxvit_rmlp_small_rw_224.sw_in1k created, param count: 64895044
Running train benchmark on maxvit_rmlp_small_rw_224.sw_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 222.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 14.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.18 GiB is allocated by PyTorch, and 221.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model maxvit_rmlp_small_rw_224.sw_in1k created, param count: 64895044
Running train benchmark on maxvit_rmlp_small_rw_224.sw_in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 74.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 40.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 21.91 GiB is allocated by PyTorch, and 471.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model maxvit_rmlp_small_rw_224.sw_in1k created, param count: 64895044
Running train benchmark on maxvit_rmlp_small_rw_224.sw_in1k for 40 steps w/ input size (3, 224, 224) and batch size 96.
Train [8/40]. 367.47 samples/sec. 261.246 ms/step.
Train [16/40]. 367.49 samples/sec. 261.232 ms/step.
Train [24/40]. 367.49 samples/sec. 261.230 ms/step.
Train [32/40]. 367.50 samples/sec. 261.221 ms/step.
Train [40/40]. 367.45 samples/sec. 261.258 ms/step.
Train benchmark of maxvit_rmlp_small_rw_224.sw_in1k done. 363.69 samples/sec, 261.26 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model maxvit_rmlp_tiny_rw_256.sw_in1k created, param count: 29148896
Running inference benchmark on maxvit_rmlp_tiny_rw_256.sw_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
Infer [8/40]. 1285.26 samples/sec. 199.182 ms/step.
Infer [16/40]. 1285.30 samples/sec. 199.176 ms/step.
Infer [24/40]. 1285.34 samples/sec. 199.169 ms/step.
Infer [32/40]. 1285.35 samples/sec. 199.168 ms/step.
Infer [40/40]. 1285.36 samples/sec. 199.167 ms/step.
Inference benchmark of maxvit_rmlp_tiny_rw_256.sw_in1k done. 1285.13 samples/sec, 199.17 ms/step
Model maxvit_rmlp_tiny_rw_256.sw_in1k created, param count: 29148896
Running train benchmark on maxvit_rmlp_tiny_rw_256.sw_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 512.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 62.06 MiB is free. Including non-PyTorch memory, this process has 23.58 GiB memory in use. Of the allocated memory 22.23 GiB is allocated by PyTorch, and 120.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model maxvit_rmlp_tiny_rw_256.sw_in1k created, param count: 29148896
Running train benchmark on maxvit_rmlp_tiny_rw_256.sw_in1k for 40 steps w/ input size (3, 256, 256) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 20.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.26 GiB is allocated by PyTorch, and 135.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model maxvit_rmlp_tiny_rw_256.sw_in1k created, param count: 29148896
Running train benchmark on maxvit_rmlp_tiny_rw_256.sw_in1k for 40 steps w/ input size (3, 256, 256) and batch size 128.
Train [8/40]. 419.75 samples/sec. 304.942 ms/step.
Train [16/40]. 419.77 samples/sec. 304.926 ms/step.
Train [24/40]. 419.78 samples/sec. 304.923 ms/step.
Train [32/40]. 419.78 samples/sec. 304.919 ms/step.
Train [40/40]. 419.79 samples/sec. 304.912 ms/step.
Train benchmark of maxvit_rmlp_tiny_rw_256.sw_in1k done. 416.17 samples/sec, 304.91 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model maxvit_small_tf_224.in1k created, param count: 68927956
Running inference benchmark on maxvit_small_tf_224.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 958.81 samples/sec. 266.997 ms/step.
Infer [16/40]. 958.50 samples/sec. 267.084 ms/step.
Infer [24/40]. 958.23 samples/sec. 267.158 ms/step.
Infer [32/40]. 958.10 samples/sec. 267.195 ms/step.
Infer [40/40]. 958.01 samples/sec. 267.222 ms/step.
Inference benchmark of maxvit_small_tf_224.in1k done. 957.86 samples/sec, 267.22 ms/step
Model maxvit_small_tf_224.in1k created, param count: 68927956
Running train benchmark on maxvit_small_tf_224.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 442.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 402.06 MiB is free. Including non-PyTorch memory, this process has 23.25 GiB memory in use. Of the allocated memory 21.73 GiB is allocated by PyTorch, and 299.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model maxvit_small_tf_224.in1k created, param count: 68927956
Running train benchmark on maxvit_small_tf_224.in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 882.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 212.06 MiB is free. Including non-PyTorch memory, this process has 23.43 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 282.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model maxvit_small_tf_224.in1k created, param count: 68927956
Running train benchmark on maxvit_small_tf_224.in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 74.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 68.06 MiB is free. Including non-PyTorch memory, this process has 23.57 GiB memory in use. Of the allocated memory 22.12 GiB is allocated by PyTorch, and 226.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model maxvit_small_tf_224.in1k created, param count: 68927956
Running train benchmark on maxvit_small_tf_224.in1k for 40 steps w/ input size (3, 224, 224) and batch size 96.
Train [8/40]. 324.84 samples/sec. 295.528 ms/step.
Train [16/40]. 324.73 samples/sec. 295.627 ms/step.
Train [24/40]. 324.63 samples/sec. 295.719 ms/step.
Train [32/40]. 324.58 samples/sec. 295.764 ms/step.
Train [40/40]. 324.56 samples/sec. 295.787 ms/step.
Train benchmark of maxvit_small_tf_224.in1k done. 321.68 samples/sec, 295.79 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model maxvit_small_tf_384.in1k created, param count: 69018676
Running inference benchmark on maxvit_small_tf_384.in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 6.75 GiB. GPU 0 has a total capacty of 23.65 GiB of which 5.82 GiB is free. Including non-PyTorch memory, this process has 17.82 GiB memory in use. Of the allocated memory 15.40 GiB is allocated by PyTorch, and 1.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model maxvit_small_tf_384.in1k created, param count: 69018676
Running inference benchmark on maxvit_small_tf_384.in1k for 40 steps w/ input size (3, 384, 384) and batch size 192.
Infer [8/40]. 290.36 samples/sec. 661.242 ms/step.
Infer [16/40]. 290.36 samples/sec. 661.258 ms/step.
Infer [24/40]. 290.34 samples/sec. 661.301 ms/step.
Infer [32/40]. 290.33 samples/sec. 661.318 ms/step.
Infer [40/40]. 290.32 samples/sec. 661.332 ms/step.
Inference benchmark of maxvit_small_tf_384.in1k done. 290.30 samples/sec, 661.33 ms/step
Model maxvit_small_tf_384.in1k created, param count: 69018676
Running train benchmark on maxvit_small_tf_384.in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 6.75 GiB. GPU 0 has a total capacty of 23.65 GiB of which 200.06 MiB is free. Including non-PyTorch memory, this process has 23.45 GiB memory in use. Of the allocated memory 21.52 GiB is allocated by PyTorch, and 709.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model maxvit_small_tf_384.in1k created, param count: 69018676
Running train benchmark on maxvit_small_tf_384.in1k for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 23.65 GiB of which 486.06 MiB is free. Including non-PyTorch memory, this process has 23.17 GiB memory in use. Of the allocated memory 21.24 GiB is allocated by PyTorch, and 712.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model maxvit_small_tf_384.in1k created, param count: 69018676
Running train benchmark on maxvit_small_tf_384.in1k for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 216.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 14.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.17 GiB is allocated by PyTorch, and 229.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model maxvit_small_tf_384.in1k created, param count: 69018676
Running train benchmark on maxvit_small_tf_384.in1k for 40 steps w/ input size (3, 384, 384) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 648.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 180.06 MiB is free. Including non-PyTorch memory, this process has 23.46 GiB memory in use. Of the allocated memory 21.82 GiB is allocated by PyTorch, and 420.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model maxvit_small_tf_384.in1k created, param count: 69018676
Running train benchmark on maxvit_small_tf_384.in1k for 40 steps w/ input size (3, 384, 384) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 864.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 276.06 MiB is free. Including non-PyTorch memory, this process has 23.37 GiB memory in use. Of the allocated memory 21.86 GiB is allocated by PyTorch, and 282.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model maxvit_small_tf_384.in1k created, param count: 69018676
Running train benchmark on maxvit_small_tf_384.in1k for 40 steps w/ input size (3, 384, 384) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 34.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 22.19 GiB is allocated by PyTorch, and 187.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model maxvit_small_tf_384.in1k created, param count: 69018676
Running train benchmark on maxvit_small_tf_384.in1k for 40 steps w/ input size (3, 384, 384) and batch size 32.
Train [8/40]. 101.06 samples/sec. 316.658 ms/step.
Train [16/40]. 101.06 samples/sec. 316.643 ms/step.
Train [24/40]. 101.06 samples/sec. 316.633 ms/step.
Train [32/40]. 101.06 samples/sec. 316.631 ms/step.
Train [40/40]. 101.06 samples/sec. 316.636 ms/step.
Train benchmark of maxvit_small_tf_384.in1k done. 100.19 samples/sec, 316.64 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model maxvit_small_tf_512.in1k created, param count: 69127540
Running inference benchmark on maxvit_small_tf_512.in1k for 40 steps w/ input size (3, 512, 512) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 12.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 5.57 GiB is free. Including non-PyTorch memory, this process has 18.07 GiB memory in use. Of the allocated memory 15.27 GiB is allocated by PyTorch, and 1.57 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model maxvit_small_tf_512.in1k created, param count: 69127540
Running inference benchmark on maxvit_small_tf_512.in1k for 40 steps w/ input size (3, 512, 512) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 9.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 584.06 MiB is free. Including non-PyTorch memory, this process has 23.07 GiB memory in use. Of the allocated memory 20.49 GiB is allocated by PyTorch, and 1.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model maxvit_small_tf_512.in1k created, param count: 69127540
Running inference benchmark on maxvit_small_tf_512.in1k for 40 steps w/ input size (3, 512, 512) and batch size 128.
Infer [8/40]. 148.86 samples/sec. 859.867 ms/step.
Infer [16/40]. 148.85 samples/sec. 859.915 ms/step.
Infer [24/40]. 148.85 samples/sec. 859.942 ms/step.
Infer [32/40]. 148.85 samples/sec. 859.950 ms/step.
Infer [40/40]. 148.84 samples/sec. 859.959 ms/step.
Inference benchmark of maxvit_small_tf_512.in1k done. 148.84 samples/sec, 859.96 ms/step
Model maxvit_small_tf_512.in1k created, param count: 69127540
Running train benchmark on maxvit_small_tf_512.in1k for 40 steps w/ input size (3, 512, 512) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 12.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 7.19 GiB is free. Including non-PyTorch memory, this process has 16.45 GiB memory in use. Of the allocated memory 14.15 GiB is allocated by PyTorch, and 1.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model maxvit_small_tf_512.in1k created, param count: 69127540
Running train benchmark on maxvit_small_tf_512.in1k for 40 steps w/ input size (3, 512, 512) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 9.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.79 GiB is free. Including non-PyTorch memory, this process has 21.86 GiB memory in use. Of the allocated memory 19.65 GiB is allocated by PyTorch, and 1001.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model maxvit_small_tf_512.in1k created, param count: 69127540
Running train benchmark on maxvit_small_tf_512.in1k for 40 steps w/ input size (3, 512, 512) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 6.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 2.57 GiB is free. Including non-PyTorch memory, this process has 21.07 GiB memory in use. Of the allocated memory 19.15 GiB is allocated by PyTorch, and 710.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model maxvit_small_tf_512.in1k created, param count: 69127540
Running train benchmark on maxvit_small_tf_512.in1k for 40 steps w/ input size (3, 512, 512) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 4.54 GiB. GPU 0 has a total capacty of 23.65 GiB of which 2.82 GiB is free. Including non-PyTorch memory, this process has 20.82 GiB memory in use. Of the allocated memory 18.90 GiB is allocated by PyTorch, and 714.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model maxvit_small_tf_512.in1k created, param count: 69127540
Running train benchmark on maxvit_small_tf_512.in1k for 40 steps w/ input size (3, 512, 512) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 1.50 GiB. GPU 0 has a total capacty of 23.65 GiB of which 362.06 MiB is free. Including non-PyTorch memory, this process has 23.29 GiB memory in use. Of the allocated memory 21.88 GiB is allocated by PyTorch, and 178.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model maxvit_small_tf_512.in1k created, param count: 69127540
Running train benchmark on maxvit_small_tf_512.in1k for 40 steps w/ input size (3, 512, 512) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 576.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 174.06 MiB is free. Including non-PyTorch memory, this process has 23.47 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 164.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model maxvit_small_tf_512.in1k created, param count: 69127540
Running train benchmark on maxvit_small_tf_512.in1k for 40 steps w/ input size (3, 512, 512) and batch size 32.
ERROR: "CUDA out of memory. Tried to allocate 782.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 36.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 22.26 GiB is allocated by PyTorch, and 117.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model maxvit_small_tf_512.in1k created, param count: 69127540
Running train benchmark on maxvit_small_tf_512.in1k for 40 steps w/ input size (3, 512, 512) and batch size 24.
ERROR: "CUDA out of memory. Tried to allocate 288.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 168.06 MiB is free. Including non-PyTorch memory, this process has 23.48 GiB memory in use. Of the allocated memory 22.12 GiB is allocated by PyTorch, and 128.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model maxvit_small_tf_512.in1k created, param count: 69127540
Running train benchmark on maxvit_small_tf_512.in1k for 40 steps w/ input size (3, 512, 512) and batch size 16.
Train [8/40]. 51.77 samples/sec. 309.065 ms/step.
Train [16/40]. 51.77 samples/sec. 309.057 ms/step.
Train [24/40]. 51.77 samples/sec. 309.065 ms/step.
Train [32/40]. 51.77 samples/sec. 309.065 ms/step.
Train [40/40]. 51.77 samples/sec. 309.066 ms/step.
Train benchmark of maxvit_small_tf_512.in1k done. 51.30 samples/sec, 309.07 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model maxvit_tiny_rw_224.sw_in1k created, param count: 29057312
Running inference benchmark on maxvit_tiny_rw_224.sw_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1709.57 samples/sec. 149.746 ms/step.
Infer [16/40]. 1709.68 samples/sec. 149.736 ms/step.
Infer [24/40]. 1709.63 samples/sec. 149.740 ms/step.
Infer [32/40]. 1709.63 samples/sec. 149.740 ms/step.
Infer [40/40]. 1709.54 samples/sec. 149.748 ms/step.
Inference benchmark of maxvit_tiny_rw_224.sw_in1k done. 1709.19 samples/sec, 149.75 ms/step
Model maxvit_tiny_rw_224.sw_in1k created, param count: 29057312
Running train benchmark on maxvit_tiny_rw_224.sw_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 148.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 24.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.26 GiB is allocated by PyTorch, and 127.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model maxvit_tiny_rw_224.sw_in1k created, param count: 29057312
Running train benchmark on maxvit_tiny_rw_224.sw_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 32.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 22.12 GiB is allocated by PyTorch, and 260.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model maxvit_tiny_rw_224.sw_in1k created, param count: 29057312
Running train benchmark on maxvit_tiny_rw_224.sw_in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
Train [8/40]. 555.67 samples/sec. 230.351 ms/step.
Train [16/40]. 555.69 samples/sec. 230.342 ms/step.
Train [24/40]. 555.69 samples/sec. 230.345 ms/step.
Train [32/40]. 555.69 samples/sec. 230.345 ms/step.
Train [40/40]. 555.68 samples/sec. 230.347 ms/step.
Train benchmark of maxvit_tiny_rw_224.sw_in1k done. 550.23 samples/sec, 230.35 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model maxvit_tiny_tf_224.in1k created, param count: 30916528
Running inference benchmark on maxvit_tiny_tf_224.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1475.82 samples/sec. 173.463 ms/step.
Infer [16/40]. 1475.82 samples/sec. 173.463 ms/step.
Infer [24/40]. 1475.56 samples/sec. 173.493 ms/step.
Infer [32/40]. 1475.29 samples/sec. 173.526 ms/step.
Infer [40/40]. 1475.11 samples/sec. 173.546 ms/step.
Inference benchmark of maxvit_tiny_tf_224.in1k done. 1474.85 samples/sec, 173.55 ms/step
Model maxvit_tiny_tf_224.in1k created, param count: 30916528
Running train benchmark on maxvit_tiny_tf_224.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 196.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 110.06 MiB is free. Including non-PyTorch memory, this process has 23.53 GiB memory in use. Of the allocated memory 22.16 GiB is allocated by PyTorch, and 143.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model maxvit_tiny_tf_224.in1k created, param count: 30916528
Running train benchmark on maxvit_tiny_tf_224.in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 74.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 16.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.24 GiB is allocated by PyTorch, and 159.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model maxvit_tiny_tf_224.in1k created, param count: 30916528
Running train benchmark on maxvit_tiny_tf_224.in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
Train [8/40]. 489.18 samples/sec. 261.663 ms/step.
Train [16/40]. 489.17 samples/sec. 261.670 ms/step.
Train [24/40]. 489.17 samples/sec. 261.670 ms/step.
Train [32/40]. 489.17 samples/sec. 261.669 ms/step.
Train [40/40]. 489.18 samples/sec. 261.661 ms/step.
Train benchmark of maxvit_tiny_tf_224.in1k done. 484.43 samples/sec, 261.66 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model maxvit_tiny_tf_384.in1k created, param count: 30977008
Running inference benchmark on maxvit_tiny_tf_384.in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 438.21 samples/sec. 584.192 ms/step.
Infer [16/40]. 438.22 samples/sec. 584.176 ms/step.
Infer [24/40]. 438.23 samples/sec. 584.174 ms/step.
Infer [32/40]. 438.21 samples/sec. 584.188 ms/step.
Infer [40/40]. 438.21 samples/sec. 584.200 ms/step.
Inference benchmark of maxvit_tiny_tf_384.in1k done. 438.17 samples/sec, 584.20 ms/step
Model maxvit_tiny_tf_384.in1k created, param count: 30977008
Running train benchmark on maxvit_tiny_tf_384.in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 4.55 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.31 GiB is free. Including non-PyTorch memory, this process has 22.33 GiB memory in use. Of the allocated memory 21.03 GiB is allocated by PyTorch, and 70.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model maxvit_tiny_tf_384.in1k created, param count: 30977008
Running train benchmark on maxvit_tiny_tf_384.in1k for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 972.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 546.06 MiB is free. Including non-PyTorch memory, this process has 23.11 GiB memory in use. Of the allocated memory 21.85 GiB is allocated by PyTorch, and 27.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model maxvit_tiny_tf_384.in1k created, param count: 30977008
Running train benchmark on maxvit_tiny_tf_384.in1k for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 576.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 18.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.14 GiB is allocated by PyTorch, and 256.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model maxvit_tiny_tf_384.in1k created, param count: 30977008
Running train benchmark on maxvit_tiny_tf_384.in1k for 40 steps w/ input size (3, 384, 384) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 864.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 452.06 MiB is free. Including non-PyTorch memory, this process has 23.20 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 179.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model maxvit_tiny_tf_384.in1k created, param count: 30977008
Running train benchmark on maxvit_tiny_tf_384.in1k for 40 steps w/ input size (3, 384, 384) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 72.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 46.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 22.27 GiB is allocated by PyTorch, and 98.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model maxvit_tiny_tf_384.in1k created, param count: 30977008
Running train benchmark on maxvit_tiny_tf_384.in1k for 40 steps w/ input size (3, 384, 384) and batch size 48.
Train [8/40]. 131.02 samples/sec. 366.350 ms/step.
Train [16/40]. 131.03 samples/sec. 366.325 ms/step.
Train [24/40]. 130.85 samples/sec. 366.830 ms/step.
Train [32/40]. 130.85 samples/sec. 366.829 ms/step.
Train [40/40]. 130.82 samples/sec. 366.924 ms/step.
Train benchmark of maxvit_tiny_tf_384.in1k done. 129.87 samples/sec, 366.92 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model maxvit_tiny_tf_512.in1k created, param count: 31049584
Running inference benchmark on maxvit_tiny_tf_512.in1k for 40 steps w/ input size (3, 512, 512) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.56 GiB is free. Including non-PyTorch memory, this process has 22.08 GiB memory in use. Of the allocated memory 18.95 GiB is allocated by PyTorch, and 1.90 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model maxvit_tiny_tf_512.in1k created, param count: 31049584
Running inference benchmark on maxvit_tiny_tf_512.in1k for 40 steps w/ input size (3, 512, 512) and batch size 192.
Infer [8/40]. 223.55 samples/sec. 858.852 ms/step.
Infer [16/40]. 223.55 samples/sec. 858.887 ms/step.
Infer [24/40]. 223.54 samples/sec. 858.902 ms/step.
Infer [32/40]. 223.55 samples/sec. 858.883 ms/step.
Infer [40/40]. 223.55 samples/sec. 858.883 ms/step.
Inference benchmark of maxvit_tiny_tf_512.in1k done. 223.53 samples/sec, 858.88 ms/step
Model maxvit_tiny_tf_512.in1k created, param count: 31049584
Running train benchmark on maxvit_tiny_tf_512.in1k for 40 steps w/ input size (3, 512, 512) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 7.18 GiB is free. Including non-PyTorch memory, this process has 16.46 GiB memory in use. Of the allocated memory 13.33 GiB is allocated by PyTorch, and 1.90 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model maxvit_tiny_tf_512.in1k created, param count: 31049584
Running train benchmark on maxvit_tiny_tf_512.in1k for 40 steps w/ input size (3, 512, 512) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 6.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 5.06 GiB is free. Including non-PyTorch memory, this process has 18.58 GiB memory in use. Of the allocated memory 16.01 GiB is allocated by PyTorch, and 1.34 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model maxvit_tiny_tf_512.in1k created, param count: 31049584
Running train benchmark on maxvit_tiny_tf_512.in1k for 40 steps w/ input size (3, 512, 512) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 4.03 GiB. GPU 0 has a total capacty of 23.65 GiB of which 3.55 GiB is free. Including non-PyTorch memory, this process has 20.09 GiB memory in use. Of the allocated memory 18.70 GiB is allocated by PyTorch, and 158.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model maxvit_tiny_tf_512.in1k created, param count: 31049584
Running train benchmark on maxvit_tiny_tf_512.in1k for 40 steps w/ input size (3, 512, 512) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 576.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 346.06 MiB is free. Including non-PyTorch memory, this process has 23.30 GiB memory in use. Of the allocated memory 21.98 GiB is allocated by PyTorch, and 96.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model maxvit_tiny_tf_512.in1k created, param count: 31049584
Running train benchmark on maxvit_tiny_tf_512.in1k for 40 steps w/ input size (3, 512, 512) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 32.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 22.32 GiB is allocated by PyTorch, and 62.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model maxvit_tiny_tf_512.in1k created, param count: 31049584
Running train benchmark on maxvit_tiny_tf_512.in1k for 40 steps w/ input size (3, 512, 512) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 768.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 144.06 MiB is free. Including non-PyTorch memory, this process has 23.50 GiB memory in use. Of the allocated memory 22.19 GiB is allocated by PyTorch, and 81.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model maxvit_tiny_tf_512.in1k created, param count: 31049584
Running train benchmark on maxvit_tiny_tf_512.in1k for 40 steps w/ input size (3, 512, 512) and batch size 32.
ERROR: "CUDA out of memory. Tried to allocate 128.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 26.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.34 GiB is allocated by PyTorch, and 49.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model maxvit_tiny_tf_512.in1k created, param count: 31049584
Running train benchmark on maxvit_tiny_tf_512.in1k for 40 steps w/ input size (3, 512, 512) and batch size 24.
Train [8/40]. 77.65 samples/sec. 309.095 ms/step.
Train [16/40]. 77.64 samples/sec. 309.127 ms/step.
Train [24/40]. 77.64 samples/sec. 309.136 ms/step.
Train [32/40]. 77.63 samples/sec. 309.140 ms/step.
Train [40/40]. 77.63 samples/sec. 309.143 ms/step.
Train benchmark of maxvit_tiny_tf_512.in1k done. 76.96 samples/sec, 309.14 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model maxvit_xlarge_tf_384.in21k_ft_in1k created, param count: 475323472
Running inference benchmark on maxvit_xlarge_tf_384.in21k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 13.50 GiB. GPU 0 has a total capacty of 23.65 GiB of which 7.78 GiB is free. Including non-PyTorch memory, this process has 15.86 GiB memory in use. Of the allocated memory 8.71 GiB is allocated by PyTorch, and 5.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model maxvit_xlarge_tf_384.in21k_ft_in1k created, param count: 475323472
Running inference benchmark on maxvit_xlarge_tf_384.in21k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 10.12 GiB. GPU 0 has a total capacty of 23.65 GiB of which 7.79 GiB is free. Including non-PyTorch memory, this process has 15.85 GiB memory in use. Of the allocated memory 14.38 GiB is allocated by PyTorch, and 252.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model maxvit_xlarge_tf_384.in21k_ft_in1k created, param count: 475323472
Running inference benchmark on maxvit_xlarge_tf_384.in21k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 6.75 GiB. GPU 0 has a total capacty of 23.65 GiB of which 2.67 GiB is free. Including non-PyTorch memory, this process has 20.97 GiB memory in use. Of the allocated memory 16.62 GiB is allocated by PyTorch, and 3.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model maxvit_xlarge_tf_384.in21k_ft_in1k created, param count: 475323472
Running inference benchmark on maxvit_xlarge_tf_384.in21k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 96.
Infer [8/40]. 71.23 samples/sec. 1347.739 ms/step.
Infer [16/40]. 71.23 samples/sec. 1347.725 ms/step.
Infer [24/40]. 71.23 samples/sec. 1347.722 ms/step.
Infer [32/40]. 71.23 samples/sec. 1347.739 ms/step.
Infer [40/40]. 71.23 samples/sec. 1347.754 ms/step.
Inference benchmark of maxvit_xlarge_tf_384.in21k_ft_in1k done. 71.23 samples/sec, 1347.75 ms/step
Model maxvit_xlarge_tf_384.in21k_ft_in1k created, param count: 475323472
Running train benchmark on maxvit_xlarge_tf_384.in21k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 3.38 GiB. GPU 0 has a total capacty of 23.65 GiB of which 838.06 MiB is free. Including non-PyTorch memory, this process has 22.82 GiB memory in use. Of the allocated memory 19.05 GiB is allocated by PyTorch, and 2.55 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model maxvit_xlarge_tf_384.in21k_ft_in1k created, param count: 475323472
Running train benchmark on maxvit_xlarge_tf_384.in21k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 10.12 GiB. GPU 0 has a total capacty of 23.65 GiB of which 2.72 GiB is free. Including non-PyTorch memory, this process has 20.92 GiB memory in use. Of the allocated memory 17.07 GiB is allocated by PyTorch, and 2.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model maxvit_xlarge_tf_384.in21k_ft_in1k created, param count: 475323472
Running train benchmark on maxvit_xlarge_tf_384.in21k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 6.75 GiB. GPU 0 has a total capacty of 23.65 GiB of which 2.56 GiB is free. Including non-PyTorch memory, this process has 21.08 GiB memory in use. Of the allocated memory 18.42 GiB is allocated by PyTorch, and 1.43 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model maxvit_xlarge_tf_384.in21k_ft_in1k created, param count: 475323472
Running train benchmark on maxvit_xlarge_tf_384.in21k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 5.06 GiB. GPU 0 has a total capacty of 23.65 GiB of which 2.78 GiB is free. Including non-PyTorch memory, this process has 20.86 GiB memory in use. Of the allocated memory 19.12 GiB is allocated by PyTorch, and 525.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model maxvit_xlarge_tf_384.in21k_ft_in1k created, param count: 475323472
Running train benchmark on maxvit_xlarge_tf_384.in21k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 972.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 456.06 MiB is free. Including non-PyTorch memory, this process has 23.20 GiB memory in use. Of the allocated memory 21.73 GiB is allocated by PyTorch, and 242.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model maxvit_xlarge_tf_384.in21k_ft_in1k created, param count: 475323472
Running train benchmark on maxvit_xlarge_tf_384.in21k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 648.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 274.06 MiB is free. Including non-PyTorch memory, this process has 23.37 GiB memory in use. Of the allocated memory 21.77 GiB is allocated by PyTorch, and 384.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model maxvit_xlarge_tf_384.in21k_ft_in1k created, param count: 475323472
Running train benchmark on maxvit_xlarge_tf_384.in21k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 32.
ERROR: "CUDA out of memory. Tried to allocate 432.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 68.06 MiB is free. Including non-PyTorch memory, this process has 23.57 GiB memory in use. Of the allocated memory 21.68 GiB is allocated by PyTorch, and 684.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model maxvit_xlarge_tf_384.in21k_ft_in1k created, param count: 475323472
Running train benchmark on maxvit_xlarge_tf_384.in21k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 24.
ERROR: "CUDA out of memory. Tried to allocate 162.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 68.06 MiB is free. Including non-PyTorch memory, this process has 23.57 GiB memory in use. Of the allocated memory 22.12 GiB is allocated by PyTorch, and 230.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model maxvit_xlarge_tf_384.in21k_ft_in1k created, param count: 475323472
Running train benchmark on maxvit_xlarge_tf_384.in21k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 16.
ERROR: "CUDA out of memory. Tried to allocate 122.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 122.06 MiB is free. Including non-PyTorch memory, this process has 23.52 GiB memory in use. Of the allocated memory 21.84 GiB is allocated by PyTorch, and 461.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 12 for retry.
Model maxvit_xlarge_tf_384.in21k_ft_in1k created, param count: 475323472
Running train benchmark on maxvit_xlarge_tf_384.in21k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 12.
ERROR: "CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 12.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 21.95 GiB is allocated by PyTorch, and 460.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 8 for retry.
Model maxvit_xlarge_tf_384.in21k_ft_in1k created, param count: 475323472
Running train benchmark on maxvit_xlarge_tf_384.in21k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 8.
Train [8/40]. 23.90 samples/sec. 334.722 ms/step.
Train [16/40]. 24.05 samples/sec. 332.647 ms/step.
Train [24/40]. 24.10 samples/sec. 331.961 ms/step.
Train [32/40]. 24.12 samples/sec. 331.624 ms/step.
Train [40/40]. 24.14 samples/sec. 331.393 ms/step.
Train benchmark of maxvit_xlarge_tf_384.in21k_ft_in1k done. 23.81 samples/sec, 331.39 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model maxvit_xlarge_tf_512.in21k_ft_in1k created, param count: 475769296
Running inference benchmark on maxvit_xlarge_tf_512.in21k_ft_in1k for 40 steps w/ input size (3, 512, 512) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 24.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 7.62 GiB is free. Including non-PyTorch memory, this process has 16.02 GiB memory in use. Of the allocated memory 14.78 GiB is allocated by PyTorch, and 12.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model maxvit_xlarge_tf_512.in21k_ft_in1k created, param count: 475769296
Running inference benchmark on maxvit_xlarge_tf_512.in21k_ft_in1k for 40 steps w/ input size (3, 512, 512) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 18.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 10.15 GiB is free. Including non-PyTorch memory, this process has 13.49 GiB memory in use. Of the allocated memory 11.34 GiB is allocated by PyTorch, and 944.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model maxvit_xlarge_tf_512.in21k_ft_in1k created, param count: 475769296
Running inference benchmark on maxvit_xlarge_tf_512.in21k_ft_in1k for 40 steps w/ input size (3, 512, 512) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 12.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 9.07 GiB is free. Including non-PyTorch memory, this process has 14.57 GiB memory in use. Of the allocated memory 7.85 GiB is allocated by PyTorch, and 5.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model maxvit_xlarge_tf_512.in21k_ft_in1k created, param count: 475769296
Running inference benchmark on maxvit_xlarge_tf_512.in21k_ft_in1k for 40 steps w/ input size (3, 512, 512) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 9.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 164.06 MiB is free. Including non-PyTorch memory, this process has 23.48 GiB memory in use. Of the allocated memory 21.88 GiB is allocated by PyTorch, and 383.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model maxvit_xlarge_tf_512.in21k_ft_in1k created, param count: 475769296
Running inference benchmark on maxvit_xlarge_tf_512.in21k_ft_in1k for 40 steps w/ input size (3, 512, 512) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 6.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 4.49 GiB is free. Including non-PyTorch memory, this process has 19.15 GiB memory in use. Of the allocated memory 14.88 GiB is allocated by PyTorch, and 3.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model maxvit_xlarge_tf_512.in21k_ft_in1k created, param count: 475769296
Running inference benchmark on maxvit_xlarge_tf_512.in21k_ft_in1k for 40 steps w/ input size (3, 512, 512) and batch size 48.
Infer [8/40]. 37.01 samples/sec. 1297.104 ms/step.
Infer [16/40]. 37.00 samples/sec. 1297.188 ms/step.
Infer [24/40]. 37.00 samples/sec. 1297.219 ms/step.
Infer [32/40]. 37.00 samples/sec. 1297.253 ms/step.
Infer [40/40]. 37.00 samples/sec. 1297.262 ms/step.
Inference benchmark of maxvit_xlarge_tf_512.in21k_ft_in1k done. 37.00 samples/sec, 1297.26 ms/step
Model maxvit_xlarge_tf_512.in21k_ft_in1k created, param count: 475769296
Running train benchmark on maxvit_xlarge_tf_512.in21k_ft_in1k for 40 steps w/ input size (3, 512, 512) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 6.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 2.74 GiB is free. Including non-PyTorch memory, this process has 20.90 GiB memory in use. Of the allocated memory 19.66 GiB is allocated by PyTorch, and 12.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model maxvit_xlarge_tf_512.in21k_ft_in1k created, param count: 475769296
Running train benchmark on maxvit_xlarge_tf_512.in21k_ft_in1k for 40 steps w/ input size (3, 512, 512) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 4.50 GiB. GPU 0 has a total capacty of 23.65 GiB of which 2.28 GiB is free. Including non-PyTorch memory, this process has 21.37 GiB memory in use. Of the allocated memory 19.49 GiB is allocated by PyTorch, and 655.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model maxvit_xlarge_tf_512.in21k_ft_in1k created, param count: 475769296
Running train benchmark on maxvit_xlarge_tf_512.in21k_ft_in1k for 40 steps w/ input size (3, 512, 512) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 3.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 2.88 GiB is free. Including non-PyTorch memory, this process has 20.76 GiB memory in use. Of the allocated memory 17.04 GiB is allocated by PyTorch, and 2.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model maxvit_xlarge_tf_512.in21k_ft_in1k created, param count: 475769296
Running train benchmark on maxvit_xlarge_tf_512.in21k_ft_in1k for 40 steps w/ input size (3, 512, 512) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 9.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 4.66 GiB is free. Including non-PyTorch memory, this process has 18.98 GiB memory in use. Of the allocated memory 15.27 GiB is allocated by PyTorch, and 2.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model maxvit_xlarge_tf_512.in21k_ft_in1k created, param count: 475769296
Running train benchmark on maxvit_xlarge_tf_512.in21k_ft_in1k for 40 steps w/ input size (3, 512, 512) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 6.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 4.49 GiB is free. Including non-PyTorch memory, this process has 19.15 GiB memory in use. Of the allocated memory 16.48 GiB is allocated by PyTorch, and 1.45 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model maxvit_xlarge_tf_512.in21k_ft_in1k created, param count: 475769296
Running train benchmark on maxvit_xlarge_tf_512.in21k_ft_in1k for 40 steps w/ input size (3, 512, 512) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 4.54 GiB. GPU 0 has a total capacty of 23.65 GiB of which 260.06 MiB is free. Including non-PyTorch memory, this process has 23.39 GiB memory in use. Of the allocated memory 21.59 GiB is allocated by PyTorch, and 582.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model maxvit_xlarge_tf_512.in21k_ft_in1k created, param count: 475769296
Running train benchmark on maxvit_xlarge_tf_512.in21k_ft_in1k for 40 steps w/ input size (3, 512, 512) and batch size 32.
ERROR: "CUDA out of memory. Tried to allocate 1.50 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.20 GiB is free. Including non-PyTorch memory, this process has 22.45 GiB memory in use. Of the allocated memory 20.72 GiB is allocated by PyTorch, and 501.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model maxvit_xlarge_tf_512.in21k_ft_in1k created, param count: 475769296
Running train benchmark on maxvit_xlarge_tf_512.in21k_ft_in1k for 40 steps w/ input size (3, 512, 512) and batch size 24.
ERROR: "CUDA out of memory. Tried to allocate 576.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 4.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 22.11 GiB is allocated by PyTorch, and 306.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model maxvit_xlarge_tf_512.in21k_ft_in1k created, param count: 475769296
Running train benchmark on maxvit_xlarge_tf_512.in21k_ft_in1k for 40 steps w/ input size (3, 512, 512) and batch size 16.
ERROR: "CUDA out of memory. Tried to allocate 768.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 658.06 MiB is free. Including non-PyTorch memory, this process has 23.00 GiB memory in use. Of the allocated memory 21.41 GiB is allocated by PyTorch, and 369.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 12 for retry.
Model maxvit_xlarge_tf_512.in21k_ft_in1k created, param count: 475769296
Running train benchmark on maxvit_xlarge_tf_512.in21k_ft_in1k for 40 steps w/ input size (3, 512, 512) and batch size 12.
ERROR: "CUDA out of memory. Tried to allocate 108.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 12.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.20 GiB is allocated by PyTorch, and 207.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 8 for retry.
Model maxvit_xlarge_tf_512.in21k_ft_in1k created, param count: 475769296
Running train benchmark on maxvit_xlarge_tf_512.in21k_ft_in1k for 40 steps w/ input size (3, 512, 512) and batch size 8.
ERROR: "CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 4.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 21.98 GiB is allocated by PyTorch, and 432.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 6 for retry.
Model maxvit_xlarge_tf_512.in21k_ft_in1k created, param count: 475769296
Running train benchmark on maxvit_xlarge_tf_512.in21k_ft_in1k for 40 steps w/ input size (3, 512, 512) and batch size 6.
ERROR: "CUDA out of memory. Tried to allocate 72.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 10.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.25 GiB is allocated by PyTorch, and 152.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 4 for retry.
Model maxvit_xlarge_tf_512.in21k_ft_in1k created, param count: 475769296
Running train benchmark on maxvit_xlarge_tf_512.in21k_ft_in1k for 40 steps w/ input size (3, 512, 512) and batch size 4.
Train [8/40]. 12.04 samples/sec. 332.301 ms/step.
Train [16/40]. 11.95 samples/sec. 334.618 ms/step.
Train [24/40]. 11.95 samples/sec. 334.680 ms/step.
Train [32/40]. 11.98 samples/sec. 333.951 ms/step.
Train [40/40]. 11.99 samples/sec. 333.627 ms/step.
Train benchmark of maxvit_xlarge_tf_512.in21k_ft_in1k done. 11.83 samples/sec, 333.63 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model maxxvit_rmlp_nano_rw_256.sw_in1k created, param count: 16779628
Running inference benchmark on maxxvit_rmlp_nano_rw_256.sw_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
Infer [8/40]. 2323.64 samples/sec. 110.172 ms/step.
Infer [16/40]. 2323.58 samples/sec. 110.175 ms/step.
Infer [24/40]. 2323.67 samples/sec. 110.171 ms/step.
Infer [32/40]. 2323.64 samples/sec. 110.172 ms/step.
Infer [40/40]. 2323.63 samples/sec. 110.172 ms/step.
Inference benchmark of maxxvit_rmlp_nano_rw_256.sw_in1k done. 2323.02 samples/sec, 110.17 ms/step
Model maxxvit_rmlp_nano_rw_256.sw_in1k created, param count: 16779628
Running train benchmark on maxxvit_rmlp_nano_rw_256.sw_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
Train [8/40]. 758.28 samples/sec. 337.607 ms/step.
Train [16/40]. 758.25 samples/sec. 337.621 ms/step.
Train [24/40]. 758.25 samples/sec. 337.619 ms/step.
Train [32/40]. 758.25 samples/sec. 337.621 ms/step.
Train [40/40]. 758.25 samples/sec. 337.622 ms/step.
Train benchmark of maxxvit_rmlp_nano_rw_256.sw_in1k done. 754.02 samples/sec, 337.62 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model maxxvit_rmlp_small_rw_256.sw_in1k created, param count: 66010516
Running inference benchmark on maxxvit_rmlp_small_rw_256.sw_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
Infer [8/40]. 1000.60 samples/sec. 255.847 ms/step.
Infer [16/40]. 1000.60 samples/sec. 255.848 ms/step.
Infer [24/40]. 1000.57 samples/sec. 255.855 ms/step.
Infer [32/40]. 1000.56 samples/sec. 255.858 ms/step.
Infer [40/40]. 1000.54 samples/sec. 255.862 ms/step.
Inference benchmark of maxxvit_rmlp_small_rw_256.sw_in1k done. 1000.37 samples/sec, 255.86 ms/step
Model maxxvit_rmlp_small_rw_256.sw_in1k created, param count: 66010516
Running train benchmark on maxxvit_rmlp_small_rw_256.sw_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 384.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 310.06 MiB is free. Including non-PyTorch memory, this process has 23.34 GiB memory in use. Of the allocated memory 22.05 GiB is allocated by PyTorch, and 57.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model maxxvit_rmlp_small_rw_256.sw_in1k created, param count: 66010516
Running train benchmark on maxxvit_rmlp_small_rw_256.sw_in1k for 40 steps w/ input size (3, 256, 256) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 72.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 40.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 22.35 GiB is allocated by PyTorch, and 16.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model maxxvit_rmlp_small_rw_256.sw_in1k created, param count: 66010516
Running train benchmark on maxxvit_rmlp_small_rw_256.sw_in1k for 40 steps w/ input size (3, 256, 256) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 4.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 22.39 GiB is allocated by PyTorch, and 18.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model maxxvit_rmlp_small_rw_256.sw_in1k created, param count: 66010516
Running train benchmark on maxxvit_rmlp_small_rw_256.sw_in1k for 40 steps w/ input size (3, 256, 256) and batch size 96.
Train [8/40]. 361.19 samples/sec. 265.785 ms/step.
Train [16/40]. 361.20 samples/sec. 265.784 ms/step.
Train [24/40]. 361.19 samples/sec. 265.787 ms/step.
Train [32/40]. 361.20 samples/sec. 265.784 ms/step.
Train [40/40]. 361.16 samples/sec. 265.808 ms/step.
Train benchmark of maxxvit_rmlp_small_rw_256.sw_in1k done. 357.96 samples/sec, 265.81 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model maxxvitv2_nano_rw_256.sw_in1k created, param count: 23696611
Running inference benchmark on maxxvitv2_nano_rw_256.sw_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
Infer [8/40]. 2255.37 samples/sec. 113.507 ms/step.
Infer [16/40]. 2254.44 samples/sec. 113.554 ms/step.
Infer [24/40]. 2254.05 samples/sec. 113.573 ms/step.
Infer [32/40]. 2253.93 samples/sec. 113.579 ms/step.
Infer [40/40]. 2253.75 samples/sec. 113.588 ms/step.
Inference benchmark of maxxvitv2_nano_rw_256.sw_in1k done. 2253.16 samples/sec, 113.59 ms/step
Model maxxvitv2_nano_rw_256.sw_in1k created, param count: 23696611
Running train benchmark on maxxvitv2_nano_rw_256.sw_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
Train [8/40]. 748.88 samples/sec. 341.844 ms/step.
Train [16/40]. 748.85 samples/sec. 341.859 ms/step.
Train [24/40]. 748.84 samples/sec. 341.861 ms/step.
Train [32/40]. 748.85 samples/sec. 341.859 ms/step.
Train [40/40]. 748.84 samples/sec. 341.861 ms/step.
Train benchmark of maxxvitv2_nano_rw_256.sw_in1k done. 746.06 samples/sec, 341.86 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model maxxvitv2_rmlp_base_rw_224.sw_in12k created, param count: 127184037
Running inference benchmark on maxxvitv2_rmlp_base_rw_224.sw_in12k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 901.42 samples/sec. 283.997 ms/step.
Infer [16/40]. 901.41 samples/sec. 284.000 ms/step.
Infer [24/40]. 901.39 samples/sec. 284.006 ms/step.
Infer [32/40]. 901.38 samples/sec. 284.010 ms/step.
Infer [40/40]. 901.36 samples/sec. 284.015 ms/step.
Inference benchmark of maxxvitv2_rmlp_base_rw_224.sw_in12k done. 901.22 samples/sec, 284.01 ms/step
Model maxxvitv2_rmlp_base_rw_224.sw_in12k created, param count: 127184037
Running train benchmark on maxxvitv2_rmlp_base_rw_224.sw_in12k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 64.06 MiB is free. Including non-PyTorch memory, this process has 23.58 GiB memory in use. Of the allocated memory 22.19 GiB is allocated by PyTorch, and 161.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model maxxvitv2_rmlp_base_rw_224.sw_in12k created, param count: 127184037
Running train benchmark on maxxvitv2_rmlp_base_rw_224.sw_in12k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 294.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 198.06 MiB is free. Including non-PyTorch memory, this process has 23.45 GiB memory in use. Of the allocated memory 21.83 GiB is allocated by PyTorch, and 399.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model maxxvitv2_rmlp_base_rw_224.sw_in12k created, param count: 127184037
Running train benchmark on maxxvitv2_rmlp_base_rw_224.sw_in12k for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 74.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 72.06 MiB is free. Including non-PyTorch memory, this process has 23.57 GiB memory in use. Of the allocated memory 22.11 GiB is allocated by PyTorch, and 230.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model maxxvitv2_rmlp_base_rw_224.sw_in12k created, param count: 127184037
Running train benchmark on maxxvitv2_rmlp_base_rw_224.sw_in12k for 40 steps w/ input size (3, 224, 224) and batch size 96.
Train [8/40]. 332.52 samples/sec. 288.707 ms/step.
Train [16/40]. 332.51 samples/sec. 288.717 ms/step.
Train [24/40]. 332.50 samples/sec. 288.718 ms/step.
Train [32/40]. 332.50 samples/sec. 288.721 ms/step.
Train [40/40]. 332.50 samples/sec. 288.722 ms/step.
Train benchmark of maxxvitv2_rmlp_base_rw_224.sw_in12k done. 329.47 samples/sec, 288.72 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model maxxvitv2_rmlp_base_rw_224.sw_in12k_ft_in1k created, param count: 116092512
Running inference benchmark on maxxvitv2_rmlp_base_rw_224.sw_in12k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 901.85 samples/sec. 283.860 ms/step.
Infer [16/40]. 901.82 samples/sec. 283.870 ms/step.
Infer [24/40]. 901.79 samples/sec. 283.879 ms/step.
Infer [32/40]. 901.79 samples/sec. 283.879 ms/step.
Infer [40/40]. 901.76 samples/sec. 283.889 ms/step.
Inference benchmark of maxxvitv2_rmlp_base_rw_224.sw_in12k_ft_in1k done. 901.62 samples/sec, 283.89 ms/step
Model maxxvitv2_rmlp_base_rw_224.sw_in12k_ft_in1k created, param count: 116092512
Running train benchmark on maxxvitv2_rmlp_base_rw_224.sw_in12k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 64.06 MiB is free. Including non-PyTorch memory, this process has 23.58 GiB memory in use. Of the allocated memory 22.17 GiB is allocated by PyTorch, and 182.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model maxxvitv2_rmlp_base_rw_224.sw_in12k_ft_in1k created, param count: 116092512
Running train benchmark on maxxvitv2_rmlp_base_rw_224.sw_in12k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 294.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 198.06 MiB is free. Including non-PyTorch memory, this process has 23.45 GiB memory in use. Of the allocated memory 21.81 GiB is allocated by PyTorch, and 420.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model maxxvitv2_rmlp_base_rw_224.sw_in12k_ft_in1k created, param count: 116092512
Running train benchmark on maxxvitv2_rmlp_base_rw_224.sw_in12k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 24.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.17 GiB is allocated by PyTorch, and 225.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model maxxvitv2_rmlp_base_rw_224.sw_in12k_ft_in1k created, param count: 116092512
Running train benchmark on maxxvitv2_rmlp_base_rw_224.sw_in12k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 96.
Train [8/40]. 332.96 samples/sec. 288.327 ms/step.
Train [16/40]. 332.95 samples/sec. 288.334 ms/step.
Train [24/40]. 332.95 samples/sec. 288.329 ms/step.
Train [32/40]. 332.96 samples/sec. 288.326 ms/step.
Train [40/40]. 332.95 samples/sec. 288.329 ms/step.
Train benchmark of maxxvitv2_rmlp_base_rw_224.sw_in12k_ft_in1k done. 329.93 samples/sec, 288.33 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model maxxvitv2_rmlp_base_rw_384.sw_in12k_ft_in1k created, param count: 116092512
Running inference benchmark on maxxvitv2_rmlp_base_rw_384.sw_in12k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 269.72 samples/sec. 949.125 ms/step.
Infer [16/40]. 269.71 samples/sec. 949.163 ms/step.
Infer [24/40]. 269.71 samples/sec. 949.182 ms/step.
Infer [32/40]. 269.70 samples/sec. 949.203 ms/step.
Infer [40/40]. 269.69 samples/sec. 949.221 ms/step.
Inference benchmark of maxxvitv2_rmlp_base_rw_384.sw_in12k_ft_in1k done. 269.68 samples/sec, 949.22 ms/step
Model maxxvitv2_rmlp_base_rw_384.sw_in12k_ft_in1k created, param count: 116092512
Running train benchmark on maxxvitv2_rmlp_base_rw_384.sw_in12k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 2.53 GiB. GPU 0 has a total capacty of 23.65 GiB of which 2.16 GiB is free. Including non-PyTorch memory, this process has 21.48 GiB memory in use. Of the allocated memory 19.96 GiB is allocated by PyTorch, and 298.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model maxxvitv2_rmlp_base_rw_384.sw_in12k_ft_in1k created, param count: 116092512
Running train benchmark on maxxvitv2_rmlp_base_rw_384.sw_in12k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 1.69 GiB. GPU 0 has a total capacty of 23.65 GiB of which 48.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 21.81 GiB is allocated by PyTorch, and 571.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model maxxvitv2_rmlp_base_rw_384.sw_in12k_ft_in1k created, param count: 116092512
Running train benchmark on maxxvitv2_rmlp_base_rw_384.sw_in12k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 1.12 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.03 GiB is free. Including non-PyTorch memory, this process has 22.61 GiB memory in use. Of the allocated memory 20.96 GiB is allocated by PyTorch, and 425.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model maxxvitv2_rmlp_base_rw_384.sw_in12k_ft_in1k created, param count: 116092512
Running train benchmark on maxxvitv2_rmlp_base_rw_384.sw_in12k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 324.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 36.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 22.23 GiB is allocated by PyTorch, and 152.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model maxxvitv2_rmlp_base_rw_384.sw_in12k_ft_in1k created, param count: 116092512
Running train benchmark on maxxvitv2_rmlp_base_rw_384.sw_in12k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 216.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 24.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.19 GiB is allocated by PyTorch, and 203.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model maxxvitv2_rmlp_base_rw_384.sw_in12k_ft_in1k created, param count: 116092512
Running train benchmark on maxxvitv2_rmlp_base_rw_384.sw_in12k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 2.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 22.34 GiB is allocated by PyTorch, and 66.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model maxxvitv2_rmlp_base_rw_384.sw_in12k_ft_in1k created, param count: 116092512
Running train benchmark on maxxvitv2_rmlp_base_rw_384.sw_in12k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 32.
Train [8/40]. 99.50 samples/sec. 321.602 ms/step.
Train [16/40]. 99.50 samples/sec. 321.600 ms/step.
Train [24/40]. 99.50 samples/sec. 321.597 ms/step.
Train [32/40]. 99.50 samples/sec. 321.603 ms/step.
Train [40/40]. 99.50 samples/sec. 321.603 ms/step.
Train benchmark of maxxvitv2_rmlp_base_rw_384.sw_in12k_ft_in1k done. 98.69 samples/sec, 321.60 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model mixer_b16_224.goog_in21k_ft_in1k created, param count: 59880472
Running inference benchmark on mixer_b16_224.goog_in21k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 2847.98 samples/sec. 89.888 ms/step.
Infer [16/40]. 2847.82 samples/sec. 89.893 ms/step.
Infer [24/40]. 2847.76 samples/sec. 89.895 ms/step.
Infer [32/40]. 2847.75 samples/sec. 89.895 ms/step.
Infer [40/40]. 2847.72 samples/sec. 89.896 ms/step.
Inference benchmark of mixer_b16_224.goog_in21k_ft_in1k done. 2846.85 samples/sec, 89.90 ms/step
Model mixer_b16_224.goog_in21k_ft_in1k created, param count: 59880472
Running train benchmark on mixer_b16_224.goog_in21k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1052.55 samples/sec. 243.218 ms/step.
Train [16/40]. 1052.58 samples/sec. 243.211 ms/step.
Train [24/40]. 1052.56 samples/sec. 243.217 ms/step.
Train [32/40]. 1052.54 samples/sec. 243.221 ms/step.
Train [40/40]. 1052.55 samples/sec. 243.219 ms/step.
Train benchmark of mixer_b16_224.goog_in21k_ft_in1k done. 1048.30 samples/sec, 243.22 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model mixer_b16_224.miil_in21k_ft_in1k created, param count: 59880472
Running inference benchmark on mixer_b16_224.miil_in21k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 2848.77 samples/sec. 89.863 ms/step.
Infer [16/40]. 2848.77 samples/sec. 89.863 ms/step.
Infer [24/40]. 2848.70 samples/sec. 89.865 ms/step.
Infer [32/40]. 2848.73 samples/sec. 89.864 ms/step.
Infer [40/40]. 2848.76 samples/sec. 89.864 ms/step.
Inference benchmark of mixer_b16_224.miil_in21k_ft_in1k done. 2847.90 samples/sec, 89.86 ms/step
Model mixer_b16_224.miil_in21k_ft_in1k created, param count: 59880472
Running train benchmark on mixer_b16_224.miil_in21k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1052.60 samples/sec. 243.207 ms/step.
Train [16/40]. 1052.65 samples/sec. 243.196 ms/step.
Train [24/40]. 1052.64 samples/sec. 243.198 ms/step.
Train [32/40]. 1052.63 samples/sec. 243.200 ms/step.
Train [40/40]. 1052.61 samples/sec. 243.204 ms/step.
Train benchmark of mixer_b16_224.miil_in21k_ft_in1k done. 1048.41 samples/sec, 243.20 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model mixer_l16_224.goog_in21k_ft_in1k created, param count: 208196168
Running inference benchmark on mixer_l16_224.goog_in21k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 928.01 samples/sec. 275.859 ms/step.
Infer [16/40]. 928.00 samples/sec. 275.861 ms/step.
Infer [24/40]. 927.91 samples/sec. 275.888 ms/step.
Infer [32/40]. 927.80 samples/sec. 275.921 ms/step.
Infer [40/40]. 927.74 samples/sec. 275.938 ms/step.
Inference benchmark of mixer_l16_224.goog_in21k_ft_in1k done. 927.60 samples/sec, 275.94 ms/step
Model mixer_l16_224.goog_in21k_ft_in1k created, param count: 208196168
Running train benchmark on mixer_l16_224.goog_in21k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 148.06 MiB is free. Including non-PyTorch memory, this process has 23.50 GiB memory in use. Of the allocated memory 22.20 GiB is allocated by PyTorch, and 64.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model mixer_l16_224.goog_in21k_ft_in1k created, param count: 208196168
Running train benchmark on mixer_l16_224.goog_in21k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 294.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 92.06 MiB is free. Including non-PyTorch memory, this process has 23.55 GiB memory in use. Of the allocated memory 22.18 GiB is allocated by PyTorch, and 142.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model mixer_l16_224.goog_in21k_ft_in1k created, param count: 208196168
Running train benchmark on mixer_l16_224.goog_in21k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
Train [8/40]. 339.77 samples/sec. 376.728 ms/step.
Train [16/40]. 339.78 samples/sec. 376.717 ms/step.
Train [24/40]. 339.78 samples/sec. 376.719 ms/step.
Train [32/40]. 339.78 samples/sec. 376.719 ms/step.
Train [40/40]. 339.78 samples/sec. 376.718 ms/step.
Train benchmark of mixer_l16_224.goog_in21k_ft_in1k done. 338.23 samples/sec, 376.72 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model mixnet_l.ft_in1k created, param count: 7329252
Running inference benchmark on mixnet_l.ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3950.57 samples/sec. 64.801 ms/step.
Infer [16/40]. 3950.57 samples/sec. 64.801 ms/step.
Infer [24/40]. 3950.54 samples/sec. 64.801 ms/step.
Infer [32/40]. 3950.51 samples/sec. 64.802 ms/step.
Infer [40/40]. 3950.53 samples/sec. 64.801 ms/step.
Inference benchmark of mixnet_l.ft_in1k done. 3949.01 samples/sec, 64.80 ms/step
Model mixnet_l.ft_in1k created, param count: 7329252
Running train benchmark on mixnet_l.ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 953.40 samples/sec. 268.514 ms/step.
Train [16/40]. 953.38 samples/sec. 268.518 ms/step.
Train [24/40]. 953.40 samples/sec. 268.512 ms/step.
Train [32/40]. 953.42 samples/sec. 268.506 ms/step.
Train [40/40]. 953.41 samples/sec. 268.511 ms/step.
Train benchmark of mixnet_l.ft_in1k done. 948.26 samples/sec, 268.51 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model mixnet_m.ft_in1k created, param count: 5014382
Running inference benchmark on mixnet_m.ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 5343.04 samples/sec. 47.913 ms/step.
Infer [16/40]. 5342.94 samples/sec. 47.914 ms/step.
Infer [24/40]. 5341.92 samples/sec. 47.923 ms/step.
Infer [32/40]. 5340.90 samples/sec. 47.932 ms/step.
Infer [40/40]. 5340.36 samples/sec. 47.937 ms/step.
Inference benchmark of mixnet_m.ft_in1k done. 5337.62 samples/sec, 47.94 ms/step
Model mixnet_m.ft_in1k created, param count: 5014382
Running train benchmark on mixnet_m.ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1230.28 samples/sec. 208.083 ms/step.
Train [16/40]. 1230.28 samples/sec. 208.083 ms/step.
Train [24/40]. 1230.27 samples/sec. 208.085 ms/step.
Train [32/40]. 1230.26 samples/sec. 208.086 ms/step.
Train [40/40]. 1230.24 samples/sec. 208.089 ms/step.
Train benchmark of mixnet_m.ft_in1k done. 1222.54 samples/sec, 208.09 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model mixnet_s.ft_in1k created, param count: 4134606
Running inference benchmark on mixnet_s.ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 7840.73 samples/sec. 32.650 ms/step.
Infer [16/40]. 7839.22 samples/sec. 32.656 ms/step.
Infer [24/40]. 7835.98 samples/sec. 32.670 ms/step.
Infer [32/40]. 7834.38 samples/sec. 32.676 ms/step.
Infer [40/40]. 7833.52 samples/sec. 32.680 ms/step.
Inference benchmark of mixnet_s.ft_in1k done. 7827.91 samples/sec, 32.68 ms/step
Model mixnet_s.ft_in1k created, param count: 4134606
Running train benchmark on mixnet_s.ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1692.75 samples/sec. 151.233 ms/step.
Train [16/40]. 1692.75 samples/sec. 151.233 ms/step.
Train [24/40]. 1692.74 samples/sec. 151.234 ms/step.
Train [32/40]. 1692.72 samples/sec. 151.236 ms/step.
Train [40/40]. 1692.67 samples/sec. 151.241 ms/step.
Train benchmark of mixnet_s.ft_in1k done. 1681.46 samples/sec, 151.24 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model mixnet_xl.ra_in1k created, param count: 11896768
Running inference benchmark on mixnet_xl.ra_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 2844.80 samples/sec. 89.989 ms/step.
Infer [16/40]. 2844.69 samples/sec. 89.992 ms/step.
Infer [24/40]. 2844.69 samples/sec. 89.992 ms/step.
Infer [32/40]. 2844.60 samples/sec. 89.995 ms/step.
Infer [40/40]. 2844.58 samples/sec. 89.996 ms/step.
Inference benchmark of mixnet_xl.ra_in1k done. 2843.75 samples/sec, 90.00 ms/step
Model mixnet_xl.ra_in1k created, param count: 11896768
Running train benchmark on mixnet_xl.ra_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 708.41 samples/sec. 361.371 ms/step.
Train [16/40]. 708.41 samples/sec. 361.374 ms/step.
Train [24/40]. 708.37 samples/sec. 361.392 ms/step.
Train [32/40]. 708.35 samples/sec. 361.402 ms/step.
Train [40/40]. 708.34 samples/sec. 361.407 ms/step.
Train benchmark of mixnet_xl.ra_in1k done. 705.09 samples/sec, 361.41 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model mnasnet_100.rmsp_in1k created, param count: 4383312
Running inference benchmark on mnasnet_100.rmsp_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 13826.09 samples/sec. 18.516 ms/step.
Infer [16/40]. 13825.47 samples/sec. 18.517 ms/step.
Infer [24/40]. 13825.43 samples/sec. 18.517 ms/step.
Infer [32/40]. 13825.24 samples/sec. 18.517 ms/step.
Infer [40/40]. 13825.67 samples/sec. 18.516 ms/step.
Inference benchmark of mnasnet_100.rmsp_in1k done. 13807.89 samples/sec, 18.52 ms/step
Model mnasnet_100.rmsp_in1k created, param count: 4383312
Running train benchmark on mnasnet_100.rmsp_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 2658.21 samples/sec. 96.306 ms/step.
Train [16/40]. 2658.17 samples/sec. 96.307 ms/step.
Train [24/40]. 2658.24 samples/sec. 96.304 ms/step.
Train [32/40]. 2658.25 samples/sec. 96.304 ms/step.
Train [40/40]. 2658.20 samples/sec. 96.306 ms/step.
Train benchmark of mnasnet_100.rmsp_in1k done. 2641.72 samples/sec, 96.31 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model mnasnet_small.lamb_in1k created, param count: 2030264
Running inference benchmark on mnasnet_small.lamb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 21328.54 samples/sec. 12.003 ms/step.
Infer [16/40]. 21324.02 samples/sec. 12.005 ms/step.
Infer [24/40]. 21319.34 samples/sec. 12.008 ms/step.
Infer [32/40]. 21314.65 samples/sec. 12.011 ms/step.
Infer [40/40]. 21315.23 samples/sec. 12.010 ms/step.
Inference benchmark of mnasnet_small.lamb_in1k done. 21279.34 samples/sec, 12.01 ms/step
Model mnasnet_small.lamb_in1k created, param count: 2030264
Running train benchmark on mnasnet_small.lamb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 4533.10 samples/sec. 56.473 ms/step.
Train [16/40]. 4533.94 samples/sec. 56.463 ms/step.
Train [24/40]. 4533.75 samples/sec. 56.465 ms/step.
Train [32/40]. 4534.05 samples/sec. 56.462 ms/step.
Train [40/40]. 4533.35 samples/sec. 56.470 ms/step.
Train benchmark of mnasnet_small.lamb_in1k done. 4482.84 samples/sec, 56.47 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model mobilenetv2_050.lamb_in1k created, param count: 1968680
Running inference benchmark on mobilenetv2_050.lamb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 22945.74 samples/sec. 11.157 ms/step.
Infer [16/40]. 22947.13 samples/sec. 11.156 ms/step.
Infer [24/40]. 22949.36 samples/sec. 11.155 ms/step.
Infer [32/40]. 22949.01 samples/sec. 11.155 ms/step.
Infer [40/40]. 22949.59 samples/sec. 11.155 ms/step.
Inference benchmark of mobilenetv2_050.lamb_in1k done. 22907.11 samples/sec, 11.15 ms/step
Model mobilenetv2_050.lamb_in1k created, param count: 1968680
Running train benchmark on mobilenetv2_050.lamb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 3592.47 samples/sec. 71.260 ms/step.
Train [16/40]. 3592.33 samples/sec. 71.263 ms/step.
Train [24/40]. 3592.34 samples/sec. 71.263 ms/step.
Train [32/40]. 3592.31 samples/sec. 71.263 ms/step.
Train [40/40]. 3592.21 samples/sec. 71.265 ms/step.
Train benchmark of mobilenetv2_050.lamb_in1k done. 3563.09 samples/sec, 71.27 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model mobilenetv2_100.ra_in1k created, param count: 3504872
Running inference benchmark on mobilenetv2_100.ra_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 11712.81 samples/sec. 21.856 ms/step.
Infer [16/40]. 11711.92 samples/sec. 21.858 ms/step.
Infer [24/40]. 11711.85 samples/sec. 21.858 ms/step.
Infer [32/40]. 11711.84 samples/sec. 21.858 ms/step.
Infer [40/40]. 11712.02 samples/sec. 21.858 ms/step.
Inference benchmark of mobilenetv2_100.ra_in1k done. 11698.82 samples/sec, 21.86 ms/step
Model mobilenetv2_100.ra_in1k created, param count: 3504872
Running train benchmark on mobilenetv2_100.ra_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 2384.90 samples/sec. 107.342 ms/step.
Train [16/40]. 2384.71 samples/sec. 107.351 ms/step.
Train [24/40]. 2384.73 samples/sec. 107.349 ms/step.
Train [32/40]. 2384.68 samples/sec. 107.352 ms/step.
Train [40/40]. 2384.71 samples/sec. 107.351 ms/step.
Train benchmark of mobilenetv2_100.ra_in1k done. 2370.96 samples/sec, 107.35 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model mobilenetv2_110d.ra_in1k created, param count: 4516520
Running inference benchmark on mobilenetv2_110d.ra_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 8852.92 samples/sec. 28.917 ms/step.
Infer [16/40]. 8852.02 samples/sec. 28.920 ms/step.
Infer [24/40]. 8851.81 samples/sec. 28.921 ms/step.
Infer [32/40]. 8851.96 samples/sec. 28.920 ms/step.
Infer [40/40]. 8851.93 samples/sec. 28.920 ms/step.
Inference benchmark of mobilenetv2_110d.ra_in1k done. 8844.30 samples/sec, 28.92 ms/step
Model mobilenetv2_110d.ra_in1k created, param count: 4516520
Running train benchmark on mobilenetv2_110d.ra_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1876.86 samples/sec. 136.398 ms/step.
Train [16/40]. 1876.70 samples/sec. 136.409 ms/step.
Train [24/40]. 1876.71 samples/sec. 136.409 ms/step.
Train [32/40]. 1876.68 samples/sec. 136.411 ms/step.
Train [40/40]. 1876.70 samples/sec. 136.410 ms/step.
Train benchmark of mobilenetv2_110d.ra_in1k done. 1866.25 samples/sec, 136.41 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model mobilenetv2_120d.ra_in1k created, param count: 5831144
Running inference benchmark on mobilenetv2_120d.ra_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 6413.33 samples/sec. 39.917 ms/step.
Infer [16/40]. 6413.16 samples/sec. 39.918 ms/step.
Infer [24/40]. 6412.80 samples/sec. 39.920 ms/step.
Infer [32/40]. 6411.33 samples/sec. 39.929 ms/step.
Infer [40/40]. 6410.34 samples/sec. 39.935 ms/step.
Inference benchmark of mobilenetv2_120d.ra_in1k done. 6406.33 samples/sec, 39.94 ms/step
Model mobilenetv2_120d.ra_in1k created, param count: 5831144
Running train benchmark on mobilenetv2_120d.ra_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1458.33 samples/sec. 175.544 ms/step.
Train [16/40]. 1458.52 samples/sec. 175.521 ms/step.
Train [24/40]. 1458.58 samples/sec. 175.514 ms/step.
Train [32/40]. 1458.59 samples/sec. 175.512 ms/step.
Train [40/40]. 1458.58 samples/sec. 175.513 ms/step.
Train benchmark of mobilenetv2_120d.ra_in1k done. 1450.72 samples/sec, 175.51 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model mobilenetv2_140.ra_in1k created, param count: 6108776
Running inference benchmark on mobilenetv2_140.ra_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 7743.38 samples/sec. 33.061 ms/step.
Infer [16/40]. 7744.16 samples/sec. 33.057 ms/step.
Infer [24/40]. 7742.38 samples/sec. 33.065 ms/step.
Infer [32/40]. 7740.82 samples/sec. 33.071 ms/step.
Infer [40/40]. 7739.99 samples/sec. 33.075 ms/step.
Inference benchmark of mobilenetv2_140.ra_in1k done. 7734.09 samples/sec, 33.08 ms/step
Model mobilenetv2_140.ra_in1k created, param count: 6108776
Running train benchmark on mobilenetv2_140.ra_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1765.48 samples/sec. 145.003 ms/step.
Train [16/40]. 1765.42 samples/sec. 145.008 ms/step.
Train [24/40]. 1765.31 samples/sec. 145.017 ms/step.
Train [32/40]. 1765.35 samples/sec. 145.014 ms/step.
Train [40/40]. 1765.32 samples/sec. 145.016 ms/step.
Train benchmark of mobilenetv2_140.ra_in1k done. 1757.24 samples/sec, 145.02 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model mobilenetv3_large_100.miil_in21k_ft_in1k created, param count: 5483032
Running inference benchmark on mobilenetv3_large_100.miil_in21k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 16218.53 samples/sec. 15.784 ms/step.
Infer [16/40]. 16218.93 samples/sec. 15.784 ms/step.
Infer [24/40]. 16215.46 samples/sec. 15.787 ms/step.
Infer [32/40]. 16212.66 samples/sec. 15.790 ms/step.
Infer [40/40]. 16211.68 samples/sec. 15.791 ms/step.
Inference benchmark of mobilenetv3_large_100.miil_in21k_ft_in1k done. 16188.01 samples/sec, 15.79 ms/step
Model mobilenetv3_large_100.miil_in21k_ft_in1k created, param count: 5483032
Running train benchmark on mobilenetv3_large_100.miil_in21k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 2942.70 samples/sec. 86.995 ms/step.
Train [16/40]. 2942.65 samples/sec. 86.997 ms/step.
Train [24/40]. 2942.58 samples/sec. 86.998 ms/step.
Train [32/40]. 2942.62 samples/sec. 86.997 ms/step.
Train [40/40]. 2942.59 samples/sec. 86.998 ms/step.
Train benchmark of mobilenetv3_large_100.miil_in21k_ft_in1k done. 2921.40 samples/sec, 87.00 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model mobilenetv3_large_100.ra_in1k created, param count: 5483032
Running inference benchmark on mobilenetv3_large_100.ra_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 16230.22 samples/sec. 15.773 ms/step.
Infer [16/40]. 16225.60 samples/sec. 15.778 ms/step.
Infer [24/40]. 16222.13 samples/sec. 15.781 ms/step.
Infer [32/40]. 16221.21 samples/sec. 15.782 ms/step.
Infer [40/40]. 16220.27 samples/sec. 15.783 ms/step.
Inference benchmark of mobilenetv3_large_100.ra_in1k done. 16196.38 samples/sec, 15.78 ms/step
Model mobilenetv3_large_100.ra_in1k created, param count: 5483032
Running train benchmark on mobilenetv3_large_100.ra_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 2945.47 samples/sec. 86.913 ms/step.
Train [16/40]. 2945.24 samples/sec. 86.920 ms/step.
Train [24/40]. 2945.14 samples/sec. 86.923 ms/step.
Train [32/40]. 2945.17 samples/sec. 86.922 ms/step.
Train [40/40]. 2945.22 samples/sec. 86.920 ms/step.
Train benchmark of mobilenetv3_large_100.ra_in1k done. 2923.68 samples/sec, 86.92 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model mobilenetv3_rw.rmsp_in1k created, param count: 5479918
Running inference benchmark on mobilenetv3_rw.rmsp_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 16575.03 samples/sec. 15.445 ms/step.
Infer [16/40]. 16572.99 samples/sec. 15.447 ms/step.
Infer [24/40]. 16572.64 samples/sec. 15.447 ms/step.
Infer [32/40]. 16571.63 samples/sec. 15.448 ms/step.
Infer [40/40]. 16571.80 samples/sec. 15.448 ms/step.
Inference benchmark of mobilenetv3_rw.rmsp_in1k done. 16547.30 samples/sec, 15.45 ms/step
Model mobilenetv3_rw.rmsp_in1k created, param count: 5479918
Running train benchmark on mobilenetv3_rw.rmsp_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 2968.09 samples/sec. 86.251 ms/step.
Train [16/40]. 2968.01 samples/sec. 86.253 ms/step.
Train [24/40]. 2967.99 samples/sec. 86.254 ms/step.
Train [32/40]. 2967.91 samples/sec. 86.256 ms/step.
Train [40/40]. 2967.87 samples/sec. 86.257 ms/step.
Train benchmark of mobilenetv3_rw.rmsp_in1k done. 2946.31 samples/sec, 86.26 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model mobilenetv3_small_050.lamb_in1k created, param count: 1593224
Running inference benchmark on mobilenetv3_small_050.lamb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 27020.57 samples/sec. 9.474 ms/step.
Infer [16/40]. 27017.48 samples/sec. 9.475 ms/step.
Infer [24/40]. 27027.82 samples/sec. 9.472 ms/step.
Infer [32/40]. 27016.02 samples/sec. 9.476 ms/step.
Infer [40/40]. 27022.28 samples/sec. 9.474 ms/step.
Inference benchmark of mobilenetv3_small_050.lamb_in1k done. 26967.71 samples/sec, 9.47 ms/step
Model mobilenetv3_small_050.lamb_in1k created, param count: 1593224
Running train benchmark on mobilenetv3_small_050.lamb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 7449.96 samples/sec. 34.363 ms/step.
Train [16/40]. 7451.92 samples/sec. 34.354 ms/step.
Train [24/40]. 7451.21 samples/sec. 34.357 ms/step.
Train [32/40]. 7450.02 samples/sec. 34.362 ms/step.
Train [40/40]. 7450.24 samples/sec. 34.361 ms/step.
Train benchmark of mobilenetv3_small_050.lamb_in1k done. 7338.89 samples/sec, 34.36 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model mobilenetv3_small_075.lamb_in1k created, param count: 2041872
Running inference benchmark on mobilenetv3_small_075.lamb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 27341.71 samples/sec. 9.363 ms/step.
Infer [16/40]. 27344.73 samples/sec. 9.362 ms/step.
Infer [24/40]. 27341.79 samples/sec. 9.363 ms/step.
Infer [32/40]. 27322.78 samples/sec. 9.369 ms/step.
Infer [40/40]. 27320.18 samples/sec. 9.370 ms/step.
Inference benchmark of mobilenetv3_small_075.lamb_in1k done. 27265.19 samples/sec, 9.37 ms/step
Model mobilenetv3_small_075.lamb_in1k created, param count: 2041872
Running train benchmark on mobilenetv3_small_075.lamb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 7047.21 samples/sec. 36.326 ms/step.
Train [16/40]. 7051.20 samples/sec. 36.306 ms/step.
Train [24/40]. 7046.34 samples/sec. 36.331 ms/step.
Train [32/40]. 7046.09 samples/sec. 36.332 ms/step.
Train [40/40]. 7047.31 samples/sec. 36.326 ms/step.
Train benchmark of mobilenetv3_small_075.lamb_in1k done. 6949.11 samples/sec, 36.33 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model mobilenetv3_small_100.lamb_in1k created, param count: 2542856
Running inference benchmark on mobilenetv3_small_100.lamb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 26947.50 samples/sec. 9.500 ms/step.
Infer [16/40]. 26940.32 samples/sec. 9.502 ms/step.
Infer [24/40]. 26929.82 samples/sec. 9.506 ms/step.
Infer [32/40]. 26930.57 samples/sec. 9.506 ms/step.
Infer [40/40]. 26921.94 samples/sec. 9.509 ms/step.
Inference benchmark of mobilenetv3_small_100.lamb_in1k done. 26865.04 samples/sec, 9.51 ms/step
Model mobilenetv3_small_100.lamb_in1k created, param count: 2542856
Running train benchmark on mobilenetv3_small_100.lamb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 6979.11 samples/sec. 36.681 ms/step.
Train [16/40]. 6985.02 samples/sec. 36.650 ms/step.
Train [24/40]. 6985.65 samples/sec. 36.647 ms/step.
Train [32/40]. 6983.52 samples/sec. 36.658 ms/step.
Train [40/40]. 6982.61 samples/sec. 36.663 ms/step.
Train benchmark of mobilenetv3_small_100.lamb_in1k done. 6885.86 samples/sec, 36.66 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model mobilevit_s.cvnets_in1k created, param count: 5578632
Running inference benchmark on mobilevit_s.cvnets_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
Infer [8/40]. 3956.94 samples/sec. 64.696 ms/step.
Infer [16/40]. 3956.94 samples/sec. 64.696 ms/step.
Infer [24/40]. 3956.88 samples/sec. 64.698 ms/step.
Infer [32/40]. 3956.80 samples/sec. 64.699 ms/step.
Infer [40/40]. 3956.83 samples/sec. 64.698 ms/step.
Inference benchmark of mobilevit_s.cvnets_in1k done. 3955.27 samples/sec, 64.70 ms/step
Model mobilevit_s.cvnets_in1k created, param count: 5578632
Running train benchmark on mobilevit_s.cvnets_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 18.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.34 GiB is allocated by PyTorch, and 52.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model mobilevit_s.cvnets_in1k created, param count: 5578632
Running train benchmark on mobilevit_s.cvnets_in1k for 40 steps w/ input size (3, 256, 256) and batch size 192.
Train [8/40]. 986.53 samples/sec. 194.622 ms/step.
Train [16/40]. 986.49 samples/sec. 194.630 ms/step.
Train [24/40]. 986.48 samples/sec. 194.632 ms/step.
Train [32/40]. 986.48 samples/sec. 194.632 ms/step.
Train [40/40]. 986.50 samples/sec. 194.627 ms/step.
Train benchmark of mobilevit_s.cvnets_in1k done. 981.24 samples/sec, 194.63 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model mobilevit_xs.cvnets_in1k created, param count: 2317848
Running inference benchmark on mobilevit_xs.cvnets_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
Infer [8/40]. 5519.77 samples/sec. 46.379 ms/step.
Infer [16/40]. 5519.46 samples/sec. 46.381 ms/step.
Infer [24/40]. 5519.34 samples/sec. 46.382 ms/step.
Infer [32/40]. 5519.22 samples/sec. 46.383 ms/step.
Infer [40/40]. 5519.11 samples/sec. 46.384 ms/step.
Inference benchmark of mobilevit_xs.cvnets_in1k done. 5516.15 samples/sec, 46.38 ms/step
Model mobilevit_xs.cvnets_in1k created, param count: 2317848
Running train benchmark on mobilevit_xs.cvnets_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
Train [8/40]. 1227.29 samples/sec. 208.589 ms/step.
Train [16/40]. 1227.43 samples/sec. 208.567 ms/step.
Train [24/40]. 1227.32 samples/sec. 208.585 ms/step.
Train [32/40]. 1227.41 samples/sec. 208.569 ms/step.
Train [40/40]. 1227.36 samples/sec. 208.578 ms/step.
Train benchmark of mobilevit_xs.cvnets_in1k done. 1221.07 samples/sec, 208.58 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model mobilevit_xxs.cvnets_in1k created, param count: 1272024
Running inference benchmark on mobilevit_xxs.cvnets_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
Infer [8/40]. 12767.14 samples/sec. 20.051 ms/step.
Infer [16/40]. 12761.96 samples/sec. 20.060 ms/step.
Infer [24/40]. 12760.11 samples/sec. 20.063 ms/step.
Infer [32/40]. 12759.16 samples/sec. 20.064 ms/step.
Infer [40/40]. 12758.76 samples/sec. 20.065 ms/step.
Inference benchmark of mobilevit_xxs.cvnets_in1k done. 12743.77 samples/sec, 20.07 ms/step
Model mobilevit_xxs.cvnets_in1k created, param count: 1272024
Running train benchmark on mobilevit_xxs.cvnets_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
Train [8/40]. 2134.51 samples/sec. 119.934 ms/step.
Train [16/40]. 2134.68 samples/sec. 119.924 ms/step.
Train [24/40]. 2134.63 samples/sec. 119.927 ms/step.
Train [32/40]. 2134.60 samples/sec. 119.929 ms/step.
Train [40/40]. 2134.65 samples/sec. 119.926 ms/step.
Train benchmark of mobilevit_xxs.cvnets_in1k done. 2119.68 samples/sec, 119.93 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model mobilevitv2_050.cvnets_in1k created, param count: 1370593
Running inference benchmark on mobilevitv2_050.cvnets_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
Infer [8/40]. 10173.57 samples/sec. 25.163 ms/step.
Infer [16/40]. 10173.60 samples/sec. 25.163 ms/step.
Infer [24/40]. 10173.51 samples/sec. 25.163 ms/step.
Infer [32/40]. 10173.69 samples/sec. 25.163 ms/step.
Infer [40/40]. 10173.40 samples/sec. 25.164 ms/step.
Inference benchmark of mobilevitv2_050.cvnets_in1k done. 10163.83 samples/sec, 25.16 ms/step
Model mobilevitv2_050.cvnets_in1k created, param count: 1370593
Running train benchmark on mobilevitv2_050.cvnets_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
Train [8/40]. 2027.86 samples/sec. 126.241 ms/step.
Train [16/40]. 2028.12 samples/sec. 126.225 ms/step.
Train [24/40]. 2028.21 samples/sec. 126.220 ms/step.
Train [32/40]. 2028.27 samples/sec. 126.216 ms/step.
Train [40/40]. 2028.24 samples/sec. 126.218 ms/step.
Train benchmark of mobilevitv2_050.cvnets_in1k done. 2016.32 samples/sec, 126.22 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model mobilevitv2_075.cvnets_in1k created, param count: 2866009
Running inference benchmark on mobilevitv2_075.cvnets_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
Infer [8/40]. 6387.80 samples/sec. 40.076 ms/step.
Infer [16/40]. 6387.87 samples/sec. 40.076 ms/step.
Infer [24/40]. 6387.83 samples/sec. 40.076 ms/step.
Infer [32/40]. 6387.58 samples/sec. 40.078 ms/step.
Infer [40/40]. 6387.58 samples/sec. 40.078 ms/step.
Inference benchmark of mobilevitv2_075.cvnets_in1k done. 6383.75 samples/sec, 40.08 ms/step
Model mobilevitv2_075.cvnets_in1k created, param count: 2866009
Running train benchmark on mobilevitv2_075.cvnets_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
Train [8/40]. 1454.97 samples/sec. 175.949 ms/step.
Train [16/40]. 1455.13 samples/sec. 175.929 ms/step.
Train [24/40]. 1455.10 samples/sec. 175.933 ms/step.
Train [32/40]. 1455.09 samples/sec. 175.934 ms/step.
Train [40/40]. 1455.10 samples/sec. 175.933 ms/step.
Train benchmark of mobilevitv2_075.cvnets_in1k done. 1447.88 samples/sec, 175.93 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model mobilevitv2_100.cvnets_in1k created, param count: 4901841
Running inference benchmark on mobilevitv2_100.cvnets_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
Infer [8/40]. 4746.47 samples/sec. 53.935 ms/step.
Infer [16/40]. 4746.54 samples/sec. 53.934 ms/step.
Infer [24/40]. 4746.68 samples/sec. 53.932 ms/step.
Infer [32/40]. 4746.70 samples/sec. 53.932 ms/step.
Infer [40/40]. 4746.64 samples/sec. 53.933 ms/step.
Inference benchmark of mobilevitv2_100.cvnets_in1k done. 4744.46 samples/sec, 53.93 ms/step
Model mobilevitv2_100.cvnets_in1k created, param count: 4901841
Running train benchmark on mobilevitv2_100.cvnets_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
Train [8/40]. 1174.55 samples/sec. 217.956 ms/step.
Train [16/40]. 1174.73 samples/sec. 217.923 ms/step.
Train [24/40]. 1174.54 samples/sec. 217.957 ms/step.
Train [32/40]. 1174.68 samples/sec. 217.932 ms/step.
Train [40/40]. 1174.74 samples/sec. 217.921 ms/step.
Train benchmark of mobilevitv2_100.cvnets_in1k done. 1169.55 samples/sec, 217.92 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model mobilevitv2_125.cvnets_in1k created, param count: 7478089
Running inference benchmark on mobilevitv2_125.cvnets_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
Infer [8/40]. 3638.78 samples/sec. 70.353 ms/step.
Infer [16/40]. 3638.60 samples/sec. 70.357 ms/step.
Infer [24/40]. 3638.59 samples/sec. 70.357 ms/step.
Infer [32/40]. 3638.62 samples/sec. 70.356 ms/step.
Infer [40/40]. 3638.61 samples/sec. 70.357 ms/step.
Inference benchmark of mobilevitv2_125.cvnets_in1k done. 3637.25 samples/sec, 70.36 ms/step
Model mobilevitv2_125.cvnets_in1k created, param count: 7478089
Running train benchmark on mobilevitv2_125.cvnets_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 320.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 204.06 MiB is free. Including non-PyTorch memory, this process has 23.44 GiB memory in use. Of the allocated memory 21.84 GiB is allocated by PyTorch, and 372.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model mobilevitv2_125.cvnets_in1k created, param count: 7478089
Running train benchmark on mobilevitv2_125.cvnets_in1k for 40 steps w/ input size (3, 256, 256) and batch size 192.
Train [8/40]. 954.35 samples/sec. 201.183 ms/step.
Train [16/40]. 954.34 samples/sec. 201.185 ms/step.
Train [24/40]. 954.39 samples/sec. 201.176 ms/step.
Train [32/40]. 954.40 samples/sec. 201.173 ms/step.
Train [40/40]. 954.40 samples/sec. 201.174 ms/step.
Train benchmark of mobilevitv2_125.cvnets_in1k done. 950.02 samples/sec, 201.17 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model mobilevitv2_150.cvnets_in1k created, param count: 10594753
Running inference benchmark on mobilevitv2_150.cvnets_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
Infer [8/40]. 2971.05 samples/sec. 86.165 ms/step.
Infer [16/40]. 2971.06 samples/sec. 86.165 ms/step.
Infer [24/40]. 2970.92 samples/sec. 86.168 ms/step.
Infer [32/40]. 2970.84 samples/sec. 86.171 ms/step.
Infer [40/40]. 2970.89 samples/sec. 86.169 ms/step.
Inference benchmark of mobilevitv2_150.cvnets_in1k done. 2969.98 samples/sec, 86.17 ms/step
Model mobilevitv2_150.cvnets_in1k created, param count: 10594753
Running train benchmark on mobilevitv2_150.cvnets_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 14.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.26 GiB is allocated by PyTorch, and 142.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model mobilevitv2_150.cvnets_in1k created, param count: 10594753
Running train benchmark on mobilevitv2_150.cvnets_in1k for 40 steps w/ input size (3, 256, 256) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 108.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 46.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 21.94 GiB is allocated by PyTorch, and 430.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model mobilevitv2_150.cvnets_in1k created, param count: 10594753
Running train benchmark on mobilevitv2_150.cvnets_in1k for 40 steps w/ input size (3, 256, 256) and batch size 128.
Train [8/40]. 816.58 samples/sec. 156.751 ms/step.
Train [16/40]. 816.46 samples/sec. 156.774 ms/step.
Train [24/40]. 816.45 samples/sec. 156.777 ms/step.
Train [32/40]. 816.47 samples/sec. 156.772 ms/step.
Train [40/40]. 816.45 samples/sec. 156.777 ms/step.
Train benchmark of mobilevitv2_150.cvnets_in1k done. 812.28 samples/sec, 156.78 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model mobilevitv2_150.cvnets_in22k_ft_in1k created, param count: 10594753
Running inference benchmark on mobilevitv2_150.cvnets_in22k_ft_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
Infer [8/40]. 2970.93 samples/sec. 86.168 ms/step.
Infer [16/40]. 2970.75 samples/sec. 86.173 ms/step.
Infer [24/40]. 2970.78 samples/sec. 86.173 ms/step.
Infer [32/40]. 2970.78 samples/sec. 86.173 ms/step.
Infer [40/40]. 2970.66 samples/sec. 86.176 ms/step.
Inference benchmark of mobilevitv2_150.cvnets_in22k_ft_in1k done. 2969.73 samples/sec, 86.18 ms/step
Model mobilevitv2_150.cvnets_in22k_ft_in1k created, param count: 10594753
Running train benchmark on mobilevitv2_150.cvnets_in22k_ft_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 14.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.26 GiB is allocated by PyTorch, and 142.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model mobilevitv2_150.cvnets_in22k_ft_in1k created, param count: 10594753
Running train benchmark on mobilevitv2_150.cvnets_in22k_ft_in1k for 40 steps w/ input size (3, 256, 256) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 108.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 44.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 21.94 GiB is allocated by PyTorch, and 432.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model mobilevitv2_150.cvnets_in22k_ft_in1k created, param count: 10594753
Running train benchmark on mobilevitv2_150.cvnets_in22k_ft_in1k for 40 steps w/ input size (3, 256, 256) and batch size 128.
Train [8/40]. 816.50 samples/sec. 156.767 ms/step.
Train [16/40]. 816.60 samples/sec. 156.748 ms/step.
Train [24/40]. 816.56 samples/sec. 156.754 ms/step.
Train [32/40]. 816.52 samples/sec. 156.763 ms/step.
Train [40/40]. 816.47 samples/sec. 156.773 ms/step.
Train benchmark of mobilevitv2_150.cvnets_in22k_ft_in1k done. 812.13 samples/sec, 156.77 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model mobilevitv2_150.cvnets_in22k_ft_in1k_384 created, param count: 10594753
Running inference benchmark on mobilevitv2_150.cvnets_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 1286.95 samples/sec. 198.920 ms/step.
Infer [16/40]. 1286.89 samples/sec. 198.929 ms/step.
Infer [24/40]. 1286.95 samples/sec. 198.920 ms/step.
Infer [32/40]. 1286.86 samples/sec. 198.933 ms/step.
Infer [40/40]. 1286.90 samples/sec. 198.927 ms/step.
Inference benchmark of mobilevitv2_150.cvnets_in22k_ft_in1k_384 done. 1286.65 samples/sec, 198.93 ms/step
Model mobilevitv2_150.cvnets_in22k_ft_in1k_384 created, param count: 10594753
Running train benchmark on mobilevitv2_150.cvnets_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 3.38 GiB. GPU 0 has a total capacty of 23.65 GiB of which 2.17 GiB is free. Including non-PyTorch memory, this process has 21.47 GiB memory in use. Of the allocated memory 19.65 GiB is allocated by PyTorch, and 598.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model mobilevitv2_150.cvnets_in22k_ft_in1k_384 created, param count: 10594753
Running train benchmark on mobilevitv2_150.cvnets_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 648.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 48.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 21.71 GiB is allocated by PyTorch, and 667.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model mobilevitv2_150.cvnets_in22k_ft_in1k_384 created, param count: 10594753
Running train benchmark on mobilevitv2_150.cvnets_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 864.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 48.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 289.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model mobilevitv2_150.cvnets_in22k_ft_in1k_384 created, param count: 10594753
Running train benchmark on mobilevitv2_150.cvnets_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 324.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 144.06 MiB is free. Including non-PyTorch memory, this process has 23.50 GiB memory in use. Of the allocated memory 22.12 GiB is allocated by PyTorch, and 150.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model mobilevitv2_150.cvnets_in22k_ft_in1k_384 created, param count: 10594753
Running train benchmark on mobilevitv2_150.cvnets_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 64.
Train [8/40]. 351.39 samples/sec. 182.133 ms/step.
Train [16/40]. 351.36 samples/sec. 182.147 ms/step.
Train [24/40]. 351.36 samples/sec. 182.149 ms/step.
Train [32/40]. 351.36 samples/sec. 182.151 ms/step.
Train [40/40]. 351.35 samples/sec. 182.153 ms/step.
Train benchmark of mobilevitv2_150.cvnets_in22k_ft_in1k_384 done. 349.65 samples/sec, 182.15 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model mobilevitv2_175.cvnets_in1k created, param count: 14251833
Running inference benchmark on mobilevitv2_175.cvnets_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
Infer [8/40]. 2481.19 samples/sec. 103.176 ms/step.
Infer [16/40]. 2481.18 samples/sec. 103.177 ms/step.
Infer [24/40]. 2481.14 samples/sec. 103.179 ms/step.
Infer [32/40]. 2481.18 samples/sec. 103.177 ms/step.
Infer [40/40]. 2481.20 samples/sec. 103.176 ms/step.
Inference benchmark of mobilevitv2_175.cvnets_in1k done. 2480.54 samples/sec, 103.18 ms/step
Model mobilevitv2_175.cvnets_in1k created, param count: 14251833
Running train benchmark on mobilevitv2_175.cvnets_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 448.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 138.06 MiB is free. Including non-PyTorch memory, this process has 23.51 GiB memory in use. Of the allocated memory 22.01 GiB is allocated by PyTorch, and 268.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model mobilevitv2_175.cvnets_in1k created, param count: 14251833
Running train benchmark on mobilevitv2_175.cvnets_in1k for 40 steps w/ input size (3, 256, 256) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 168.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 102.06 MiB is free. Including non-PyTorch memory, this process has 23.54 GiB memory in use. Of the allocated memory 22.10 GiB is allocated by PyTorch, and 213.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model mobilevitv2_175.cvnets_in1k created, param count: 14251833
Running train benchmark on mobilevitv2_175.cvnets_in1k for 40 steps w/ input size (3, 256, 256) and batch size 128.
Train [8/40]. 680.55 samples/sec. 188.083 ms/step.
Train [16/40]. 680.61 samples/sec. 188.067 ms/step.
Train [24/40]. 680.57 samples/sec. 188.078 ms/step.
Train [32/40]. 680.59 samples/sec. 188.072 ms/step.
Train [40/40]. 680.62 samples/sec. 188.063 ms/step.
Train benchmark of mobilevitv2_175.cvnets_in1k done. 677.35 samples/sec, 188.06 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model mobilevitv2_175.cvnets_in22k_ft_in1k created, param count: 14251833
Running inference benchmark on mobilevitv2_175.cvnets_in22k_ft_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
Infer [8/40]. 2481.55 samples/sec. 103.161 ms/step.
Infer [16/40]. 2481.54 samples/sec. 103.162 ms/step.
Infer [24/40]. 2481.51 samples/sec. 103.163 ms/step.
Infer [32/40]. 2481.45 samples/sec. 103.166 ms/step.
Infer [40/40]. 2481.38 samples/sec. 103.168 ms/step.
Inference benchmark of mobilevitv2_175.cvnets_in22k_ft_in1k done. 2480.73 samples/sec, 103.17 ms/step
Model mobilevitv2_175.cvnets_in22k_ft_in1k created, param count: 14251833
Running train benchmark on mobilevitv2_175.cvnets_in22k_ft_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 448.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 138.06 MiB is free. Including non-PyTorch memory, this process has 23.51 GiB memory in use. Of the allocated memory 22.01 GiB is allocated by PyTorch, and 268.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model mobilevitv2_175.cvnets_in22k_ft_in1k created, param count: 14251833
Running train benchmark on mobilevitv2_175.cvnets_in22k_ft_in1k for 40 steps w/ input size (3, 256, 256) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 168.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 58.06 MiB is free. Including non-PyTorch memory, this process has 23.58 GiB memory in use. Of the allocated memory 22.10 GiB is allocated by PyTorch, and 257.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model mobilevitv2_175.cvnets_in22k_ft_in1k created, param count: 14251833
Running train benchmark on mobilevitv2_175.cvnets_in22k_ft_in1k for 40 steps w/ input size (3, 256, 256) and batch size 128.
Train [8/40]. 680.65 samples/sec. 188.055 ms/step.
Train [16/40]. 680.67 samples/sec. 188.051 ms/step.
Train [24/40]. 680.61 samples/sec. 188.067 ms/step.
Train [32/40]. 680.61 samples/sec. 188.068 ms/step.
Train [40/40]. 680.62 samples/sec. 188.064 ms/step.
Train benchmark of mobilevitv2_175.cvnets_in22k_ft_in1k done. 677.33 samples/sec, 188.06 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model mobilevitv2_175.cvnets_in22k_ft_in1k_384 created, param count: 14251833
Running inference benchmark on mobilevitv2_175.cvnets_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 1079.69 samples/sec. 237.105 ms/step.
Infer [16/40]. 1079.58 samples/sec. 237.130 ms/step.
Infer [24/40]. 1079.47 samples/sec. 237.153 ms/step.
Infer [32/40]. 1079.44 samples/sec. 237.160 ms/step.
Infer [40/40]. 1079.44 samples/sec. 237.161 ms/step.
Inference benchmark of mobilevitv2_175.cvnets_in22k_ft_in1k_384 done. 1079.24 samples/sec, 237.16 ms/step
Model mobilevitv2_175.cvnets_in22k_ft_in1k_384 created, param count: 14251833
Running train benchmark on mobilevitv2_175.cvnets_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 3.94 GiB. GPU 0 has a total capacty of 23.65 GiB of which 2.87 GiB is free. Including non-PyTorch memory, this process has 20.77 GiB memory in use. Of the allocated memory 18.96 GiB is allocated by PyTorch, and 596.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model mobilevitv2_175.cvnets_in22k_ft_in1k_384 created, param count: 14251833
Running train benchmark on mobilevitv2_175.cvnets_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 2.95 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.63 GiB is free. Including non-PyTorch memory, this process has 22.01 GiB memory in use. Of the allocated memory 20.13 GiB is allocated by PyTorch, and 661.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model mobilevitv2_175.cvnets_in22k_ft_in1k_384 created, param count: 14251833
Running train benchmark on mobilevitv2_175.cvnets_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 1008.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 400.06 MiB is free. Including non-PyTorch memory, this process has 23.25 GiB memory in use. Of the allocated memory 21.81 GiB is allocated by PyTorch, and 217.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model mobilevitv2_175.cvnets_in22k_ft_in1k_384 created, param count: 14251833
Running train benchmark on mobilevitv2_175.cvnets_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 190.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 140.06 MiB is free. Including non-PyTorch memory, this process has 23.50 GiB memory in use. Of the allocated memory 22.09 GiB is allocated by PyTorch, and 184.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model mobilevitv2_175.cvnets_in22k_ft_in1k_384 created, param count: 14251833
Running train benchmark on mobilevitv2_175.cvnets_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 64.
Train [8/40]. 298.16 samples/sec. 214.652 ms/step.
Train [16/40]. 298.15 samples/sec. 214.656 ms/step.
Train [24/40]. 298.14 samples/sec. 214.664 ms/step.
Train [32/40]. 298.14 samples/sec. 214.666 ms/step.
Train [40/40]. 298.13 samples/sec. 214.671 ms/step.
Train benchmark of mobilevitv2_175.cvnets_in22k_ft_in1k_384 done. 296.75 samples/sec, 214.67 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model mobilevitv2_200.cvnets_in1k created, param count: 18449329
Running inference benchmark on mobilevitv2_200.cvnets_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
Infer [8/40]. 2134.45 samples/sec. 119.937 ms/step.
Infer [16/40]. 2134.44 samples/sec. 119.938 ms/step.
Infer [24/40]. 2134.51 samples/sec. 119.934 ms/step.
Infer [32/40]. 2134.48 samples/sec. 119.935 ms/step.
Infer [40/40]. 2134.38 samples/sec. 119.941 ms/step.
Inference benchmark of mobilevitv2_200.cvnets_in1k done. 2133.82 samples/sec, 119.94 ms/step
Model mobilevitv2_200.cvnets_in1k created, param count: 18449329
Running train benchmark on mobilevitv2_200.cvnets_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 70.06 MiB is free. Including non-PyTorch memory, this process has 23.57 GiB memory in use. Of the allocated memory 22.15 GiB is allocated by PyTorch, and 199.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model mobilevitv2_200.cvnets_in1k created, param count: 18449329
Running train benchmark on mobilevitv2_200.cvnets_in1k for 40 steps w/ input size (3, 256, 256) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 188.06 MiB is free. Including non-PyTorch memory, this process has 23.46 GiB memory in use. Of the allocated memory 22.06 GiB is allocated by PyTorch, and 170.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model mobilevitv2_200.cvnets_in1k created, param count: 18449329
Running train benchmark on mobilevitv2_200.cvnets_in1k for 40 steps w/ input size (3, 256, 256) and batch size 128.
Train [8/40]. 606.34 samples/sec. 211.101 ms/step.
Train [16/40]. 606.33 samples/sec. 211.105 ms/step.
Train [24/40]. 606.31 samples/sec. 211.112 ms/step.
Train [32/40]. 606.29 samples/sec. 211.121 ms/step.
Train [40/40]. 606.29 samples/sec. 211.119 ms/step.
Train benchmark of mobilevitv2_200.cvnets_in1k done. 603.46 samples/sec, 211.12 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model mobilevitv2_200.cvnets_in22k_ft_in1k created, param count: 18449329
Running inference benchmark on mobilevitv2_200.cvnets_in22k_ft_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
Infer [8/40]. 2135.15 samples/sec. 119.898 ms/step.
Infer [16/40]. 2134.94 samples/sec. 119.910 ms/step.
Infer [24/40]. 2134.81 samples/sec. 119.917 ms/step.
Infer [32/40]. 2134.72 samples/sec. 119.922 ms/step.
Infer [40/40]. 2134.61 samples/sec. 119.928 ms/step.
Inference benchmark of mobilevitv2_200.cvnets_in22k_ft_in1k done. 2134.04 samples/sec, 119.93 ms/step
Model mobilevitv2_200.cvnets_in22k_ft_in1k created, param count: 18449329
Running train benchmark on mobilevitv2_200.cvnets_in22k_ft_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 70.06 MiB is free. Including non-PyTorch memory, this process has 23.57 GiB memory in use. Of the allocated memory 22.15 GiB is allocated by PyTorch, and 199.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model mobilevitv2_200.cvnets_in22k_ft_in1k created, param count: 18449329
Running train benchmark on mobilevitv2_200.cvnets_in22k_ft_in1k for 40 steps w/ input size (3, 256, 256) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 188.06 MiB is free. Including non-PyTorch memory, this process has 23.46 GiB memory in use. Of the allocated memory 22.06 GiB is allocated by PyTorch, and 170.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model mobilevitv2_200.cvnets_in22k_ft_in1k created, param count: 18449329
Running train benchmark on mobilevitv2_200.cvnets_in22k_ft_in1k for 40 steps w/ input size (3, 256, 256) and batch size 128.
Train [8/40]. 606.33 samples/sec. 211.105 ms/step.
Train [16/40]. 606.30 samples/sec. 211.115 ms/step.
Train [24/40]. 606.28 samples/sec. 211.122 ms/step.
Train [32/40]. 606.29 samples/sec. 211.120 ms/step.
Train [40/40]. 606.27 samples/sec. 211.127 ms/step.
Train benchmark of mobilevitv2_200.cvnets_in22k_ft_in1k done. 603.57 samples/sec, 211.13 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model mobilevitv2_200.cvnets_in22k_ft_in1k_384 created, param count: 18449329
Running inference benchmark on mobilevitv2_200.cvnets_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 926.15 samples/sec. 276.414 ms/step.
Infer [16/40]. 926.43 samples/sec. 276.331 ms/step.
Infer [24/40]. 926.03 samples/sec. 276.449 ms/step.
Infer [32/40]. 926.10 samples/sec. 276.428 ms/step.
Infer [40/40]. 925.87 samples/sec. 276.497 ms/step.
Inference benchmark of mobilevitv2_200.cvnets_in22k_ft_in1k_384 done. 925.73 samples/sec, 276.50 ms/step
Model mobilevitv2_200.cvnets_in22k_ft_in1k_384 created, param count: 18449329
Running train benchmark on mobilevitv2_200.cvnets_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 4.50 GiB. GPU 0 has a total capacty of 23.65 GiB of which 198.06 MiB is free. Including non-PyTorch memory, this process has 23.45 GiB memory in use. Of the allocated memory 21.64 GiB is allocated by PyTorch, and 591.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model mobilevitv2_200.cvnets_in22k_ft_in1k_384 created, param count: 18449329
Running train benchmark on mobilevitv2_200.cvnets_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 3.38 GiB. GPU 0 has a total capacty of 23.65 GiB of which 2.15 GiB is free. Including non-PyTorch memory, this process has 21.49 GiB memory in use. Of the allocated memory 19.61 GiB is allocated by PyTorch, and 656.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model mobilevitv2_200.cvnets_in22k_ft_in1k_384 created, param count: 18449329
Running train benchmark on mobilevitv2_200.cvnets_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 1.12 GiB. GPU 0 has a total capacty of 23.65 GiB of which 762.06 MiB is free. Including non-PyTorch memory, this process has 22.90 GiB memory in use. Of the allocated memory 21.53 GiB is allocated by PyTorch, and 135.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model mobilevitv2_200.cvnets_in22k_ft_in1k_384 created, param count: 18449329
Running train benchmark on mobilevitv2_200.cvnets_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 864.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 38.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 22.07 GiB is allocated by PyTorch, and 311.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model mobilevitv2_200.cvnets_in22k_ft_in1k_384 created, param count: 18449329
Running train benchmark on mobilevitv2_200.cvnets_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 108.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 30.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 22.05 GiB is allocated by PyTorch, and 340.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model mobilevitv2_200.cvnets_in22k_ft_in1k_384 created, param count: 18449329
Running train benchmark on mobilevitv2_200.cvnets_in22k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 48.
Train [8/40]. 265.90 samples/sec. 180.516 ms/step.
Train [16/40]. 265.92 samples/sec. 180.507 ms/step.
Train [24/40]. 265.91 samples/sec. 180.509 ms/step.
Train [32/40]. 265.90 samples/sec. 180.521 ms/step.
Train [40/40]. 265.90 samples/sec. 180.520 ms/step.
Train benchmark of mobilevitv2_200.cvnets_in22k_ft_in1k_384 done. 264.60 samples/sec, 180.52 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model mvitv2_base.fb_in1k created, param count: 51472744
Running inference benchmark on mvitv2_base.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1186.95 samples/sec. 215.678 ms/step.
Infer [16/40]. 1186.89 samples/sec. 215.690 ms/step.
Infer [24/40]. 1186.87 samples/sec. 215.693 ms/step.
Infer [32/40]. 1186.86 samples/sec. 215.696 ms/step.
Infer [40/40]. 1186.83 samples/sec. 215.700 ms/step.
Inference benchmark of mvitv2_base.fb_in1k done. 1186.65 samples/sec, 215.70 ms/step
Model mvitv2_base.fb_in1k created, param count: 51472744
Running train benchmark on mvitv2_base.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 76.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 12.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.15 GiB is allocated by PyTorch, and 257.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model mvitv2_base.fb_in1k created, param count: 51472744
Running train benchmark on mvitv2_base.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 20.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.10 GiB is allocated by PyTorch, and 297.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model mvitv2_base.fb_in1k created, param count: 51472744
Running train benchmark on mvitv2_base.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
Train [8/40]. 379.06 samples/sec. 337.675 ms/step.
Train [16/40]. 379.02 samples/sec. 337.712 ms/step.
Train [24/40]. 379.07 samples/sec. 337.665 ms/step.
Train [32/40]. 379.07 samples/sec. 337.667 ms/step.
Train [40/40]. 379.07 samples/sec. 337.667 ms/step.
Train benchmark of mvitv2_base.fb_in1k done. 375.39 samples/sec, 337.67 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model mvitv2_base_cls.fb_inw21k created, param count: 65444032
Running inference benchmark on mvitv2_base_cls.fb_inw21k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1296.10 samples/sec. 197.516 ms/step.
Infer [16/40]. 1295.95 samples/sec. 197.538 ms/step.
Infer [24/40]. 1295.88 samples/sec. 197.549 ms/step.
Infer [32/40]. 1295.83 samples/sec. 197.556 ms/step.
Infer [40/40]. 1295.82 samples/sec. 197.558 ms/step.
Inference benchmark of mvitv2_base_cls.fb_inw21k done. 1295.62 samples/sec, 197.56 ms/step
Model mvitv2_base_cls.fb_inw21k created, param count: 65444032
Running train benchmark on mvitv2_base_cls.fb_inw21k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 24.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.06 GiB is allocated by PyTorch, and 328.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model mvitv2_base_cls.fb_inw21k created, param count: 65444032
Running train benchmark on mvitv2_base_cls.fb_inw21k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 84.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 76.06 MiB is free. Including non-PyTorch memory, this process has 23.57 GiB memory in use. Of the allocated memory 22.05 GiB is allocated by PyTorch, and 291.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model mvitv2_base_cls.fb_inw21k created, param count: 65444032
Running train benchmark on mvitv2_base_cls.fb_inw21k for 40 steps w/ input size (3, 224, 224) and batch size 128.
Train [8/40]. 349.89 samples/sec. 365.830 ms/step.
Train [16/40]. 349.87 samples/sec. 365.850 ms/step.
Train [24/40]. 349.87 samples/sec. 365.852 ms/step.
Train [32/40]. 349.85 samples/sec. 365.874 ms/step.
Train [40/40]. 349.84 samples/sec. 365.884 ms/step.
Train benchmark of mvitv2_base_cls.fb_inw21k done. 346.47 samples/sec, 365.88 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model mvitv2_huge_cls.fb_inw21k created, param count: 694804128
Running inference benchmark on mvitv2_huge_cls.fb_inw21k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 181.29 samples/sec. 1412.131 ms/step.
Infer [16/40]. 181.26 samples/sec. 1412.309 ms/step.
Infer [24/40]. 181.26 samples/sec. 1412.355 ms/step.
Infer [32/40]. 181.26 samples/sec. 1412.362 ms/step.
Infer [40/40]. 181.25 samples/sec. 1412.378 ms/step.
Inference benchmark of mvitv2_huge_cls.fb_inw21k done. 181.25 samples/sec, 1412.38 ms/step
Model mvitv2_huge_cls.fb_inw21k created, param count: 694804128
Running train benchmark on mvitv2_huge_cls.fb_inw21k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 1.15 GiB. GPU 0 has a total capacty of 23.65 GiB of which 656.06 MiB is free. Including non-PyTorch memory, this process has 23.00 GiB memory in use. Of the allocated memory 20.44 GiB is allocated by PyTorch, and 1.33 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model mvitv2_huge_cls.fb_inw21k created, param count: 694804128
Running train benchmark on mvitv2_huge_cls.fb_inw21k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 884.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 562.06 MiB is free. Including non-PyTorch memory, this process has 23.09 GiB memory in use. Of the allocated memory 20.74 GiB is allocated by PyTorch, and 1.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model mvitv2_huge_cls.fb_inw21k created, param count: 694804128
Running train benchmark on mvitv2_huge_cls.fb_inw21k for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 226.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 124.06 MiB is free. Including non-PyTorch memory, this process has 23.52 GiB memory in use. Of the allocated memory 22.06 GiB is allocated by PyTorch, and 231.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model mvitv2_huge_cls.fb_inw21k created, param count: 694804128
Running train benchmark on mvitv2_huge_cls.fb_inw21k for 40 steps w/ input size (3, 224, 224) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 170.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 40.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 22.16 GiB is allocated by PyTorch, and 218.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model mvitv2_huge_cls.fb_inw21k created, param count: 694804128
Running train benchmark on mvitv2_huge_cls.fb_inw21k for 40 steps w/ input size (3, 224, 224) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 4.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 21.89 GiB is allocated by PyTorch, and 526.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model mvitv2_huge_cls.fb_inw21k created, param count: 694804128
Running train benchmark on mvitv2_huge_cls.fb_inw21k for 40 steps w/ input size (3, 224, 224) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 6.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.04 GiB is allocated by PyTorch, and 373.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model mvitv2_huge_cls.fb_inw21k created, param count: 694804128
Running train benchmark on mvitv2_huge_cls.fb_inw21k for 40 steps w/ input size (3, 224, 224) and batch size 32.
ERROR: "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 10.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 21.26 GiB is allocated by PyTorch, and 1.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model mvitv2_huge_cls.fb_inw21k created, param count: 694804128
Running train benchmark on mvitv2_huge_cls.fb_inw21k for 40 steps w/ input size (3, 224, 224) and batch size 24.
ERROR: "CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 10.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 19.67 GiB is allocated by PyTorch, and 2.73 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model mvitv2_huge_cls.fb_inw21k created, param count: 694804128
Running train benchmark on mvitv2_huge_cls.fb_inw21k for 40 steps w/ input size (3, 224, 224) and batch size 16.
Train [8/40]. 24.87 samples/sec. 643.335 ms/step.
Train [16/40]. 24.85 samples/sec. 643.885 ms/step.
Train [24/40]. 24.86 samples/sec. 643.550 ms/step.
Train [32/40]. 24.87 samples/sec. 643.471 ms/step.
Train [40/40]. 24.87 samples/sec. 643.399 ms/step.
Train benchmark of mvitv2_huge_cls.fb_inw21k done. 24.44 samples/sec, 643.40 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model mvitv2_large.fb_in1k created, param count: 217992952
Running inference benchmark on mvitv2_large.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 382.55 samples/sec. 669.193 ms/step.
Infer [16/40]. 382.53 samples/sec. 669.223 ms/step.
Infer [24/40]. 382.53 samples/sec. 669.227 ms/step.
Infer [32/40]. 382.53 samples/sec. 669.230 ms/step.
Infer [40/40]. 382.52 samples/sec. 669.238 ms/step.
Inference benchmark of mvitv2_large.fb_in1k done. 382.50 samples/sec, 669.24 ms/step
Model mvitv2_large.fb_in1k created, param count: 217992952
Running train benchmark on mvitv2_large.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 60.06 MiB is free. Including non-PyTorch memory, this process has 23.58 GiB memory in use. Of the allocated memory 21.95 GiB is allocated by PyTorch, and 411.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model mvitv2_large.fb_in1k created, param count: 217992952
Running train benchmark on mvitv2_large.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 226.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 118.06 MiB is free. Including non-PyTorch memory, this process has 23.53 GiB memory in use. Of the allocated memory 21.87 GiB is allocated by PyTorch, and 431.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model mvitv2_large.fb_in1k created, param count: 217992952
Running train benchmark on mvitv2_large.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 76.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 40.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 21.88 GiB is allocated by PyTorch, and 504.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model mvitv2_large.fb_in1k created, param count: 217992952
Running train benchmark on mvitv2_large.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 84.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 12.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 21.41 GiB is allocated by PyTorch, and 1014.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model mvitv2_large.fb_in1k created, param count: 217992952
Running train benchmark on mvitv2_large.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 50.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 21.50 GiB is allocated by PyTorch, and 884.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model mvitv2_large.fb_in1k created, param count: 217992952
Running train benchmark on mvitv2_large.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 48.
Train [8/40]. 116.02 samples/sec. 413.719 ms/step.
Train [16/40]. 116.20 samples/sec. 413.089 ms/step.
Train [24/40]. 116.50 samples/sec. 412.024 ms/step.
Train [32/40]. 116.67 samples/sec. 411.410 ms/step.
Train [40/40]. 116.71 samples/sec. 411.258 ms/step.
Train benchmark of mvitv2_large.fb_in1k done. 114.97 samples/sec, 411.26 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model mvitv2_large_cls.fb_inw21k created, param count: 234583216
Running inference benchmark on mvitv2_large_cls.fb_inw21k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 411.69 samples/sec. 621.823 ms/step.
Infer [16/40]. 411.70 samples/sec. 621.819 ms/step.
Infer [24/40]. 411.69 samples/sec. 621.820 ms/step.
Infer [32/40]. 411.69 samples/sec. 621.824 ms/step.
Infer [40/40]. 411.69 samples/sec. 621.830 ms/step.
Inference benchmark of mvitv2_large_cls.fb_inw21k done. 411.66 samples/sec, 621.83 ms/step
Model mvitv2_large_cls.fb_inw21k created, param count: 234583216
Running train benchmark on mvitv2_large_cls.fb_inw21k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 442.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 436.06 MiB is free. Including non-PyTorch memory, this process has 23.21 GiB memory in use. Of the allocated memory 21.03 GiB is allocated by PyTorch, and 976.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model mvitv2_large_cls.fb_inw21k created, param count: 234583216
Running train benchmark on mvitv2_large_cls.fb_inw21k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 226.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 94.06 MiB is free. Including non-PyTorch memory, this process has 23.55 GiB memory in use. Of the allocated memory 21.69 GiB is allocated by PyTorch, and 642.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model mvitv2_large_cls.fb_inw21k created, param count: 234583216
Running train benchmark on mvitv2_large_cls.fb_inw21k for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 102.06 MiB is free. Including non-PyTorch memory, this process has 23.54 GiB memory in use. Of the allocated memory 21.65 GiB is allocated by PyTorch, and 670.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model mvitv2_large_cls.fb_inw21k created, param count: 234583216
Running train benchmark on mvitv2_large_cls.fb_inw21k for 40 steps w/ input size (3, 224, 224) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 8.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 21.46 GiB is allocated by PyTorch, and 959.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model mvitv2_large_cls.fb_inw21k created, param count: 234583216
Running train benchmark on mvitv2_large_cls.fb_inw21k for 40 steps w/ input size (3, 224, 224) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 84.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 56.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 21.50 GiB is allocated by PyTorch, and 871.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model mvitv2_large_cls.fb_inw21k created, param count: 234583216
Running train benchmark on mvitv2_large_cls.fb_inw21k for 40 steps w/ input size (3, 224, 224) and batch size 48.
Train [8/40]. 109.07 samples/sec. 440.088 ms/step.
Train [16/40]. 109.08 samples/sec. 440.041 ms/step.
Train [24/40]. 109.08 samples/sec. 440.044 ms/step.
Train [32/40]. 109.08 samples/sec. 440.027 ms/step.
Train [40/40]. 109.08 samples/sec. 440.051 ms/step.
Train benchmark of mvitv2_large_cls.fb_inw21k done. 107.45 samples/sec, 440.05 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model mvitv2_small.fb_in1k created, param count: 34870216
Running inference benchmark on mvitv2_small.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1695.24 samples/sec. 151.011 ms/step.
Infer [16/40]. 1695.09 samples/sec. 151.025 ms/step.
Infer [24/40]. 1695.09 samples/sec. 151.024 ms/step.
Infer [32/40]. 1695.09 samples/sec. 151.024 ms/step.
Infer [40/40]. 1695.07 samples/sec. 151.026 ms/step.
Inference benchmark of mvitv2_small.fb_in1k done. 1694.71 samples/sec, 151.03 ms/step
Model mvitv2_small.fb_in1k created, param count: 34870216
Running train benchmark on mvitv2_small.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 74.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 26.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 313.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model mvitv2_small.fb_in1k created, param count: 34870216
Running train benchmark on mvitv2_small.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
Train [8/40]. 526.25 samples/sec. 364.844 ms/step.
Train [16/40]. 526.26 samples/sec. 364.838 ms/step.
Train [24/40]. 526.26 samples/sec. 364.838 ms/step.
Train [32/40]. 526.27 samples/sec. 364.835 ms/step.
Train [40/40]. 526.26 samples/sec. 364.839 ms/step.
Train benchmark of mvitv2_small.fb_in1k done. 522.40 samples/sec, 364.84 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model mvitv2_tiny.fb_in1k created, param count: 24173320
Running inference benchmark on mvitv2_tiny.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 2264.76 samples/sec. 113.036 ms/step.
Infer [16/40]. 2263.62 samples/sec. 113.093 ms/step.
Infer [24/40]. 2263.22 samples/sec. 113.113 ms/step.
Infer [32/40]. 2262.95 samples/sec. 113.127 ms/step.
Infer [40/40]. 2262.85 samples/sec. 113.132 ms/step.
Inference benchmark of mvitv2_tiny.fb_in1k done. 2262.25 samples/sec, 113.13 ms/step
Model mvitv2_tiny.fb_in1k created, param count: 24173320
Running train benchmark on mvitv2_tiny.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 690.82 samples/sec. 370.572 ms/step.
Train [16/40]. 690.83 samples/sec. 370.570 ms/step.
Train [24/40]. 690.83 samples/sec. 370.567 ms/step.
Train [32/40]. 690.84 samples/sec. 370.565 ms/step.
Train [40/40]. 690.84 samples/sec. 370.562 ms/step.
Train benchmark of mvitv2_tiny.fb_in1k done. 687.63 samples/sec, 370.56 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model nasnetalarge.tf_in1k created, param count: 88753150
Running inference benchmark on nasnetalarge.tf_in1k for 40 steps w/ input size (3, 331, 331) and batch size 256.
Infer [8/40]. 475.36 samples/sec. 538.539 ms/step.
Infer [16/40]. 475.35 samples/sec. 538.548 ms/step.
Infer [24/40]. 475.35 samples/sec. 538.550 ms/step.
Infer [32/40]. 475.35 samples/sec. 538.549 ms/step.
Infer [40/40]. 475.35 samples/sec. 538.551 ms/step.
Inference benchmark of nasnetalarge.tf_in1k done. 475.31 samples/sec, 538.55 ms/step
Model nasnetalarge.tf_in1k created, param count: 88753150
Running train benchmark on nasnetalarge.tf_in1k for 40 steps w/ input size (3, 331, 331) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 566.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 190.06 MiB is free. Including non-PyTorch memory, this process has 23.46 GiB memory in use. Of the allocated memory 22.18 GiB is allocated by PyTorch, and 48.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model nasnetalarge.tf_in1k created, param count: 88753150
Running train benchmark on nasnetalarge.tf_in1k for 40 steps w/ input size (3, 331, 331) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 244.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 226.06 MiB is free. Including non-PyTorch memory, this process has 23.42 GiB memory in use. Of the allocated memory 21.89 GiB is allocated by PyTorch, and 301.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model nasnetalarge.tf_in1k created, param count: 88753150
Running train benchmark on nasnetalarge.tf_in1k for 40 steps w/ input size (3, 331, 331) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 74.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 40.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 21.22 GiB is allocated by PyTorch, and 1.15 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model nasnetalarge.tf_in1k created, param count: 88753150
Running train benchmark on nasnetalarge.tf_in1k for 40 steps w/ input size (3, 331, 331) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 26.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 21.76 GiB is allocated by PyTorch, and 633.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model nasnetalarge.tf_in1k created, param count: 88753150
Running train benchmark on nasnetalarge.tf_in1k for 40 steps w/ input size (3, 331, 331) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 74.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 60.06 MiB is free. Including non-PyTorch memory, this process has 23.58 GiB memory in use. Of the allocated memory 21.55 GiB is allocated by PyTorch, and 815.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model nasnetalarge.tf_in1k created, param count: 88753150
Running train benchmark on nasnetalarge.tf_in1k for 40 steps w/ input size (3, 331, 331) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 14.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.02 GiB is allocated by PyTorch, and 380.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model nasnetalarge.tf_in1k created, param count: 88753150
Running train benchmark on nasnetalarge.tf_in1k for 40 steps w/ input size (3, 331, 331) and batch size 32.
Train [8/40]. 125.86 samples/sec. 254.244 ms/step.
Train [16/40]. 125.91 samples/sec. 254.158 ms/step.
Train [24/40]. 125.90 samples/sec. 254.163 ms/step.
Train [32/40]. 125.90 samples/sec. 254.173 ms/step.
Train [40/40]. 125.89 samples/sec. 254.182 ms/step.
Train benchmark of nasnetalarge.tf_in1k done. 124.37 samples/sec, 254.18 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model nest_base_jx.goog_in1k created, param count: 67723368
Running inference benchmark on nest_base_jx.goog_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1199.37 samples/sec. 213.445 ms/step.
Infer [16/40]. 1199.31 samples/sec. 213.456 ms/step.
Infer [24/40]. 1199.25 samples/sec. 213.466 ms/step.
Infer [32/40]. 1199.28 samples/sec. 213.462 ms/step.
Infer [40/40]. 1199.26 samples/sec. 213.464 ms/step.
Inference benchmark of nest_base_jx.goog_in1k done. 1199.03 samples/sec, 213.46 ms/step
Model nest_base_jx.goog_in1k created, param count: 67723368
Running train benchmark on nest_base_jx.goog_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 302.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 256.06 MiB is free. Including non-PyTorch memory, this process has 23.39 GiB memory in use. Of the allocated memory 22.00 GiB is allocated by PyTorch, and 164.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model nest_base_jx.goog_in1k created, param count: 67723368
Running train benchmark on nest_base_jx.goog_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 226.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 46.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 22.11 GiB is allocated by PyTorch, and 254.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model nest_base_jx.goog_in1k created, param count: 67723368
Running train benchmark on nest_base_jx.goog_in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
Train [8/40]. 438.47 samples/sec. 291.927 ms/step.
Train [16/40]. 438.45 samples/sec. 291.935 ms/step.
Train [24/40]. 438.45 samples/sec. 291.940 ms/step.
Train [32/40]. 438.45 samples/sec. 291.939 ms/step.
Train [40/40]. 438.45 samples/sec. 291.940 ms/step.
Train benchmark of nest_base_jx.goog_in1k done. 435.58 samples/sec, 291.94 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model nest_small_jx.goog_in1k created, param count: 38351176
Running inference benchmark on nest_small_jx.goog_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1659.26 samples/sec. 154.286 ms/step.
Infer [16/40]. 1658.68 samples/sec. 154.340 ms/step.
Infer [24/40]. 1658.53 samples/sec. 154.354 ms/step.
Infer [32/40]. 1658.43 samples/sec. 154.363 ms/step.
Infer [40/40]. 1658.38 samples/sec. 154.368 ms/step.
Inference benchmark of nest_small_jx.goog_in1k done. 1658.03 samples/sec, 154.37 ms/step
Model nest_small_jx.goog_in1k created, param count: 38351176
Running train benchmark on nest_small_jx.goog_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 226.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 104.06 MiB is free. Including non-PyTorch memory, this process has 23.54 GiB memory in use. Of the allocated memory 22.09 GiB is allocated by PyTorch, and 221.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model nest_small_jx.goog_in1k created, param count: 38351176
Running train benchmark on nest_small_jx.goog_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
Train [8/40]. 607.94 samples/sec. 315.819 ms/step.
Train [16/40]. 607.96 samples/sec. 315.812 ms/step.
Train [24/40]. 607.96 samples/sec. 315.810 ms/step.
Train [32/40]. 607.96 samples/sec. 315.808 ms/step.
Train [40/40]. 607.96 samples/sec. 315.811 ms/step.
Train benchmark of nest_small_jx.goog_in1k done. 604.29 samples/sec, 315.81 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model nest_tiny_jx.goog_in1k created, param count: 17057608
Running inference benchmark on nest_tiny_jx.goog_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 2575.83 samples/sec. 99.386 ms/step.
Infer [16/40]. 2575.05 samples/sec. 99.415 ms/step.
Infer [24/40]. 2574.83 samples/sec. 99.424 ms/step.
Infer [32/40]. 2574.77 samples/sec. 99.426 ms/step.
Infer [40/40]. 2574.70 samples/sec. 99.429 ms/step.
Inference benchmark of nest_tiny_jx.goog_in1k done. 2574.01 samples/sec, 99.43 ms/step
Model nest_tiny_jx.goog_in1k created, param count: 17057608
Running train benchmark on nest_tiny_jx.goog_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 919.09 samples/sec. 278.536 ms/step.
Train [16/40]. 919.07 samples/sec. 278.543 ms/step.
Train [24/40]. 919.08 samples/sec. 278.540 ms/step.
Train [32/40]. 919.07 samples/sec. 278.543 ms/step.
Train [40/40]. 919.07 samples/sec. 278.542 ms/step.
Train benchmark of nest_tiny_jx.goog_in1k done. 915.14 samples/sec, 278.54 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model nf_regnet_b1.ra2_in1k created, param count: 10223984
Running inference benchmark on nf_regnet_b1.ra2_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 4990.81 samples/sec. 51.294 ms/step.
Infer [16/40]. 4990.78 samples/sec. 51.295 ms/step.
Infer [24/40]. 4990.62 samples/sec. 51.296 ms/step.
Infer [32/40]. 4990.54 samples/sec. 51.297 ms/step.
Infer [40/40]. 4990.40 samples/sec. 51.298 ms/step.
Inference benchmark of nf_regnet_b1.ra2_in1k done. 4988.00 samples/sec, 51.30 ms/step
Model nf_regnet_b1.ra2_in1k created, param count: 10223984
Running train benchmark on nf_regnet_b1.ra2_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 1634.35 samples/sec. 156.637 ms/step.
Train [16/40]. 1634.36 samples/sec. 156.637 ms/step.
Train [24/40]. 1634.32 samples/sec. 156.640 ms/step.
Train [32/40]. 1634.32 samples/sec. 156.640 ms/step.
Train [40/40]. 1634.32 samples/sec. 156.640 ms/step.
Train benchmark of nf_regnet_b1.ra2_in1k done. 1622.15 samples/sec, 156.64 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model nf_resnet50.ra2_in1k created, param count: 25557032
Running inference benchmark on nf_resnet50.ra2_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 2107.24 samples/sec. 121.486 ms/step.
Infer [16/40]. 2106.49 samples/sec. 121.529 ms/step.
Infer [24/40]. 2105.71 samples/sec. 121.574 ms/step.
Infer [32/40]. 2105.61 samples/sec. 121.580 ms/step.
Infer [40/40]. 2105.25 samples/sec. 121.601 ms/step.
Inference benchmark of nf_resnet50.ra2_in1k done. 2104.76 samples/sec, 121.60 ms/step
Model nf_resnet50.ra2_in1k created, param count: 25557032
Running train benchmark on nf_resnet50.ra2_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 865.67 samples/sec. 295.724 ms/step.
Train [16/40]. 865.65 samples/sec. 295.733 ms/step.
Train [24/40]. 865.69 samples/sec. 295.717 ms/step.
Train [32/40]. 865.68 samples/sec. 295.720 ms/step.
Train [40/40]. 865.69 samples/sec. 295.718 ms/step.
Train benchmark of nf_resnet50.ra2_in1k done. 862.39 samples/sec, 295.72 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model nfnet_l0.ra2_in1k created, param count: 35074488
Running inference benchmark on nfnet_l0.ra2_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 2017.81 samples/sec. 126.870 ms/step.
Infer [16/40]. 2017.78 samples/sec. 126.872 ms/step.
Infer [24/40]. 2017.72 samples/sec. 126.876 ms/step.
Infer [32/40]. 2017.70 samples/sec. 126.877 ms/step.
Infer [40/40]. 2017.70 samples/sec. 126.877 ms/step.
Inference benchmark of nfnet_l0.ra2_in1k done. 2017.22 samples/sec, 126.88 ms/step
Model nfnet_l0.ra2_in1k created, param count: 35074488
Running train benchmark on nfnet_l0.ra2_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 729.58 samples/sec. 350.885 ms/step.
Train [16/40]. 729.60 samples/sec. 350.877 ms/step.
Train [24/40]. 729.59 samples/sec. 350.884 ms/step.
Train [32/40]. 729.58 samples/sec. 350.886 ms/step.
Train [40/40]. 729.57 samples/sec. 350.890 ms/step.
Train benchmark of nfnet_l0.ra2_in1k done. 726.73 samples/sec, 350.89 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model pit_b_224.in1k created, param count: 73764840
Running inference benchmark on pit_b_224.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3561.62 samples/sec. 71.877 ms/step.
Infer [16/40]. 3561.59 samples/sec. 71.878 ms/step.
Infer [24/40]. 3561.64 samples/sec. 71.877 ms/step.
Infer [32/40]. 3561.24 samples/sec. 71.885 ms/step.
Infer [40/40]. 3561.22 samples/sec. 71.886 ms/step.
Inference benchmark of pit_b_224.in1k done. 3559.91 samples/sec, 71.89 ms/step
Model pit_b_224.in1k created, param count: 73764840
Running train benchmark on pit_b_224.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1045.01 samples/sec. 244.975 ms/step.
Train [16/40]. 1044.95 samples/sec. 244.987 ms/step.
Train [24/40]. 1044.78 samples/sec. 245.028 ms/step.
Train [32/40]. 1044.77 samples/sec. 245.030 ms/step.
Train [40/40]. 1044.54 samples/sec. 245.084 ms/step.
Train benchmark of pit_b_224.in1k done. 1040.11 samples/sec, 245.08 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model pit_b_distilled_224.in1k created, param count: 74790096
Running inference benchmark on pit_b_distilled_224.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3534.26 samples/sec. 72.434 ms/step.
Infer [16/40]. 3534.72 samples/sec. 72.424 ms/step.
Infer [24/40]. 3534.95 samples/sec. 72.420 ms/step.
Infer [32/40]. 3534.80 samples/sec. 72.423 ms/step.
Infer [40/40]. 3534.52 samples/sec. 72.428 ms/step.
Inference benchmark of pit_b_distilled_224.in1k done. 3533.24 samples/sec, 72.43 ms/step
Model pit_b_distilled_224.in1k created, param count: 74790096
Running train benchmark on pit_b_distilled_224.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1039.21 samples/sec. 246.341 ms/step.
Train [16/40]. 1039.30 samples/sec. 246.321 ms/step.
Train [24/40]. 1039.24 samples/sec. 246.335 ms/step.
Train [32/40]. 1038.95 samples/sec. 246.403 ms/step.
Train [40/40]. 1038.90 samples/sec. 246.415 ms/step.
Train benchmark of pit_b_distilled_224.in1k done. 1034.30 samples/sec, 246.41 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model pit_s_224.in1k created, param count: 23461912
Running inference benchmark on pit_s_224.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 11386.89 samples/sec. 22.482 ms/step.
Infer [16/40]. 11384.29 samples/sec. 22.487 ms/step.
Infer [24/40]. 11381.33 samples/sec. 22.493 ms/step.
Infer [32/40]. 11379.98 samples/sec. 22.496 ms/step.
Infer [40/40]. 11378.90 samples/sec. 22.498 ms/step.
Inference benchmark of pit_s_224.in1k done. 11366.02 samples/sec, 22.50 ms/step
Model pit_s_224.in1k created, param count: 23461912
Running train benchmark on pit_s_224.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 2895.10 samples/sec. 88.425 ms/step.
Train [16/40]. 2895.24 samples/sec. 88.421 ms/step.
Train [24/40]. 2895.06 samples/sec. 88.426 ms/step.
Train [32/40]. 2895.21 samples/sec. 88.422 ms/step.
Train [40/40]. 2895.12 samples/sec. 88.425 ms/step.
Train benchmark of pit_s_224.in1k done. 2871.24 samples/sec, 88.42 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model pit_s_distilled_224.in1k created, param count: 24039056
Running inference benchmark on pit_s_distilled_224.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 11340.46 samples/sec. 22.574 ms/step.
Infer [16/40]. 11339.34 samples/sec. 22.576 ms/step.
Infer [24/40]. 11334.93 samples/sec. 22.585 ms/step.
Infer [32/40]. 11332.20 samples/sec. 22.590 ms/step.
Infer [40/40]. 11330.93 samples/sec. 22.593 ms/step.
Inference benchmark of pit_s_distilled_224.in1k done. 11318.69 samples/sec, 22.59 ms/step
Model pit_s_distilled_224.in1k created, param count: 24039056
Running train benchmark on pit_s_distilled_224.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 2873.29 samples/sec. 89.097 ms/step.
Train [16/40]. 2873.16 samples/sec. 89.100 ms/step.
Train [24/40]. 2873.02 samples/sec. 89.105 ms/step.
Train [32/40]. 2873.03 samples/sec. 89.105 ms/step.
Train [40/40]. 2873.05 samples/sec. 89.104 ms/step.
Train benchmark of pit_s_distilled_224.in1k done. 2850.33 samples/sec, 89.10 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model pit_ti_224.in1k created, param count: 4847272
Running inference benchmark on pit_ti_224.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 26923.04 samples/sec. 9.509 ms/step.
Infer [16/40]. 26924.20 samples/sec. 9.508 ms/step.
Infer [24/40]. 26918.33 samples/sec. 9.510 ms/step.
Infer [32/40]. 26927.73 samples/sec. 9.507 ms/step.
Infer [40/40]. 26931.42 samples/sec. 9.506 ms/step.
Inference benchmark of pit_ti_224.in1k done. 26873.06 samples/sec, 9.51 ms/step
Model pit_ti_224.in1k created, param count: 4847272
Running train benchmark on pit_ti_224.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 6096.78 samples/sec. 41.989 ms/step.
Train [16/40]. 6099.79 samples/sec. 41.969 ms/step.
Train [24/40]. 6097.67 samples/sec. 41.983 ms/step.
Train [32/40]. 6097.40 samples/sec. 41.985 ms/step.
Train [40/40]. 6098.68 samples/sec. 41.976 ms/step.
Train benchmark of pit_ti_224.in1k done. 6004.85 samples/sec, 41.98 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model pit_ti_distilled_224.in1k created, param count: 5104336
Running inference benchmark on pit_ti_distilled_224.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 26381.58 samples/sec. 9.704 ms/step.
Infer [16/40]. 26386.34 samples/sec. 9.702 ms/step.
Infer [24/40]. 26393.30 samples/sec. 9.699 ms/step.
Infer [32/40]. 26395.91 samples/sec. 9.698 ms/step.
Infer [40/40]. 26388.06 samples/sec. 9.701 ms/step.
Inference benchmark of pit_ti_distilled_224.in1k done. 26332.70 samples/sec, 9.70 ms/step
Model pit_ti_distilled_224.in1k created, param count: 5104336
Running train benchmark on pit_ti_distilled_224.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 6056.46 samples/sec. 42.269 ms/step.
Train [16/40]. 6055.82 samples/sec. 42.273 ms/step.
Train [24/40]. 6029.90 samples/sec. 42.455 ms/step.
Train [32/40]. 6010.45 samples/sec. 42.592 ms/step.
Train [40/40]. 6019.27 samples/sec. 42.530 ms/step.
Train benchmark of pit_ti_distilled_224.in1k done. 5928.53 samples/sec, 42.53 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model pit_xs_224.in1k created, param count: 10618888
Running inference benchmark on pit_xs_224.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 18971.96 samples/sec. 13.494 ms/step.
Infer [16/40]. 18969.31 samples/sec. 13.495 ms/step.
Infer [24/40]. 18967.27 samples/sec. 13.497 ms/step.
Infer [32/40]. 18966.95 samples/sec. 13.497 ms/step.
Infer [40/40]. 18965.07 samples/sec. 13.499 ms/step.
Inference benchmark of pit_xs_224.in1k done. 18933.37 samples/sec, 13.50 ms/step
Model pit_xs_224.in1k created, param count: 10618888
Running train benchmark on pit_xs_224.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 4508.69 samples/sec. 56.779 ms/step.
Train [16/40]. 4508.84 samples/sec. 56.777 ms/step.
Train [24/40]. 4508.65 samples/sec. 56.780 ms/step.
Train [32/40]. 4508.51 samples/sec. 56.782 ms/step.
Train [40/40]. 4508.53 samples/sec. 56.781 ms/step.
Train benchmark of pit_xs_224.in1k done. 4456.44 samples/sec, 56.78 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model pit_xs_distilled_224.in1k created, param count: 11003984
Running inference benchmark on pit_xs_distilled_224.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 18524.95 samples/sec. 13.819 ms/step.
Infer [16/40]. 18694.14 samples/sec. 13.694 ms/step.
Infer [24/40]. 18752.03 samples/sec. 13.652 ms/step.
Infer [32/40]. 18781.20 samples/sec. 13.631 ms/step.
Infer [40/40]. 18798.70 samples/sec. 13.618 ms/step.
Inference benchmark of pit_xs_distilled_224.in1k done. 18766.06 samples/sec, 13.62 ms/step
Model pit_xs_distilled_224.in1k created, param count: 11003984
Running train benchmark on pit_xs_distilled_224.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 4468.37 samples/sec. 57.292 ms/step.
Train [16/40]. 4468.54 samples/sec. 57.289 ms/step.
Train [24/40]. 4468.52 samples/sec. 57.290 ms/step.
Train [32/40]. 4468.17 samples/sec. 57.294 ms/step.
Train [40/40]. 4468.33 samples/sec. 57.292 ms/step.
Train benchmark of pit_xs_distilled_224.in1k done. 4415.95 samples/sec, 57.29 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model pnasnet5large.tf_in1k created, param count: 86057668
Running inference benchmark on pnasnet5large.tf_in1k for 40 steps w/ input size (3, 331, 331) and batch size 256.
Infer [8/40]. 446.49 samples/sec. 573.364 ms/step.
Infer [16/40]. 446.49 samples/sec. 573.364 ms/step.
Infer [24/40]. 446.46 samples/sec. 573.394 ms/step.
Infer [32/40]. 446.45 samples/sec. 573.415 ms/step.
Infer [40/40]. 446.44 samples/sec. 573.431 ms/step.
Inference benchmark of pnasnet5large.tf_in1k done. 446.40 samples/sec, 573.43 ms/step
Model pnasnet5large.tf_in1k created, param count: 86057668
Running train benchmark on pnasnet5large.tf_in1k for 40 steps w/ input size (3, 331, 331) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 182.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 110.06 MiB is free. Including non-PyTorch memory, this process has 23.53 GiB memory in use. Of the allocated memory 22.27 GiB is allocated by PyTorch, and 30.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model pnasnet5large.tf_in1k created, param count: 86057668
Running train benchmark on pnasnet5large.tf_in1k for 40 steps w/ input size (3, 331, 331) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 540.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 470.06 MiB is free. Including non-PyTorch memory, this process has 23.18 GiB memory in use. Of the allocated memory 21.67 GiB is allocated by PyTorch, and 285.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model pnasnet5large.tf_in1k created, param count: 86057668
Running train benchmark on pnasnet5large.tf_in1k for 40 steps w/ input size (3, 331, 331) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 182.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 64.06 MiB is free. Including non-PyTorch memory, this process has 23.58 GiB memory in use. Of the allocated memory 22.09 GiB is allocated by PyTorch, and 263.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model pnasnet5large.tf_in1k created, param count: 86057668
Running train benchmark on pnasnet5large.tf_in1k for 40 steps w/ input size (3, 331, 331) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 350.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 318.06 MiB is free. Including non-PyTorch memory, this process has 23.33 GiB memory in use. Of the allocated memory 21.76 GiB is allocated by PyTorch, and 346.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model pnasnet5large.tf_in1k created, param count: 86057668
Running train benchmark on pnasnet5large.tf_in1k for 40 steps w/ input size (3, 331, 331) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 30.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 22.04 GiB is allocated by PyTorch, and 350.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model pnasnet5large.tf_in1k created, param count: 86057668
Running train benchmark on pnasnet5large.tf_in1k for 40 steps w/ input size (3, 331, 331) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 18.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 14.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 21.88 GiB is allocated by PyTorch, and 524.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model pnasnet5large.tf_in1k created, param count: 86057668
Running train benchmark on pnasnet5large.tf_in1k for 40 steps w/ input size (3, 331, 331) and batch size 32.
Train [8/40]. 132.36 samples/sec. 241.766 ms/step.
Train [16/40]. 132.35 samples/sec. 241.791 ms/step.
Train [24/40]. 132.32 samples/sec. 241.840 ms/step.
Train [32/40]. 132.30 samples/sec. 241.876 ms/step.
Train [40/40]. 132.29 samples/sec. 241.885 ms/step.
Train benchmark of pnasnet5large.tf_in1k done. 130.93 samples/sec, 241.88 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model poolformer_m36.sail_in1k created, param count: 56172520
Running inference benchmark on poolformer_m36.sail_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1802.39 samples/sec. 142.034 ms/step.
Infer [16/40]. 1802.36 samples/sec. 142.036 ms/step.
Infer [24/40]. 1802.35 samples/sec. 142.037 ms/step.
Infer [32/40]. 1802.29 samples/sec. 142.041 ms/step.
Infer [40/40]. 1802.25 samples/sec. 142.045 ms/step.
Inference benchmark of poolformer_m36.sail_in1k done. 1801.87 samples/sec, 142.04 ms/step
Model poolformer_m36.sail_in1k created, param count: 56172520
Running train benchmark on poolformer_m36.sail_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 148.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 84.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 22.26 GiB is allocated by PyTorch, and 71.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model poolformer_m36.sail_in1k created, param count: 56172520
Running train benchmark on poolformer_m36.sail_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
Train [8/40]. 621.34 samples/sec. 309.009 ms/step.
Train [16/40]. 621.36 samples/sec. 308.998 ms/step.
Train [24/40]. 621.31 samples/sec. 309.025 ms/step.
Train [32/40]. 621.30 samples/sec. 309.031 ms/step.
Train [40/40]. 621.29 samples/sec. 309.036 ms/step.
Train benchmark of poolformer_m36.sail_in1k done. 618.11 samples/sec, 309.04 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model poolformer_m48.sail_in1k created, param count: 73473448
Running inference benchmark on poolformer_m48.sail_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1359.89 samples/sec. 188.251 ms/step.
Infer [16/40]. 1359.94 samples/sec. 188.243 ms/step.
Infer [24/40]. 1359.93 samples/sec. 188.244 ms/step.
Infer [32/40]. 1359.90 samples/sec. 188.250 ms/step.
Infer [40/40]. 1359.91 samples/sec. 188.247 ms/step.
Inference benchmark of poolformer_m48.sail_in1k done. 1359.66 samples/sec, 188.25 ms/step
Model poolformer_m48.sail_in1k created, param count: 73473448
Running train benchmark on poolformer_m48.sail_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 294.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 212.06 MiB is free. Including non-PyTorch memory, this process has 23.43 GiB memory in use. Of the allocated memory 22.18 GiB is allocated by PyTorch, and 24.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model poolformer_m48.sail_in1k created, param count: 73473448
Running train benchmark on poolformer_m48.sail_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 12.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 21.95 GiB is allocated by PyTorch, and 456.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model poolformer_m48.sail_in1k created, param count: 73473448
Running train benchmark on poolformer_m48.sail_in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
Train [8/40]. 492.32 samples/sec. 259.994 ms/step.
Train [16/40]. 492.15 samples/sec. 260.081 ms/step.
Train [24/40]. 492.09 samples/sec. 260.113 ms/step.
Train [32/40]. 492.06 samples/sec. 260.130 ms/step.
Train [40/40]. 492.03 samples/sec. 260.145 ms/step.
Train benchmark of poolformer_m48.sail_in1k done. 488.58 samples/sec, 260.14 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model poolformer_s12.sail_in1k created, param count: 11915176
Running inference benchmark on poolformer_s12.sail_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 7606.59 samples/sec. 33.655 ms/step.
Infer [16/40]. 7602.62 samples/sec. 33.673 ms/step.
Infer [24/40]. 7600.50 samples/sec. 33.682 ms/step.
Infer [32/40]. 7599.33 samples/sec. 33.687 ms/step.
Infer [40/40]. 7598.47 samples/sec. 33.691 ms/step.
Inference benchmark of poolformer_s12.sail_in1k done. 7592.66 samples/sec, 33.69 ms/step
Model poolformer_s12.sail_in1k created, param count: 11915176
Running train benchmark on poolformer_s12.sail_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 2690.11 samples/sec. 95.163 ms/step.
Train [16/40]. 2690.00 samples/sec. 95.167 ms/step.
Train [24/40]. 2689.93 samples/sec. 95.170 ms/step.
Train [32/40]. 2689.84 samples/sec. 95.173 ms/step.
Train [40/40]. 2689.80 samples/sec. 95.174 ms/step.
Train benchmark of poolformer_s12.sail_in1k done. 2672.88 samples/sec, 95.17 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model poolformer_s24.sail_in1k created, param count: 21388968
Running inference benchmark on poolformer_s24.sail_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3924.33 samples/sec. 65.234 ms/step.
Infer [16/40]. 3924.14 samples/sec. 65.237 ms/step.
Infer [24/40]. 3923.91 samples/sec. 65.241 ms/step.
Infer [32/40]. 3923.36 samples/sec. 65.250 ms/step.
Infer [40/40]. 3923.02 samples/sec. 65.256 ms/step.
Inference benchmark of poolformer_s24.sail_in1k done. 3921.40 samples/sec, 65.26 ms/step
Model poolformer_s24.sail_in1k created, param count: 21388968
Running train benchmark on poolformer_s24.sail_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1393.04 samples/sec. 183.771 ms/step.
Train [16/40]. 1393.03 samples/sec. 183.772 ms/step.
Train [24/40]. 1392.98 samples/sec. 183.778 ms/step.
Train [32/40]. 1392.90 samples/sec. 183.789 ms/step.
Train [40/40]. 1392.89 samples/sec. 183.790 ms/step.
Train benchmark of poolformer_s24.sail_in1k done. 1384.67 samples/sec, 183.79 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model poolformer_s36.sail_in1k created, param count: 30862760
Running inference benchmark on poolformer_s36.sail_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 2645.08 samples/sec. 96.784 ms/step.
Infer [16/40]. 2644.55 samples/sec. 96.803 ms/step.
Infer [24/40]. 2644.09 samples/sec. 96.820 ms/step.
Infer [32/40]. 2643.87 samples/sec. 96.828 ms/step.
Infer [40/40]. 2643.72 samples/sec. 96.833 ms/step.
Inference benchmark of poolformer_s36.sail_in1k done. 2642.99 samples/sec, 96.83 ms/step
Model poolformer_s36.sail_in1k created, param count: 30862760
Running train benchmark on poolformer_s36.sail_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 939.65 samples/sec. 272.442 ms/step.
Train [16/40]. 939.61 samples/sec. 272.454 ms/step.
Train [24/40]. 939.58 samples/sec. 272.462 ms/step.
Train [32/40]. 939.56 samples/sec. 272.467 ms/step.
Train [40/40]. 939.55 samples/sec. 272.470 ms/step.
Train benchmark of poolformer_s36.sail_in1k done. 934.31 samples/sec, 272.47 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model poolformerv2_m36.sail_in1k created, param count: 56077168
Running inference benchmark on poolformerv2_m36.sail_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1244.25 samples/sec. 205.747 ms/step.
Infer [16/40]. 1244.31 samples/sec. 205.737 ms/step.
Infer [24/40]. 1244.31 samples/sec. 205.737 ms/step.
Infer [32/40]. 1243.75 samples/sec. 205.829 ms/step.
Infer [40/40]. 1243.40 samples/sec. 205.887 ms/step.
Inference benchmark of poolformerv2_m36.sail_in1k done. 1243.18 samples/sec, 205.89 ms/step
Model poolformerv2_m36.sail_in1k created, param count: 56077168
Running train benchmark on poolformerv2_m36.sail_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 294.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 200.06 MiB is free. Including non-PyTorch memory, this process has 23.45 GiB memory in use. Of the allocated memory 21.96 GiB is allocated by PyTorch, and 262.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model poolformerv2_m36.sail_in1k created, param count: 56077168
Running train benchmark on poolformerv2_m36.sail_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 110.06 MiB is free. Including non-PyTorch memory, this process has 23.53 GiB memory in use. Of the allocated memory 22.09 GiB is allocated by PyTorch, and 212.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model poolformerv2_m36.sail_in1k created, param count: 56077168
Running train benchmark on poolformerv2_m36.sail_in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
Train [8/40]. 424.93 samples/sec. 301.225 ms/step.
Train [16/40]. 424.88 samples/sec. 301.265 ms/step.
Train [24/40]. 424.68 samples/sec. 301.401 ms/step.
Train [32/40]. 424.58 samples/sec. 301.472 ms/step.
Train [40/40]. 424.52 samples/sec. 301.515 ms/step.
Train benchmark of poolformerv2_m36.sail_in1k done. 422.41 samples/sec, 301.51 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model poolformerv2_m48.sail_in1k created, param count: 73346056
Running inference benchmark on poolformerv2_m48.sail_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 937.42 samples/sec. 273.089 ms/step.
Infer [16/40]. 937.40 samples/sec. 273.096 ms/step.
Infer [24/40]. 937.40 samples/sec. 273.095 ms/step.
Infer [32/40]. 937.40 samples/sec. 273.097 ms/step.
Infer [40/40]. 937.39 samples/sec. 273.097 ms/step.
Inference benchmark of poolformerv2_m48.sail_in1k done. 937.25 samples/sec, 273.10 ms/step
Model poolformerv2_m48.sail_in1k created, param count: 73346056
Running train benchmark on poolformerv2_m48.sail_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 294.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 198.06 MiB is free. Including non-PyTorch memory, this process has 23.45 GiB memory in use. Of the allocated memory 21.99 GiB is allocated by PyTorch, and 231.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model poolformerv2_m48.sail_in1k created, param count: 73346056
Running train benchmark on poolformerv2_m48.sail_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 104.06 MiB is free. Including non-PyTorch memory, this process has 23.54 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 232.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model poolformerv2_m48.sail_in1k created, param count: 73346056
Running train benchmark on poolformerv2_m48.sail_in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 74.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 38.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 21.92 GiB is allocated by PyTorch, and 463.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model poolformerv2_m48.sail_in1k created, param count: 73346056
Running train benchmark on poolformerv2_m48.sail_in1k for 40 steps w/ input size (3, 224, 224) and batch size 96.
Train [8/40]. 319.78 samples/sec. 300.207 ms/step.
Train [16/40]. 319.78 samples/sec. 300.204 ms/step.
Train [24/40]. 319.78 samples/sec. 300.204 ms/step.
Train [32/40]. 319.78 samples/sec. 300.203 ms/step.
Train [40/40]. 319.78 samples/sec. 300.205 ms/step.
Train benchmark of poolformerv2_m48.sail_in1k done. 317.93 samples/sec, 300.20 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model poolformerv2_s12.sail_in1k created, param count: 11891712
Running inference benchmark on poolformerv2_s12.sail_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 5122.91 samples/sec. 49.972 ms/step.
Infer [16/40]. 5118.08 samples/sec. 50.019 ms/step.
Infer [24/40]. 5115.75 samples/sec. 50.042 ms/step.
Infer [32/40]. 5114.59 samples/sec. 50.053 ms/step.
Infer [40/40]. 5113.91 samples/sec. 50.060 ms/step.
Inference benchmark of poolformerv2_s12.sail_in1k done. 5111.40 samples/sec, 50.06 ms/step
Model poolformerv2_s12.sail_in1k created, param count: 11891712
Running train benchmark on poolformerv2_s12.sail_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1698.38 samples/sec. 150.732 ms/step.
Train [16/40]. 1698.42 samples/sec. 150.728 ms/step.
Train [24/40]. 1698.45 samples/sec. 150.725 ms/step.
Train [32/40]. 1698.47 samples/sec. 150.724 ms/step.
Train [40/40]. 1698.45 samples/sec. 150.726 ms/step.
Train benchmark of poolformerv2_s12.sail_in1k done. 1691.03 samples/sec, 150.73 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model poolformerv2_s24.sail_in1k created, param count: 21341464
Running inference benchmark on poolformerv2_s24.sail_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 2651.87 samples/sec. 96.536 ms/step.
Infer [16/40]. 2651.83 samples/sec. 96.537 ms/step.
Infer [24/40]. 2651.74 samples/sec. 96.540 ms/step.
Infer [32/40]. 2651.67 samples/sec. 96.543 ms/step.
Infer [40/40]. 2651.65 samples/sec. 96.544 ms/step.
Inference benchmark of poolformerv2_s24.sail_in1k done. 2650.91 samples/sec, 96.54 ms/step
Model poolformerv2_s24.sail_in1k created, param count: 21341464
Running train benchmark on poolformerv2_s24.sail_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 896.53 samples/sec. 285.545 ms/step.
Train [16/40]. 896.54 samples/sec. 285.542 ms/step.
Train [24/40]. 896.54 samples/sec. 285.542 ms/step.
Train [32/40]. 896.53 samples/sec. 285.545 ms/step.
Train [40/40]. 896.52 samples/sec. 285.548 ms/step.
Train benchmark of poolformerv2_s24.sail_in1k done. 892.81 samples/sec, 285.55 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model poolformerv2_s36.sail_in1k created, param count: 30791216
Running inference benchmark on poolformerv2_s36.sail_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1790.20 samples/sec. 143.000 ms/step.
Infer [16/40]. 1790.11 samples/sec. 143.008 ms/step.
Infer [24/40]. 1790.12 samples/sec. 143.007 ms/step.
Infer [32/40]. 1790.13 samples/sec. 143.006 ms/step.
Infer [40/40]. 1790.12 samples/sec. 143.007 ms/step.
Inference benchmark of poolformerv2_s36.sail_in1k done. 1789.72 samples/sec, 143.01 ms/step
Model poolformerv2_s36.sail_in1k created, param count: 30791216
Running train benchmark on poolformerv2_s36.sail_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 124.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 100.06 MiB is free. Including non-PyTorch memory, this process has 23.54 GiB memory in use. Of the allocated memory 22.10 GiB is allocated by PyTorch, and 210.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model poolformerv2_s36.sail_in1k created, param count: 30791216
Running train benchmark on poolformerv2_s36.sail_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
Train [8/40]. 617.92 samples/sec. 310.718 ms/step.
Train [16/40]. 617.93 samples/sec. 310.715 ms/step.
Train [24/40]. 617.94 samples/sec. 310.712 ms/step.
Train [32/40]. 617.94 samples/sec. 310.710 ms/step.
Train [40/40]. 617.94 samples/sec. 310.712 ms/step.
Train benchmark of poolformerv2_s36.sail_in1k done. 614.97 samples/sec, 310.71 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model pvt_v2_b0.in1k created, param count: 3666760
Running inference benchmark on pvt_v2_b0.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 10942.30 samples/sec. 23.395 ms/step.
Infer [16/40]. 10942.79 samples/sec. 23.394 ms/step.
Infer [24/40]. 10943.67 samples/sec. 23.393 ms/step.
Infer [32/40]. 10943.89 samples/sec. 23.392 ms/step.
Infer [40/40]. 10943.91 samples/sec. 23.392 ms/step.
Inference benchmark of pvt_v2_b0.in1k done. 10932.39 samples/sec, 23.39 ms/step
Model pvt_v2_b0.in1k created, param count: 3666760
Running train benchmark on pvt_v2_b0.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 3149.06 samples/sec. 81.294 ms/step.
Train [16/40]. 3145.98 samples/sec. 81.374 ms/step.
Train [24/40]. 3144.91 samples/sec. 81.401 ms/step.
Train [32/40]. 3144.33 samples/sec. 81.416 ms/step.
Train [40/40]. 3144.02 samples/sec. 81.424 ms/step.
Train benchmark of pvt_v2_b0.in1k done. 3115.05 samples/sec, 81.42 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model pvt_v2_b1.in1k created, param count: 14009000
Running inference benchmark on pvt_v2_b1.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 5915.67 samples/sec. 43.275 ms/step.
Infer [16/40]. 5915.63 samples/sec. 43.275 ms/step.
Infer [24/40]. 5915.49 samples/sec. 43.276 ms/step.
Infer [32/40]. 5915.42 samples/sec. 43.277 ms/step.
Infer [40/40]. 5915.56 samples/sec. 43.276 ms/step.
Inference benchmark of pvt_v2_b1.in1k done. 5911.95 samples/sec, 43.28 ms/step
Model pvt_v2_b1.in1k created, param count: 14009000
Running train benchmark on pvt_v2_b1.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1890.51 samples/sec. 135.413 ms/step.
Train [16/40]. 1890.44 samples/sec. 135.418 ms/step.
Train [24/40]. 1890.47 samples/sec. 135.416 ms/step.
Train [32/40]. 1890.45 samples/sec. 135.417 ms/step.
Train [40/40]. 1890.44 samples/sec. 135.418 ms/step.
Train benchmark of pvt_v2_b1.in1k done. 1878.67 samples/sec, 135.42 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model pvt_v2_b2.in1k created, param count: 25362856
Running inference benchmark on pvt_v2_b2.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3495.70 samples/sec. 73.233 ms/step.
Infer [16/40]. 3495.45 samples/sec. 73.238 ms/step.
Infer [24/40]. 3495.49 samples/sec. 73.237 ms/step.
Infer [32/40]. 3495.41 samples/sec. 73.239 ms/step.
Infer [40/40]. 3495.33 samples/sec. 73.241 ms/step.
Inference benchmark of pvt_v2_b2.in1k done. 3494.01 samples/sec, 73.24 ms/step
Model pvt_v2_b2.in1k created, param count: 25362856
Running train benchmark on pvt_v2_b2.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1129.95 samples/sec. 226.559 ms/step.
Train [16/40]. 1129.91 samples/sec. 226.566 ms/step.
Train [24/40]. 1129.92 samples/sec. 226.565 ms/step.
Train [32/40]. 1129.91 samples/sec. 226.566 ms/step.
Train [40/40]. 1129.89 samples/sec. 226.571 ms/step.
Train benchmark of pvt_v2_b2.in1k done. 1121.59 samples/sec, 226.57 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model pvt_v2_b2_li.in1k created, param count: 22553512
Running inference benchmark on pvt_v2_b2_li.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3018.93 samples/sec. 84.798 ms/step.
Infer [16/40]. 3017.40 samples/sec. 84.841 ms/step.
Infer [24/40]. 3016.86 samples/sec. 84.856 ms/step.
Infer [32/40]. 3016.66 samples/sec. 84.862 ms/step.
Infer [40/40]. 3016.53 samples/sec. 84.866 ms/step.
Inference benchmark of pvt_v2_b2_li.in1k done. 3015.58 samples/sec, 84.87 ms/step
Model pvt_v2_b2_li.in1k created, param count: 22553512
Running train benchmark on pvt_v2_b2_li.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1005.46 samples/sec. 254.609 ms/step.
Train [16/40]. 1005.45 samples/sec. 254.612 ms/step.
Train [24/40]. 1005.44 samples/sec. 254.614 ms/step.
Train [32/40]. 1005.47 samples/sec. 254.608 ms/step.
Train [40/40]. 1005.47 samples/sec. 254.606 ms/step.
Train benchmark of pvt_v2_b2_li.in1k done. 998.47 samples/sec, 254.61 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model pvt_v2_b3.in1k created, param count: 45238696
Running inference benchmark on pvt_v2_b3.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 2581.87 samples/sec. 99.153 ms/step.
Infer [16/40]. 2581.81 samples/sec. 99.155 ms/step.
Infer [24/40]. 2581.72 samples/sec. 99.159 ms/step.
Infer [32/40]. 2581.66 samples/sec. 99.161 ms/step.
Infer [40/40]. 2581.60 samples/sec. 99.163 ms/step.
Inference benchmark of pvt_v2_b3.in1k done. 2580.89 samples/sec, 99.16 ms/step
Model pvt_v2_b3.in1k created, param count: 45238696
Running train benchmark on pvt_v2_b3.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 8.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.19 GiB is allocated by PyTorch, and 217.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model pvt_v2_b3.in1k created, param count: 45238696
Running train benchmark on pvt_v2_b3.in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
Train [8/40]. 833.55 samples/sec. 230.341 ms/step.
Train [16/40]. 833.54 samples/sec. 230.342 ms/step.
Train [24/40]. 833.57 samples/sec. 230.336 ms/step.
Train [32/40]. 833.58 samples/sec. 230.332 ms/step.
Train [40/40]. 833.58 samples/sec. 230.333 ms/step.
Train benchmark of pvt_v2_b3.in1k done. 825.46 samples/sec, 230.33 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model pvt_v2_b4.in1k created, param count: 62556072
Running inference benchmark on pvt_v2_b4.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1853.19 samples/sec. 138.140 ms/step.
Infer [16/40]. 1853.08 samples/sec. 138.148 ms/step.
Infer [24/40]. 1853.07 samples/sec. 138.149 ms/step.
Infer [32/40]. 1853.05 samples/sec. 138.151 ms/step.
Infer [40/40]. 1853.02 samples/sec. 138.153 ms/step.
Inference benchmark of pvt_v2_b4.in1k done. 1852.62 samples/sec, 138.15 ms/step
Model pvt_v2_b4.in1k created, param count: 62556072
Running train benchmark on pvt_v2_b4.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 2.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 22.34 GiB is allocated by PyTorch, and 65.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model pvt_v2_b4.in1k created, param count: 62556072
Running train benchmark on pvt_v2_b4.in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 92.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 80.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 22.11 GiB is allocated by PyTorch, and 229.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model pvt_v2_b4.in1k created, param count: 62556072
Running train benchmark on pvt_v2_b4.in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
Train [8/40]. 608.28 samples/sec. 210.429 ms/step.
Train [16/40]. 608.30 samples/sec. 210.424 ms/step.
Train [24/40]. 608.30 samples/sec. 210.423 ms/step.
Train [32/40]. 608.28 samples/sec. 210.431 ms/step.
Train [40/40]. 608.27 samples/sec. 210.432 ms/step.
Train benchmark of pvt_v2_b4.in1k done. 599.53 samples/sec, 210.43 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model pvt_v2_b5.in1k created, param count: 81956008
Running inference benchmark on pvt_v2_b5.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1901.78 samples/sec. 134.611 ms/step.
Infer [16/40]. 1901.77 samples/sec. 134.611 ms/step.
Infer [24/40]. 1901.75 samples/sec. 134.613 ms/step.
Infer [32/40]. 1901.71 samples/sec. 134.616 ms/step.
Infer [40/40]. 1901.68 samples/sec. 134.618 ms/step.
Inference benchmark of pvt_v2_b5.in1k done. 1901.25 samples/sec, 134.62 ms/step
Model pvt_v2_b5.in1k created, param count: 81956008
Running train benchmark on pvt_v2_b5.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 4.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 22.10 GiB is allocated by PyTorch, and 312.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model pvt_v2_b5.in1k created, param count: 81956008
Running train benchmark on pvt_v2_b5.in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 92.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 42.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 22.00 GiB is allocated by PyTorch, and 373.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model pvt_v2_b5.in1k created, param count: 81956008
Running train benchmark on pvt_v2_b5.in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
Train [8/40]. 596.94 samples/sec. 214.429 ms/step.
Train [16/40]. 596.81 samples/sec. 214.472 ms/step.
Train [24/40]. 596.69 samples/sec. 214.516 ms/step.
Train [32/40]. 596.66 samples/sec. 214.527 ms/step.
Train [40/40]. 596.63 samples/sec. 214.539 ms/step.
Train benchmark of pvt_v2_b5.in1k done. 586.54 samples/sec, 214.54 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model regnetv_040.ra3_in1k created, param count: 20640640
Running inference benchmark on regnetv_040.ra3_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 2315.65 samples/sec. 110.552 ms/step.
Infer [16/40]. 2315.38 samples/sec. 110.565 ms/step.
Infer [24/40]. 2315.20 samples/sec. 110.573 ms/step.
Infer [32/40]. 2315.01 samples/sec. 110.583 ms/step.
Infer [40/40]. 2314.89 samples/sec. 110.589 ms/step.
Inference benchmark of regnetv_040.ra3_in1k done. 2314.28 samples/sec, 110.59 ms/step
Model regnetv_040.ra3_in1k created, param count: 20640640
Running train benchmark on regnetv_040.ra3_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 82.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 30.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 22.17 GiB is allocated by PyTorch, and 209.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model regnetv_040.ra3_in1k created, param count: 20640640
Running train benchmark on regnetv_040.ra3_in1k for 40 steps w/ input size (3, 288, 288) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 34.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 16.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.00 GiB is allocated by PyTorch, and 397.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model regnetv_040.ra3_in1k created, param count: 20640640
Running train benchmark on regnetv_040.ra3_in1k for 40 steps w/ input size (3, 288, 288) and batch size 128.
Train [8/40]. 648.37 samples/sec. 197.418 ms/step.
Train [16/40]. 648.34 samples/sec. 197.428 ms/step.
Train [24/40]. 648.34 samples/sec. 197.426 ms/step.
Train [32/40]. 648.15 samples/sec. 197.484 ms/step.
Train [40/40]. 648.04 samples/sec. 197.520 ms/step.
Train benchmark of regnetv_040.ra3_in1k done. 644.26 samples/sec, 197.52 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model regnetv_064.ra3_in1k created, param count: 30576052
Running inference benchmark on regnetv_064.ra3_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 1388.65 samples/sec. 184.352 ms/step.
Infer [16/40]. 1388.40 samples/sec. 184.385 ms/step.
Infer [24/40]. 1388.14 samples/sec. 184.419 ms/step.
Infer [32/40]. 1388.03 samples/sec. 184.434 ms/step.
Infer [40/40]. 1387.94 samples/sec. 184.446 ms/step.
Inference benchmark of regnetv_064.ra3_in1k done. 1387.68 samples/sec, 184.45 ms/step
Model regnetv_064.ra3_in1k created, param count: 30576052
Running train benchmark on regnetv_064.ra3_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 184.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 120.06 MiB is free. Including non-PyTorch memory, this process has 23.52 GiB memory in use. Of the allocated memory 21.96 GiB is allocated by PyTorch, and 336.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model regnetv_064.ra3_in1k created, param count: 30576052
Running train benchmark on regnetv_064.ra3_in1k for 40 steps w/ input size (3, 288, 288) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 70.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 46.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 22.15 GiB is allocated by PyTorch, and 214.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model regnetv_064.ra3_in1k created, param count: 30576052
Running train benchmark on regnetv_064.ra3_in1k for 40 steps w/ input size (3, 288, 288) and batch size 128.
Train [8/40]. 424.27 samples/sec. 301.692 ms/step.
Train [16/40]. 424.28 samples/sec. 301.686 ms/step.
Train [24/40]. 424.27 samples/sec. 301.692 ms/step.
Train [32/40]. 424.26 samples/sec. 301.705 ms/step.
Train [40/40]. 424.26 samples/sec. 301.701 ms/step.
Train benchmark of regnetv_064.ra3_in1k done. 422.27 samples/sec, 301.70 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model regnetx_002.pycls_in1k created, param count: 2684792
Running inference benchmark on regnetx_002.pycls_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 25580.49 samples/sec. 10.008 ms/step.
Infer [16/40]. 25566.78 samples/sec. 10.013 ms/step.
Infer [24/40]. 25564.01 samples/sec. 10.014 ms/step.
Infer [32/40]. 25556.84 samples/sec. 10.017 ms/step.
Infer [40/40]. 25551.62 samples/sec. 10.019 ms/step.
Inference benchmark of regnetx_002.pycls_in1k done. 25500.89 samples/sec, 10.02 ms/step
Model regnetx_002.pycls_in1k created, param count: 2684792
Running train benchmark on regnetx_002.pycls_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 5321.49 samples/sec. 48.107 ms/step.
Train [16/40]. 5320.27 samples/sec. 48.118 ms/step.
Train [24/40]. 5319.75 samples/sec. 48.123 ms/step.
Train [32/40]. 5319.70 samples/sec. 48.123 ms/step.
Train [40/40]. 5320.12 samples/sec. 48.119 ms/step.
Train benchmark of regnetx_002.pycls_in1k done. 5265.29 samples/sec, 48.12 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model regnetx_004.pycls_in1k created, param count: 5157512
Running inference benchmark on regnetx_004.pycls_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 15805.56 samples/sec. 16.197 ms/step.
Infer [16/40]. 15803.00 samples/sec. 16.199 ms/step.
Infer [24/40]. 15798.13 samples/sec. 16.204 ms/step.
Infer [32/40]. 15795.93 samples/sec. 16.207 ms/step.
Infer [40/40]. 15790.77 samples/sec. 16.212 ms/step.
Inference benchmark of regnetx_004.pycls_in1k done. 15769.30 samples/sec, 16.21 ms/step
Model regnetx_004.pycls_in1k created, param count: 5157512
Running train benchmark on regnetx_004.pycls_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 3849.03 samples/sec. 66.510 ms/step.
Train [16/40]. 3847.55 samples/sec. 66.536 ms/step.
Train [24/40]. 3848.03 samples/sec. 66.527 ms/step.
Train [32/40]. 3848.02 samples/sec. 66.528 ms/step.
Train [40/40]. 3847.99 samples/sec. 66.528 ms/step.
Train benchmark of regnetx_004.pycls_in1k done. 3808.80 samples/sec, 66.53 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model regnetx_004_tv.tv2_in1k created, param count: 5495976
Running inference benchmark on regnetx_004_tv.tv2_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 15777.70 samples/sec. 16.225 ms/step.
Infer [16/40]. 15774.45 samples/sec. 16.229 ms/step.
Infer [24/40]. 15770.06 samples/sec. 16.233 ms/step.
Infer [32/40]. 15771.00 samples/sec. 16.232 ms/step.
Infer [40/40]. 15770.96 samples/sec. 16.232 ms/step.
Inference benchmark of regnetx_004_tv.tv2_in1k done. 15748.87 samples/sec, 16.23 ms/step
Model regnetx_004_tv.tv2_in1k created, param count: 5495976
Running train benchmark on regnetx_004_tv.tv2_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 3844.13 samples/sec. 66.595 ms/step.
Train [16/40]. 3842.96 samples/sec. 66.615 ms/step.
Train [24/40]. 3843.81 samples/sec. 66.601 ms/step.
Train [32/40]. 3843.44 samples/sec. 66.607 ms/step.
Train [40/40]. 3843.38 samples/sec. 66.608 ms/step.
Train benchmark of regnetx_004_tv.tv2_in1k done. 3805.12 samples/sec, 66.61 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model regnetx_006.pycls_in1k created, param count: 6196040
Running inference benchmark on regnetx_006.pycls_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 16069.32 samples/sec. 15.931 ms/step.
Infer [16/40]. 16069.80 samples/sec. 15.931 ms/step.
Infer [24/40]. 16067.03 samples/sec. 15.933 ms/step.
Infer [32/40]. 16067.32 samples/sec. 15.933 ms/step.
Infer [40/40]. 16067.27 samples/sec. 15.933 ms/step.
Inference benchmark of regnetx_006.pycls_in1k done. 16043.60 samples/sec, 15.93 ms/step
Model regnetx_006.pycls_in1k created, param count: 6196040
Running train benchmark on regnetx_006.pycls_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 3285.11 samples/sec. 77.927 ms/step.
Train [16/40]. 3285.78 samples/sec. 77.911 ms/step.
Train [24/40]. 3285.86 samples/sec. 77.910 ms/step.
Train [32/40]. 3285.65 samples/sec. 77.914 ms/step.
Train [40/40]. 3285.63 samples/sec. 77.915 ms/step.
Train benchmark of regnetx_006.pycls_in1k done. 3261.44 samples/sec, 77.92 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model regnetx_008.pycls_in1k created, param count: 7259656
Running inference benchmark on regnetx_008.pycls_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 12683.74 samples/sec. 20.183 ms/step.
Infer [16/40]. 12682.69 samples/sec. 20.185 ms/step.
Infer [24/40]. 12681.83 samples/sec. 20.186 ms/step.
Infer [32/40]. 12682.32 samples/sec. 20.186 ms/step.
Infer [40/40]. 12681.93 samples/sec. 20.186 ms/step.
Inference benchmark of regnetx_008.pycls_in1k done. 12667.15 samples/sec, 20.19 ms/step
Model regnetx_008.pycls_in1k created, param count: 7259656
Running train benchmark on regnetx_008.pycls_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 2828.43 samples/sec. 90.510 ms/step.
Train [16/40]. 2828.31 samples/sec. 90.513 ms/step.
Train [24/40]. 2828.26 samples/sec. 90.515 ms/step.
Train [32/40]. 2828.35 samples/sec. 90.512 ms/step.
Train [40/40]. 2828.35 samples/sec. 90.512 ms/step.
Train benchmark of regnetx_008.pycls_in1k done. 2810.19 samples/sec, 90.51 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model regnetx_008.tv2_in1k created, param count: 7259656
Running inference benchmark on regnetx_008.tv2_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 12592.78 samples/sec. 20.329 ms/step.
Infer [16/40]. 12638.22 samples/sec. 20.256 ms/step.
Infer [24/40]. 12654.17 samples/sec. 20.230 ms/step.
Infer [32/40]. 12660.00 samples/sec. 20.221 ms/step.
Infer [40/40]. 12658.44 samples/sec. 20.224 ms/step.
Inference benchmark of regnetx_008.tv2_in1k done. 12643.79 samples/sec, 20.22 ms/step
Model regnetx_008.tv2_in1k created, param count: 7259656
Running train benchmark on regnetx_008.tv2_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 2828.01 samples/sec. 90.523 ms/step.
Train [16/40]. 2827.68 samples/sec. 90.534 ms/step.
Train [24/40]. 2826.82 samples/sec. 90.561 ms/step.
Train [32/40]. 2826.59 samples/sec. 90.569 ms/step.
Train [40/40]. 2826.27 samples/sec. 90.579 ms/step.
Train benchmark of regnetx_008.tv2_in1k done. 2808.15 samples/sec, 90.58 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model regnetx_016.pycls_in1k created, param count: 9190136
Running inference benchmark on regnetx_016.pycls_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 7297.17 samples/sec. 35.082 ms/step.
Infer [16/40]. 7296.72 samples/sec. 35.084 ms/step.
Infer [24/40]. 7296.44 samples/sec. 35.086 ms/step.
Infer [32/40]. 7296.35 samples/sec. 35.086 ms/step.
Infer [40/40]. 7295.71 samples/sec. 35.089 ms/step.
Inference benchmark of regnetx_016.pycls_in1k done. 7290.58 samples/sec, 35.09 ms/step
Model regnetx_016.pycls_in1k created, param count: 9190136
Running train benchmark on regnetx_016.pycls_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1678.21 samples/sec. 152.543 ms/step.
Train [16/40]. 1678.22 samples/sec. 152.543 ms/step.
Train [24/40]. 1678.20 samples/sec. 152.544 ms/step.
Train [32/40]. 1678.29 samples/sec. 152.536 ms/step.
Train [40/40]. 1678.31 samples/sec. 152.534 ms/step.
Train benchmark of regnetx_016.pycls_in1k done. 1670.32 samples/sec, 152.53 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model regnetx_016.tv2_in1k created, param count: 9190136
Running inference benchmark on regnetx_016.tv2_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 7288.05 samples/sec. 35.126 ms/step.
Infer [16/40]. 7285.71 samples/sec. 35.137 ms/step.
Infer [24/40]. 7285.20 samples/sec. 35.140 ms/step.
Infer [32/40]. 7284.96 samples/sec. 35.141 ms/step.
Infer [40/40]. 7284.67 samples/sec. 35.142 ms/step.
Inference benchmark of regnetx_016.tv2_in1k done. 7279.58 samples/sec, 35.14 ms/step
Model regnetx_016.tv2_in1k created, param count: 9190136
Running train benchmark on regnetx_016.tv2_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1678.77 samples/sec. 152.493 ms/step.
Train [16/40]. 1678.96 samples/sec. 152.475 ms/step.
Train [24/40]. 1678.91 samples/sec. 152.480 ms/step.
Train [32/40]. 1678.90 samples/sec. 152.481 ms/step.
Train [40/40]. 1678.79 samples/sec. 152.490 ms/step.
Train benchmark of regnetx_016.tv2_in1k done. 1670.87 samples/sec, 152.49 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model regnetx_032.pycls_in1k created, param count: 15296552
Running inference benchmark on regnetx_032.pycls_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 4712.11 samples/sec. 54.328 ms/step.
Infer [16/40]. 4712.17 samples/sec. 54.327 ms/step.
Infer [24/40]. 4712.13 samples/sec. 54.328 ms/step.
Infer [32/40]. 4710.61 samples/sec. 54.345 ms/step.
Infer [40/40]. 4709.43 samples/sec. 54.359 ms/step.
Inference benchmark of regnetx_032.pycls_in1k done. 4707.22 samples/sec, 54.36 ms/step
Model regnetx_032.pycls_in1k created, param count: 15296552
Running train benchmark on regnetx_032.pycls_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1243.71 samples/sec. 205.836 ms/step.
Train [16/40]. 1244.01 samples/sec. 205.787 ms/step.
Train [24/40]. 1243.91 samples/sec. 205.802 ms/step.
Train [32/40]. 1244.08 samples/sec. 205.775 ms/step.
Train [40/40]. 1244.00 samples/sec. 205.788 ms/step.
Train benchmark of regnetx_032.pycls_in1k done. 1237.94 samples/sec, 205.79 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model regnetx_032.tv2_in1k created, param count: 15296552
Running inference benchmark on regnetx_032.tv2_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 4708.06 samples/sec. 54.375 ms/step.
Infer [16/40]. 4708.13 samples/sec. 54.374 ms/step.
Infer [24/40]. 4708.32 samples/sec. 54.372 ms/step.
Infer [32/40]. 4707.91 samples/sec. 54.377 ms/step.
Infer [40/40]. 4707.60 samples/sec. 54.380 ms/step.
Inference benchmark of regnetx_032.tv2_in1k done. 4705.37 samples/sec, 54.38 ms/step
Model regnetx_032.tv2_in1k created, param count: 15296552
Running train benchmark on regnetx_032.tv2_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1243.89 samples/sec. 205.806 ms/step.
Train [16/40]. 1243.58 samples/sec. 205.858 ms/step.
Train [24/40]. 1243.61 samples/sec. 205.852 ms/step.
Train [32/40]. 1243.62 samples/sec. 205.851 ms/step.
Train [40/40]. 1243.40 samples/sec. 205.887 ms/step.
Train benchmark of regnetx_032.tv2_in1k done. 1237.32 samples/sec, 205.89 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model regnetx_040.pycls_in1k created, param count: 22118248
Running inference benchmark on regnetx_040.pycls_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 4001.69 samples/sec. 63.973 ms/step.
Infer [16/40]. 4001.95 samples/sec. 63.969 ms/step.
Infer [24/40]. 4002.02 samples/sec. 63.968 ms/step.
Infer [32/40]. 4001.84 samples/sec. 63.971 ms/step.
Infer [40/40]. 4001.88 samples/sec. 63.970 ms/step.
Inference benchmark of regnetx_040.pycls_in1k done. 4000.17 samples/sec, 63.97 ms/step
Model regnetx_040.pycls_in1k created, param count: 22118248
Running train benchmark on regnetx_040.pycls_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1107.70 samples/sec. 231.110 ms/step.
Train [16/40]. 1107.54 samples/sec. 231.143 ms/step.
Train [24/40]. 1107.46 samples/sec. 231.160 ms/step.
Train [32/40]. 1107.37 samples/sec. 231.178 ms/step.
Train [40/40]. 1107.38 samples/sec. 231.177 ms/step.
Train benchmark of regnetx_040.pycls_in1k done. 1102.64 samples/sec, 231.18 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model regnetx_064.pycls_in1k created, param count: 26209256
Running inference benchmark on regnetx_064.pycls_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 2956.64 samples/sec. 86.585 ms/step.
Infer [16/40]. 2956.21 samples/sec. 86.597 ms/step.
Infer [24/40]. 2955.76 samples/sec. 86.610 ms/step.
Infer [32/40]. 2955.49 samples/sec. 86.618 ms/step.
Infer [40/40]. 2955.33 samples/sec. 86.623 ms/step.
Inference benchmark of regnetx_064.pycls_in1k done. 2954.39 samples/sec, 86.62 ms/step
Model regnetx_064.pycls_in1k created, param count: 26209256
Running train benchmark on regnetx_064.pycls_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 833.01 samples/sec. 307.321 ms/step.
Train [16/40]. 833.02 samples/sec. 307.317 ms/step.
Train [24/40]. 833.01 samples/sec. 307.320 ms/step.
Train [32/40]. 833.02 samples/sec. 307.315 ms/step.
Train [40/40]. 833.02 samples/sec. 307.314 ms/step.
Train benchmark of regnetx_064.pycls_in1k done. 830.51 samples/sec, 307.31 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model regnetx_080.pycls_in1k created, param count: 39572648
Running inference benchmark on regnetx_080.pycls_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3023.20 samples/sec. 84.679 ms/step.
Infer [16/40]. 3023.12 samples/sec. 84.681 ms/step.
Infer [24/40]. 3022.89 samples/sec. 84.687 ms/step.
Infer [32/40]. 3022.84 samples/sec. 84.688 ms/step.
Infer [40/40]. 3022.83 samples/sec. 84.689 ms/step.
Inference benchmark of regnetx_080.pycls_in1k done. 3021.85 samples/sec, 84.69 ms/step
Model regnetx_080.pycls_in1k created, param count: 39572648
Running train benchmark on regnetx_080.pycls_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 880.16 samples/sec. 290.856 ms/step.
Train [16/40]. 880.14 samples/sec. 290.861 ms/step.
Train [24/40]. 880.15 samples/sec. 290.859 ms/step.
Train [32/40]. 880.16 samples/sec. 290.857 ms/step.
Train [40/40]. 880.15 samples/sec. 290.859 ms/step.
Train benchmark of regnetx_080.pycls_in1k done. 876.77 samples/sec, 290.86 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model regnetx_080.tv2_in1k created, param count: 39572648
Running inference benchmark on regnetx_080.tv2_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3025.53 samples/sec. 84.613 ms/step.
Infer [16/40]. 3025.15 samples/sec. 84.624 ms/step.
Infer [24/40]. 3025.09 samples/sec. 84.626 ms/step.
Infer [32/40]. 3024.89 samples/sec. 84.631 ms/step.
Infer [40/40]. 3024.52 samples/sec. 84.641 ms/step.
Inference benchmark of regnetx_080.tv2_in1k done. 3023.54 samples/sec, 84.64 ms/step
Model regnetx_080.tv2_in1k created, param count: 39572648
Running train benchmark on regnetx_080.tv2_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 880.21 samples/sec. 290.839 ms/step.
Train [16/40]. 880.22 samples/sec. 290.835 ms/step.
Train [24/40]. 880.25 samples/sec. 290.826 ms/step.
Train [32/40]. 880.26 samples/sec. 290.823 ms/step.
Train [40/40]. 880.26 samples/sec. 290.823 ms/step.
Train benchmark of regnetx_080.tv2_in1k done. 876.86 samples/sec, 290.82 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model regnetx_120.pycls_in1k created, param count: 46106056
Running inference benchmark on regnetx_120.pycls_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 2022.64 samples/sec. 126.567 ms/step.
Infer [16/40]. 2022.55 samples/sec. 126.573 ms/step.
Infer [24/40]. 2022.59 samples/sec. 126.571 ms/step.
Infer [32/40]. 2022.65 samples/sec. 126.566 ms/step.
Infer [40/40]. 2022.69 samples/sec. 126.564 ms/step.
Inference benchmark of regnetx_120.pycls_in1k done. 2022.19 samples/sec, 126.56 ms/step
Model regnetx_120.pycls_in1k created, param count: 46106056
Running train benchmark on regnetx_120.pycls_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 607.24 samples/sec. 421.580 ms/step.
Train [16/40]. 607.25 samples/sec. 421.574 ms/step.
Train [24/40]. 607.24 samples/sec. 421.579 ms/step.
Train [32/40]. 607.23 samples/sec. 421.583 ms/step.
Train [40/40]. 607.21 samples/sec. 421.599 ms/step.
Train benchmark of regnetx_120.pycls_in1k done. 605.70 samples/sec, 421.60 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model regnetx_160.pycls_in1k created, param count: 54278536
Running inference benchmark on regnetx_160.pycls_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1750.80 samples/sec. 146.219 ms/step.
Infer [16/40]. 1750.80 samples/sec. 146.219 ms/step.
Infer [24/40]. 1750.87 samples/sec. 146.213 ms/step.
Infer [32/40]. 1750.86 samples/sec. 146.214 ms/step.
Infer [40/40]. 1750.83 samples/sec. 146.216 ms/step.
Inference benchmark of regnetx_160.pycls_in1k done. 1750.44 samples/sec, 146.22 ms/step
Model regnetx_160.pycls_in1k created, param count: 54278536
Running train benchmark on regnetx_160.pycls_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 86.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 36.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 21.53 GiB is allocated by PyTorch, and 867.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model regnetx_160.pycls_in1k created, param count: 54278536
Running train benchmark on regnetx_160.pycls_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
Train [8/40]. 527.80 samples/sec. 363.774 ms/step.
Train [16/40]. 527.81 samples/sec. 363.764 ms/step.
Train [24/40]. 527.82 samples/sec. 363.758 ms/step.
Train [32/40]. 527.81 samples/sec. 363.767 ms/step.
Train [40/40]. 527.81 samples/sec. 363.770 ms/step.
Train benchmark of regnetx_160.pycls_in1k done. 526.16 samples/sec, 363.77 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model regnetx_160.tv2_in1k created, param count: 54278536
Running inference benchmark on regnetx_160.tv2_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1596.29 samples/sec. 160.372 ms/step.
Infer [16/40]. 1596.21 samples/sec. 160.380 ms/step.
Infer [24/40]. 1596.20 samples/sec. 160.381 ms/step.
Infer [32/40]. 1596.11 samples/sec. 160.390 ms/step.
Infer [40/40]. 1596.12 samples/sec. 160.389 ms/step.
Inference benchmark of regnetx_160.tv2_in1k done. 1595.78 samples/sec, 160.39 ms/step
Model regnetx_160.tv2_in1k created, param count: 54278536
Running train benchmark on regnetx_160.tv2_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 86.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 50.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 21.53 GiB is allocated by PyTorch, and 849.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model regnetx_160.tv2_in1k created, param count: 54278536
Running train benchmark on regnetx_160.tv2_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
Train [8/40]. 527.76 samples/sec. 363.804 ms/step.
Train [16/40]. 527.78 samples/sec. 363.786 ms/step.
Train [24/40]. 527.75 samples/sec. 363.807 ms/step.
Train [32/40]. 527.76 samples/sec. 363.800 ms/step.
Train [40/40]. 527.74 samples/sec. 363.817 ms/step.
Train benchmark of regnetx_160.tv2_in1k done. 526.10 samples/sec, 363.82 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model regnetx_320.pycls_in1k created, param count: 107811560
Running inference benchmark on regnetx_320.pycls_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 865.23 samples/sec. 295.874 ms/step.
Infer [16/40]. 865.29 samples/sec. 295.855 ms/step.
Infer [24/40]. 865.28 samples/sec. 295.857 ms/step.
Infer [32/40]. 865.21 samples/sec. 295.880 ms/step.
Infer [40/40]. 865.21 samples/sec. 295.883 ms/step.
Inference benchmark of regnetx_320.pycls_in1k done. 865.08 samples/sec, 295.88 ms/step
Model regnetx_320.pycls_in1k created, param count: 107811560
Running train benchmark on regnetx_320.pycls_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 258.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 160.06 MiB is free. Including non-PyTorch memory, this process has 23.48 GiB memory in use. Of the allocated memory 22.03 GiB is allocated by PyTorch, and 226.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model regnetx_320.pycls_in1k created, param count: 107811560
Running train benchmark on regnetx_320.pycls_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 76.06 MiB is free. Including non-PyTorch memory, this process has 23.57 GiB memory in use. Of the allocated memory 22.14 GiB is allocated by PyTorch, and 203.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model regnetx_320.pycls_in1k created, param count: 107811560
Running train benchmark on regnetx_320.pycls_in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
Train [8/40]. 286.18 samples/sec. 447.273 ms/step.
Train [16/40]. 286.15 samples/sec. 447.311 ms/step.
Train [24/40]. 286.16 samples/sec. 447.306 ms/step.
Train [32/40]. 286.15 samples/sec. 447.311 ms/step.
Train [40/40]. 286.16 samples/sec. 447.307 ms/step.
Train benchmark of regnetx_320.pycls_in1k done. 285.40 samples/sec, 447.31 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model regnetx_320.tv2_in1k created, param count: 107811560
Running inference benchmark on regnetx_320.tv2_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 812.34 samples/sec. 315.138 ms/step.
Infer [16/40]. 812.07 samples/sec. 315.245 ms/step.
Infer [24/40]. 811.98 samples/sec. 315.279 ms/step.
Infer [32/40]. 811.93 samples/sec. 315.297 ms/step.
Infer [40/40]. 811.91 samples/sec. 315.306 ms/step.
Inference benchmark of regnetx_320.tv2_in1k done. 811.79 samples/sec, 315.31 ms/step
Model regnetx_320.tv2_in1k created, param count: 107811560
Running train benchmark on regnetx_320.tv2_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 258.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 190.06 MiB is free. Including non-PyTorch memory, this process has 23.46 GiB memory in use. Of the allocated memory 22.05 GiB is allocated by PyTorch, and 176.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model regnetx_320.tv2_in1k created, param count: 107811560
Running train benchmark on regnetx_320.tv2_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 50.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 22.14 GiB is allocated by PyTorch, and 229.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model regnetx_320.tv2_in1k created, param count: 107811560
Running train benchmark on regnetx_320.tv2_in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
Train [8/40]. 286.14 samples/sec. 447.336 ms/step.
Train [16/40]. 286.13 samples/sec. 447.350 ms/step.
Train [24/40]. 286.13 samples/sec. 447.352 ms/step.
Train [32/40]. 286.12 samples/sec. 447.361 ms/step.
Train [40/40]. 286.12 samples/sec. 447.369 ms/step.
Train benchmark of regnetx_320.tv2_in1k done. 285.36 samples/sec, 447.37 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model regnety_002.pycls_in1k created, param count: 3162996
Running inference benchmark on regnety_002.pycls_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 17118.81 samples/sec. 14.954 ms/step.
Infer [16/40]. 17102.16 samples/sec. 14.969 ms/step.
Infer [24/40]. 17099.50 samples/sec. 14.971 ms/step.
Infer [32/40]. 17094.92 samples/sec. 14.975 ms/step.
Infer [40/40]. 17088.16 samples/sec. 14.981 ms/step.
Inference benchmark of regnety_002.pycls_in1k done. 17063.06 samples/sec, 14.98 ms/step
Model regnety_002.pycls_in1k created, param count: 3162996
Running train benchmark on regnety_002.pycls_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 4401.08 samples/sec. 58.167 ms/step.
Train [16/40]. 4400.09 samples/sec. 58.181 ms/step.
Train [24/40]. 4399.50 samples/sec. 58.188 ms/step.
Train [32/40]. 4398.97 samples/sec. 58.195 ms/step.
Train [40/40]. 4398.22 samples/sec. 58.205 ms/step.
Train benchmark of regnety_002.pycls_in1k done. 4348.69 samples/sec, 58.20 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model regnety_004.pycls_in1k created, param count: 4344144
Running inference benchmark on regnety_004.pycls_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 14155.98 samples/sec. 18.084 ms/step.
Infer [16/40]. 14141.06 samples/sec. 18.103 ms/step.
Infer [24/40]. 14142.84 samples/sec. 18.101 ms/step.
Infer [32/40]. 14142.88 samples/sec. 18.101 ms/step.
Infer [40/40]. 14141.08 samples/sec. 18.103 ms/step.
Inference benchmark of regnety_004.pycls_in1k done. 14123.61 samples/sec, 18.10 ms/step
Model regnety_004.pycls_in1k created, param count: 4344144
Running train benchmark on regnety_004.pycls_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 3250.94 samples/sec. 78.746 ms/step.
Train [16/40]. 3250.09 samples/sec. 78.767 ms/step.
Train [24/40]. 3249.07 samples/sec. 78.792 ms/step.
Train [32/40]. 3249.23 samples/sec. 78.788 ms/step.
Train [40/40]. 3248.32 samples/sec. 78.810 ms/step.
Train benchmark of regnety_004.pycls_in1k done. 3216.94 samples/sec, 78.81 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model regnety_004.tv2_in1k created, param count: 4344144
Running inference benchmark on regnety_004.tv2_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 14205.15 samples/sec. 18.022 ms/step.
Infer [16/40]. 14199.37 samples/sec. 18.029 ms/step.
Infer [24/40]. 14190.77 samples/sec. 18.040 ms/step.
Infer [32/40]. 14182.64 samples/sec. 18.050 ms/step.
Infer [40/40]. 14181.24 samples/sec. 18.052 ms/step.
Inference benchmark of regnety_004.tv2_in1k done. 14163.69 samples/sec, 18.05 ms/step
Model regnety_004.tv2_in1k created, param count: 4344144
Running train benchmark on regnety_004.tv2_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 3247.55 samples/sec. 78.829 ms/step.
Train [16/40]. 3247.18 samples/sec. 78.838 ms/step.
Train [24/40]. 3246.97 samples/sec. 78.843 ms/step.
Train [32/40]. 3247.13 samples/sec. 78.839 ms/step.
Train [40/40]. 3247.48 samples/sec. 78.830 ms/step.
Train benchmark of regnety_004.tv2_in1k done. 3216.21 samples/sec, 78.83 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model regnety_006.pycls_in1k created, param count: 6055160
Running inference benchmark on regnety_006.pycls_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 14727.73 samples/sec. 17.382 ms/step.
Infer [16/40]. 14723.09 samples/sec. 17.388 ms/step.
Infer [24/40]. 14721.96 samples/sec. 17.389 ms/step.
Infer [32/40]. 14718.48 samples/sec. 17.393 ms/step.
Infer [40/40]. 14719.45 samples/sec. 17.392 ms/step.
Inference benchmark of regnety_006.pycls_in1k done. 14699.60 samples/sec, 17.39 ms/step
Model regnety_006.pycls_in1k created, param count: 6055160
Running train benchmark on regnety_006.pycls_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 2985.22 samples/sec. 85.756 ms/step.
Train [16/40]. 2985.10 samples/sec. 85.759 ms/step.
Train [24/40]. 2985.27 samples/sec. 85.754 ms/step.
Train [32/40]. 2985.30 samples/sec. 85.754 ms/step.
Train [40/40]. 2985.25 samples/sec. 85.755 ms/step.
Train benchmark of regnety_006.pycls_in1k done. 2959.79 samples/sec, 85.75 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model regnety_008.pycls_in1k created, param count: 6263168
Running inference benchmark on regnety_008.pycls_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 11975.65 samples/sec. 21.377 ms/step.
Infer [16/40]. 11975.58 samples/sec. 21.377 ms/step.
Infer [24/40]. 11975.40 samples/sec. 21.377 ms/step.
Infer [32/40]. 11975.48 samples/sec. 21.377 ms/step.
Infer [40/40]. 11975.37 samples/sec. 21.377 ms/step.
Inference benchmark of regnety_008.pycls_in1k done. 11961.92 samples/sec, 21.38 ms/step
Model regnety_008.pycls_in1k created, param count: 6263168
Running train benchmark on regnety_008.pycls_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 2623.24 samples/sec. 97.589 ms/step.
Train [16/40]. 2623.29 samples/sec. 97.587 ms/step.
Train [24/40]. 2623.33 samples/sec. 97.586 ms/step.
Train [32/40]. 2623.29 samples/sec. 97.587 ms/step.
Train [40/40]. 2623.29 samples/sec. 97.587 ms/step.
Train benchmark of regnety_008.pycls_in1k done. 2604.26 samples/sec, 97.59 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model regnety_008_tv.tv2_in1k created, param count: 6432512
Running inference benchmark on regnety_008_tv.tv2_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 11355.75 samples/sec. 22.544 ms/step.
Infer [16/40]. 11352.09 samples/sec. 22.551 ms/step.
Infer [24/40]. 11353.09 samples/sec. 22.549 ms/step.
Infer [32/40]. 11353.25 samples/sec. 22.549 ms/step.
Infer [40/40]. 11352.62 samples/sec. 22.550 ms/step.
Inference benchmark of regnety_008_tv.tv2_in1k done. 11340.36 samples/sec, 22.55 ms/step
Model regnety_008_tv.tv2_in1k created, param count: 6432512
Running train benchmark on regnety_008_tv.tv2_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 2478.85 samples/sec. 103.274 ms/step.
Train [16/40]. 2478.40 samples/sec. 103.293 ms/step.
Train [24/40]. 2479.04 samples/sec. 103.266 ms/step.
Train [32/40]. 2479.21 samples/sec. 103.259 ms/step.
Train [40/40]. 2479.15 samples/sec. 103.261 ms/step.
Train benchmark of regnety_008_tv.tv2_in1k done. 2462.01 samples/sec, 103.26 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model regnety_016.pycls_in1k created, param count: 11202430
Running inference benchmark on regnety_016.pycls_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 7213.32 samples/sec. 35.490 ms/step.
Infer [16/40]. 7212.96 samples/sec. 35.492 ms/step.
Infer [24/40]. 7212.69 samples/sec. 35.493 ms/step.
Infer [32/40]. 7213.05 samples/sec. 35.491 ms/step.
Infer [40/40]. 7212.87 samples/sec. 35.492 ms/step.
Inference benchmark of regnety_016.pycls_in1k done. 7207.80 samples/sec, 35.49 ms/step
Model regnety_016.pycls_in1k created, param count: 11202430
Running train benchmark on regnety_016.pycls_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1552.16 samples/sec. 164.931 ms/step.
Train [16/40]. 1552.24 samples/sec. 164.923 ms/step.
Train [24/40]. 1552.24 samples/sec. 164.923 ms/step.
Train [32/40]. 1552.22 samples/sec. 164.925 ms/step.
Train [40/40]. 1552.20 samples/sec. 164.927 ms/step.
Train benchmark of regnety_016.pycls_in1k done. 1540.85 samples/sec, 164.93 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model regnety_016.tv2_in1k created, param count: 11202430
Running inference benchmark on regnety_016.tv2_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 7211.34 samples/sec. 35.500 ms/step.
Infer [16/40]. 7212.21 samples/sec. 35.495 ms/step.
Infer [24/40]. 7212.10 samples/sec. 35.496 ms/step.
Infer [32/40]. 7212.56 samples/sec. 35.494 ms/step.
Infer [40/40]. 7212.43 samples/sec. 35.494 ms/step.
Inference benchmark of regnety_016.tv2_in1k done. 7207.36 samples/sec, 35.49 ms/step
Model regnety_016.tv2_in1k created, param count: 11202430
Running train benchmark on regnety_016.tv2_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1551.77 samples/sec. 164.973 ms/step.
Train [16/40]. 1551.90 samples/sec. 164.959 ms/step.
Train [24/40]. 1551.88 samples/sec. 164.961 ms/step.
Train [32/40]. 1551.87 samples/sec. 164.963 ms/step.
Train [40/40]. 1551.88 samples/sec. 164.961 ms/step.
Train benchmark of regnety_016.tv2_in1k done. 1540.60 samples/sec, 164.96 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model regnety_032.pycls_in1k created, param count: 19436338
Running inference benchmark on regnety_032.pycls_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 4342.18 samples/sec. 58.957 ms/step.
Infer [16/40]. 4341.80 samples/sec. 58.962 ms/step.
Infer [24/40]. 4341.79 samples/sec. 58.962 ms/step.
Infer [32/40]. 4341.76 samples/sec. 58.962 ms/step.
Infer [40/40]. 4341.59 samples/sec. 58.965 ms/step.
Inference benchmark of regnety_032.pycls_in1k done. 4339.64 samples/sec, 58.97 ms/step
Model regnety_032.pycls_in1k created, param count: 19436338
Running train benchmark on regnety_032.pycls_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1072.28 samples/sec. 238.743 ms/step.
Train [16/40]. 1071.90 samples/sec. 238.828 ms/step.
Train [24/40]. 1072.11 samples/sec. 238.782 ms/step.
Train [32/40]. 1072.17 samples/sec. 238.767 ms/step.
Train [40/40]. 1072.14 samples/sec. 238.775 ms/step.
Train benchmark of regnety_032.pycls_in1k done. 1066.60 samples/sec, 238.78 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model regnety_032.ra_in1k created, param count: 19436338
Running inference benchmark on regnety_032.ra_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 2463.71 samples/sec. 103.908 ms/step.
Infer [16/40]. 2463.69 samples/sec. 103.909 ms/step.
Infer [24/40]. 2463.67 samples/sec. 103.910 ms/step.
Infer [32/40]. 2463.64 samples/sec. 103.911 ms/step.
Infer [40/40]. 2463.57 samples/sec. 103.914 ms/step.
Inference benchmark of regnety_032.ra_in1k done. 2462.92 samples/sec, 103.91 ms/step
Model regnety_032.ra_in1k created, param count: 19436338
Running train benchmark on regnety_032.ra_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 621.91 samples/sec. 411.633 ms/step.
Train [16/40]. 621.79 samples/sec. 411.712 ms/step.
Train [24/40]. 621.89 samples/sec. 411.652 ms/step.
Train [32/40]. 621.91 samples/sec. 411.633 ms/step.
Train [40/40]. 621.87 samples/sec. 411.659 ms/step.
Train benchmark of regnety_032.ra_in1k done. 619.82 samples/sec, 411.66 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model regnety_032.tv2_in1k created, param count: 19436338
Running inference benchmark on regnety_032.tv2_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 4341.33 samples/sec. 58.968 ms/step.
Infer [16/40]. 4341.06 samples/sec. 58.972 ms/step.
Infer [24/40]. 4340.87 samples/sec. 58.974 ms/step.
Infer [32/40]. 4340.92 samples/sec. 58.974 ms/step.
Infer [40/40]. 4340.83 samples/sec. 58.975 ms/step.
Inference benchmark of regnety_032.tv2_in1k done. 4339.00 samples/sec, 58.98 ms/step
Model regnety_032.tv2_in1k created, param count: 19436338
Running train benchmark on regnety_032.tv2_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1071.06 samples/sec. 239.017 ms/step.
Train [16/40]. 1071.39 samples/sec. 238.941 ms/step.
Train [24/40]. 1071.52 samples/sec. 238.914 ms/step.
Train [32/40]. 1071.60 samples/sec. 238.896 ms/step.
Train [40/40]. 1071.51 samples/sec. 238.916 ms/step.
Train benchmark of regnety_032.tv2_in1k done. 1066.14 samples/sec, 238.92 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model regnety_040.pycls_in1k created, param count: 20646656
Running inference benchmark on regnety_040.pycls_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 4022.24 samples/sec. 63.646 ms/step.
Infer [16/40]. 4022.29 samples/sec. 63.645 ms/step.
Infer [24/40]. 4022.60 samples/sec. 63.640 ms/step.
Infer [32/40]. 4022.59 samples/sec. 63.641 ms/step.
Infer [40/40]. 4022.53 samples/sec. 63.641 ms/step.
Inference benchmark of regnety_040.pycls_in1k done. 4020.93 samples/sec, 63.64 ms/step
Model regnety_040.pycls_in1k created, param count: 20646656
Running train benchmark on regnety_040.pycls_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1085.32 samples/sec. 235.876 ms/step.
Train [16/40]. 1085.26 samples/sec. 235.889 ms/step.
Train [24/40]. 1085.25 samples/sec. 235.890 ms/step.
Train [32/40]. 1085.23 samples/sec. 235.895 ms/step.
Train [40/40]. 1085.25 samples/sec. 235.890 ms/step.
Train benchmark of regnety_040.pycls_in1k done. 1079.58 samples/sec, 235.89 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model regnety_040.ra3_in1k created, param count: 20646656
Running inference benchmark on regnety_040.ra3_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 2285.23 samples/sec. 112.024 ms/step.
Infer [16/40]. 2285.62 samples/sec. 112.005 ms/step.
Infer [24/40]. 2285.75 samples/sec. 111.998 ms/step.
Infer [32/40]. 2286.07 samples/sec. 111.983 ms/step.
Infer [40/40]. 2286.14 samples/sec. 111.979 ms/step.
Inference benchmark of regnety_040.ra3_in1k done. 2285.56 samples/sec, 111.98 ms/step
Model regnety_040.ra3_in1k created, param count: 20646656
Running train benchmark on regnety_040.ra3_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 633.53 samples/sec. 404.084 ms/step.
Train [16/40]. 633.51 samples/sec. 404.097 ms/step.
Train [24/40]. 633.49 samples/sec. 404.109 ms/step.
Train [32/40]. 633.35 samples/sec. 404.202 ms/step.
Train [40/40]. 633.23 samples/sec. 404.273 ms/step.
Train benchmark of regnety_040.ra3_in1k done. 631.07 samples/sec, 404.27 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model regnety_064.pycls_in1k created, param count: 30583252
Running inference benchmark on regnety_064.pycls_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 2566.39 samples/sec. 99.751 ms/step.
Infer [16/40]. 2564.76 samples/sec. 99.814 ms/step.
Infer [24/40]. 2564.20 samples/sec. 99.836 ms/step.
Infer [32/40]. 2564.00 samples/sec. 99.844 ms/step.
Infer [40/40]. 2563.86 samples/sec. 99.850 ms/step.
Inference benchmark of regnety_064.pycls_in1k done. 2563.19 samples/sec, 99.85 ms/step
Model regnety_064.pycls_in1k created, param count: 30583252
Running train benchmark on regnety_064.pycls_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 737.04 samples/sec. 347.336 ms/step.
Train [16/40]. 737.00 samples/sec. 347.354 ms/step.
Train [24/40]. 736.96 samples/sec. 347.374 ms/step.
Train [32/40]. 736.93 samples/sec. 347.388 ms/step.
Train [40/40]. 736.95 samples/sec. 347.380 ms/step.
Train benchmark of regnety_064.pycls_in1k done. 733.78 samples/sec, 347.38 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model regnety_064.ra3_in1k created, param count: 30583252
Running inference benchmark on regnety_064.ra3_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 1454.29 samples/sec. 176.031 ms/step.
Infer [16/40]. 1454.37 samples/sec. 176.021 ms/step.
Infer [24/40]. 1454.43 samples/sec. 176.014 ms/step.
Infer [32/40]. 1454.46 samples/sec. 176.011 ms/step.
Infer [40/40]. 1454.41 samples/sec. 176.017 ms/step.
Inference benchmark of regnety_064.ra3_in1k done. 1454.13 samples/sec, 176.02 ms/step
Model regnety_064.ra3_in1k created, param count: 30583252
Running train benchmark on regnety_064.ra3_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 92.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 40.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 22.21 GiB is allocated by PyTorch, and 164.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model regnety_064.ra3_in1k created, param count: 30583252
Running train benchmark on regnety_064.ra3_in1k for 40 steps w/ input size (3, 288, 288) and batch size 192.
Train [8/40]. 428.80 samples/sec. 447.756 ms/step.
Train [16/40]. 428.82 samples/sec. 447.745 ms/step.
Train [24/40]. 428.83 samples/sec. 447.728 ms/step.
Train [32/40]. 428.88 samples/sec. 447.682 ms/step.
Train [40/40]. 428.88 samples/sec. 447.683 ms/step.
Train benchmark of regnety_064.ra3_in1k done. 427.35 samples/sec, 447.68 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model regnety_080.pycls_in1k created, param count: 39180068
Running inference benchmark on regnety_080.pycls_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 2473.26 samples/sec. 103.507 ms/step.
Infer [16/40]. 2472.76 samples/sec. 103.528 ms/step.
Infer [24/40]. 2473.00 samples/sec. 103.518 ms/step.
Infer [32/40]. 2472.88 samples/sec. 103.523 ms/step.
Infer [40/40]. 2472.70 samples/sec. 103.531 ms/step.
Inference benchmark of regnety_080.pycls_in1k done. 2472.03 samples/sec, 103.53 ms/step
Model regnety_080.pycls_in1k created, param count: 39180068
Running train benchmark on regnety_080.pycls_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 700.00 samples/sec. 365.715 ms/step.
Train [16/40]. 700.06 samples/sec. 365.686 ms/step.
Train [24/40]. 700.05 samples/sec. 365.689 ms/step.
Train [32/40]. 700.06 samples/sec. 365.681 ms/step.
Train [40/40]. 700.07 samples/sec. 365.678 ms/step.
Train benchmark of regnety_080.pycls_in1k done. 697.74 samples/sec, 365.68 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model regnety_080.ra3_in1k created, param count: 39180068
Running inference benchmark on regnety_080.ra3_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 1370.02 samples/sec. 186.858 ms/step.
Infer [16/40]. 1369.94 samples/sec. 186.870 ms/step.
Infer [24/40]. 1369.96 samples/sec. 186.866 ms/step.
Infer [32/40]. 1369.97 samples/sec. 186.866 ms/step.
Infer [40/40]. 1369.92 samples/sec. 186.872 ms/step.
Inference benchmark of regnety_080.ra3_in1k done. 1369.64 samples/sec, 186.87 ms/step
Model regnety_080.ra3_in1k created, param count: 39180068
Running train benchmark on regnety_080.ra3_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 142.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 36.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 22.33 GiB is allocated by PyTorch, and 46.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model regnety_080.ra3_in1k created, param count: 39180068
Running train benchmark on regnety_080.ra3_in1k for 40 steps w/ input size (3, 288, 288) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 108.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 34.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 22.16 GiB is allocated by PyTorch, and 217.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model regnety_080.ra3_in1k created, param count: 39180068
Running train benchmark on regnety_080.ra3_in1k for 40 steps w/ input size (3, 288, 288) and batch size 128.
Train [8/40]. 419.42 samples/sec. 305.183 ms/step.
Train [16/40]. 419.40 samples/sec. 305.198 ms/step.
Train [24/40]. 419.41 samples/sec. 305.192 ms/step.
Train [32/40]. 419.41 samples/sec. 305.194 ms/step.
Train [40/40]. 419.40 samples/sec. 305.194 ms/step.
Train benchmark of regnety_080.ra3_in1k done. 417.79 samples/sec, 305.19 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model regnety_080_tv.tv2_in1k created, param count: 39381472
Running inference benchmark on regnety_080_tv.tv2_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 2290.44 samples/sec. 111.769 ms/step.
Infer [16/40]. 2290.14 samples/sec. 111.784 ms/step.
Infer [24/40]. 2289.86 samples/sec. 111.797 ms/step.
Infer [32/40]. 2289.71 samples/sec. 111.805 ms/step.
Infer [40/40]. 2289.65 samples/sec. 111.807 ms/step.
Inference benchmark of regnety_080_tv.tv2_in1k done. 2289.06 samples/sec, 111.81 ms/step
Model regnety_080_tv.tv2_in1k created, param count: 39381472
Running train benchmark on regnety_080_tv.tv2_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 652.35 samples/sec. 392.426 ms/step.
Train [16/40]. 652.37 samples/sec. 392.414 ms/step.
Train [24/40]. 652.40 samples/sec. 392.398 ms/step.
Train [32/40]. 652.41 samples/sec. 392.394 ms/step.
Train [40/40]. 652.40 samples/sec. 392.398 ms/step.
Train benchmark of regnety_080_tv.tv2_in1k done. 650.38 samples/sec, 392.40 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model regnety_120.pycls_in1k created, param count: 51822544
Running inference benchmark on regnety_120.pycls_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1888.17 samples/sec. 135.581 ms/step.
Infer [16/40]. 1887.55 samples/sec. 135.625 ms/step.
Infer [24/40]. 1887.32 samples/sec. 135.642 ms/step.
Infer [32/40]. 1887.20 samples/sec. 135.651 ms/step.
Infer [40/40]. 1887.08 samples/sec. 135.659 ms/step.
Inference benchmark of regnety_120.pycls_in1k done. 1886.67 samples/sec, 135.66 ms/step
Model regnety_120.pycls_in1k created, param count: 51822544
Running train benchmark on regnety_120.pycls_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 86.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 68.06 MiB is free. Including non-PyTorch memory, this process has 23.57 GiB memory in use. Of the allocated memory 21.12 GiB is allocated by PyTorch, and 1.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model regnety_120.pycls_in1k created, param count: 51822544
Running train benchmark on regnety_120.pycls_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
Train [8/40]. 559.39 samples/sec. 343.228 ms/step.
Train [16/40]. 559.41 samples/sec. 343.221 ms/step.
Train [24/40]. 559.41 samples/sec. 343.219 ms/step.
Train [32/40]. 559.41 samples/sec. 343.219 ms/step.
Train [40/40]. 559.41 samples/sec. 343.221 ms/step.
Train benchmark of regnety_120.pycls_in1k done. 557.32 samples/sec, 343.22 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model regnety_120.sw_in12k created, param count: 76072405
Running inference benchmark on regnety_120.sw_in12k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 1059.22 samples/sec. 241.688 ms/step.
Infer [16/40]. 1059.18 samples/sec. 241.697 ms/step.
Infer [24/40]. 1059.14 samples/sec. 241.705 ms/step.
Infer [32/40]. 1059.15 samples/sec. 241.703 ms/step.
Infer [40/40]. 1059.15 samples/sec. 241.703 ms/step.
Inference benchmark of regnety_120.sw_in12k done. 1058.98 samples/sec, 241.70 ms/step
Model regnety_120.sw_in12k created, param count: 76072405
Running train benchmark on regnety_120.sw_in12k for 40 steps w/ input size (3, 288, 288) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 284.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 232.06 MiB is free. Including non-PyTorch memory, this process has 23.41 GiB memory in use. Of the allocated memory 21.69 GiB is allocated by PyTorch, and 504.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model regnety_120.sw_in12k created, param count: 76072405
Running train benchmark on regnety_120.sw_in12k for 40 steps w/ input size (3, 288, 288) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 108.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 66.06 MiB is free. Including non-PyTorch memory, this process has 23.58 GiB memory in use. Of the allocated memory 21.71 GiB is allocated by PyTorch, and 645.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model regnety_120.sw_in12k created, param count: 76072405
Running train benchmark on regnety_120.sw_in12k for 40 steps w/ input size (3, 288, 288) and batch size 128.
Train [8/40]. 330.96 samples/sec. 386.754 ms/step.
Train [16/40]. 330.96 samples/sec. 386.754 ms/step.
Train [24/40]. 330.96 samples/sec. 386.753 ms/step.
Train [32/40]. 330.96 samples/sec. 386.755 ms/step.
Train [40/40]. 330.96 samples/sec. 386.753 ms/step.
Train benchmark of regnety_120.sw_in12k done. 329.87 samples/sec, 386.75 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model regnety_120.sw_in12k_ft_in1k created, param count: 51822544
Running inference benchmark on regnety_120.sw_in12k_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 1059.47 samples/sec. 241.631 ms/step.
Infer [16/40]. 1059.44 samples/sec. 241.637 ms/step.
Infer [24/40]. 1059.42 samples/sec. 241.641 ms/step.
Infer [32/40]. 1059.42 samples/sec. 241.641 ms/step.
Infer [40/40]. 1059.42 samples/sec. 241.642 ms/step.
Inference benchmark of regnety_120.sw_in12k_ft_in1k done. 1059.24 samples/sec, 241.64 ms/step
Model regnety_120.sw_in12k_ft_in1k created, param count: 51822544
Running train benchmark on regnety_120.sw_in12k_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 284.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 232.06 MiB is free. Including non-PyTorch memory, this process has 23.41 GiB memory in use. Of the allocated memory 21.92 GiB is allocated by PyTorch, and 266.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model regnety_120.sw_in12k_ft_in1k created, param count: 51822544
Running train benchmark on regnety_120.sw_in12k_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 108.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 76.06 MiB is free. Including non-PyTorch memory, this process has 23.57 GiB memory in use. Of the allocated memory 21.77 GiB is allocated by PyTorch, and 576.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model regnety_120.sw_in12k_ft_in1k created, param count: 51822544
Running train benchmark on regnety_120.sw_in12k_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 128.
Train [8/40]. 331.74 samples/sec. 385.850 ms/step.
Train [16/40]. 331.74 samples/sec. 385.842 ms/step.
Train [24/40]. 331.74 samples/sec. 385.843 ms/step.
Train [32/40]. 331.74 samples/sec. 385.843 ms/step.
Train [40/40]. 331.74 samples/sec. 385.842 ms/step.
Train benchmark of regnety_120.sw_in12k_ft_in1k done. 330.62 samples/sec, 385.84 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model regnety_160.deit_in1k created, param count: 83590140
Running inference benchmark on regnety_160.deit_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 939.55 samples/sec. 272.472 ms/step.
Infer [16/40]. 939.53 samples/sec. 272.476 ms/step.
Infer [24/40]. 939.56 samples/sec. 272.467 ms/step.
Infer [32/40]. 939.58 samples/sec. 272.463 ms/step.
Infer [40/40]. 939.60 samples/sec. 272.456 ms/step.
Inference benchmark of regnety_160.deit_in1k done. 939.45 samples/sec, 272.46 ms/step
Model regnety_160.deit_in1k created, param count: 83590140
Running train benchmark on regnety_160.deit_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 284.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 228.06 MiB is free. Including non-PyTorch memory, this process has 23.42 GiB memory in use. Of the allocated memory 21.70 GiB is allocated by PyTorch, and 494.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model regnety_160.deit_in1k created, param count: 83590140
Running train benchmark on regnety_160.deit_in1k for 40 steps w/ input size (3, 288, 288) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 148.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 106.06 MiB is free. Including non-PyTorch memory, this process has 23.54 GiB memory in use. Of the allocated memory 21.60 GiB is allocated by PyTorch, and 726.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model regnety_160.deit_in1k created, param count: 83590140
Running train benchmark on regnety_160.deit_in1k for 40 steps w/ input size (3, 288, 288) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 240.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 86.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 20.22 GiB is allocated by PyTorch, and 2.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model regnety_160.deit_in1k created, param count: 83590140
Running train benchmark on regnety_160.deit_in1k for 40 steps w/ input size (3, 288, 288) and batch size 96.
Train [8/40]. 287.44 samples/sec. 333.985 ms/step.
Train [16/40]. 287.39 samples/sec. 334.045 ms/step.
Train [24/40]. 287.39 samples/sec. 334.046 ms/step.
Train [32/40]. 287.41 samples/sec. 334.022 ms/step.
Train [40/40]. 287.38 samples/sec. 334.048 ms/step.
Train benchmark of regnety_160.deit_in1k done. 286.32 samples/sec, 334.05 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model regnety_160.lion_in12k_ft_in1k created, param count: 83590140
Running inference benchmark on regnety_160.lion_in12k_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 940.35 samples/sec. 272.239 ms/step.
Infer [16/40]. 940.23 samples/sec. 272.274 ms/step.
Infer [24/40]. 940.01 samples/sec. 272.339 ms/step.
Infer [32/40]. 939.94 samples/sec. 272.358 ms/step.
Infer [40/40]. 939.90 samples/sec. 272.371 ms/step.
Inference benchmark of regnety_160.lion_in12k_ft_in1k done. 939.74 samples/sec, 272.37 ms/step
Model regnety_160.lion_in12k_ft_in1k created, param count: 83590140
Running train benchmark on regnety_160.lion_in12k_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 284.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 228.06 MiB is free. Including non-PyTorch memory, this process has 23.42 GiB memory in use. Of the allocated memory 21.70 GiB is allocated by PyTorch, and 494.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model regnety_160.lion_in12k_ft_in1k created, param count: 83590140
Running train benchmark on regnety_160.lion_in12k_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 148.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 110.06 MiB is free. Including non-PyTorch memory, this process has 23.53 GiB memory in use. Of the allocated memory 21.45 GiB is allocated by PyTorch, and 868.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model regnety_160.lion_in12k_ft_in1k created, param count: 83590140
Running train benchmark on regnety_160.lion_in12k_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 128.
Train [8/40]. 279.50 samples/sec. 457.956 ms/step.
Train [16/40]. 279.50 samples/sec. 457.966 ms/step.
Train [24/40]. 279.46 samples/sec. 458.026 ms/step.
Train [32/40]. 279.41 samples/sec. 458.108 ms/step.
Train [40/40]. 279.38 samples/sec. 458.150 ms/step.
Train benchmark of regnety_160.lion_in12k_ft_in1k done. 278.61 samples/sec, 458.15 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model regnety_160.pycls_in1k created, param count: 83590140
Running inference benchmark on regnety_160.pycls_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1619.48 samples/sec. 158.075 ms/step.
Infer [16/40]. 1619.38 samples/sec. 158.085 ms/step.
Infer [24/40]. 1619.43 samples/sec. 158.081 ms/step.
Infer [32/40]. 1619.44 samples/sec. 158.079 ms/step.
Infer [40/40]. 1619.44 samples/sec. 158.079 ms/step.
Inference benchmark of regnety_160.pycls_in1k done. 1619.10 samples/sec, 158.08 ms/step
Model regnety_160.pycls_in1k created, param count: 83590140
Running train benchmark on regnety_160.pycls_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 118.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 82.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 21.50 GiB is allocated by PyTorch, and 850.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model regnety_160.pycls_in1k created, param count: 83590140
Running train benchmark on regnety_160.pycls_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
Train [8/40]. 484.85 samples/sec. 396.002 ms/step.
Train [16/40]. 484.81 samples/sec. 396.034 ms/step.
Train [24/40]. 484.77 samples/sec. 396.061 ms/step.
Train [32/40]. 484.73 samples/sec. 396.101 ms/step.
Train [40/40]. 484.72 samples/sec. 396.103 ms/step.
Train benchmark of regnety_160.pycls_in1k done. 483.15 samples/sec, 396.10 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model regnety_160.sw_in12k created, param count: 116323665
Running inference benchmark on regnety_160.sw_in12k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 939.10 samples/sec. 272.601 ms/step.
Infer [16/40]. 939.05 samples/sec. 272.616 ms/step.
Infer [24/40]. 939.05 samples/sec. 272.617 ms/step.
Infer [32/40]. 939.05 samples/sec. 272.616 ms/step.
Infer [40/40]. 939.04 samples/sec. 272.619 ms/step.
Inference benchmark of regnety_160.sw_in12k done. 938.89 samples/sec, 272.62 ms/step
Model regnety_160.sw_in12k created, param count: 116323665
Running train benchmark on regnety_160.sw_in12k for 40 steps w/ input size (3, 288, 288) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 284.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 228.06 MiB is free. Including non-PyTorch memory, this process has 23.42 GiB memory in use. Of the allocated memory 21.76 GiB is allocated by PyTorch, and 431.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model regnety_160.sw_in12k created, param count: 116323665
Running train benchmark on regnety_160.sw_in12k for 40 steps w/ input size (3, 288, 288) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 148.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 110.06 MiB is free. Including non-PyTorch memory, this process has 23.53 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 806.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model regnety_160.sw_in12k created, param count: 116323665
Running train benchmark on regnety_160.sw_in12k for 40 steps w/ input size (3, 288, 288) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 240.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 96.06 MiB is free. Including non-PyTorch memory, this process has 23.55 GiB memory in use. Of the allocated memory 20.40 GiB is allocated by PyTorch, and 1.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model regnety_160.sw_in12k created, param count: 116323665
Running train benchmark on regnety_160.sw_in12k for 40 steps w/ input size (3, 288, 288) and batch size 96.
Train [8/40]. 286.19 samples/sec. 335.441 ms/step.
Train [16/40]. 286.22 samples/sec. 335.405 ms/step.
Train [24/40]. 286.24 samples/sec. 335.381 ms/step.
Train [32/40]. 286.23 samples/sec. 335.391 ms/step.
Train [40/40]. 286.23 samples/sec. 335.395 ms/step.
Train benchmark of regnety_160.sw_in12k done. 285.14 samples/sec, 335.39 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model regnety_160.sw_in12k_ft_in1k created, param count: 83590140
Running inference benchmark on regnety_160.sw_in12k_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 939.75 samples/sec. 272.414 ms/step.
Infer [16/40]. 939.74 samples/sec. 272.416 ms/step.
Infer [24/40]. 939.70 samples/sec. 272.426 ms/step.
Infer [32/40]. 939.71 samples/sec. 272.424 ms/step.
Infer [40/40]. 939.72 samples/sec. 272.421 ms/step.
Inference benchmark of regnety_160.sw_in12k_ft_in1k done. 939.57 samples/sec, 272.42 ms/step
Model regnety_160.sw_in12k_ft_in1k created, param count: 83590140
Running train benchmark on regnety_160.sw_in12k_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 284.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 228.06 MiB is free. Including non-PyTorch memory, this process has 23.42 GiB memory in use. Of the allocated memory 21.70 GiB is allocated by PyTorch, and 494.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model regnety_160.sw_in12k_ft_in1k created, param count: 83590140
Running train benchmark on regnety_160.sw_in12k_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 148.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 110.06 MiB is free. Including non-PyTorch memory, this process has 23.53 GiB memory in use. Of the allocated memory 21.45 GiB is allocated by PyTorch, and 868.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model regnety_160.sw_in12k_ft_in1k created, param count: 83590140
Running train benchmark on regnety_160.sw_in12k_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 128.
Train [8/40]. 279.21 samples/sec. 458.443 ms/step.
Train [16/40]. 279.22 samples/sec. 458.420 ms/step.
Train [24/40]. 279.22 samples/sec. 458.424 ms/step.
Train [32/40]. 279.21 samples/sec. 458.440 ms/step.
Train [40/40]. 279.21 samples/sec. 458.440 ms/step.
Train benchmark of regnety_160.sw_in12k_ft_in1k done. 278.43 samples/sec, 458.44 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model regnety_160.swag_ft_in1k created, param count: 83590140
Running inference benchmark on regnety_160.swag_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 580.54 samples/sec. 440.967 ms/step.
Infer [16/40]. 580.52 samples/sec. 440.983 ms/step.
Infer [24/40]. 580.51 samples/sec. 440.989 ms/step.
Infer [32/40]. 580.52 samples/sec. 440.987 ms/step.
Infer [40/40]. 580.52 samples/sec. 440.984 ms/step.
Inference benchmark of regnety_160.swag_ft_in1k done. 580.46 samples/sec, 440.98 ms/step
Model regnety_160.swag_ft_in1k created, param count: 83590140
Running train benchmark on regnety_160.swag_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 1008.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 766.06 MiB is free. Including non-PyTorch memory, this process has 22.89 GiB memory in use. Of the allocated memory 21.20 GiB is allocated by PyTorch, and 474.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model regnety_160.swag_ft_in1k created, param count: 83590140
Running train benchmark on regnety_160.swag_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 378.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 30.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 21.85 GiB is allocated by PyTorch, and 545.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model regnety_160.swag_ft_in1k created, param count: 83590140
Running train benchmark on regnety_160.swag_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 174.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 118.06 MiB is free. Including non-PyTorch memory, this process has 23.53 GiB memory in use. Of the allocated memory 21.82 GiB is allocated by PyTorch, and 480.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model regnety_160.swag_ft_in1k created, param count: 83590140
Running train benchmark on regnety_160.swag_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 130.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 78.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 21.63 GiB is allocated by PyTorch, and 718.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model regnety_160.swag_ft_in1k created, param count: 83590140
Running train benchmark on regnety_160.swag_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 64.
Train [8/40]. 169.30 samples/sec. 378.025 ms/step.
Train [16/40]. 169.29 samples/sec. 378.059 ms/step.
Train [24/40]. 169.27 samples/sec. 378.094 ms/step.
Train [32/40]. 169.27 samples/sec. 378.097 ms/step.
Train [40/40]. 169.27 samples/sec. 378.086 ms/step.
Train benchmark of regnety_160.swag_ft_in1k done. 168.71 samples/sec, 378.09 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model regnety_160.swag_lc_in1k created, param count: 83590140
Running inference benchmark on regnety_160.swag_lc_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1619.83 samples/sec. 158.041 ms/step.
Infer [16/40]. 1619.85 samples/sec. 158.040 ms/step.
Infer [24/40]. 1619.89 samples/sec. 158.036 ms/step.
Infer [32/40]. 1619.88 samples/sec. 158.036 ms/step.
Infer [40/40]. 1619.86 samples/sec. 158.038 ms/step.
Inference benchmark of regnety_160.swag_lc_in1k done. 1619.52 samples/sec, 158.04 ms/step
Model regnety_160.swag_lc_in1k created, param count: 83590140
Running train benchmark on regnety_160.swag_lc_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 118.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 104.06 MiB is free. Including non-PyTorch memory, this process has 23.54 GiB memory in use. Of the allocated memory 21.61 GiB is allocated by PyTorch, and 708.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model regnety_160.swag_lc_in1k created, param count: 83590140
Running train benchmark on regnety_160.swag_lc_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
Train [8/40]. 485.02 samples/sec. 395.862 ms/step.
Train [16/40]. 485.03 samples/sec. 395.855 ms/step.
Train [24/40]. 485.05 samples/sec. 395.836 ms/step.
Train [32/40]. 485.03 samples/sec. 395.854 ms/step.
Train [40/40]. 485.02 samples/sec. 395.857 ms/step.
Train benchmark of regnety_160.swag_lc_in1k done. 483.43 samples/sec, 395.86 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model regnety_160.tv2_in1k created, param count: 83590140
Running inference benchmark on regnety_160.tv2_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1619.96 samples/sec. 158.028 ms/step.
Infer [16/40]. 1619.82 samples/sec. 158.042 ms/step.
Infer [24/40]. 1619.74 samples/sec. 158.050 ms/step.
Infer [32/40]. 1619.72 samples/sec. 158.052 ms/step.
Infer [40/40]. 1619.67 samples/sec. 158.057 ms/step.
Inference benchmark of regnety_160.tv2_in1k done. 1619.33 samples/sec, 158.06 ms/step
Model regnety_160.tv2_in1k created, param count: 83590140
Running train benchmark on regnety_160.tv2_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 118.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 104.06 MiB is free. Including non-PyTorch memory, this process has 23.54 GiB memory in use. Of the allocated memory 21.61 GiB is allocated by PyTorch, and 708.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model regnety_160.tv2_in1k created, param count: 83590140
Running train benchmark on regnety_160.tv2_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
Train [8/40]. 484.99 samples/sec. 395.886 ms/step.
Train [16/40]. 485.01 samples/sec. 395.867 ms/step.
Train [24/40]. 485.02 samples/sec. 395.861 ms/step.
Train [32/40]. 485.04 samples/sec. 395.843 ms/step.
Train [40/40]. 485.03 samples/sec. 395.852 ms/step.
Train benchmark of regnety_160.tv2_in1k done. 483.43 samples/sec, 395.85 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model regnety_320.pycls_in1k created, param count: 145046770
Running inference benchmark on regnety_320.pycls_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 973.28 samples/sec. 263.029 ms/step.
Infer [16/40]. 973.27 samples/sec. 263.031 ms/step.
Infer [24/40]. 973.27 samples/sec. 263.029 ms/step.
Infer [32/40]. 973.23 samples/sec. 263.041 ms/step.
Infer [40/40]. 973.21 samples/sec. 263.047 ms/step.
Inference benchmark of regnety_320.pycls_in1k done. 973.05 samples/sec, 263.05 ms/step
Model regnety_320.pycls_in1k created, param count: 145046770
Running train benchmark on regnety_320.pycls_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 102.06 MiB is free. Including non-PyTorch memory, this process has 23.54 GiB memory in use. Of the allocated memory 21.96 GiB is allocated by PyTorch, and 351.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model regnety_320.pycls_in1k created, param count: 145046770
Running train benchmark on regnety_320.pycls_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 58.06 MiB is free. Including non-PyTorch memory, this process has 23.58 GiB memory in use. Of the allocated memory 22.02 GiB is allocated by PyTorch, and 343.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model regnety_320.pycls_in1k created, param count: 145046770
Running train benchmark on regnety_320.pycls_in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
Train [8/40]. 312.89 samples/sec. 409.092 ms/step.
Train [16/40]. 312.90 samples/sec. 409.081 ms/step.
Train [24/40]. 312.90 samples/sec. 409.073 ms/step.
Train [32/40]. 312.90 samples/sec. 409.077 ms/step.
Train [40/40]. 312.90 samples/sec. 409.075 ms/step.
Train benchmark of regnety_320.pycls_in1k done. 311.85 samples/sec, 409.07 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model regnety_320.seer created, param count: 141333770
Running inference benchmark on regnety_320.seer for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 872.33 samples/sec. 293.466 ms/step.
Infer [16/40]. 872.02 samples/sec. 293.572 ms/step.
Infer [24/40]. 872.46 samples/sec. 293.424 ms/step.
Infer [32/40]. 872.74 samples/sec. 293.328 ms/step.
Infer [40/40]. 872.24 samples/sec. 293.496 ms/step.
Inference benchmark of regnety_320.seer done. 872.10 samples/sec, 293.50 ms/step
Model regnety_320.seer created, param count: 141333770
Running train benchmark on regnety_320.seer for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 102.06 MiB is free. Including non-PyTorch memory, this process has 23.54 GiB memory in use. Of the allocated memory 21.96 GiB is allocated by PyTorch, and 358.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model regnety_320.seer created, param count: 141333770
Running train benchmark on regnety_320.seer for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 10.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 21.82 GiB is allocated by PyTorch, and 594.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model regnety_320.seer created, param count: 141333770
Running train benchmark on regnety_320.seer for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model regnety_320.seer created, param count: 141333770
Running train benchmark on regnety_320.seer for 40 steps w/ input size (3, 224, 224) and batch size 96.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model regnety_320.seer created, param count: 141333770
Running train benchmark on regnety_320.seer for 40 steps w/ input size (3, 224, 224) and batch size 64.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model regnety_320.seer created, param count: 141333770
Running train benchmark on regnety_320.seer for 40 steps w/ input size (3, 224, 224) and batch size 48.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model regnety_320.seer created, param count: 141333770
Running train benchmark on regnety_320.seer for 40 steps w/ input size (3, 224, 224) and batch size 32.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model regnety_320.seer created, param count: 141333770
Running train benchmark on regnety_320.seer for 40 steps w/ input size (3, 224, 224) and batch size 24.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model regnety_320.seer created, param count: 141333770
Running train benchmark on regnety_320.seer for 40 steps w/ input size (3, 224, 224) and batch size 16.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 12 for retry.
Model regnety_320.seer created, param count: 141333770
Running train benchmark on regnety_320.seer for 40 steps w/ input size (3, 224, 224) and batch size 12.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 8 for retry.
Model regnety_320.seer created, param count: 141333770
Running train benchmark on regnety_320.seer for 40 steps w/ input size (3, 224, 224) and batch size 8.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 6 for retry.
Model regnety_320.seer created, param count: 141333770
Running train benchmark on regnety_320.seer for 40 steps w/ input size (3, 224, 224) and batch size 6.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 4 for retry.
Model regnety_320.seer created, param count: 141333770
Running train benchmark on regnety_320.seer for 40 steps w/ input size (3, 224, 224) and batch size 4.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 3 for retry.
Model regnety_320.seer created, param count: 141333770
Running train benchmark on regnety_320.seer for 40 steps w/ input size (3, 224, 224) and batch size 3.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 2 for retry.
Model regnety_320.seer created, param count: 141333770
Running train benchmark on regnety_320.seer for 40 steps w/ input size (3, 224, 224) and batch size 2.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 1 for retry.
Model regnety_320.seer created, param count: 141333770
Running train benchmark on regnety_320.seer for 40 steps w/ input size (3, 224, 224) and batch size 1.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 0 for retry.
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model regnety_320.seer_ft_in1k created, param count: 145046770
Running inference benchmark on regnety_320.seer_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 361.37 samples/sec. 708.411 ms/step.
Infer [16/40]. 361.39 samples/sec. 708.375 ms/step.
Infer [24/40]. 361.38 samples/sec. 708.391 ms/step.
Infer [32/40]. 361.35 samples/sec. 708.451 ms/step.
Infer [40/40]. 361.33 samples/sec. 708.500 ms/step.
Inference benchmark of regnety_320.seer_ft_in1k done. 361.30 samples/sec, 708.50 ms/step
Model regnety_320.seer_ft_in1k created, param count: 145046770
Running train benchmark on regnety_320.seer_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 1.02 GiB. GPU 0 has a total capacty of 23.65 GiB of which 140.06 MiB is free. Including non-PyTorch memory, this process has 23.50 GiB memory in use. Of the allocated memory 20.99 GiB is allocated by PyTorch, and 1.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model regnety_320.seer_ft_in1k created, param count: 145046770
Running train benchmark on regnety_320.seer_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 588.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 42.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 21.94 GiB is allocated by PyTorch, and 434.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model regnety_320.seer_ft_in1k created, param count: 145046770
Running train benchmark on regnety_320.seer_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 392.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 90.06 MiB is free. Including non-PyTorch memory, this process has 23.55 GiB memory in use. Of the allocated memory 21.99 GiB is allocated by PyTorch, and 340.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model regnety_320.seer_ft_in1k created, param count: 145046770
Running train benchmark on regnety_320.seer_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 588.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 554.06 MiB is free. Including non-PyTorch memory, this process has 23.10 GiB memory in use. Of the allocated memory 21.45 GiB is allocated by PyTorch, and 431.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model regnety_320.seer_ft_in1k created, param count: 145046770
Running train benchmark on regnety_320.seer_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 2.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 21.86 GiB is allocated by PyTorch, and 558.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model regnety_320.seer_ft_in1k created, param count: 145046770
Running train benchmark on regnety_320.seer_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 48.
Train [8/40]. 106.07 samples/sec. 452.519 ms/step.
Train [16/40]. 106.07 samples/sec. 452.536 ms/step.
Train [24/40]. 106.06 samples/sec. 452.587 ms/step.
Train [32/40]. 106.06 samples/sec. 452.573 ms/step.
Train [40/40]. 106.06 samples/sec. 452.578 ms/step.
Train benchmark of regnety_320.seer_ft_in1k done. 105.72 samples/sec, 452.58 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model regnety_320.swag_ft_in1k created, param count: 145046770
Running inference benchmark on regnety_320.swag_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 361.47 samples/sec. 708.225 ms/step.
Infer [16/40]. 361.42 samples/sec. 708.309 ms/step.
Infer [24/40]. 361.38 samples/sec. 708.405 ms/step.
Infer [32/40]. 361.31 samples/sec. 708.526 ms/step.
Infer [40/40]. 361.26 samples/sec. 708.634 ms/step.
Inference benchmark of regnety_320.swag_ft_in1k done. 361.23 samples/sec, 708.63 ms/step
Model regnety_320.swag_ft_in1k created, param count: 145046770
Running train benchmark on regnety_320.swag_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 1.02 GiB. GPU 0 has a total capacty of 23.65 GiB of which 140.06 MiB is free. Including non-PyTorch memory, this process has 23.50 GiB memory in use. Of the allocated memory 20.99 GiB is allocated by PyTorch, and 1.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model regnety_320.swag_ft_in1k created, param count: 145046770
Running train benchmark on regnety_320.swag_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 588.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 42.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 21.94 GiB is allocated by PyTorch, and 434.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model regnety_320.swag_ft_in1k created, param count: 145046770
Running train benchmark on regnety_320.swag_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 392.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 352.06 MiB is free. Including non-PyTorch memory, this process has 23.30 GiB memory in use. Of the allocated memory 21.60 GiB is allocated by PyTorch, and 471.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model regnety_320.swag_ft_in1k created, param count: 145046770
Running train benchmark on regnety_320.swag_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 588.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 456.06 MiB is free. Including non-PyTorch memory, this process has 23.20 GiB memory in use. Of the allocated memory 21.45 GiB is allocated by PyTorch, and 529.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model regnety_320.swag_ft_in1k created, param count: 145046770
Running train benchmark on regnety_320.swag_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 18.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.06 GiB is allocated by PyTorch, and 338.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model regnety_320.swag_ft_in1k created, param count: 145046770
Running train benchmark on regnety_320.swag_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 48.
Train [8/40]. 106.06 samples/sec. 452.587 ms/step.
Train [16/40]. 106.07 samples/sec. 452.513 ms/step.
Train [24/40]. 106.07 samples/sec. 452.540 ms/step.
Train [32/40]. 106.06 samples/sec. 452.571 ms/step.
Train [40/40]. 106.06 samples/sec. 452.559 ms/step.
Train benchmark of regnety_320.swag_ft_in1k done. 105.72 samples/sec, 452.56 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model regnety_320.swag_lc_in1k created, param count: 145046770
Running inference benchmark on regnety_320.swag_lc_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 869.69 samples/sec. 294.357 ms/step.
Infer [16/40]. 869.90 samples/sec. 294.287 ms/step.
Infer [24/40]. 871.45 samples/sec. 293.765 ms/step.
Infer [32/40]. 872.66 samples/sec. 293.356 ms/step.
Infer [40/40]. 872.36 samples/sec. 293.458 ms/step.
Inference benchmark of regnety_320.swag_lc_in1k done. 872.22 samples/sec, 293.46 ms/step
Model regnety_320.swag_lc_in1k created, param count: 145046770
Running train benchmark on regnety_320.swag_lc_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 102.06 MiB is free. Including non-PyTorch memory, this process has 23.54 GiB memory in use. Of the allocated memory 21.96 GiB is allocated by PyTorch, and 351.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model regnety_320.swag_lc_in1k created, param count: 145046770
Running train benchmark on regnety_320.swag_lc_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 10.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 21.83 GiB is allocated by PyTorch, and 586.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model regnety_320.swag_lc_in1k created, param count: 145046770
Running train benchmark on regnety_320.swag_lc_in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
Train [8/40]. 312.90 samples/sec. 409.073 ms/step.
Train [16/40]. 312.90 samples/sec. 409.079 ms/step.
Train [24/40]. 312.88 samples/sec. 409.100 ms/step.
Train [32/40]. 312.88 samples/sec. 409.101 ms/step.
Train [40/40]. 312.89 samples/sec. 409.094 ms/step.
Train benchmark of regnety_320.swag_lc_in1k done. 311.80 samples/sec, 409.09 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model regnety_320.tv2_in1k created, param count: 145046770
Running inference benchmark on regnety_320.tv2_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 874.62 samples/sec. 292.698 ms/step.
Infer [16/40]. 869.73 samples/sec. 294.343 ms/step.
Infer [24/40]. 870.77 samples/sec. 293.994 ms/step.
Infer [32/40]. 871.35 samples/sec. 293.796 ms/step.
Infer [40/40]. 870.09 samples/sec. 294.224 ms/step.
Inference benchmark of regnety_320.tv2_in1k done. 869.95 samples/sec, 294.22 ms/step
Model regnety_320.tv2_in1k created, param count: 145046770
Running train benchmark on regnety_320.tv2_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 102.06 MiB is free. Including non-PyTorch memory, this process has 23.54 GiB memory in use. Of the allocated memory 21.96 GiB is allocated by PyTorch, and 351.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model regnety_320.tv2_in1k created, param count: 145046770
Running train benchmark on regnety_320.tv2_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 10.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 21.83 GiB is allocated by PyTorch, and 586.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model regnety_320.tv2_in1k created, param count: 145046770
Running train benchmark on regnety_320.tv2_in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
Train [8/40]. 312.91 samples/sec. 409.067 ms/step.
Train [16/40]. 312.91 samples/sec. 409.058 ms/step.
Train [24/40]. 312.90 samples/sec. 409.070 ms/step.
Train [32/40]. 312.91 samples/sec. 409.067 ms/step.
Train [40/40]. 312.90 samples/sec. 409.074 ms/step.
Train benchmark of regnety_320.tv2_in1k done. 311.83 samples/sec, 409.07 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model regnety_640.seer created, param count: 276457786
Running inference benchmark on regnety_640.seer for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 626.69 samples/sec. 408.498 ms/step.
Infer [16/40]. 626.66 samples/sec. 408.516 ms/step.
Infer [24/40]. 626.66 samples/sec. 408.518 ms/step.
Infer [32/40]. 626.64 samples/sec. 408.527 ms/step.
Infer [40/40]. 626.64 samples/sec. 408.530 ms/step.
Inference benchmark of regnety_640.seer done. 626.56 samples/sec, 408.53 ms/step
Model regnety_640.seer created, param count: 276457786
Running train benchmark on regnety_640.seer for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 378.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 124.06 MiB is free. Including non-PyTorch memory, this process has 23.52 GiB memory in use. Of the allocated memory 21.96 GiB is allocated by PyTorch, and 334.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model regnety_640.seer created, param count: 276457786
Running train benchmark on regnety_640.seer for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 142.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 124.06 MiB is free. Including non-PyTorch memory, this process has 23.52 GiB memory in use. Of the allocated memory 21.99 GiB is allocated by PyTorch, and 300.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model regnety_640.seer created, param count: 276457786
Running train benchmark on regnety_640.seer for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 2.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 21.38 GiB is allocated by PyTorch, and 1.03 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model regnety_640.seer created, param count: 276457786
Running train benchmark on regnety_640.seer for 40 steps w/ input size (3, 224, 224) and batch size 96.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model regnety_640.seer created, param count: 276457786
Running train benchmark on regnety_640.seer for 40 steps w/ input size (3, 224, 224) and batch size 64.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model regnety_640.seer created, param count: 276457786
Running train benchmark on regnety_640.seer for 40 steps w/ input size (3, 224, 224) and batch size 48.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model regnety_640.seer created, param count: 276457786
Running train benchmark on regnety_640.seer for 40 steps w/ input size (3, 224, 224) and batch size 32.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model regnety_640.seer created, param count: 276457786
Running train benchmark on regnety_640.seer for 40 steps w/ input size (3, 224, 224) and batch size 24.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model regnety_640.seer created, param count: 276457786
Running train benchmark on regnety_640.seer for 40 steps w/ input size (3, 224, 224) and batch size 16.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 12 for retry.
Model regnety_640.seer created, param count: 276457786
Running train benchmark on regnety_640.seer for 40 steps w/ input size (3, 224, 224) and batch size 12.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 8 for retry.
Model regnety_640.seer created, param count: 276457786
Running train benchmark on regnety_640.seer for 40 steps w/ input size (3, 224, 224) and batch size 8.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 6 for retry.
Model regnety_640.seer created, param count: 276457786
Running train benchmark on regnety_640.seer for 40 steps w/ input size (3, 224, 224) and batch size 6.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 4 for retry.
Model regnety_640.seer created, param count: 276457786
Running train benchmark on regnety_640.seer for 40 steps w/ input size (3, 224, 224) and batch size 4.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 3 for retry.
Model regnety_640.seer created, param count: 276457786
Running train benchmark on regnety_640.seer for 40 steps w/ input size (3, 224, 224) and batch size 3.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 2 for retry.
Model regnety_640.seer created, param count: 276457786
Running train benchmark on regnety_640.seer for 40 steps w/ input size (3, 224, 224) and batch size 2.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 1 for retry.
Model regnety_640.seer created, param count: 276457786
Running train benchmark on regnety_640.seer for 40 steps w/ input size (3, 224, 224) and batch size 1.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 0 for retry.
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model regnety_640.seer_ft_in1k created, param count: 281378786
Running inference benchmark on regnety_640.seer_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 222.63 samples/sec. 1149.870 ms/step.
Infer [16/40]. 222.57 samples/sec. 1150.178 ms/step.
Infer [24/40]. 222.57 samples/sec. 1150.216 ms/step.
Infer [32/40]. 222.55 samples/sec. 1150.324 ms/step.
Infer [40/40]. 222.53 samples/sec. 1150.429 ms/step.
Inference benchmark of regnety_640.seer_ft_in1k done. 222.52 samples/sec, 1150.43 ms/step
Model regnety_640.seer_ft_in1k created, param count: 281378786
Running train benchmark on regnety_640.seer_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 1.44 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.06 GiB is free. Including non-PyTorch memory, this process has 22.58 GiB memory in use. Of the allocated memory 20.62 GiB is allocated by PyTorch, and 751.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model regnety_640.seer_ft_in1k created, param count: 281378786
Running train benchmark on regnety_640.seer_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 1.08 GiB. GPU 0 has a total capacty of 23.65 GiB of which 550.06 MiB is free. Including non-PyTorch memory, this process has 23.10 GiB memory in use. Of the allocated memory 21.02 GiB is allocated by PyTorch, and 872.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model regnety_640.seer_ft_in1k created, param count: 281378786
Running train benchmark on regnety_640.seer_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 554.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 426.06 MiB is free. Including non-PyTorch memory, this process has 23.22 GiB memory in use. Of the allocated memory 21.75 GiB is allocated by PyTorch, and 245.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model regnety_640.seer_ft_in1k created, param count: 281378786
Running train benchmark on regnety_640.seer_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 416.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 370.06 MiB is free. Including non-PyTorch memory, this process has 23.28 GiB memory in use. Of the allocated memory 21.73 GiB is allocated by PyTorch, and 320.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model regnety_640.seer_ft_in1k created, param count: 281378786
Running train benchmark on regnety_640.seer_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 140.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 68.06 MiB is free. Including non-PyTorch memory, this process has 23.57 GiB memory in use. Of the allocated memory 21.97 GiB is allocated by PyTorch, and 382.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model regnety_640.seer_ft_in1k created, param count: 281378786
Running train benchmark on regnety_640.seer_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 104.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 16.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 21.90 GiB is allocated by PyTorch, and 504.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model regnety_640.seer_ft_in1k created, param count: 281378786
Running train benchmark on regnety_640.seer_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 32.
Train [8/40]. 66.77 samples/sec. 479.261 ms/step.
Train [16/40]. 66.77 samples/sec. 479.256 ms/step.
Train [24/40]. 66.77 samples/sec. 479.249 ms/step.
Train [32/40]. 66.77 samples/sec. 479.246 ms/step.
Train [40/40]. 66.77 samples/sec. 479.251 ms/step.
Train benchmark of regnety_640.seer_ft_in1k done. 66.57 samples/sec, 479.25 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model regnety_1280.seer created, param count: 637419894
Running inference benchmark on regnety_1280.seer for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 321.61 samples/sec. 795.989 ms/step.
Infer [16/40]. 321.60 samples/sec. 796.020 ms/step.
Infer [24/40]. 321.58 samples/sec. 796.061 ms/step.
Infer [32/40]. 321.56 samples/sec. 796.111 ms/step.
Infer [40/40]. 321.54 samples/sec. 796.175 ms/step.
Inference benchmark of regnety_1280.seer done. 321.52 samples/sec, 796.17 ms/step
Model regnety_1280.seer created, param count: 637419894
Running train benchmark on regnety_1280.seer for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 406.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 382.06 MiB is free. Including non-PyTorch memory, this process has 23.27 GiB memory in use. Of the allocated memory 21.80 GiB is allocated by PyTorch, and 240.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model regnety_1280.seer created, param count: 637419894
Running train benchmark on regnety_1280.seer for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 304.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 156.06 MiB is free. Including non-PyTorch memory, this process has 23.49 GiB memory in use. Of the allocated memory 21.70 GiB is allocated by PyTorch, and 565.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model regnety_1280.seer created, param count: 637419894
Running train benchmark on regnety_1280.seer for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 140.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 68.06 MiB is free. Including non-PyTorch memory, this process has 23.57 GiB memory in use. Of the allocated memory 21.87 GiB is allocated by PyTorch, and 479.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model regnety_1280.seer created, param count: 637419894
Running train benchmark on regnety_1280.seer for 40 steps w/ input size (3, 224, 224) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 106.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 22.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 21.42 GiB is allocated by PyTorch, and 987.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model regnety_1280.seer created, param count: 637419894
Running train benchmark on regnety_1280.seer for 40 steps w/ input size (3, 224, 224) and batch size 64.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model regnety_1280.seer created, param count: 637419894
Running train benchmark on regnety_1280.seer for 40 steps w/ input size (3, 224, 224) and batch size 48.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model regnety_1280.seer created, param count: 637419894
Running train benchmark on regnety_1280.seer for 40 steps w/ input size (3, 224, 224) and batch size 32.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model regnety_1280.seer created, param count: 637419894
Running train benchmark on regnety_1280.seer for 40 steps w/ input size (3, 224, 224) and batch size 24.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model regnety_1280.seer created, param count: 637419894
Running train benchmark on regnety_1280.seer for 40 steps w/ input size (3, 224, 224) and batch size 16.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 12 for retry.
Model regnety_1280.seer created, param count: 637419894
Running train benchmark on regnety_1280.seer for 40 steps w/ input size (3, 224, 224) and batch size 12.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 8 for retry.
Model regnety_1280.seer created, param count: 637419894
Running train benchmark on regnety_1280.seer for 40 steps w/ input size (3, 224, 224) and batch size 8.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 6 for retry.
Model regnety_1280.seer created, param count: 637419894
Running train benchmark on regnety_1280.seer for 40 steps w/ input size (3, 224, 224) and batch size 6.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 4 for retry.
Model regnety_1280.seer created, param count: 637419894
Running train benchmark on regnety_1280.seer for 40 steps w/ input size (3, 224, 224) and batch size 4.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 3 for retry.
Model regnety_1280.seer created, param count: 637419894
Running train benchmark on regnety_1280.seer for 40 steps w/ input size (3, 224, 224) and batch size 3.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 2 for retry.
Model regnety_1280.seer created, param count: 637419894
Running train benchmark on regnety_1280.seer for 40 steps w/ input size (3, 224, 224) and batch size 2.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 1 for retry.
Model regnety_1280.seer created, param count: 637419894
Running train benchmark on regnety_1280.seer for 40 steps w/ input size (3, 224, 224) and batch size 1.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 0 for retry.
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model regnety_1280.seer_ft_in1k created, param count: 644812894
Running inference benchmark on regnety_1280.seer_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 110.70 samples/sec. 2312.634 ms/step.
Infer [16/40]. 110.81 samples/sec. 2310.326 ms/step.
Infer [24/40]. 110.74 samples/sec. 2311.674 ms/step.
Infer [32/40]. 110.68 samples/sec. 2312.899 ms/step.
Infer [40/40]. 110.65 samples/sec. 2313.628 ms/step.
Inference benchmark of regnety_1280.seer_ft_in1k done. 110.65 samples/sec, 2313.63 ms/step
Model regnety_1280.seer_ft_in1k created, param count: 644812894
Running train benchmark on regnety_1280.seer_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 2.32 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.26 GiB is free. Including non-PyTorch memory, this process has 22.38 GiB memory in use. Of the allocated memory 21.12 GiB is allocated by PyTorch, and 28.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model regnety_1280.seer_ft_in1k created, param count: 644812894
Running train benchmark on regnety_1280.seer_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 1.74 GiB. GPU 0 has a total capacty of 23.65 GiB of which 468.06 MiB is free. Including non-PyTorch memory, this process has 23.18 GiB memory in use. Of the allocated memory 21.38 GiB is allocated by PyTorch, and 585.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model regnety_1280.seer_ft_in1k created, param count: 644812894
Running train benchmark on regnety_1280.seer_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.08 GiB is free. Including non-PyTorch memory, this process has 22.56 GiB memory in use. Of the allocated memory 20.45 GiB is allocated by PyTorch, and 899.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model regnety_1280.seer_ft_in1k created, param count: 644812894
Running train benchmark on regnety_1280.seer_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 1.74 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.44 GiB is free. Including non-PyTorch memory, this process has 22.20 GiB memory in use. Of the allocated memory 20.01 GiB is allocated by PyTorch, and 978.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model regnety_1280.seer_ft_in1k created, param count: 644812894
Running train benchmark on regnety_1280.seer_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 298.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 266.06 MiB is free. Including non-PyTorch memory, this process has 23.38 GiB memory in use. Of the allocated memory 21.58 GiB is allocated by PyTorch, and 584.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model regnety_1280.seer_ft_in1k created, param count: 644812894
Running train benchmark on regnety_1280.seer_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 48.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 21.72 GiB is allocated by PyTorch, and 659.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model regnety_1280.seer_ft_in1k created, param count: 644812894
Running train benchmark on regnety_1280.seer_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 32.
ERROR: "CUDA out of memory. Tried to allocate 104.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 54.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 21.20 GiB is allocated by PyTorch, and 1.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model regnety_1280.seer_ft_in1k created, param count: 644812894
Running train benchmark on regnety_1280.seer_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 24.
ERROR: "CUDA out of memory. Tried to allocate 78.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 60.06 MiB is free. Including non-PyTorch memory, this process has 23.58 GiB memory in use. Of the allocated memory 20.36 GiB is allocated by PyTorch, and 1.99 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model regnety_1280.seer_ft_in1k created, param count: 644812894
Running train benchmark on regnety_1280.seer_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 16.
Train [8/40]. 34.81 samples/sec. 459.702 ms/step.
Train [16/40]. 34.80 samples/sec. 459.715 ms/step.
Train [24/40]. 34.80 samples/sec. 459.722 ms/step.
Train [32/40]. 34.80 samples/sec. 459.722 ms/step.
Train [40/40]. 34.80 samples/sec. 459.740 ms/step.
Train benchmark of regnety_1280.seer_ft_in1k done. 34.67 samples/sec, 459.74 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model regnety_1280.swag_ft_in1k created, param count: 644812894
Running inference benchmark on regnety_1280.swag_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 110.75 samples/sec. 2311.538 ms/step.
Infer [16/40]. 110.74 samples/sec. 2311.670 ms/step.
Infer [24/40]. 110.78 samples/sec. 2310.899 ms/step.
Infer [32/40]. 110.74 samples/sec. 2311.810 ms/step.
Infer [40/40]. 110.71 samples/sec. 2312.293 ms/step.
Inference benchmark of regnety_1280.swag_ft_in1k done. 110.71 samples/sec, 2312.29 ms/step
Model regnety_1280.swag_ft_in1k created, param count: 644812894
Running train benchmark on regnety_1280.swag_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 2.32 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.26 GiB is free. Including non-PyTorch memory, this process has 22.38 GiB memory in use. Of the allocated memory 21.12 GiB is allocated by PyTorch, and 28.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model regnety_1280.swag_ft_in1k created, param count: 644812894
Running train benchmark on regnety_1280.swag_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 1.74 GiB. GPU 0 has a total capacty of 23.65 GiB of which 468.06 MiB is free. Including non-PyTorch memory, this process has 23.18 GiB memory in use. Of the allocated memory 21.38 GiB is allocated by PyTorch, and 585.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model regnety_1280.swag_ft_in1k created, param count: 644812894
Running train benchmark on regnety_1280.swag_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.08 GiB is free. Including non-PyTorch memory, this process has 22.56 GiB memory in use. Of the allocated memory 20.45 GiB is allocated by PyTorch, and 899.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model regnety_1280.swag_ft_in1k created, param count: 644812894
Running train benchmark on regnety_1280.swag_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 1.74 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.44 GiB is free. Including non-PyTorch memory, this process has 22.20 GiB memory in use. Of the allocated memory 20.01 GiB is allocated by PyTorch, and 978.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model regnety_1280.swag_ft_in1k created, param count: 644812894
Running train benchmark on regnety_1280.swag_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 298.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 268.06 MiB is free. Including non-PyTorch memory, this process has 23.38 GiB memory in use. Of the allocated memory 21.58 GiB is allocated by PyTorch, and 584.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model regnety_1280.swag_ft_in1k created, param count: 644812894
Running train benchmark on regnety_1280.swag_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 50.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 21.72 GiB is allocated by PyTorch, and 657.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model regnety_1280.swag_ft_in1k created, param count: 644812894
Running train benchmark on regnety_1280.swag_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 32.
ERROR: "CUDA out of memory. Tried to allocate 104.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 42.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 21.10 GiB is allocated by PyTorch, and 1.27 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model regnety_1280.swag_ft_in1k created, param count: 644812894
Running train benchmark on regnety_1280.swag_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 24.
ERROR: "CUDA out of memory. Tried to allocate 78.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 48.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 20.26 GiB is allocated by PyTorch, and 2.10 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model regnety_1280.swag_ft_in1k created, param count: 644812894
Running train benchmark on regnety_1280.swag_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 16.
Train [8/40]. 34.82 samples/sec. 459.500 ms/step.
Train [16/40]. 34.82 samples/sec. 459.496 ms/step.
Train [24/40]. 34.82 samples/sec. 459.499 ms/step.
Train [32/40]. 34.82 samples/sec. 459.504 ms/step.
Train [40/40]. 34.82 samples/sec. 459.551 ms/step.
Train benchmark of regnety_1280.swag_ft_in1k done. 34.69 samples/sec, 459.55 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model regnety_1280.swag_lc_in1k created, param count: 644812894
Running inference benchmark on regnety_1280.swag_lc_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 308.80 samples/sec. 829.008 ms/step.
Infer [16/40]. 308.89 samples/sec. 828.772 ms/step.
Infer [24/40]. 308.79 samples/sec. 829.053 ms/step.
Infer [32/40]. 308.77 samples/sec. 829.085 ms/step.
Infer [40/40]. 308.66 samples/sec. 829.384 ms/step.
Inference benchmark of regnety_1280.swag_lc_in1k done. 308.64 samples/sec, 829.38 ms/step
Model regnety_1280.swag_lc_in1k created, param count: 644812894
Running train benchmark on regnety_1280.swag_lc_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 406.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 366.06 MiB is free. Including non-PyTorch memory, this process has 23.28 GiB memory in use. Of the allocated memory 21.81 GiB is allocated by PyTorch, and 242.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model regnety_1280.swag_lc_in1k created, param count: 644812894
Running train benchmark on regnety_1280.swag_lc_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 304.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 140.06 MiB is free. Including non-PyTorch memory, this process has 23.50 GiB memory in use. Of the allocated memory 21.72 GiB is allocated by PyTorch, and 568.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model regnety_1280.swag_lc_in1k created, param count: 644812894
Running train benchmark on regnety_1280.swag_lc_in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 140.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 54.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 21.89 GiB is allocated by PyTorch, and 479.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model regnety_1280.swag_lc_in1k created, param count: 644812894
Running train benchmark on regnety_1280.swag_lc_in1k for 40 steps w/ input size (3, 224, 224) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 106.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 42.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 21.33 GiB is allocated by PyTorch, and 1.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model regnety_1280.swag_lc_in1k created, param count: 644812894
Running train benchmark on regnety_1280.swag_lc_in1k for 40 steps w/ input size (3, 224, 224) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 70.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 70.06 MiB is free. Including non-PyTorch memory, this process has 23.57 GiB memory in use. Of the allocated memory 20.12 GiB is allocated by PyTorch, and 2.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model regnety_1280.swag_lc_in1k created, param count: 644812894
Running train benchmark on regnety_1280.swag_lc_in1k for 40 steps w/ input size (3, 224, 224) and batch size 48.
Train [8/40]. 99.60 samples/sec. 481.920 ms/step.
Train [16/40]. 99.60 samples/sec. 481.927 ms/step.
Train [24/40]. 99.59 samples/sec. 481.977 ms/step.
Train [32/40]. 99.59 samples/sec. 481.976 ms/step.
Train [40/40]. 99.59 samples/sec. 481.979 ms/step.
Train benchmark of regnety_1280.swag_lc_in1k done. 99.21 samples/sec, 481.98 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model regnety_2560.seer_ft_in1k created, param count: 1282601138
Running inference benchmark on regnety_2560.seer_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 13.11 GiB. GPU 0 has a total capacty of 23.65 GiB of which 5.99 GiB is free. Including non-PyTorch memory, this process has 17.65 GiB memory in use. Of the allocated memory 16.29 GiB is allocated by PyTorch, and 121.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model regnety_2560.seer_ft_in1k created, param count: 1282601138
Running inference benchmark on regnety_2560.seer_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 9.84 GiB. GPU 0 has a total capacty of 23.65 GiB of which 8.09 GiB is free. Including non-PyTorch memory, this process has 15.55 GiB memory in use. Of the allocated memory 12.83 GiB is allocated by PyTorch, and 1.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model regnety_2560.seer_ft_in1k created, param count: 1282601138
Running inference benchmark on regnety_2560.seer_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 6.56 GiB. GPU 0 has a total capacty of 23.65 GiB of which 6.17 GiB is free. Including non-PyTorch memory, this process has 17.47 GiB memory in use. Of the allocated memory 9.36 GiB is allocated by PyTorch, and 6.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model regnety_2560.seer_ft_in1k created, param count: 1282601138
Running inference benchmark on regnety_2560.seer_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 96.
Infer [8/40]. 66.20 samples/sec. 1450.233 ms/step.
Infer [16/40]. 66.20 samples/sec. 1450.248 ms/step.
Infer [24/40]. 66.19 samples/sec. 1450.329 ms/step.
Infer [32/40]. 66.19 samples/sec. 1450.441 ms/step.
Infer [40/40]. 66.18 samples/sec. 1450.550 ms/step.
Inference benchmark of regnety_2560.seer_ft_in1k done. 66.18 samples/sec, 1450.55 ms/step
Model regnety_2560.seer_ft_in1k created, param count: 1282601138
Running train benchmark on regnety_2560.seer_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 13.11 GiB. GPU 0 has a total capacty of 23.65 GiB of which 5.43 GiB is free. Including non-PyTorch memory, this process has 18.21 GiB memory in use. Of the allocated memory 16.86 GiB is allocated by PyTorch, and 121.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model regnety_2560.seer_ft_in1k created, param count: 1282601138
Running train benchmark on regnety_2560.seer_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 9.84 GiB. GPU 0 has a total capacty of 23.65 GiB of which 8.09 GiB is free. Including non-PyTorch memory, this process has 15.55 GiB memory in use. Of the allocated memory 13.26 GiB is allocated by PyTorch, and 1.06 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model regnety_2560.seer_ft_in1k created, param count: 1282601138
Running train benchmark on regnety_2560.seer_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 1.64 GiB. GPU 0 has a total capacty of 23.65 GiB of which 702.06 MiB is free. Including non-PyTorch memory, this process has 22.96 GiB memory in use. Of the allocated memory 21.12 GiB is allocated by PyTorch, and 621.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model regnety_2560.seer_ft_in1k created, param count: 1282601138
Running train benchmark on regnety_2560.seer_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 1.23 GiB. GPU 0 has a total capacty of 23.65 GiB of which 930.06 MiB is free. Including non-PyTorch memory, this process has 22.73 GiB memory in use. Of the allocated memory 20.13 GiB is allocated by PyTorch, and 1.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model regnety_2560.seer_ft_in1k created, param count: 1282601138
Running train benchmark on regnety_2560.seer_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 1.64 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.48 GiB is free. Including non-PyTorch memory, this process has 22.16 GiB memory in use. Of the allocated memory 19.97 GiB is allocated by PyTorch, and 980.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model regnety_2560.seer_ft_in1k created, param count: 1282601138
Running train benchmark on regnety_2560.seer_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 316.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 162.06 MiB is free. Including non-PyTorch memory, this process has 23.48 GiB memory in use. Of the allocated memory 21.11 GiB is allocated by PyTorch, and 1.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model regnety_2560.seer_ft_in1k created, param count: 1282601138
Running train benchmark on regnety_2560.seer_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 32.
ERROR: "CUDA out of memory. Tried to allocate 210.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 102.06 MiB is free. Including non-PyTorch memory, this process has 23.54 GiB memory in use. Of the allocated memory 21.44 GiB is allocated by PyTorch, and 884.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model regnety_2560.seer_ft_in1k created, param count: 1282601138
Running train benchmark on regnety_2560.seer_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 24.
ERROR: "CUDA out of memory. Tried to allocate 110.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 34.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 21.11 GiB is allocated by PyTorch, and 1.26 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model regnety_2560.seer_ft_in1k created, param count: 1282601138
Running train benchmark on regnety_2560.seer_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 16.
ERROR: "CUDA out of memory. Tried to allocate 74.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 32.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 19.52 GiB is allocated by PyTorch, and 2.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 12 for retry.
Model regnety_2560.seer_ft_in1k created, param count: 1282601138
Running train benchmark on regnety_2560.seer_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 12.
ERROR: "CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 28.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 19.02 GiB is allocated by PyTorch, and 3.36 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 8 for retry.
Model regnety_2560.seer_ft_in1k created, param count: 1282601138
Running train benchmark on regnety_2560.seer_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 8.
Train [8/40]. 17.57 samples/sec. 455.443 ms/step.
Train [16/40]. 17.56 samples/sec. 455.477 ms/step.
Train [24/40]. 17.56 samples/sec. 455.480 ms/step.
Train [32/40]. 17.56 samples/sec. 455.519 ms/step.
Train [40/40]. 17.56 samples/sec. 455.536 ms/step.
Train benchmark of regnety_2560.seer_ft_in1k done. 17.50 samples/sec, 455.54 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model regnetz_040.ra3_in1k created, param count: 27116800
Running inference benchmark on regnetz_040.ra3_in1k for 40 steps w/ input size (3, 320, 320) and batch size 256.
Infer [8/40]. 1330.78 samples/sec. 192.369 ms/step.
Infer [16/40]. 1330.71 samples/sec. 192.378 ms/step.
Infer [24/40]. 1330.71 samples/sec. 192.378 ms/step.
Infer [32/40]. 1330.74 samples/sec. 192.375 ms/step.
Infer [40/40]. 1330.79 samples/sec. 192.367 ms/step.
Inference benchmark of regnetz_040.ra3_in1k done. 1330.54 samples/sec, 192.37 ms/step
Model regnetz_040.ra3_in1k created, param count: 27116800
Running train benchmark on regnetz_040.ra3_in1k for 40 steps w/ input size (3, 320, 320) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 326.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 290.06 MiB is free. Including non-PyTorch memory, this process has 23.36 GiB memory in use. Of the allocated memory 21.96 GiB is allocated by PyTorch, and 164.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model regnetz_040.ra3_in1k created, param count: 27116800
Running train benchmark on regnetz_040.ra3_in1k for 40 steps w/ input size (3, 320, 320) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 244.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 144.06 MiB is free. Including non-PyTorch memory, this process has 23.50 GiB memory in use. Of the allocated memory 22.09 GiB is allocated by PyTorch, and 183.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model regnetz_040.ra3_in1k created, param count: 27116800
Running train benchmark on regnetz_040.ra3_in1k for 40 steps w/ input size (3, 320, 320) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 94.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 84.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 253.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model regnetz_040.ra3_in1k created, param count: 27116800
Running train benchmark on regnetz_040.ra3_in1k for 40 steps w/ input size (3, 320, 320) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 40.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 34.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 22.07 GiB is allocated by PyTorch, and 315.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model regnetz_040.ra3_in1k created, param count: 27116800
Running train benchmark on regnetz_040.ra3_in1k for 40 steps w/ input size (3, 320, 320) and batch size 64.
Train [8/40]. 353.39 samples/sec. 181.104 ms/step.
Train [16/40]. 353.37 samples/sec. 181.112 ms/step.
Train [24/40]. 353.38 samples/sec. 181.107 ms/step.
Train [32/40]. 353.39 samples/sec. 181.104 ms/step.
Train [40/40]. 353.38 samples/sec. 181.108 ms/step.
Train benchmark of regnetz_040.ra3_in1k done. 350.85 samples/sec, 181.11 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model regnetz_040_h.ra3_in1k created, param count: 28938880
Running inference benchmark on regnetz_040_h.ra3_in1k for 40 steps w/ input size (3, 320, 320) and batch size 256.
Infer [8/40]. 1314.92 samples/sec. 194.688 ms/step.
Infer [16/40]. 1314.51 samples/sec. 194.749 ms/step.
Infer [24/40]. 1314.37 samples/sec. 194.770 ms/step.
Infer [32/40]. 1314.27 samples/sec. 194.785 ms/step.
Infer [40/40]. 1314.17 samples/sec. 194.800 ms/step.
Inference benchmark of regnetz_040_h.ra3_in1k done. 1313.92 samples/sec, 194.80 ms/step
Model regnetz_040_h.ra3_in1k created, param count: 28938880
Running train benchmark on regnetz_040_h.ra3_in1k for 40 steps w/ input size (3, 320, 320) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 326.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 290.06 MiB is free. Including non-PyTorch memory, this process has 23.36 GiB memory in use. Of the allocated memory 21.97 GiB is allocated by PyTorch, and 161.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model regnetz_040_h.ra3_in1k created, param count: 28938880
Running train benchmark on regnetz_040_h.ra3_in1k for 40 steps w/ input size (3, 320, 320) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 244.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 92.06 MiB is free. Including non-PyTorch memory, this process has 23.55 GiB memory in use. Of the allocated memory 22.09 GiB is allocated by PyTorch, and 232.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model regnetz_040_h.ra3_in1k created, param count: 28938880
Running train benchmark on regnetz_040_h.ra3_in1k for 40 steps w/ input size (3, 320, 320) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 94.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 24.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.17 GiB is allocated by PyTorch, and 217.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model regnetz_040_h.ra3_in1k created, param count: 28938880
Running train benchmark on regnetz_040_h.ra3_in1k for 40 steps w/ input size (3, 320, 320) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 40.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 16.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.07 GiB is allocated by PyTorch, and 333.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model regnetz_040_h.ra3_in1k created, param count: 28938880
Running train benchmark on regnetz_040_h.ra3_in1k for 40 steps w/ input size (3, 320, 320) and batch size 64.
Train [8/40]. 352.28 samples/sec. 181.673 ms/step.
Train [16/40]. 352.29 samples/sec. 181.669 ms/step.
Train [24/40]. 352.27 samples/sec. 181.677 ms/step.
Train [32/40]. 352.26 samples/sec. 181.682 ms/step.
Train [40/40]. 352.27 samples/sec. 181.681 ms/step.
Train benchmark of regnetz_040_h.ra3_in1k done. 349.76 samples/sec, 181.68 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model regnetz_b16.ra3_in1k created, param count: 9715480
Running inference benchmark on regnetz_b16.ra3_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 3160.78 samples/sec. 80.993 ms/step.
Infer [16/40]. 3160.62 samples/sec. 80.997 ms/step.
Infer [24/40]. 3160.53 samples/sec. 80.999 ms/step.
Infer [32/40]. 3160.30 samples/sec. 81.005 ms/step.
Infer [40/40]. 3160.28 samples/sec. 81.005 ms/step.
Inference benchmark of regnetz_b16.ra3_in1k done. 3159.28 samples/sec, 81.00 ms/step
Model regnetz_b16.ra3_in1k created, param count: 9715480
Running train benchmark on regnetz_b16.ra3_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 92.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 90.06 MiB is free. Including non-PyTorch memory, this process has 23.55 GiB memory in use. Of the allocated memory 22.09 GiB is allocated by PyTorch, and 235.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model regnetz_b16.ra3_in1k created, param count: 9715480
Running train benchmark on regnetz_b16.ra3_in1k for 40 steps w/ input size (3, 288, 288) and batch size 192.
Train [8/40]. 757.47 samples/sec. 253.474 ms/step.
Train [16/40]. 757.50 samples/sec. 253.466 ms/step.
Train [24/40]. 757.51 samples/sec. 253.461 ms/step.
Train [32/40]. 757.49 samples/sec. 253.469 ms/step.
Train [40/40]. 757.50 samples/sec. 253.464 ms/step.
Train benchmark of regnetz_b16.ra3_in1k done. 753.81 samples/sec, 253.46 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model regnetz_c16.ra3_in1k created, param count: 13459880
Running inference benchmark on regnetz_c16.ra3_in1k for 40 steps w/ input size (3, 320, 320) and batch size 256.
Infer [8/40]. 1954.99 samples/sec. 130.947 ms/step.
Infer [16/40]. 1954.99 samples/sec. 130.947 ms/step.
Infer [24/40]. 1954.86 samples/sec. 130.955 ms/step.
Infer [32/40]. 1954.78 samples/sec. 130.961 ms/step.
Infer [40/40]. 1954.77 samples/sec. 130.962 ms/step.
Inference benchmark of regnetz_c16.ra3_in1k done. 1954.32 samples/sec, 130.96 ms/step
Model regnetz_c16.ra3_in1k created, param count: 13459880
Running train benchmark on regnetz_c16.ra3_in1k for 40 steps w/ input size (3, 320, 320) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 300.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 140.06 MiB is free. Including non-PyTorch memory, this process has 23.50 GiB memory in use. Of the allocated memory 21.97 GiB is allocated by PyTorch, and 305.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model regnetz_c16.ra3_in1k created, param count: 13459880
Running train benchmark on regnetz_c16.ra3_in1k for 40 steps w/ input size (3, 320, 320) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 114.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 14.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.06 GiB is allocated by PyTorch, and 338.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model regnetz_c16.ra3_in1k created, param count: 13459880
Running train benchmark on regnetz_c16.ra3_in1k for 40 steps w/ input size (3, 320, 320) and batch size 128.
Train [8/40]. 489.40 samples/sec. 261.547 ms/step.
Train [16/40]. 489.33 samples/sec. 261.583 ms/step.
Train [24/40]. 489.31 samples/sec. 261.594 ms/step.
Train [32/40]. 489.28 samples/sec. 261.607 ms/step.
Train [40/40]. 489.28 samples/sec. 261.611 ms/step.
Train benchmark of regnetz_c16.ra3_in1k done. 486.93 samples/sec, 261.61 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model regnetz_c16_evos.ch_in1k created, param count: 13487816
Running inference benchmark on regnetz_c16_evos.ch_in1k for 40 steps w/ input size (3, 320, 320) and batch size 256.
Infer [8/40]. 724.34 samples/sec. 353.426 ms/step.
Infer [16/40]. 724.16 samples/sec. 353.513 ms/step.
Infer [24/40]. 724.16 samples/sec. 353.512 ms/step.
Infer [32/40]. 724.14 samples/sec. 353.525 ms/step.
Infer [40/40]. 724.13 samples/sec. 353.529 ms/step.
Inference benchmark of regnetz_c16_evos.ch_in1k done. 724.03 samples/sec, 353.53 ms/step
Model regnetz_c16_evos.ch_in1k created, param count: 13487816
Running train benchmark on regnetz_c16_evos.ch_in1k for 40 steps w/ input size (3, 320, 320) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 600.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 336.06 MiB is free. Including non-PyTorch memory, this process has 23.31 GiB memory in use. Of the allocated memory 21.97 GiB is allocated by PyTorch, and 116.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model regnetz_c16_evos.ch_in1k created, param count: 13487816
Running train benchmark on regnetz_c16_evos.ch_in1k for 40 steps w/ input size (3, 320, 320) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 450.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 82.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 22.20 GiB is allocated by PyTorch, and 133.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model regnetz_c16_evos.ch_in1k created, param count: 13487816
Running train benchmark on regnetz_c16_evos.ch_in1k for 40 steps w/ input size (3, 320, 320) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 150.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 116.06 MiB is free. Including non-PyTorch memory, this process has 23.53 GiB memory in use. Of the allocated memory 22.23 GiB is allocated by PyTorch, and 70.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model regnetz_c16_evos.ch_in1k created, param count: 13487816
Running train benchmark on regnetz_c16_evos.ch_in1k for 40 steps w/ input size (3, 320, 320) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 30.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 4.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 22.16 GiB is allocated by PyTorch, and 250.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model regnetz_c16_evos.ch_in1k created, param count: 13487816
Running train benchmark on regnetz_c16_evos.ch_in1k for 40 steps w/ input size (3, 320, 320) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 76.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 70.06 MiB is free. Including non-PyTorch memory, this process has 23.57 GiB memory in use. Of the allocated memory 22.20 GiB is allocated by PyTorch, and 144.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model regnetz_c16_evos.ch_in1k created, param count: 13487816
Running train benchmark on regnetz_c16_evos.ch_in1k for 40 steps w/ input size (3, 320, 320) and batch size 48.
Train [8/40]. 208.94 samples/sec. 229.729 ms/step.
Train [16/40]. 208.94 samples/sec. 229.735 ms/step.
Train [24/40]. 208.94 samples/sec. 229.732 ms/step.
Train [32/40]. 208.94 samples/sec. 229.730 ms/step.
Train [40/40]. 208.94 samples/sec. 229.733 ms/step.
Train benchmark of regnetz_c16_evos.ch_in1k done. 207.41 samples/sec, 229.73 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model regnetz_d8.ra3_in1k created, param count: 23373792
Running inference benchmark on regnetz_d8.ra3_in1k for 40 steps w/ input size (3, 320, 320) and batch size 256.
Infer [8/40]. 1315.74 samples/sec. 194.568 ms/step.
Infer [16/40]. 1315.54 samples/sec. 194.596 ms/step.
Infer [24/40]. 1315.42 samples/sec. 194.615 ms/step.
Infer [32/40]. 1315.35 samples/sec. 194.625 ms/step.
Infer [40/40]. 1315.31 samples/sec. 194.631 ms/step.
Inference benchmark of regnetz_d8.ra3_in1k done. 1315.06 samples/sec, 194.63 ms/step
Model regnetz_d8.ra3_in1k created, param count: 23373792
Running train benchmark on regnetz_d8.ra3_in1k for 40 steps w/ input size (3, 320, 320) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 800.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 268.06 MiB is free. Including non-PyTorch memory, this process has 23.38 GiB memory in use. Of the allocated memory 21.99 GiB is allocated by PyTorch, and 164.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model regnetz_d8.ra3_in1k created, param count: 23373792
Running train benchmark on regnetz_d8.ra3_in1k for 40 steps w/ input size (3, 320, 320) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 76.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 28.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 22.22 GiB is allocated by PyTorch, and 161.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model regnetz_d8.ra3_in1k created, param count: 23373792
Running train benchmark on regnetz_d8.ra3_in1k for 40 steps w/ input size (3, 320, 320) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 2.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 22.11 GiB is allocated by PyTorch, and 300.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model regnetz_d8.ra3_in1k created, param count: 23373792
Running train benchmark on regnetz_d8.ra3_in1k for 40 steps w/ input size (3, 320, 320) and batch size 96.
Train [8/40]. 345.92 samples/sec. 277.523 ms/step.
Train [16/40]. 345.93 samples/sec. 277.514 ms/step.
Train [24/40]. 345.92 samples/sec. 277.518 ms/step.
Train [32/40]. 345.92 samples/sec. 277.520 ms/step.
Train [40/40]. 345.92 samples/sec. 277.520 ms/step.
Train benchmark of regnetz_d8.ra3_in1k done. 344.22 samples/sec, 277.52 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model regnetz_d8_evos.ch_in1k created, param count: 23464296
Running inference benchmark on regnetz_d8_evos.ch_in1k for 40 steps w/ input size (3, 320, 320) and batch size 256.
Infer [8/40]. 474.14 samples/sec. 539.923 ms/step.
Infer [16/40]. 474.12 samples/sec. 539.949 ms/step.
Infer [24/40]. 474.14 samples/sec. 539.924 ms/step.
Infer [32/40]. 474.10 samples/sec. 539.971 ms/step.
Infer [40/40]. 474.07 samples/sec. 539.999 ms/step.
Inference benchmark of regnetz_d8_evos.ch_in1k done. 474.04 samples/sec, 540.00 ms/step
Model regnetz_d8_evos.ch_in1k created, param count: 23464296
Running train benchmark on regnetz_d8_evos.ch_in1k for 40 steps w/ input size (3, 320, 320) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 1.56 GiB. GPU 0 has a total capacty of 23.65 GiB of which 276.06 MiB is free. Including non-PyTorch memory, this process has 23.37 GiB memory in use. Of the allocated memory 21.30 GiB is allocated by PyTorch, and 857.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model regnetz_d8_evos.ch_in1k created, param count: 23464296
Running train benchmark on regnetz_d8_evos.ch_in1k for 40 steps w/ input size (3, 320, 320) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 1.17 GiB. GPU 0 has a total capacty of 23.65 GiB of which 168.06 MiB is free. Including non-PyTorch memory, this process has 23.48 GiB memory in use. Of the allocated memory 22.14 GiB is allocated by PyTorch, and 104.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model regnetz_d8_evos.ch_in1k created, param count: 23464296
Running train benchmark on regnetz_d8_evos.ch_in1k for 40 steps w/ input size (3, 320, 320) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 800.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 262.06 MiB is free. Including non-PyTorch memory, this process has 23.38 GiB memory in use. Of the allocated memory 22.01 GiB is allocated by PyTorch, and 146.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model regnetz_d8_evos.ch_in1k created, param count: 23464296
Running train benchmark on regnetz_d8_evos.ch_in1k for 40 steps w/ input size (3, 320, 320) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 600.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 366.06 MiB is free. Including non-PyTorch memory, this process has 23.28 GiB memory in use. Of the allocated memory 21.96 GiB is allocated by PyTorch, and 97.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model regnetz_d8_evos.ch_in1k created, param count: 23464296
Running train benchmark on regnetz_d8_evos.ch_in1k for 40 steps w/ input size (3, 320, 320) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 26.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.32 GiB is allocated by PyTorch, and 59.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model regnetz_d8_evos.ch_in1k created, param count: 23464296
Running train benchmark on regnetz_d8_evos.ch_in1k for 40 steps w/ input size (3, 320, 320) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 76.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 56.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 22.23 GiB is allocated by PyTorch, and 131.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model regnetz_d8_evos.ch_in1k created, param count: 23464296
Running train benchmark on regnetz_d8_evos.ch_in1k for 40 steps w/ input size (3, 320, 320) and batch size 32.
Train [8/40]. 141.92 samples/sec. 225.475 ms/step.
Train [16/40]. 141.92 samples/sec. 225.477 ms/step.
Train [24/40]. 141.92 samples/sec. 225.481 ms/step.
Train [32/40]. 141.91 samples/sec. 225.487 ms/step.
Train [40/40]. 141.92 samples/sec. 225.487 ms/step.
Train benchmark of regnetz_d8_evos.ch_in1k done. 140.78 samples/sec, 225.49 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model regnetz_d32.ra3_in1k created, param count: 27576288
Running inference benchmark on regnetz_d32.ra3_in1k for 40 steps w/ input size (3, 320, 320) and batch size 256.
Infer [8/40]. 1310.72 samples/sec. 195.313 ms/step.
Infer [16/40]. 1310.72 samples/sec. 195.313 ms/step.
Infer [24/40]. 1310.73 samples/sec. 195.310 ms/step.
Infer [32/40]. 1310.73 samples/sec. 195.312 ms/step.
Infer [40/40]. 1310.69 samples/sec. 195.317 ms/step.
Inference benchmark of regnetz_d32.ra3_in1k done. 1310.44 samples/sec, 195.32 ms/step
Model regnetz_d32.ra3_in1k created, param count: 27576288
Running train benchmark on regnetz_d32.ra3_in1k for 40 steps w/ input size (3, 320, 320) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 800.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 258.06 MiB is free. Including non-PyTorch memory, this process has 23.39 GiB memory in use. Of the allocated memory 21.99 GiB is allocated by PyTorch, and 166.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model regnetz_d32.ra3_in1k created, param count: 27576288
Running train benchmark on regnetz_d32.ra3_in1k for 40 steps w/ input size (3, 320, 320) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 76.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 54.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 22.23 GiB is allocated by PyTorch, and 130.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model regnetz_d32.ra3_in1k created, param count: 27576288
Running train benchmark on regnetz_d32.ra3_in1k for 40 steps w/ input size (3, 320, 320) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 76.06 MiB is free. Including non-PyTorch memory, this process has 23.57 GiB memory in use. Of the allocated memory 22.12 GiB is allocated by PyTorch, and 218.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model regnetz_d32.ra3_in1k created, param count: 27576288
Running train benchmark on regnetz_d32.ra3_in1k for 40 steps w/ input size (3, 320, 320) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 34.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 2.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 21.98 GiB is allocated by PyTorch, and 439.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model regnetz_d32.ra3_in1k created, param count: 27576288
Running train benchmark on regnetz_d32.ra3_in1k for 40 steps w/ input size (3, 320, 320) and batch size 64.
Train [8/40]. 346.95 samples/sec. 184.467 ms/step.
Train [16/40]. 346.96 samples/sec. 184.461 ms/step.
Train [24/40]. 346.94 samples/sec. 184.470 ms/step.
Train [32/40]. 346.95 samples/sec. 184.467 ms/step.
Train [40/40]. 346.95 samples/sec. 184.467 ms/step.
Train benchmark of regnetz_d32.ra3_in1k done. 344.77 samples/sec, 184.47 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model regnetz_e8.ra3_in1k created, param count: 57698176
Running inference benchmark on regnetz_e8.ra3_in1k for 40 steps w/ input size (3, 320, 320) and batch size 256.
Infer [8/40]. 740.22 samples/sec. 345.845 ms/step.
Infer [16/40]. 740.21 samples/sec. 345.850 ms/step.
Infer [24/40]. 740.18 samples/sec. 345.863 ms/step.
Infer [32/40]. 740.12 samples/sec. 345.890 ms/step.
Infer [40/40]. 740.11 samples/sec. 345.896 ms/step.
Inference benchmark of regnetz_e8.ra3_in1k done. 740.01 samples/sec, 345.90 ms/step
Model regnetz_e8.ra3_in1k created, param count: 57698176
Running train benchmark on regnetz_e8.ra3_in1k for 40 steps w/ input size (3, 320, 320) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 1.17 GiB. GPU 0 has a total capacty of 23.65 GiB of which 888.06 MiB is free. Including non-PyTorch memory, this process has 22.77 GiB memory in use. Of the allocated memory 21.27 GiB is allocated by PyTorch, and 279.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model regnetz_e8.ra3_in1k created, param count: 57698176
Running train benchmark on regnetz_e8.ra3_in1k for 40 steps w/ input size (3, 320, 320) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 900.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 326.06 MiB is free. Including non-PyTorch memory, this process has 23.32 GiB memory in use. Of the allocated memory 21.70 GiB is allocated by PyTorch, and 397.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model regnetz_e8.ra3_in1k created, param count: 57698176
Running train benchmark on regnetz_e8.ra3_in1k for 40 steps w/ input size (3, 320, 320) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 300.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 168.06 MiB is free. Including non-PyTorch memory, this process has 23.48 GiB memory in use. Of the allocated memory 21.98 GiB is allocated by PyTorch, and 267.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model regnetz_e8.ra3_in1k created, param count: 57698176
Running train benchmark on regnetz_e8.ra3_in1k for 40 steps w/ input size (3, 320, 320) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 226.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 98.06 MiB is free. Including non-PyTorch memory, this process has 23.54 GiB memory in use. Of the allocated memory 22.04 GiB is allocated by PyTorch, and 281.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model regnetz_e8.ra3_in1k created, param count: 57698176
Running train benchmark on regnetz_e8.ra3_in1k for 40 steps w/ input size (3, 320, 320) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 76.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 52.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 230.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model regnetz_e8.ra3_in1k created, param count: 57698176
Running train benchmark on regnetz_e8.ra3_in1k for 40 steps w/ input size (3, 320, 320) and batch size 48.
Train [8/40]. 202.19 samples/sec. 237.398 ms/step.
Train [16/40]. 202.20 samples/sec. 237.389 ms/step.
Train [24/40]. 202.20 samples/sec. 237.387 ms/step.
Train [32/40]. 202.20 samples/sec. 237.386 ms/step.
Train [40/40]. 202.20 samples/sec. 237.386 ms/step.
Train benchmark of regnetz_e8.ra3_in1k done. 200.90 samples/sec, 237.39 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model repvgg_a2.rvgg_in1k created, param count: 28210600
Running inference benchmark on repvgg_a2.rvgg_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 5710.68 samples/sec. 44.828 ms/step.
Infer [16/40]. 5711.30 samples/sec. 44.823 ms/step.
Infer [24/40]. 5711.26 samples/sec. 44.824 ms/step.
Infer [32/40]. 5711.30 samples/sec. 44.823 ms/step.
Infer [40/40]. 5711.30 samples/sec. 44.823 ms/step.
Inference benchmark of repvgg_a2.rvgg_in1k done. 5708.13 samples/sec, 44.82 ms/step
Model repvgg_a2.rvgg_in1k created, param count: 28210600
Running train benchmark on repvgg_a2.rvgg_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1558.07 samples/sec. 164.306 ms/step.
Train [16/40]. 1558.09 samples/sec. 164.304 ms/step.
Train [24/40]. 1558.04 samples/sec. 164.309 ms/step.
Train [32/40]. 1558.01 samples/sec. 164.313 ms/step.
Train [40/40]. 1558.01 samples/sec. 164.312 ms/step.
Train benchmark of repvgg_a2.rvgg_in1k done. 1550.88 samples/sec, 164.31 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model repvgg_b0.rvgg_in1k created, param count: 15817960
Running inference benchmark on repvgg_b0.rvgg_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3819.50 samples/sec. 67.024 ms/step.
Infer [16/40]. 3821.23 samples/sec. 66.994 ms/step.
Infer [24/40]. 3810.62 samples/sec. 67.181 ms/step.
Infer [32/40]. 3816.42 samples/sec. 67.078 ms/step.
Infer [40/40]. 3817.74 samples/sec. 67.055 ms/step.
Inference benchmark of repvgg_b0.rvgg_in1k done. 3816.32 samples/sec, 67.06 ms/step
Model repvgg_b0.rvgg_in1k created, param count: 15817960
Running train benchmark on repvgg_b0.rvgg_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1422.86 samples/sec. 179.919 ms/step.
Train [16/40]. 1422.18 samples/sec. 180.006 ms/step.
Train [24/40]. 1421.09 samples/sec. 180.144 ms/step.
Train [32/40]. 1420.51 samples/sec. 180.217 ms/step.
Train [40/40]. 1420.63 samples/sec. 180.201 ms/step.
Train benchmark of repvgg_b0.rvgg_in1k done. 1413.65 samples/sec, 180.20 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model repvgg_b1.rvgg_in1k created, param count: 57415016
Running inference benchmark on repvgg_b1.rvgg_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 2794.25 samples/sec. 91.617 ms/step.
Infer [16/40]. 2794.35 samples/sec. 91.614 ms/step.
Infer [24/40]. 2794.27 samples/sec. 91.616 ms/step.
Infer [32/40]. 2794.29 samples/sec. 91.615 ms/step.
Infer [40/40]. 2794.28 samples/sec. 91.616 ms/step.
Inference benchmark of repvgg_b1.rvgg_in1k done. 2793.44 samples/sec, 91.62 ms/step
Model repvgg_b1.rvgg_in1k created, param count: 57415016
Running train benchmark on repvgg_b1.rvgg_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 819.39 samples/sec. 312.428 ms/step.
Train [16/40]. 819.37 samples/sec. 312.435 ms/step.
Train [24/40]. 819.36 samples/sec. 312.439 ms/step.
Train [32/40]. 819.36 samples/sec. 312.440 ms/step.
Train [40/40]. 819.37 samples/sec. 312.437 ms/step.
Train benchmark of repvgg_b1.rvgg_in1k done. 816.61 samples/sec, 312.44 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model repvgg_b1g4.rvgg_in1k created, param count: 39966056
Running inference benchmark on repvgg_b1g4.rvgg_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3273.34 samples/sec. 78.207 ms/step.
Infer [16/40]. 3272.40 samples/sec. 78.230 ms/step.
Infer [24/40]. 3272.25 samples/sec. 78.234 ms/step.
Infer [32/40]. 3272.12 samples/sec. 78.237 ms/step.
Infer [40/40]. 3272.04 samples/sec. 78.239 ms/step.
Inference benchmark of repvgg_b1g4.rvgg_in1k done. 3270.96 samples/sec, 78.24 ms/step
Model repvgg_b1g4.rvgg_in1k created, param count: 39966056
Running train benchmark on repvgg_b1g4.rvgg_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 925.72 samples/sec. 276.540 ms/step.
Train [16/40]. 925.68 samples/sec. 276.553 ms/step.
Train [24/40]. 925.69 samples/sec. 276.550 ms/step.
Train [32/40]. 925.68 samples/sec. 276.552 ms/step.
Train [40/40]. 925.70 samples/sec. 276.547 ms/step.
Train benchmark of repvgg_b1g4.rvgg_in1k done. 922.34 samples/sec, 276.55 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model repvgg_b2.rvgg_in1k created, param count: 89022376
Running inference benchmark on repvgg_b2.rvgg_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 2010.41 samples/sec. 127.337 ms/step.
Infer [16/40]. 2010.30 samples/sec. 127.344 ms/step.
Infer [24/40]. 2010.27 samples/sec. 127.346 ms/step.
Infer [32/40]. 2010.28 samples/sec. 127.345 ms/step.
Infer [40/40]. 2010.27 samples/sec. 127.346 ms/step.
Inference benchmark of repvgg_b2.rvgg_in1k done. 2009.79 samples/sec, 127.35 ms/step
Model repvgg_b2.rvgg_in1k created, param count: 89022376
Running train benchmark on repvgg_b2.rvgg_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 599.69 samples/sec. 426.885 ms/step.
Train [16/40]. 599.70 samples/sec. 426.879 ms/step.
Train [24/40]. 599.70 samples/sec. 426.881 ms/step.
Train [32/40]. 599.69 samples/sec. 426.886 ms/step.
Train [40/40]. 599.69 samples/sec. 426.889 ms/step.
Train benchmark of repvgg_b2.rvgg_in1k done. 598.15 samples/sec, 426.89 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model repvgg_b2g4.rvgg_in1k created, param count: 61758376
Running inference benchmark on repvgg_b2g4.rvgg_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 2135.93 samples/sec. 119.854 ms/step.
Infer [16/40]. 2135.73 samples/sec. 119.866 ms/step.
Infer [24/40]. 2135.82 samples/sec. 119.860 ms/step.
Infer [32/40]. 2135.82 samples/sec. 119.860 ms/step.
Infer [40/40]. 2135.88 samples/sec. 119.857 ms/step.
Inference benchmark of repvgg_b2g4.rvgg_in1k done. 2135.36 samples/sec, 119.86 ms/step
Model repvgg_b2g4.rvgg_in1k created, param count: 61758376
Running train benchmark on repvgg_b2g4.rvgg_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 660.49 samples/sec. 387.591 ms/step.
Train [16/40]. 660.45 samples/sec. 387.612 ms/step.
Train [24/40]. 660.48 samples/sec. 387.596 ms/step.
Train [32/40]. 660.48 samples/sec. 387.595 ms/step.
Train [40/40]. 660.48 samples/sec. 387.597 ms/step.
Train benchmark of repvgg_b2g4.rvgg_in1k done. 658.63 samples/sec, 387.60 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model repvgg_b3.rvgg_in1k created, param count: 123085288
Running inference benchmark on repvgg_b3.rvgg_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1571.10 samples/sec. 162.943 ms/step.
Infer [16/40]. 1571.25 samples/sec. 162.927 ms/step.
Infer [24/40]. 1571.20 samples/sec. 162.933 ms/step.
Infer [32/40]. 1571.23 samples/sec. 162.929 ms/step.
Infer [40/40]. 1571.19 samples/sec. 162.934 ms/step.
Inference benchmark of repvgg_b3.rvgg_in1k done. 1570.85 samples/sec, 162.93 ms/step
Model repvgg_b3.rvgg_in1k created, param count: 123085288
Running train benchmark on repvgg_b3.rvgg_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 482.09 samples/sec. 531.025 ms/step.
Train [16/40]. 482.07 samples/sec. 531.039 ms/step.
Train [24/40]. 482.07 samples/sec. 531.045 ms/step.
Train [32/40]. 482.06 samples/sec. 531.049 ms/step.
Train [40/40]. 482.07 samples/sec. 531.046 ms/step.
Train benchmark of repvgg_b3.rvgg_in1k done. 481.08 samples/sec, 531.05 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model repvgg_b3g4.rvgg_in1k created, param count: 83825128
Running inference benchmark on repvgg_b3g4.rvgg_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1724.95 samples/sec. 148.410 ms/step.
Infer [16/40]. 1724.73 samples/sec. 148.429 ms/step.
Infer [24/40]. 1724.68 samples/sec. 148.433 ms/step.
Infer [32/40]. 1724.77 samples/sec. 148.426 ms/step.
Infer [40/40]. 1724.66 samples/sec. 148.435 ms/step.
Inference benchmark of repvgg_b3g4.rvgg_in1k done. 1724.29 samples/sec, 148.44 ms/step
Model repvgg_b3g4.rvgg_in1k created, param count: 83825128
Running train benchmark on repvgg_b3g4.rvgg_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 551.20 samples/sec. 464.439 ms/step.
Train [16/40]. 551.18 samples/sec. 464.458 ms/step.
Train [24/40]. 551.19 samples/sec. 464.449 ms/step.
Train [32/40]. 551.18 samples/sec. 464.457 ms/step.
Train [40/40]. 551.18 samples/sec. 464.458 ms/step.
Train benchmark of repvgg_b3g4.rvgg_in1k done. 549.90 samples/sec, 464.46 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model res2net50_14w_8s.in1k created, param count: 25059816
Running inference benchmark on res2net50_14w_8s.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3724.44 samples/sec. 68.735 ms/step.
Infer [16/40]. 3724.41 samples/sec. 68.736 ms/step.
Infer [24/40]. 3724.28 samples/sec. 68.738 ms/step.
Infer [32/40]. 3724.24 samples/sec. 68.739 ms/step.
Infer [40/40]. 3723.80 samples/sec. 68.747 ms/step.
Inference benchmark of res2net50_14w_8s.in1k done. 3722.43 samples/sec, 68.75 ms/step
Model res2net50_14w_8s.in1k created, param count: 25059816
Running train benchmark on res2net50_14w_8s.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 951.51 samples/sec. 269.045 ms/step.
Train [16/40]. 951.51 samples/sec. 269.045 ms/step.
Train [24/40]. 951.52 samples/sec. 269.043 ms/step.
Train [32/40]. 951.53 samples/sec. 269.039 ms/step.
Train [40/40]. 951.54 samples/sec. 269.037 ms/step.
Train benchmark of res2net50_14w_8s.in1k done. 946.09 samples/sec, 269.04 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model res2net50_26w_4s.in1k created, param count: 25699120
Running inference benchmark on res2net50_26w_4s.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3643.70 samples/sec. 70.258 ms/step.
Infer [16/40]. 3643.77 samples/sec. 70.257 ms/step.
Infer [24/40]. 3643.79 samples/sec. 70.257 ms/step.
Infer [32/40]. 3643.14 samples/sec. 70.269 ms/step.
Infer [40/40]. 3642.41 samples/sec. 70.283 ms/step.
Inference benchmark of res2net50_26w_4s.in1k done. 3641.10 samples/sec, 70.28 ms/step
Model res2net50_26w_4s.in1k created, param count: 25699120
Running train benchmark on res2net50_26w_4s.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1038.17 samples/sec. 246.587 ms/step.
Train [16/40]. 1038.16 samples/sec. 246.590 ms/step.
Train [24/40]. 1038.17 samples/sec. 246.588 ms/step.
Train [32/40]. 1038.16 samples/sec. 246.591 ms/step.
Train [40/40]. 1038.15 samples/sec. 246.592 ms/step.
Train benchmark of res2net50_26w_4s.in1k done. 1033.57 samples/sec, 246.59 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model res2net50_26w_6s.in1k created, param count: 37051448
Running inference benchmark on res2net50_26w_6s.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 2856.85 samples/sec. 89.609 ms/step.
Infer [16/40]. 2857.00 samples/sec. 89.605 ms/step.
Infer [24/40]. 2856.94 samples/sec. 89.606 ms/step.
Infer [32/40]. 2856.91 samples/sec. 89.607 ms/step.
Infer [40/40]. 2856.90 samples/sec. 89.608 ms/step.
Inference benchmark of res2net50_26w_6s.in1k done. 2856.02 samples/sec, 89.61 ms/step
Model res2net50_26w_6s.in1k created, param count: 37051448
Running train benchmark on res2net50_26w_6s.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 804.07 samples/sec. 318.382 ms/step.
Train [16/40]. 804.04 samples/sec. 318.391 ms/step.
Train [24/40]. 804.04 samples/sec. 318.393 ms/step.
Train [32/40]. 804.04 samples/sec. 318.393 ms/step.
Train [40/40]. 804.04 samples/sec. 318.393 ms/step.
Train benchmark of res2net50_26w_6s.in1k done. 800.37 samples/sec, 318.39 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model res2net50_26w_8s.in1k created, param count: 48403776
Running inference benchmark on res2net50_26w_8s.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 2341.46 samples/sec. 109.333 ms/step.
Infer [16/40]. 2341.28 samples/sec. 109.342 ms/step.
Infer [24/40]. 2341.26 samples/sec. 109.343 ms/step.
Infer [32/40]. 2341.30 samples/sec. 109.341 ms/step.
Infer [40/40]. 2341.29 samples/sec. 109.341 ms/step.
Inference benchmark of res2net50_26w_8s.in1k done. 2340.67 samples/sec, 109.34 ms/step
Model res2net50_26w_8s.in1k created, param count: 48403776
Running train benchmark on res2net50_26w_8s.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 660.91 samples/sec. 387.348 ms/step.
Train [16/40]. 660.90 samples/sec. 387.350 ms/step.
Train [24/40]. 660.91 samples/sec. 387.347 ms/step.
Train [32/40]. 660.91 samples/sec. 387.347 ms/step.
Train [40/40]. 660.90 samples/sec. 387.348 ms/step.
Train benchmark of res2net50_26w_8s.in1k done. 657.92 samples/sec, 387.35 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model res2net50_48w_2s.in1k created, param count: 25287304
Running inference benchmark on res2net50_48w_2s.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3985.85 samples/sec. 64.227 ms/step.
Infer [16/40]. 3985.66 samples/sec. 64.230 ms/step.
Infer [24/40]. 3985.64 samples/sec. 64.231 ms/step.
Infer [32/40]. 3985.57 samples/sec. 64.232 ms/step.
Infer [40/40]. 3985.62 samples/sec. 64.231 ms/step.
Inference benchmark of res2net50_48w_2s.in1k done. 3983.97 samples/sec, 64.23 ms/step
Model res2net50_48w_2s.in1k created, param count: 25287304
Running train benchmark on res2net50_48w_2s.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1176.37 samples/sec. 217.618 ms/step.
Train [16/40]. 1176.41 samples/sec. 217.612 ms/step.
Train [24/40]. 1176.40 samples/sec. 217.613 ms/step.
Train [32/40]. 1176.39 samples/sec. 217.616 ms/step.
Train [40/40]. 1176.40 samples/sec. 217.614 ms/step.
Train benchmark of res2net50_48w_2s.in1k done. 1171.73 samples/sec, 217.61 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model res2net50d.in1k created, param count: 25718352
Running inference benchmark on res2net50d.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3469.78 samples/sec. 73.780 ms/step.
Infer [16/40]. 3469.85 samples/sec. 73.778 ms/step.
Infer [24/40]. 3469.73 samples/sec. 73.781 ms/step.
Infer [32/40]. 3469.75 samples/sec. 73.780 ms/step.
Infer [40/40]. 3469.76 samples/sec. 73.780 ms/step.
Inference benchmark of res2net50d.in1k done. 3468.52 samples/sec, 73.78 ms/step
Model res2net50d.in1k created, param count: 25718352
Running train benchmark on res2net50d.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 945.95 samples/sec. 270.627 ms/step.
Train [16/40]. 945.94 samples/sec. 270.629 ms/step.
Train [24/40]. 945.93 samples/sec. 270.633 ms/step.
Train [32/40]. 945.93 samples/sec. 270.632 ms/step.
Train [40/40]. 945.94 samples/sec. 270.631 ms/step.
Train benchmark of res2net50d.in1k done. 941.83 samples/sec, 270.63 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model res2net101_26w_4s.in1k created, param count: 45206688
Running inference benchmark on res2net101_26w_4s.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 2351.37 samples/sec. 108.873 ms/step.
Infer [16/40]. 2351.42 samples/sec. 108.870 ms/step.
Infer [24/40]. 2351.45 samples/sec. 108.869 ms/step.
Infer [32/40]. 2351.49 samples/sec. 108.867 ms/step.
Infer [40/40]. 2351.47 samples/sec. 108.868 ms/step.
Inference benchmark of res2net101_26w_4s.in1k done. 2350.87 samples/sec, 108.87 ms/step
Model res2net101_26w_4s.in1k created, param count: 45206688
Running train benchmark on res2net101_26w_4s.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 687.08 samples/sec. 372.589 ms/step.
Train [16/40]. 687.08 samples/sec. 372.592 ms/step.
Train [24/40]. 687.07 samples/sec. 372.595 ms/step.
Train [32/40]. 687.07 samples/sec. 372.596 ms/step.
Train [40/40]. 687.07 samples/sec. 372.597 ms/step.
Train benchmark of res2net101_26w_4s.in1k done. 683.46 samples/sec, 372.60 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model res2net101d.in1k created, param count: 45225920
Running inference benchmark on res2net101d.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 2281.76 samples/sec. 112.194 ms/step.
Infer [16/40]. 2281.66 samples/sec. 112.199 ms/step.
Infer [24/40]. 2281.55 samples/sec. 112.204 ms/step.
Infer [32/40]. 2281.56 samples/sec. 112.204 ms/step.
Infer [40/40]. 2281.54 samples/sec. 112.205 ms/step.
Inference benchmark of res2net101d.in1k done. 2280.97 samples/sec, 112.20 ms/step
Model res2net101d.in1k created, param count: 45225920
Running train benchmark on res2net101d.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 644.84 samples/sec. 396.995 ms/step.
Train [16/40]. 644.87 samples/sec. 396.981 ms/step.
Train [24/40]. 644.87 samples/sec. 396.978 ms/step.
Train [32/40]. 644.88 samples/sec. 396.974 ms/step.
Train [40/40]. 644.88 samples/sec. 396.973 ms/step.
Train benchmark of res2net101d.in1k done. 641.66 samples/sec, 396.97 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model res2next50.in1k created, param count: 24671464
Running inference benchmark on res2next50.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3487.55 samples/sec. 73.404 ms/step.
Infer [16/40]. 3487.55 samples/sec. 73.404 ms/step.
Infer [24/40]. 3487.56 samples/sec. 73.404 ms/step.
Infer [32/40]. 3487.25 samples/sec. 73.410 ms/step.
Infer [40/40]. 3486.86 samples/sec. 73.418 ms/step.
Inference benchmark of res2next50.in1k done. 3485.62 samples/sec, 73.42 ms/step
Model res2next50.in1k created, param count: 24671464
Running train benchmark on res2next50.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1001.09 samples/sec. 255.721 ms/step.
Train [16/40]. 1001.07 samples/sec. 255.727 ms/step.
Train [24/40]. 1001.06 samples/sec. 255.730 ms/step.
Train [32/40]. 1000.90 samples/sec. 255.769 ms/step.
Train [40/40]. 1000.76 samples/sec. 255.807 ms/step.
Train benchmark of res2next50.in1k done. 996.41 samples/sec, 255.81 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resmlp_12_224.fb_dino created, param count: 15350872
Running inference benchmark on resmlp_12_224.fb_dino for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 8644.77 samples/sec. 29.613 ms/step.
Infer [16/40]. 8644.79 samples/sec. 29.613 ms/step.
Infer [24/40]. 8644.97 samples/sec. 29.613 ms/step.
Infer [32/40]. 8645.03 samples/sec. 29.612 ms/step.
Infer [40/40]. 8644.91 samples/sec. 29.613 ms/step.
Inference benchmark of resmlp_12_224.fb_dino done. 8637.98 samples/sec, 29.61 ms/step
Model resmlp_12_224.fb_dino created, param count: 15350872
Running train benchmark on resmlp_12_224.fb_dino for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 2946.29 samples/sec. 86.889 ms/step.
Train [16/40]. 2946.22 samples/sec. 86.891 ms/step.
Train [24/40]. 2946.26 samples/sec. 86.890 ms/step.
Train [32/40]. 2946.30 samples/sec. 86.889 ms/step.
Train [40/40]. 2946.29 samples/sec. 86.889 ms/step.
Train benchmark of resmlp_12_224.fb_dino done. 2925.13 samples/sec, 86.89 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resmlp_12_224.fb_distilled_in1k created, param count: 15350872
Running inference benchmark on resmlp_12_224.fb_distilled_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 8635.74 samples/sec. 29.644 ms/step.
Infer [16/40]. 8632.89 samples/sec. 29.654 ms/step.
Infer [24/40]. 8630.41 samples/sec. 29.663 ms/step.
Infer [32/40]. 8629.45 samples/sec. 29.666 ms/step.
Infer [40/40]. 8629.21 samples/sec. 29.667 ms/step.
Inference benchmark of resmlp_12_224.fb_distilled_in1k done. 8622.19 samples/sec, 29.67 ms/step
Model resmlp_12_224.fb_distilled_in1k created, param count: 15350872
Running train benchmark on resmlp_12_224.fb_distilled_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 2945.38 samples/sec. 86.916 ms/step.
Train [16/40]. 2945.43 samples/sec. 86.914 ms/step.
Train [24/40]. 2945.46 samples/sec. 86.913 ms/step.
Train [32/40]. 2945.47 samples/sec. 86.913 ms/step.
Train [40/40]. 2945.51 samples/sec. 86.912 ms/step.
Train benchmark of resmlp_12_224.fb_distilled_in1k done. 2923.34 samples/sec, 86.91 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resmlp_12_224.fb_in1k created, param count: 15350872
Running inference benchmark on resmlp_12_224.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 8646.95 samples/sec. 29.606 ms/step.
Infer [16/40]. 8646.03 samples/sec. 29.609 ms/step.
Infer [24/40]. 8645.73 samples/sec. 29.610 ms/step.
Infer [32/40]. 8645.54 samples/sec. 29.611 ms/step.
Infer [40/40]. 8645.27 samples/sec. 29.612 ms/step.
Inference benchmark of resmlp_12_224.fb_in1k done. 8638.25 samples/sec, 29.61 ms/step
Model resmlp_12_224.fb_in1k created, param count: 15350872
Running train benchmark on resmlp_12_224.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 2946.23 samples/sec. 86.891 ms/step.
Train [16/40]. 2945.99 samples/sec. 86.898 ms/step.
Train [24/40]. 2945.98 samples/sec. 86.898 ms/step.
Train [32/40]. 2945.95 samples/sec. 86.899 ms/step.
Train [40/40]. 2945.98 samples/sec. 86.898 ms/step.
Train benchmark of resmlp_12_224.fb_in1k done. 2924.50 samples/sec, 86.90 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resmlp_24_224.fb_dino created, param count: 30020680
Running inference benchmark on resmlp_24_224.fb_dino for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 4391.93 samples/sec. 58.289 ms/step.
Infer [16/40]. 4391.87 samples/sec. 58.290 ms/step.
Infer [24/40]. 4391.97 samples/sec. 58.288 ms/step.
Infer [32/40]. 4391.93 samples/sec. 58.289 ms/step.
Infer [40/40]. 4391.34 samples/sec. 58.297 ms/step.
Inference benchmark of resmlp_24_224.fb_dino done. 4389.35 samples/sec, 58.30 ms/step
Model resmlp_24_224.fb_dino created, param count: 30020680
Running train benchmark on resmlp_24_224.fb_dino for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1495.89 samples/sec. 171.136 ms/step.
Train [16/40]. 1495.21 samples/sec. 171.214 ms/step.
Train [24/40]. 1494.94 samples/sec. 171.245 ms/step.
Train [32/40]. 1494.83 samples/sec. 171.257 ms/step.
Train [40/40]. 1494.77 samples/sec. 171.264 ms/step.
Train benchmark of resmlp_24_224.fb_dino done. 1484.25 samples/sec, 171.26 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resmlp_24_224.fb_distilled_in1k created, param count: 30020680
Running inference benchmark on resmlp_24_224.fb_distilled_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 4395.89 samples/sec. 58.236 ms/step.
Infer [16/40]. 4395.67 samples/sec. 58.239 ms/step.
Infer [24/40]. 4394.44 samples/sec. 58.255 ms/step.
Infer [32/40]. 4391.96 samples/sec. 58.288 ms/step.
Infer [40/40]. 4390.56 samples/sec. 58.307 ms/step.
Inference benchmark of resmlp_24_224.fb_distilled_in1k done. 4388.65 samples/sec, 58.31 ms/step
Model resmlp_24_224.fb_distilled_in1k created, param count: 30020680
Running train benchmark on resmlp_24_224.fb_distilled_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1497.39 samples/sec. 170.965 ms/step.
Train [16/40]. 1497.40 samples/sec. 170.963 ms/step.
Train [24/40]. 1497.37 samples/sec. 170.966 ms/step.
Train [32/40]. 1497.37 samples/sec. 170.966 ms/step.
Train [40/40]. 1497.37 samples/sec. 170.966 ms/step.
Train benchmark of resmlp_24_224.fb_distilled_in1k done. 1487.22 samples/sec, 170.97 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resmlp_24_224.fb_in1k created, param count: 30020680
Running inference benchmark on resmlp_24_224.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 4395.30 samples/sec. 58.244 ms/step.
Infer [16/40]. 4395.41 samples/sec. 58.243 ms/step.
Infer [24/40]. 4394.22 samples/sec. 58.258 ms/step.
Infer [32/40]. 4391.56 samples/sec. 58.294 ms/step.
Infer [40/40]. 4389.96 samples/sec. 58.315 ms/step.
Inference benchmark of resmlp_24_224.fb_in1k done. 4388.03 samples/sec, 58.31 ms/step
Model resmlp_24_224.fb_in1k created, param count: 30020680
Running train benchmark on resmlp_24_224.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1497.45 samples/sec. 170.958 ms/step.
Train [16/40]. 1497.36 samples/sec. 170.968 ms/step.
Train [24/40]. 1497.35 samples/sec. 170.969 ms/step.
Train [32/40]. 1497.33 samples/sec. 170.971 ms/step.
Train [40/40]. 1497.35 samples/sec. 170.969 ms/step.
Train benchmark of resmlp_24_224.fb_in1k done. 1487.24 samples/sec, 170.97 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resmlp_36_224.fb_distilled_in1k created, param count: 44690488
Running inference benchmark on resmlp_36_224.fb_distilled_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 2939.70 samples/sec. 87.084 ms/step.
Infer [16/40]. 2939.76 samples/sec. 87.082 ms/step.
Infer [24/40]. 2939.70 samples/sec. 87.084 ms/step.
Infer [32/40]. 2939.65 samples/sec. 87.085 ms/step.
Infer [40/40]. 2939.62 samples/sec. 87.086 ms/step.
Inference benchmark of resmlp_36_224.fb_distilled_in1k done. 2938.74 samples/sec, 87.09 ms/step
Model resmlp_36_224.fb_distilled_in1k created, param count: 44690488
Running train benchmark on resmlp_36_224.fb_distilled_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1003.15 samples/sec. 255.196 ms/step.
Train [16/40]. 1003.19 samples/sec. 255.186 ms/step.
Train [24/40]. 1003.18 samples/sec. 255.189 ms/step.
Train [32/40]. 1003.17 samples/sec. 255.192 ms/step.
Train [40/40]. 1003.18 samples/sec. 255.189 ms/step.
Train benchmark of resmlp_36_224.fb_distilled_in1k done. 996.59 samples/sec, 255.19 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resmlp_36_224.fb_in1k created, param count: 44690488
Running inference benchmark on resmlp_36_224.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 2934.61 samples/sec. 87.235 ms/step.
Infer [16/40]. 2934.83 samples/sec. 87.228 ms/step.
Infer [24/40]. 2934.82 samples/sec. 87.229 ms/step.
Infer [32/40]. 2934.82 samples/sec. 87.228 ms/step.
Infer [40/40]. 2934.82 samples/sec. 87.228 ms/step.
Inference benchmark of resmlp_36_224.fb_in1k done. 2933.94 samples/sec, 87.23 ms/step
Model resmlp_36_224.fb_in1k created, param count: 44690488
Running train benchmark on resmlp_36_224.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1003.27 samples/sec. 255.166 ms/step.
Train [16/40]. 1003.26 samples/sec. 255.169 ms/step.
Train [24/40]. 1003.24 samples/sec. 255.172 ms/step.
Train [32/40]. 1003.23 samples/sec. 255.176 ms/step.
Train [40/40]. 1003.25 samples/sec. 255.171 ms/step.
Train benchmark of resmlp_36_224.fb_in1k done. 996.74 samples/sec, 255.17 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resmlp_big_24_224.fb_distilled_in1k created, param count: 129138280
Running inference benchmark on resmlp_big_24_224.fb_distilled_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 408.58 samples/sec. 626.556 ms/step.
Infer [16/40]. 408.43 samples/sec. 626.789 ms/step.
Infer [24/40]. 408.37 samples/sec. 626.885 ms/step.
Infer [32/40]. 408.33 samples/sec. 626.941 ms/step.
Infer [40/40]. 408.31 samples/sec. 626.969 ms/step.
Inference benchmark of resmlp_big_24_224.fb_distilled_in1k done. 408.28 samples/sec, 626.97 ms/step
Model resmlp_big_24_224.fb_distilled_in1k created, param count: 129138280
Running train benchmark on resmlp_big_24_224.fb_distilled_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 1.15 GiB. GPU 0 has a total capacty of 23.65 GiB of which 60.06 MiB is free. Including non-PyTorch memory, this process has 23.58 GiB memory in use. Of the allocated memory 22.15 GiB is allocated by PyTorch, and 204.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model resmlp_big_24_224.fb_distilled_in1k created, param count: 129138280
Running train benchmark on resmlp_big_24_224.fb_distilled_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 222.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 216.06 MiB is free. Including non-PyTorch memory, this process has 23.43 GiB memory in use. Of the allocated memory 21.84 GiB is allocated by PyTorch, and 362.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model resmlp_big_24_224.fb_distilled_in1k created, param count: 129138280
Running train benchmark on resmlp_big_24_224.fb_distilled_in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 588.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 438.06 MiB is free. Including non-PyTorch memory, this process has 23.21 GiB memory in use. Of the allocated memory 21.89 GiB is allocated by PyTorch, and 94.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model resmlp_big_24_224.fb_distilled_in1k created, param count: 129138280
Running train benchmark on resmlp_big_24_224.fb_distilled_in1k for 40 steps w/ input size (3, 224, 224) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 442.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 102.06 MiB is free. Including non-PyTorch memory, this process has 23.54 GiB memory in use. Of the allocated memory 22.06 GiB is allocated by PyTorch, and 255.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model resmlp_big_24_224.fb_distilled_in1k created, param count: 129138280
Running train benchmark on resmlp_big_24_224.fb_distilled_in1k for 40 steps w/ input size (3, 224, 224) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 294.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 218.06 MiB is free. Including non-PyTorch memory, this process has 23.43 GiB memory in use. Of the allocated memory 22.16 GiB is allocated by PyTorch, and 39.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model resmlp_big_24_224.fb_distilled_in1k created, param count: 129138280
Running train benchmark on resmlp_big_24_224.fb_distilled_in1k for 40 steps w/ input size (3, 224, 224) and batch size 48.
Train [8/40]. 135.28 samples/sec. 354.813 ms/step.
Train [16/40]. 135.28 samples/sec. 354.816 ms/step.
Train [24/40]. 135.28 samples/sec. 354.820 ms/step.
Train [32/40]. 135.28 samples/sec. 354.819 ms/step.
Train [40/40]. 135.28 samples/sec. 354.819 ms/step.
Train benchmark of resmlp_big_24_224.fb_distilled_in1k done. 134.72 samples/sec, 354.82 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resmlp_big_24_224.fb_in1k created, param count: 129138280
Running inference benchmark on resmlp_big_24_224.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 408.29 samples/sec. 627.005 ms/step.
Infer [16/40]. 408.26 samples/sec. 627.055 ms/step.
Infer [24/40]. 408.23 samples/sec. 627.091 ms/step.
Infer [32/40]. 408.22 samples/sec. 627.118 ms/step.
Infer [40/40]. 408.19 samples/sec. 627.152 ms/step.
Inference benchmark of resmlp_big_24_224.fb_in1k done. 408.16 samples/sec, 627.15 ms/step
Model resmlp_big_24_224.fb_in1k created, param count: 129138280
Running train benchmark on resmlp_big_24_224.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 1.15 GiB. GPU 0 has a total capacty of 23.65 GiB of which 60.06 MiB is free. Including non-PyTorch memory, this process has 23.58 GiB memory in use. Of the allocated memory 22.15 GiB is allocated by PyTorch, and 204.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model resmlp_big_24_224.fb_in1k created, param count: 129138280
Running train benchmark on resmlp_big_24_224.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 222.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 216.06 MiB is free. Including non-PyTorch memory, this process has 23.43 GiB memory in use. Of the allocated memory 21.84 GiB is allocated by PyTorch, and 362.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model resmlp_big_24_224.fb_in1k created, param count: 129138280
Running train benchmark on resmlp_big_24_224.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 588.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 438.06 MiB is free. Including non-PyTorch memory, this process has 23.21 GiB memory in use. Of the allocated memory 21.89 GiB is allocated by PyTorch, and 94.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model resmlp_big_24_224.fb_in1k created, param count: 129138280
Running train benchmark on resmlp_big_24_224.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 442.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 102.06 MiB is free. Including non-PyTorch memory, this process has 23.54 GiB memory in use. Of the allocated memory 22.06 GiB is allocated by PyTorch, and 255.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model resmlp_big_24_224.fb_in1k created, param count: 129138280
Running train benchmark on resmlp_big_24_224.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 294.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 218.06 MiB is free. Including non-PyTorch memory, this process has 23.43 GiB memory in use. Of the allocated memory 22.16 GiB is allocated by PyTorch, and 39.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model resmlp_big_24_224.fb_in1k created, param count: 129138280
Running train benchmark on resmlp_big_24_224.fb_in1k for 40 steps w/ input size (3, 224, 224) and batch size 48.
Train [8/40]. 135.27 samples/sec. 354.845 ms/step.
Train [16/40]. 135.26 samples/sec. 354.865 ms/step.
Train [24/40]. 135.26 samples/sec. 354.868 ms/step.
Train [32/40]. 135.26 samples/sec. 354.869 ms/step.
Train [40/40]. 135.26 samples/sec. 354.871 ms/step.
Train benchmark of resmlp_big_24_224.fb_in1k done. 134.69 samples/sec, 354.87 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resmlp_big_24_224.fb_in22k_ft_in1k created, param count: 129138280
Running inference benchmark on resmlp_big_24_224.fb_in22k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 408.30 samples/sec. 626.993 ms/step.
Infer [16/40]. 408.28 samples/sec. 627.027 ms/step.
Infer [24/40]. 408.27 samples/sec. 627.042 ms/step.
Infer [32/40]. 408.18 samples/sec. 627.178 ms/step.
Infer [40/40]. 408.09 samples/sec. 627.305 ms/step.
Inference benchmark of resmlp_big_24_224.fb_in22k_ft_in1k done. 408.06 samples/sec, 627.30 ms/step
Model resmlp_big_24_224.fb_in22k_ft_in1k created, param count: 129138280
Running train benchmark on resmlp_big_24_224.fb_in22k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 1.15 GiB. GPU 0 has a total capacty of 23.65 GiB of which 60.06 MiB is free. Including non-PyTorch memory, this process has 23.58 GiB memory in use. Of the allocated memory 22.15 GiB is allocated by PyTorch, and 204.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model resmlp_big_24_224.fb_in22k_ft_in1k created, param count: 129138280
Running train benchmark on resmlp_big_24_224.fb_in22k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 222.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 216.06 MiB is free. Including non-PyTorch memory, this process has 23.43 GiB memory in use. Of the allocated memory 21.84 GiB is allocated by PyTorch, and 362.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model resmlp_big_24_224.fb_in22k_ft_in1k created, param count: 129138280
Running train benchmark on resmlp_big_24_224.fb_in22k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 588.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 438.06 MiB is free. Including non-PyTorch memory, this process has 23.21 GiB memory in use. Of the allocated memory 21.89 GiB is allocated by PyTorch, and 94.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model resmlp_big_24_224.fb_in22k_ft_in1k created, param count: 129138280
Running train benchmark on resmlp_big_24_224.fb_in22k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 442.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 102.06 MiB is free. Including non-PyTorch memory, this process has 23.54 GiB memory in use. Of the allocated memory 22.06 GiB is allocated by PyTorch, and 255.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model resmlp_big_24_224.fb_in22k_ft_in1k created, param count: 129138280
Running train benchmark on resmlp_big_24_224.fb_in22k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 294.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 218.06 MiB is free. Including non-PyTorch memory, this process has 23.43 GiB memory in use. Of the allocated memory 22.16 GiB is allocated by PyTorch, and 39.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model resmlp_big_24_224.fb_in22k_ft_in1k created, param count: 129138280
Running train benchmark on resmlp_big_24_224.fb_in22k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 48.
Train [8/40]. 135.30 samples/sec. 354.774 ms/step.
Train [16/40]. 135.30 samples/sec. 354.770 ms/step.
Train [24/40]. 135.30 samples/sec. 354.776 ms/step.
Train [32/40]. 135.30 samples/sec. 354.777 ms/step.
Train [40/40]. 135.30 samples/sec. 354.777 ms/step.
Train benchmark of resmlp_big_24_224.fb_in22k_ft_in1k done. 134.74 samples/sec, 354.78 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnest14d.gluon_in1k created, param count: 10611688
Running inference benchmark on resnest14d.gluon_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 6053.18 samples/sec. 42.292 ms/step.
Infer [16/40]. 6052.85 samples/sec. 42.294 ms/step.
Infer [24/40]. 6052.71 samples/sec. 42.295 ms/step.
Infer [32/40]. 6052.85 samples/sec. 42.294 ms/step.
Infer [40/40]. 6052.71 samples/sec. 42.295 ms/step.
Inference benchmark of resnest14d.gluon_in1k done. 6049.09 samples/sec, 42.30 ms/step
Model resnest14d.gluon_in1k created, param count: 10611688
Running train benchmark on resnest14d.gluon_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1647.17 samples/sec. 155.418 ms/step.
Train [16/40]. 1647.01 samples/sec. 155.433 ms/step.
Train [24/40]. 1647.06 samples/sec. 155.428 ms/step.
Train [32/40]. 1647.04 samples/sec. 155.431 ms/step.
Train [40/40]. 1647.06 samples/sec. 155.429 ms/step.
Train benchmark of resnest14d.gluon_in1k done. 1641.34 samples/sec, 155.43 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnest26d.gluon_in1k created, param count: 17069448
Running inference benchmark on resnest26d.gluon_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 4403.52 samples/sec. 58.135 ms/step.
Infer [16/40]. 4403.68 samples/sec. 58.133 ms/step.
Infer [24/40]. 4403.63 samples/sec. 58.134 ms/step.
Infer [32/40]. 4403.53 samples/sec. 58.135 ms/step.
Infer [40/40]. 4403.53 samples/sec. 58.135 ms/step.
Inference benchmark of resnest26d.gluon_in1k done. 4401.55 samples/sec, 58.13 ms/step
Model resnest26d.gluon_in1k created, param count: 17069448
Running train benchmark on resnest26d.gluon_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1240.25 samples/sec. 206.410 ms/step.
Train [16/40]. 1240.19 samples/sec. 206.421 ms/step.
Train [24/40]. 1240.19 samples/sec. 206.419 ms/step.
Train [32/40]. 1240.19 samples/sec. 206.419 ms/step.
Train [40/40]. 1240.19 samples/sec. 206.421 ms/step.
Train benchmark of resnest26d.gluon_in1k done. 1235.52 samples/sec, 206.42 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnest50d.in1k created, param count: 27483240
Running inference benchmark on resnest50d.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3027.50 samples/sec. 84.558 ms/step.
Infer [16/40]. 3027.49 samples/sec. 84.559 ms/step.
Infer [24/40]. 3027.48 samples/sec. 84.559 ms/step.
Infer [32/40]. 3027.43 samples/sec. 84.560 ms/step.
Infer [40/40]. 3027.37 samples/sec. 84.562 ms/step.
Inference benchmark of resnest50d.in1k done. 3026.40 samples/sec, 84.56 ms/step
Model resnest50d.in1k created, param count: 27483240
Running train benchmark on resnest50d.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 874.57 samples/sec. 292.716 ms/step.
Train [16/40]. 874.56 samples/sec. 292.717 ms/step.
Train [24/40]. 874.57 samples/sec. 292.714 ms/step.
Train [32/40]. 874.58 samples/sec. 292.713 ms/step.
Train [40/40]. 874.58 samples/sec. 292.713 ms/step.
Train benchmark of resnest50d.in1k done. 870.82 samples/sec, 292.71 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnest50d_1s4x24d.in1k created, param count: 25677000
Running inference benchmark on resnest50d_1s4x24d.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3474.05 samples/sec. 73.689 ms/step.
Infer [16/40]. 3473.98 samples/sec. 73.691 ms/step.
Infer [24/40]. 3473.87 samples/sec. 73.693 ms/step.
Infer [32/40]. 3473.97 samples/sec. 73.691 ms/step.
Infer [40/40]. 3473.98 samples/sec. 73.691 ms/step.
Inference benchmark of resnest50d_1s4x24d.in1k done. 3472.73 samples/sec, 73.69 ms/step
Model resnest50d_1s4x24d.in1k created, param count: 25677000
Running train benchmark on resnest50d_1s4x24d.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 943.83 samples/sec. 271.235 ms/step.
Train [16/40]. 943.83 samples/sec. 271.234 ms/step.
Train [24/40]. 943.80 samples/sec. 271.243 ms/step.
Train [32/40]. 943.80 samples/sec. 271.243 ms/step.
Train [40/40]. 943.80 samples/sec. 271.245 ms/step.
Train benchmark of resnest50d_1s4x24d.in1k done. 939.49 samples/sec, 271.25 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnest50d_4s2x40d.in1k created, param count: 30417592
Running inference benchmark on resnest50d_4s2x40d.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 2286.65 samples/sec. 111.954 ms/step.
Infer [16/40]. 2286.71 samples/sec. 111.951 ms/step.
Infer [24/40]. 2286.81 samples/sec. 111.946 ms/step.
Infer [32/40]. 2286.81 samples/sec. 111.946 ms/step.
Infer [40/40]. 2286.87 samples/sec. 111.943 ms/step.
Inference benchmark of resnest50d_4s2x40d.in1k done. 2286.28 samples/sec, 111.94 ms/step
Model resnest50d_4s2x40d.in1k created, param count: 30417592
Running train benchmark on resnest50d_4s2x40d.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 674.68 samples/sec. 379.438 ms/step.
Train [16/40]. 674.62 samples/sec. 379.474 ms/step.
Train [24/40]. 674.65 samples/sec. 379.457 ms/step.
Train [32/40]. 674.66 samples/sec. 379.450 ms/step.
Train [40/40]. 674.66 samples/sec. 379.448 ms/step.
Train benchmark of resnest50d_4s2x40d.in1k done. 672.26 samples/sec, 379.45 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnest101e.in1k created, param count: 48275016
Running inference benchmark on resnest101e.in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
Infer [8/40]. 1510.91 samples/sec. 169.434 ms/step.
Infer [16/40]. 1510.93 samples/sec. 169.432 ms/step.
Infer [24/40]. 1510.98 samples/sec. 169.427 ms/step.
Infer [32/40]. 1510.92 samples/sec. 169.433 ms/step.
Infer [40/40]. 1510.95 samples/sec. 169.430 ms/step.
Inference benchmark of resnest101e.in1k done. 1510.65 samples/sec, 169.43 ms/step
Model resnest101e.in1k created, param count: 48275016
Running train benchmark on resnest101e.in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 128.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 118.06 MiB is free. Including non-PyTorch memory, this process has 23.53 GiB memory in use. Of the allocated memory 22.24 GiB is allocated by PyTorch, and 57.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model resnest101e.in1k created, param count: 48275016
Running train benchmark on resnest101e.in1k for 40 steps w/ input size (3, 256, 256) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 46.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 22.22 GiB is allocated by PyTorch, and 149.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model resnest101e.in1k created, param count: 48275016
Running train benchmark on resnest101e.in1k for 40 steps w/ input size (3, 256, 256) and batch size 128.
Train [8/40]. 467.89 samples/sec. 273.567 ms/step.
Train [16/40]. 467.93 samples/sec. 273.545 ms/step.
Train [24/40]. 467.96 samples/sec. 273.527 ms/step.
Train [32/40]. 467.96 samples/sec. 273.528 ms/step.
Train [40/40]. 467.95 samples/sec. 273.531 ms/step.
Train benchmark of resnest101e.in1k done. 464.47 samples/sec, 273.53 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnest200e.in1k created, param count: 70201544
Running inference benchmark on resnest200e.in1k for 40 steps w/ input size (3, 320, 320) and batch size 256.
Infer [8/40]. 507.09 samples/sec. 504.837 ms/step.
Infer [16/40]. 507.10 samples/sec. 504.836 ms/step.
Infer [24/40]. 507.09 samples/sec. 504.839 ms/step.
Infer [32/40]. 507.10 samples/sec. 504.835 ms/step.
Infer [40/40]. 507.09 samples/sec. 504.838 ms/step.
Inference benchmark of resnest200e.in1k done. 507.06 samples/sec, 504.84 ms/step
Model resnest200e.in1k created, param count: 70201544
Running train benchmark on resnest200e.in1k for 40 steps w/ input size (3, 320, 320) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 400.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 38.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 22.17 GiB is allocated by PyTorch, and 205.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model resnest200e.in1k created, param count: 70201544
Running train benchmark on resnest200e.in1k for 40 steps w/ input size (3, 320, 320) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 300.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 92.06 MiB is free. Including non-PyTorch memory, this process has 23.55 GiB memory in use. Of the allocated memory 21.87 GiB is allocated by PyTorch, and 460.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model resnest200e.in1k created, param count: 70201544
Running train benchmark on resnest200e.in1k for 40 steps w/ input size (3, 320, 320) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 44.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 21.90 GiB is allocated by PyTorch, and 472.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model resnest200e.in1k created, param count: 70201544
Running train benchmark on resnest200e.in1k for 40 steps w/ input size (3, 320, 320) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 76.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 34.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 21.88 GiB is allocated by PyTorch, and 511.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model resnest200e.in1k created, param count: 70201544
Running train benchmark on resnest200e.in1k for 40 steps w/ input size (3, 320, 320) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 48.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 22.09 GiB is allocated by PyTorch, and 281.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model resnest200e.in1k created, param count: 70201544
Running train benchmark on resnest200e.in1k for 40 steps w/ input size (3, 320, 320) and batch size 48.
Train [8/40]. 164.33 samples/sec. 292.095 ms/step.
Train [16/40]. 164.41 samples/sec. 291.950 ms/step.
Train [24/40]. 164.51 samples/sec. 291.779 ms/step.
Train [32/40]. 164.79 samples/sec. 291.283 ms/step.
Train [40/40]. 165.28 samples/sec. 290.414 ms/step.
Train benchmark of resnest200e.in1k done. 163.39 samples/sec, 290.41 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnest269e.in1k created, param count: 110929480
Running inference benchmark on resnest269e.in1k for 40 steps w/ input size (3, 416, 416) and batch size 256.
Infer [8/40]. 239.39 samples/sec. 1069.381 ms/step.
Infer [16/40]. 239.37 samples/sec. 1069.465 ms/step.
Infer [24/40]. 239.37 samples/sec. 1069.492 ms/step.
Infer [32/40]. 239.36 samples/sec. 1069.509 ms/step.
Infer [40/40]. 239.36 samples/sec. 1069.528 ms/step.
Inference benchmark of resnest269e.in1k done. 239.35 samples/sec, 1069.53 ms/step
Model resnest269e.in1k created, param count: 110929480
Running train benchmark on resnest269e.in1k for 40 steps w/ input size (3, 416, 416) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 1.32 GiB. GPU 0 has a total capacty of 23.65 GiB of which 726.06 MiB is free. Including non-PyTorch memory, this process has 22.93 GiB memory in use. Of the allocated memory 20.94 GiB is allocated by PyTorch, and 780.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model resnest269e.in1k created, param count: 110929480
Running train benchmark on resnest269e.in1k for 40 steps w/ input size (3, 416, 416) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 254.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 210.06 MiB is free. Including non-PyTorch memory, this process has 23.44 GiB memory in use. Of the allocated memory 21.95 GiB is allocated by PyTorch, and 257.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model resnest269e.in1k created, param count: 110929480
Running train benchmark on resnest269e.in1k for 40 steps w/ input size (3, 416, 416) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 338.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 160.06 MiB is free. Including non-PyTorch memory, this process has 23.48 GiB memory in use. Of the allocated memory 22.06 GiB is allocated by PyTorch, and 196.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model resnest269e.in1k created, param count: 110929480
Running train benchmark on resnest269e.in1k for 40 steps w/ input size (3, 416, 416) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 20.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.19 GiB is allocated by PyTorch, and 201.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model resnest269e.in1k created, param count: 110929480
Running train benchmark on resnest269e.in1k for 40 steps w/ input size (3, 416, 416) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 44.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 2.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 22.28 GiB is allocated by PyTorch, and 125.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model resnest269e.in1k created, param count: 110929480
Running train benchmark on resnest269e.in1k for 40 steps w/ input size (3, 416, 416) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 128.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 42.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 22.20 GiB is allocated by PyTorch, and 174.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model resnest269e.in1k created, param count: 110929480
Running train benchmark on resnest269e.in1k for 40 steps w/ input size (3, 416, 416) and batch size 32.
ERROR: "CUDA out of memory. Tried to allocate 44.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 16.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.07 GiB is allocated by PyTorch, and 331.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model resnest269e.in1k created, param count: 110929480
Running train benchmark on resnest269e.in1k for 40 steps w/ input size (3, 416, 416) and batch size 24.
Train [8/40]. 72.67 samples/sec. 330.239 ms/step.
Train [16/40]. 72.68 samples/sec. 330.195 ms/step.
Train [24/40]. 72.69 samples/sec. 330.181 ms/step.
Train [32/40]. 72.68 samples/sec. 330.198 ms/step.
Train [40/40]. 72.69 samples/sec. 330.184 ms/step.
Train benchmark of resnest269e.in1k done. 71.71 samples/sec, 330.18 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnet10t.c3_in1k created, param count: 5435488
Running inference benchmark on resnet10t.c3_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 11184.87 samples/sec. 22.888 ms/step.
Infer [16/40]. 11238.72 samples/sec. 22.778 ms/step.
Infer [24/40]. 11203.65 samples/sec. 22.850 ms/step.
Infer [32/40]. 11191.69 samples/sec. 22.874 ms/step.
Infer [40/40]. 11169.80 samples/sec. 22.919 ms/step.
Inference benchmark of resnet10t.c3_in1k done. 11157.99 samples/sec, 22.92 ms/step
Model resnet10t.c3_in1k created, param count: 5435488
Running train benchmark on resnet10t.c3_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 3488.91 samples/sec. 73.375 ms/step.
Train [16/40]. 3488.34 samples/sec. 73.387 ms/step.
Train [24/40]. 3488.89 samples/sec. 73.376 ms/step.
Train [32/40]. 3490.39 samples/sec. 73.344 ms/step.
Train [40/40]. 3492.82 samples/sec. 73.293 ms/step.
Train benchmark of resnet10t.c3_in1k done. 3478.12 samples/sec, 73.29 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnet14t.c3_in1k created, param count: 10081632
Running inference benchmark on resnet14t.c3_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 8330.09 samples/sec. 30.732 ms/step.
Infer [16/40]. 8330.13 samples/sec. 30.732 ms/step.
Infer [24/40]. 8345.11 samples/sec. 30.677 ms/step.
Infer [32/40]. 8342.07 samples/sec. 30.688 ms/step.
Infer [40/40]. 8340.01 samples/sec. 30.695 ms/step.
Inference benchmark of resnet14t.c3_in1k done. 8332.93 samples/sec, 30.70 ms/step
Model resnet14t.c3_in1k created, param count: 10081632
Running train benchmark on resnet14t.c3_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 2307.93 samples/sec. 110.922 ms/step.
Train [16/40]. 2308.33 samples/sec. 110.903 ms/step.
Train [24/40]. 2308.18 samples/sec. 110.910 ms/step.
Train [32/40]. 2308.61 samples/sec. 110.889 ms/step.
Train [40/40]. 2308.06 samples/sec. 110.915 ms/step.
Train benchmark of resnet14t.c3_in1k done. 2299.64 samples/sec, 110.92 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnet18.a1_in1k created, param count: 11689512
Running inference benchmark on resnet18.a1_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 8617.87 samples/sec. 29.706 ms/step.
Infer [16/40]. 8617.36 samples/sec. 29.707 ms/step.
Infer [24/40]. 8617.34 samples/sec. 29.708 ms/step.
Infer [32/40]. 8616.77 samples/sec. 29.710 ms/step.
Infer [40/40]. 8616.48 samples/sec. 29.711 ms/step.
Inference benchmark of resnet18.a1_in1k done. 8609.02 samples/sec, 29.71 ms/step
Model resnet18.a1_in1k created, param count: 11689512
Running train benchmark on resnet18.a1_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 2347.74 samples/sec. 109.041 ms/step.
Train [16/40]. 2347.72 samples/sec. 109.042 ms/step.
Train [24/40]. 2347.71 samples/sec. 109.043 ms/step.
Train [32/40]. 2347.62 samples/sec. 109.047 ms/step.
Train [40/40]. 2347.61 samples/sec. 109.047 ms/step.
Train benchmark of resnet18.a1_in1k done. 2338.72 samples/sec, 109.05 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnet18.a2_in1k created, param count: 11689512
Running inference benchmark on resnet18.a2_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 8619.35 samples/sec. 29.701 ms/step.
Infer [16/40]. 8617.70 samples/sec. 29.706 ms/step.
Infer [24/40]. 8617.46 samples/sec. 29.707 ms/step.
Infer [32/40]. 8617.26 samples/sec. 29.708 ms/step.
Infer [40/40]. 8617.01 samples/sec. 29.709 ms/step.
Inference benchmark of resnet18.a2_in1k done. 8609.55 samples/sec, 29.71 ms/step
Model resnet18.a2_in1k created, param count: 11689512
Running train benchmark on resnet18.a2_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 2350.21 samples/sec. 108.926 ms/step.
Train [16/40]. 2350.13 samples/sec. 108.930 ms/step.
Train [24/40]. 2350.18 samples/sec. 108.928 ms/step.
Train [32/40]. 2350.25 samples/sec. 108.924 ms/step.
Train [40/40]. 2350.24 samples/sec. 108.925 ms/step.
Train benchmark of resnet18.a2_in1k done. 2341.24 samples/sec, 108.92 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnet18.a3_in1k created, param count: 11689512
Running inference benchmark on resnet18.a3_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 7626.50 samples/sec. 33.567 ms/step.
Infer [16/40]. 7709.43 samples/sec. 33.206 ms/step.
Infer [24/40]. 7697.93 samples/sec. 33.256 ms/step.
Infer [32/40]. 7713.92 samples/sec. 33.187 ms/step.
Infer [40/40]. 7761.72 samples/sec. 32.982 ms/step.
Inference benchmark of resnet18.a3_in1k done. 7755.64 samples/sec, 32.98 ms/step
Model resnet18.a3_in1k created, param count: 11689512
Running train benchmark on resnet18.a3_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 3049.30 samples/sec. 83.954 ms/step.
Train [16/40]. 3062.39 samples/sec. 83.595 ms/step.
Train [24/40]. 3063.66 samples/sec. 83.560 ms/step.
Train [32/40]. 3065.70 samples/sec. 83.504 ms/step.
Train [40/40]. 3066.31 samples/sec. 83.488 ms/step.
Train benchmark of resnet18.a3_in1k done. 3052.37 samples/sec, 83.49 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnet18.fb_ssl_yfcc100m_ft_in1k created, param count: 11689512
Running inference benchmark on resnet18.fb_ssl_yfcc100m_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 7591.35 samples/sec. 33.723 ms/step.
Infer [16/40]. 7637.24 samples/sec. 33.520 ms/step.
Infer [24/40]. 7732.82 samples/sec. 33.106 ms/step.
Infer [32/40]. 7719.11 samples/sec. 33.164 ms/step.
Infer [40/40]. 7730.23 samples/sec. 33.117 ms/step.
Inference benchmark of resnet18.fb_ssl_yfcc100m_ft_in1k done. 7724.00 samples/sec, 33.12 ms/step
Model resnet18.fb_ssl_yfcc100m_ft_in1k created, param count: 11689512
Running train benchmark on resnet18.fb_ssl_yfcc100m_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 3141.93 samples/sec. 81.479 ms/step.
Train [16/40]. 3130.07 samples/sec. 81.787 ms/step.
Train [24/40]. 3131.64 samples/sec. 81.746 ms/step.
Train [32/40]. 3133.05 samples/sec. 81.709 ms/step.
Train [40/40]. 3126.94 samples/sec. 81.869 ms/step.
Train benchmark of resnet18.fb_ssl_yfcc100m_ft_in1k done. 3112.18 samples/sec, 81.87 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnet18.fb_swsl_ig1b_ft_in1k created, param count: 11689512
Running inference benchmark on resnet18.fb_swsl_ig1b_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 7522.57 samples/sec. 34.031 ms/step.
Infer [16/40]. 7703.98 samples/sec. 33.230 ms/step.
Infer [24/40]. 7765.55 samples/sec. 32.966 ms/step.
Infer [32/40]. 7799.82 samples/sec. 32.821 ms/step.
Infer [40/40]. 7804.59 samples/sec. 32.801 ms/step.
Inference benchmark of resnet18.fb_swsl_ig1b_ft_in1k done. 7798.44 samples/sec, 32.80 ms/step
Model resnet18.fb_swsl_ig1b_ft_in1k created, param count: 11689512
Running train benchmark on resnet18.fb_swsl_ig1b_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 3109.10 samples/sec. 82.339 ms/step.
Train [16/40]. 3099.92 samples/sec. 82.583 ms/step.
Train [24/40]. 3111.90 samples/sec. 82.265 ms/step.
Train [32/40]. 3112.65 samples/sec. 82.245 ms/step.
Train [40/40]. 3115.15 samples/sec. 82.179 ms/step.
Train benchmark of resnet18.fb_swsl_ig1b_ft_in1k done. 3100.73 samples/sec, 82.18 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnet18.gluon_in1k created, param count: 11689512
Running inference benchmark on resnet18.gluon_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 7594.02 samples/sec. 33.711 ms/step.
Infer [16/40]. 7680.04 samples/sec. 33.333 ms/step.
Infer [24/40]. 7741.07 samples/sec. 33.070 ms/step.
Infer [32/40]. 7707.67 samples/sec. 33.214 ms/step.
Infer [40/40]. 7708.90 samples/sec. 33.208 ms/step.
Inference benchmark of resnet18.gluon_in1k done. 7702.82 samples/sec, 33.21 ms/step
Model resnet18.gluon_in1k created, param count: 11689512
Running train benchmark on resnet18.gluon_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 3138.06 samples/sec. 81.579 ms/step.
Train [16/40]. 3126.58 samples/sec. 81.879 ms/step.
Train [24/40]. 3132.50 samples/sec. 81.724 ms/step.
Train [32/40]. 3134.60 samples/sec. 81.669 ms/step.
Train [40/40]. 3131.42 samples/sec. 81.752 ms/step.
Train benchmark of resnet18.gluon_in1k done. 3116.90 samples/sec, 81.75 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnet18.tv_in1k created, param count: 11689512
Running inference benchmark on resnet18.tv_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 7672.88 samples/sec. 33.364 ms/step.
Infer [16/40]. 7840.14 samples/sec. 32.652 ms/step.
Infer [24/40]. 7856.90 samples/sec. 32.583 ms/step.
Infer [32/40]. 7822.64 samples/sec. 32.726 ms/step.
Infer [40/40]. 7819.81 samples/sec. 32.737 ms/step.
Inference benchmark of resnet18.tv_in1k done. 7813.69 samples/sec, 32.74 ms/step
Model resnet18.tv_in1k created, param count: 11689512
Running train benchmark on resnet18.tv_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 3109.00 samples/sec. 82.342 ms/step.
Train [16/40]. 3107.98 samples/sec. 82.369 ms/step.
Train [24/40]. 3109.10 samples/sec. 82.339 ms/step.
Train [32/40]. 3111.95 samples/sec. 82.264 ms/step.
Train [40/40]. 3109.68 samples/sec. 82.324 ms/step.
Train benchmark of resnet18.tv_in1k done. 3095.46 samples/sec, 82.32 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnet18d.ra2_in1k created, param count: 11708744
Running inference benchmark on resnet18d.ra2_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 6999.43 samples/sec. 36.574 ms/step.
Infer [16/40]. 6999.17 samples/sec. 36.576 ms/step.
Infer [24/40]. 6999.14 samples/sec. 36.576 ms/step.
Infer [32/40]. 6999.21 samples/sec. 36.576 ms/step.
Infer [40/40]. 6999.20 samples/sec. 36.576 ms/step.
Inference benchmark of resnet18d.ra2_in1k done. 6994.41 samples/sec, 36.58 ms/step
Model resnet18d.ra2_in1k created, param count: 11708744
Running train benchmark on resnet18d.ra2_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 1712.73 samples/sec. 149.469 ms/step.
Train [16/40]. 1712.64 samples/sec. 149.477 ms/step.
Train [24/40]. 1712.63 samples/sec. 149.478 ms/step.
Train [32/40]. 1712.67 samples/sec. 149.474 ms/step.
Train [40/40]. 1712.68 samples/sec. 149.474 ms/step.
Train benchmark of resnet18d.ra2_in1k done. 1707.11 samples/sec, 149.47 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnet26.bt_in1k created, param count: 15995176
Running inference benchmark on resnet26.bt_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 3926.90 samples/sec. 65.191 ms/step.
Infer [16/40]. 3926.62 samples/sec. 65.196 ms/step.
Infer [24/40]. 3926.64 samples/sec. 65.196 ms/step.
Infer [32/40]. 3926.69 samples/sec. 65.195 ms/step.
Infer [40/40]. 3926.60 samples/sec. 65.196 ms/step.
Inference benchmark of resnet26.bt_in1k done. 3925.03 samples/sec, 65.20 ms/step
Model resnet26.bt_in1k created, param count: 15995176
Running train benchmark on resnet26.bt_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 1194.07 samples/sec. 214.393 ms/step.
Train [16/40]. 1194.10 samples/sec. 214.387 ms/step.
Train [24/40]. 1194.08 samples/sec. 214.392 ms/step.
Train [32/40]. 1194.09 samples/sec. 214.389 ms/step.
Train [40/40]. 1194.08 samples/sec. 214.392 ms/step.
Train benchmark of resnet26.bt_in1k done. 1190.42 samples/sec, 214.39 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnet26d.bt_in1k created, param count: 16014408
Running inference benchmark on resnet26d.bt_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 3556.25 samples/sec. 71.986 ms/step.
Infer [16/40]. 3555.27 samples/sec. 72.006 ms/step.
Infer [24/40]. 3554.96 samples/sec. 72.012 ms/step.
Infer [32/40]. 3554.70 samples/sec. 72.017 ms/step.
Infer [40/40]. 3554.58 samples/sec. 72.020 ms/step.
Inference benchmark of resnet26d.bt_in1k done. 3553.26 samples/sec, 72.02 ms/step
Model resnet26d.bt_in1k created, param count: 16014408
Running train benchmark on resnet26d.bt_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 1002.35 samples/sec. 255.400 ms/step.
Train [16/40]. 1002.29 samples/sec. 255.415 ms/step.
Train [24/40]. 1002.27 samples/sec. 255.421 ms/step.
Train [32/40]. 1002.27 samples/sec. 255.421 ms/step.
Train [40/40]. 1002.25 samples/sec. 255.426 ms/step.
Train benchmark of resnet26d.bt_in1k done. 999.37 samples/sec, 255.43 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnet26t.ra2_in1k created, param count: 16011872
Running inference benchmark on resnet26t.ra2_in1k for 40 steps w/ input size (3, 320, 320) and batch size 256.
Infer [8/40]. 3033.01 samples/sec. 84.405 ms/step.
Infer [16/40]. 3032.94 samples/sec. 84.407 ms/step.
Infer [24/40]. 3032.64 samples/sec. 84.415 ms/step.
Infer [32/40]. 3032.50 samples/sec. 84.419 ms/step.
Infer [40/40]. 3032.48 samples/sec. 84.419 ms/step.
Inference benchmark of resnet26t.ra2_in1k done. 3031.49 samples/sec, 84.42 ms/step
Model resnet26t.ra2_in1k created, param count: 16011872
Running train benchmark on resnet26t.ra2_in1k for 40 steps w/ input size (3, 320, 320) and batch size 256.
Train [8/40]. 832.62 samples/sec. 307.462 ms/step.
Train [16/40]. 832.95 samples/sec. 307.342 ms/step.
Train [24/40]. 833.19 samples/sec. 307.254 ms/step.
Train [32/40]. 833.19 samples/sec. 307.255 ms/step.
Train [40/40]. 833.21 samples/sec. 307.247 ms/step.
Train benchmark of resnet26t.ra2_in1k done. 831.14 samples/sec, 307.25 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnet32ts.ra2_in1k created, param count: 17963616
Running inference benchmark on resnet32ts.ra2_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 3247.63 samples/sec. 78.827 ms/step.
Infer [16/40]. 3247.40 samples/sec. 78.832 ms/step.
Infer [24/40]. 3247.29 samples/sec. 78.835 ms/step.
Infer [32/40]. 3247.28 samples/sec. 78.835 ms/step.
Infer [40/40]. 3247.24 samples/sec. 78.836 ms/step.
Inference benchmark of resnet32ts.ra2_in1k done. 3246.14 samples/sec, 78.84 ms/step
Model resnet32ts.ra2_in1k created, param count: 17963616
Running train benchmark on resnet32ts.ra2_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 883.84 samples/sec. 289.645 ms/step.
Train [16/40]. 883.88 samples/sec. 289.632 ms/step.
Train [24/40]. 883.90 samples/sec. 289.625 ms/step.
Train [32/40]. 883.88 samples/sec. 289.631 ms/step.
Train [40/40]. 883.87 samples/sec. 289.635 ms/step.
Train benchmark of resnet32ts.ra2_in1k done. 881.40 samples/sec, 289.63 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnet33ts.ra2_in1k created, param count: 19676256
Running inference benchmark on resnet33ts.ra2_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 3209.26 samples/sec. 79.769 ms/step.
Infer [16/40]. 3209.15 samples/sec. 79.772 ms/step.
Infer [24/40]. 3209.03 samples/sec. 79.775 ms/step.
Infer [32/40]. 3209.00 samples/sec. 79.776 ms/step.
Infer [40/40]. 3208.92 samples/sec. 79.778 ms/step.
Inference benchmark of resnet33ts.ra2_in1k done. 3207.87 samples/sec, 79.78 ms/step
Model resnet33ts.ra2_in1k created, param count: 19676256
Running train benchmark on resnet33ts.ra2_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 874.80 samples/sec. 292.638 ms/step.
Train [16/40]. 874.79 samples/sec. 292.643 ms/step.
Train [24/40]. 874.79 samples/sec. 292.641 ms/step.
Train [32/40]. 874.81 samples/sec. 292.636 ms/step.
Train [40/40]. 874.81 samples/sec. 292.636 ms/step.
Train benchmark of resnet33ts.ra2_in1k done. 872.38 samples/sec, 292.64 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnet34.a1_in1k created, param count: 21797672
Running inference benchmark on resnet34.a1_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 5210.71 samples/sec. 49.130 ms/step.
Infer [16/40]. 5210.27 samples/sec. 49.134 ms/step.
Infer [24/40]. 5209.97 samples/sec. 49.137 ms/step.
Infer [32/40]. 5209.63 samples/sec. 49.140 ms/step.
Infer [40/40]. 5209.53 samples/sec. 49.141 ms/step.
Inference benchmark of resnet34.a1_in1k done. 5206.84 samples/sec, 49.14 ms/step
Model resnet34.a1_in1k created, param count: 21797672
Running train benchmark on resnet34.a1_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 1452.34 samples/sec. 176.267 ms/step.
Train [16/40]. 1452.27 samples/sec. 176.276 ms/step.
Train [24/40]. 1452.19 samples/sec. 176.286 ms/step.
Train [32/40]. 1452.19 samples/sec. 176.286 ms/step.
Train [40/40]. 1452.17 samples/sec. 176.288 ms/step.
Train benchmark of resnet34.a1_in1k done. 1446.99 samples/sec, 176.29 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnet34.a2_in1k created, param count: 21797672
Running inference benchmark on resnet34.a2_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 5212.74 samples/sec. 49.110 ms/step.
Infer [16/40]. 5212.16 samples/sec. 49.116 ms/step.
Infer [24/40]. 5211.82 samples/sec. 49.119 ms/step.
Infer [32/40]. 5211.68 samples/sec. 49.120 ms/step.
Infer [40/40]. 5211.49 samples/sec. 49.122 ms/step.
Inference benchmark of resnet34.a2_in1k done. 5208.82 samples/sec, 49.12 ms/step
Model resnet34.a2_in1k created, param count: 21797672
Running train benchmark on resnet34.a2_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 1452.13 samples/sec. 176.292 ms/step.
Train [16/40]. 1452.13 samples/sec. 176.292 ms/step.
Train [24/40]. 1452.16 samples/sec. 176.289 ms/step.
Train [32/40]. 1452.16 samples/sec. 176.289 ms/step.
Train [40/40]. 1452.18 samples/sec. 176.287 ms/step.
Train benchmark of resnet34.a2_in1k done. 1446.88 samples/sec, 176.29 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnet34.a3_in1k created, param count: 21797672
Running inference benchmark on resnet34.a3_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 4398.72 samples/sec. 58.199 ms/step.
Infer [16/40]. 4417.49 samples/sec. 57.952 ms/step.
Infer [24/40]. 4430.57 samples/sec. 57.780 ms/step.
Infer [32/40]. 4454.77 samples/sec. 57.467 ms/step.
Infer [40/40]. 4448.18 samples/sec. 57.552 ms/step.
Inference benchmark of resnet34.a3_in1k done. 4446.19 samples/sec, 57.55 ms/step
Model resnet34.a3_in1k created, param count: 21797672
Running train benchmark on resnet34.a3_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1797.56 samples/sec. 142.415 ms/step.
Train [16/40]. 1793.46 samples/sec. 142.741 ms/step.
Train [24/40]. 1791.05 samples/sec. 142.933 ms/step.
Train [32/40]. 1791.84 samples/sec. 142.870 ms/step.
Train [40/40]. 1790.31 samples/sec. 142.992 ms/step.
Train benchmark of resnet34.a3_in1k done. 1783.03 samples/sec, 142.99 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnet34.bt_in1k created, param count: 21797672
Running inference benchmark on resnet34.bt_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 5211.81 samples/sec. 49.119 ms/step.
Infer [16/40]. 5211.59 samples/sec. 49.121 ms/step.
Infer [24/40]. 5211.43 samples/sec. 49.123 ms/step.
Infer [32/40]. 5211.26 samples/sec. 49.124 ms/step.
Infer [40/40]. 5211.16 samples/sec. 49.125 ms/step.
Inference benchmark of resnet34.bt_in1k done. 5208.47 samples/sec, 49.12 ms/step
Model resnet34.bt_in1k created, param count: 21797672
Running train benchmark on resnet34.bt_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 1451.97 samples/sec. 176.312 ms/step.
Train [16/40]. 1451.99 samples/sec. 176.310 ms/step.
Train [24/40]. 1452.03 samples/sec. 176.305 ms/step.
Train [32/40]. 1452.06 samples/sec. 176.301 ms/step.
Train [40/40]. 1452.06 samples/sec. 176.301 ms/step.
Train benchmark of resnet34.bt_in1k done. 1446.57 samples/sec, 176.30 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnet34.gluon_in1k created, param count: 21797672
Running inference benchmark on resnet34.gluon_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 4406.03 samples/sec. 58.102 ms/step.
Infer [16/40]. 4453.71 samples/sec. 57.480 ms/step.
Infer [24/40]. 4433.55 samples/sec. 57.742 ms/step.
Infer [32/40]. 4432.24 samples/sec. 57.759 ms/step.
Infer [40/40]. 4424.14 samples/sec. 57.864 ms/step.
Inference benchmark of resnet34.gluon_in1k done. 4422.08 samples/sec, 57.86 ms/step
Model resnet34.gluon_in1k created, param count: 21797672
Running train benchmark on resnet34.gluon_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1788.17 samples/sec. 143.163 ms/step.
Train [16/40]. 1790.67 samples/sec. 142.963 ms/step.
Train [24/40]. 1790.86 samples/sec. 142.948 ms/step.
Train [32/40]. 1790.40 samples/sec. 142.985 ms/step.
Train [40/40]. 1790.45 samples/sec. 142.981 ms/step.
Train benchmark of resnet34.gluon_in1k done. 1782.75 samples/sec, 142.98 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnet34.tv_in1k created, param count: 21797672
Running inference benchmark on resnet34.tv_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 4433.91 samples/sec. 57.737 ms/step.
Infer [16/40]. 4446.24 samples/sec. 57.577 ms/step.
Infer [24/40]. 4440.36 samples/sec. 57.653 ms/step.
Infer [32/40]. 4442.23 samples/sec. 57.629 ms/step.
Infer [40/40]. 4438.19 samples/sec. 57.681 ms/step.
Inference benchmark of resnet34.tv_in1k done. 4436.23 samples/sec, 57.68 ms/step
Model resnet34.tv_in1k created, param count: 21797672
Running train benchmark on resnet34.tv_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1819.39 samples/sec. 140.706 ms/step.
Train [16/40]. 1821.60 samples/sec. 140.536 ms/step.
Train [24/40]. 1818.85 samples/sec. 140.748 ms/step.
Train [32/40]. 1816.11 samples/sec. 140.961 ms/step.
Train [40/40]. 1816.20 samples/sec. 140.954 ms/step.
Train benchmark of resnet34.tv_in1k done. 1808.48 samples/sec, 140.95 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnet34d.ra2_in1k created, param count: 21816904
Running inference benchmark on resnet34d.ra2_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 4575.90 samples/sec. 55.945 ms/step.
Infer [16/40]. 4575.70 samples/sec. 55.948 ms/step.
Infer [24/40]. 4575.64 samples/sec. 55.949 ms/step.
Infer [32/40]. 4575.57 samples/sec. 55.949 ms/step.
Infer [40/40]. 4575.52 samples/sec. 55.950 ms/step.
Inference benchmark of resnet34d.ra2_in1k done. 4573.47 samples/sec, 55.95 ms/step
Model resnet34d.ra2_in1k created, param count: 21816904
Running train benchmark on resnet34d.ra2_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 1180.38 samples/sec. 216.879 ms/step.
Train [16/40]. 1180.41 samples/sec. 216.874 ms/step.
Train [24/40]. 1180.40 samples/sec. 216.876 ms/step.
Train [32/40]. 1180.41 samples/sec. 216.874 ms/step.
Train [40/40]. 1180.41 samples/sec. 216.875 ms/step.
Train benchmark of resnet34d.ra2_in1k done. 1176.16 samples/sec, 216.88 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnet50.a1_in1k created, param count: 25557032
Running inference benchmark on resnet50.a1_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 2536.96 samples/sec. 100.908 ms/step.
Infer [16/40]. 2536.79 samples/sec. 100.915 ms/step.
Infer [24/40]. 2536.81 samples/sec. 100.914 ms/step.
Infer [32/40]. 2536.77 samples/sec. 100.916 ms/step.
Infer [40/40]. 2536.80 samples/sec. 100.915 ms/step.
Inference benchmark of resnet50.a1_in1k done. 2536.08 samples/sec, 100.92 ms/step
Model resnet50.a1_in1k created, param count: 25557032
Running train benchmark on resnet50.a1_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 772.23 samples/sec. 331.506 ms/step.
Train [16/40]. 772.17 samples/sec. 331.531 ms/step.
Train [24/40]. 772.20 samples/sec. 331.522 ms/step.
Train [32/40]. 772.20 samples/sec. 331.521 ms/step.
Train [40/40]. 772.15 samples/sec. 331.540 ms/step.
Train benchmark of resnet50.a1_in1k done. 769.79 samples/sec, 331.54 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnet50.a1h_in1k created, param count: 25557032
Running inference benchmark on resnet50.a1h_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3414.66 samples/sec. 74.971 ms/step.
Infer [16/40]. 3414.36 samples/sec. 74.977 ms/step.
Infer [24/40]. 3415.30 samples/sec. 74.957 ms/step.
Infer [32/40]. 3414.73 samples/sec. 74.969 ms/step.
Infer [40/40]. 3412.91 samples/sec. 75.009 ms/step.
Inference benchmark of resnet50.a1h_in1k done. 3411.63 samples/sec, 75.01 ms/step
Model resnet50.a1h_in1k created, param count: 25557032
Running train benchmark on resnet50.a1h_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1215.73 samples/sec. 210.573 ms/step.
Train [16/40]. 1217.03 samples/sec. 210.348 ms/step.
Train [24/40]. 1216.89 samples/sec. 210.372 ms/step.
Train [32/40]. 1216.28 samples/sec. 210.478 ms/step.
Train [40/40]. 1216.53 samples/sec. 210.434 ms/step.
Train benchmark of resnet50.a1h_in1k done. 1211.23 samples/sec, 210.43 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnet50.a2_in1k created, param count: 25557032
Running inference benchmark on resnet50.a2_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 2536.86 samples/sec. 100.912 ms/step.
Infer [16/40]. 2536.76 samples/sec. 100.916 ms/step.
Infer [24/40]. 2536.72 samples/sec. 100.918 ms/step.
Infer [32/40]. 2536.67 samples/sec. 100.920 ms/step.
Infer [40/40]. 2536.58 samples/sec. 100.923 ms/step.
Inference benchmark of resnet50.a2_in1k done. 2535.83 samples/sec, 100.92 ms/step
Model resnet50.a2_in1k created, param count: 25557032
Running train benchmark on resnet50.a2_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 771.64 samples/sec. 331.761 ms/step.
Train [16/40]. 771.62 samples/sec. 331.770 ms/step.
Train [24/40]. 771.61 samples/sec. 331.774 ms/step.
Train [32/40]. 771.64 samples/sec. 331.761 ms/step.
Train [40/40]. 771.63 samples/sec. 331.764 ms/step.
Train benchmark of resnet50.a2_in1k done. 769.22 samples/sec, 331.76 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnet50.a3_in1k created, param count: 25557032
Running inference benchmark on resnet50.a3_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3480.58 samples/sec. 73.551 ms/step.
Infer [16/40]. 3491.12 samples/sec. 73.329 ms/step.
Infer [24/40]. 3491.87 samples/sec. 73.313 ms/step.
Infer [32/40]. 3492.60 samples/sec. 73.298 ms/step.
Infer [40/40]. 3485.04 samples/sec. 73.457 ms/step.
Inference benchmark of resnet50.a3_in1k done. 3482.68 samples/sec, 73.46 ms/step
Model resnet50.a3_in1k created, param count: 25557032
Running train benchmark on resnet50.a3_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1211.01 samples/sec. 211.394 ms/step.
Train [16/40]. 1210.22 samples/sec. 211.532 ms/step.
Train [24/40]. 1210.00 samples/sec. 211.570 ms/step.
Train [32/40]. 1208.79 samples/sec. 211.783 ms/step.
Train [40/40]. 1208.14 samples/sec. 211.896 ms/step.
Train benchmark of resnet50.a3_in1k done. 1202.88 samples/sec, 211.90 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnet50.am_in1k created, param count: 25557032
Running inference benchmark on resnet50.am_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3423.34 samples/sec. 74.781 ms/step.
Infer [16/40]. 3433.76 samples/sec. 74.554 ms/step.
Infer [24/40]. 3429.85 samples/sec. 74.639 ms/step.
Infer [32/40]. 3431.12 samples/sec. 74.611 ms/step.
Infer [40/40]. 3428.61 samples/sec. 74.666 ms/step.
Inference benchmark of resnet50.am_in1k done. 3427.39 samples/sec, 74.67 ms/step
Model resnet50.am_in1k created, param count: 25557032
Running train benchmark on resnet50.am_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1211.14 samples/sec. 211.370 ms/step.
Train [16/40]. 1209.54 samples/sec. 211.651 ms/step.
Train [24/40]. 1208.35 samples/sec. 211.859 ms/step.
Train [32/40]. 1207.86 samples/sec. 211.945 ms/step.
Train [40/40]. 1208.11 samples/sec. 211.902 ms/step.
Train benchmark of resnet50.am_in1k done. 1202.98 samples/sec, 211.90 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnet50.b1k_in1k created, param count: 25557032
Running inference benchmark on resnet50.b1k_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 2536.82 samples/sec. 100.914 ms/step.
Infer [16/40]. 2536.03 samples/sec. 100.945 ms/step.
Infer [24/40]. 2535.70 samples/sec. 100.958 ms/step.
Infer [32/40]. 2535.60 samples/sec. 100.962 ms/step.
Infer [40/40]. 2535.52 samples/sec. 100.965 ms/step.
Inference benchmark of resnet50.b1k_in1k done. 2534.76 samples/sec, 100.97 ms/step
Model resnet50.b1k_in1k created, param count: 25557032
Running train benchmark on resnet50.b1k_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 771.55 samples/sec. 331.798 ms/step.
Train [16/40]. 771.55 samples/sec. 331.799 ms/step.
Train [24/40]. 771.56 samples/sec. 331.797 ms/step.
Train [32/40]. 771.56 samples/sec. 331.797 ms/step.
Train [40/40]. 771.56 samples/sec. 331.795 ms/step.
Train benchmark of resnet50.b1k_in1k done. 769.13 samples/sec, 331.80 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnet50.b2k_in1k created, param count: 25557032
Running inference benchmark on resnet50.b2k_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 2536.72 samples/sec. 100.918 ms/step.
Infer [16/40]. 2536.58 samples/sec. 100.923 ms/step.
Infer [24/40]. 2536.50 samples/sec. 100.926 ms/step.
Infer [32/40]. 2536.46 samples/sec. 100.928 ms/step.
Infer [40/40]. 2536.42 samples/sec. 100.930 ms/step.
Inference benchmark of resnet50.b2k_in1k done. 2535.67 samples/sec, 100.93 ms/step
Model resnet50.b2k_in1k created, param count: 25557032
Running train benchmark on resnet50.b2k_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 771.58 samples/sec. 331.786 ms/step.
Train [16/40]. 771.60 samples/sec. 331.779 ms/step.
Train [24/40]. 771.60 samples/sec. 331.779 ms/step.
Train [32/40]. 771.58 samples/sec. 331.786 ms/step.
Train [40/40]. 771.59 samples/sec. 331.782 ms/step.
Train benchmark of resnet50.b2k_in1k done. 769.15 samples/sec, 331.78 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnet50.bt_in1k created, param count: 25557032
Running inference benchmark on resnet50.bt_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 2535.12 samples/sec. 100.981 ms/step.
Infer [16/40]. 2535.21 samples/sec. 100.978 ms/step.
Infer [24/40]. 2535.17 samples/sec. 100.979 ms/step.
Infer [32/40]. 2535.16 samples/sec. 100.980 ms/step.
Infer [40/40]. 2535.08 samples/sec. 100.983 ms/step.
Inference benchmark of resnet50.bt_in1k done. 2534.32 samples/sec, 100.98 ms/step
Model resnet50.bt_in1k created, param count: 25557032
Running train benchmark on resnet50.bt_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 771.60 samples/sec. 331.776 ms/step.
Train [16/40]. 771.60 samples/sec. 331.776 ms/step.
Train [24/40]. 771.60 samples/sec. 331.776 ms/step.
Train [32/40]. 771.60 samples/sec. 331.779 ms/step.
Train [40/40]. 771.60 samples/sec. 331.780 ms/step.
Train benchmark of resnet50.bt_in1k done. 769.15 samples/sec, 331.78 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnet50.c1_in1k created, param count: 25557032
Running inference benchmark on resnet50.c1_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 2535.20 samples/sec. 100.978 ms/step.
Infer [16/40]. 2535.14 samples/sec. 100.981 ms/step.
Infer [24/40]. 2535.25 samples/sec. 100.976 ms/step.
Infer [32/40]. 2535.25 samples/sec. 100.976 ms/step.
Infer [40/40]. 2535.19 samples/sec. 100.979 ms/step.
Inference benchmark of resnet50.c1_in1k done. 2534.42 samples/sec, 100.98 ms/step
Model resnet50.c1_in1k created, param count: 25557032
Running train benchmark on resnet50.c1_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 771.60 samples/sec. 331.779 ms/step.
Train [16/40]. 771.58 samples/sec. 331.786 ms/step.
Train [24/40]. 771.57 samples/sec. 331.789 ms/step.
Train [32/40]. 771.57 samples/sec. 331.790 ms/step.
Train [40/40]. 771.58 samples/sec. 331.788 ms/step.
Train benchmark of resnet50.c1_in1k done. 769.13 samples/sec, 331.79 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnet50.c2_in1k created, param count: 25557032
Running inference benchmark on resnet50.c2_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 2535.37 samples/sec. 100.972 ms/step.
Infer [16/40]. 2535.35 samples/sec. 100.972 ms/step.
Infer [24/40]. 2535.35 samples/sec. 100.972 ms/step.
Infer [32/40]. 2535.35 samples/sec. 100.972 ms/step.
Infer [40/40]. 2535.34 samples/sec. 100.973 ms/step.
Inference benchmark of resnet50.c2_in1k done. 2534.64 samples/sec, 100.97 ms/step
Model resnet50.c2_in1k created, param count: 25557032
Running train benchmark on resnet50.c2_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 771.60 samples/sec. 331.778 ms/step.
Train [16/40]. 771.61 samples/sec. 331.774 ms/step.
Train [24/40]. 771.60 samples/sec. 331.776 ms/step.
Train [32/40]. 771.61 samples/sec. 331.775 ms/step.
Train [40/40]. 771.61 samples/sec. 331.775 ms/step.
Train benchmark of resnet50.c2_in1k done. 769.27 samples/sec, 331.77 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnet50.d_in1k created, param count: 25557032
Running inference benchmark on resnet50.d_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 2535.21 samples/sec. 100.978 ms/step.
Infer [16/40]. 2535.15 samples/sec. 100.980 ms/step.
Infer [24/40]. 2535.08 samples/sec. 100.983 ms/step.
Infer [32/40]. 2535.05 samples/sec. 100.984 ms/step.
Infer [40/40]. 2535.05 samples/sec. 100.984 ms/step.
Inference benchmark of resnet50.d_in1k done. 2534.35 samples/sec, 100.98 ms/step
Model resnet50.d_in1k created, param count: 25557032
Running train benchmark on resnet50.d_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 771.63 samples/sec. 331.763 ms/step.
Train [16/40]. 771.63 samples/sec. 331.763 ms/step.
Train [24/40]. 771.63 samples/sec. 331.766 ms/step.
Train [32/40]. 771.63 samples/sec. 331.767 ms/step.
Train [40/40]. 771.63 samples/sec. 331.767 ms/step.
Train benchmark of resnet50.d_in1k done. 769.27 samples/sec, 331.77 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnet50.fb_ssl_yfcc100m_ft_in1k created, param count: 25557032
Running inference benchmark on resnet50.fb_ssl_yfcc100m_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3428.25 samples/sec. 74.674 ms/step.
Infer [16/40]. 3424.72 samples/sec. 74.751 ms/step.
Infer [24/40]. 3420.74 samples/sec. 74.838 ms/step.
Infer [32/40]. 3420.24 samples/sec. 74.849 ms/step.
Infer [40/40]. 3420.11 samples/sec. 74.851 ms/step.
Inference benchmark of resnet50.fb_ssl_yfcc100m_ft_in1k done. 3418.90 samples/sec, 74.85 ms/step
Model resnet50.fb_ssl_yfcc100m_ft_in1k created, param count: 25557032
Running train benchmark on resnet50.fb_ssl_yfcc100m_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1209.38 samples/sec. 211.679 ms/step.
Train [16/40]. 1207.92 samples/sec. 211.934 ms/step.
Train [24/40]. 1207.71 samples/sec. 211.972 ms/step.
Train [32/40]. 1207.53 samples/sec. 212.003 ms/step.
Train [40/40]. 1207.21 samples/sec. 212.060 ms/step.
Train benchmark of resnet50.fb_ssl_yfcc100m_ft_in1k done. 1202.11 samples/sec, 212.06 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnet50.fb_swsl_ig1b_ft_in1k created, param count: 25557032
Running inference benchmark on resnet50.fb_swsl_ig1b_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3415.27 samples/sec. 74.957 ms/step.
Infer [16/40]. 3424.40 samples/sec. 74.758 ms/step.
Infer [24/40]. 3419.21 samples/sec. 74.871 ms/step.
Infer [32/40]. 3417.91 samples/sec. 74.900 ms/step.
Infer [40/40]. 3417.97 samples/sec. 74.898 ms/step.
Inference benchmark of resnet50.fb_swsl_ig1b_ft_in1k done. 3416.75 samples/sec, 74.90 ms/step
Model resnet50.fb_swsl_ig1b_ft_in1k created, param count: 25557032
Running train benchmark on resnet50.fb_swsl_ig1b_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1207.90 samples/sec. 211.938 ms/step.
Train [16/40]. 1206.78 samples/sec. 212.135 ms/step.
Train [24/40]. 1206.89 samples/sec. 212.116 ms/step.
Train [32/40]. 1206.58 samples/sec. 212.170 ms/step.
Train [40/40]. 1206.32 samples/sec. 212.216 ms/step.
Train benchmark of resnet50.fb_swsl_ig1b_ft_in1k done. 1201.11 samples/sec, 212.22 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnet50.gluon_in1k created, param count: 25557032
Running inference benchmark on resnet50.gluon_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3425.46 samples/sec. 74.735 ms/step.
Infer [16/40]. 3428.22 samples/sec. 74.674 ms/step.
Infer [24/40]. 3424.36 samples/sec. 74.759 ms/step.
Infer [32/40]. 3424.71 samples/sec. 74.751 ms/step.
Infer [40/40]. 3423.18 samples/sec. 74.784 ms/step.
Inference benchmark of resnet50.gluon_in1k done. 3421.95 samples/sec, 74.78 ms/step
Model resnet50.gluon_in1k created, param count: 25557032
Running train benchmark on resnet50.gluon_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1208.83 samples/sec. 211.775 ms/step.
Train [16/40]. 1208.37 samples/sec. 211.856 ms/step.
Train [24/40]. 1207.59 samples/sec. 211.993 ms/step.
Train [32/40]. 1208.13 samples/sec. 211.897 ms/step.
Train [40/40]. 1208.20 samples/sec. 211.885 ms/step.
Train benchmark of resnet50.gluon_in1k done. 1203.03 samples/sec, 211.88 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnet50.ra_in1k created, param count: 25557032
Running inference benchmark on resnet50.ra_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 2536.47 samples/sec. 100.928 ms/step.
Infer [16/40]. 2536.41 samples/sec. 100.930 ms/step.
Infer [24/40]. 2536.43 samples/sec. 100.929 ms/step.
Infer [32/40]. 2536.42 samples/sec. 100.930 ms/step.
Infer [40/40]. 2536.36 samples/sec. 100.932 ms/step.
Inference benchmark of resnet50.ra_in1k done. 2535.64 samples/sec, 100.93 ms/step
Model resnet50.ra_in1k created, param count: 25557032
Running train benchmark on resnet50.ra_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 771.57 samples/sec. 331.792 ms/step.
Train [16/40]. 771.58 samples/sec. 331.789 ms/step.
Train [24/40]. 771.58 samples/sec. 331.785 ms/step.
Train [32/40]. 771.57 samples/sec. 331.790 ms/step.
Train [40/40]. 771.58 samples/sec. 331.786 ms/step.
Train benchmark of resnet50.ra_in1k done. 769.18 samples/sec, 331.79 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnet50.ram_in1k created, param count: 25557032
Running inference benchmark on resnet50.ram_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 2535.47 samples/sec. 100.968 ms/step.
Infer [16/40]. 2535.35 samples/sec. 100.972 ms/step.
Infer [24/40]. 2535.23 samples/sec. 100.977 ms/step.
Infer [32/40]. 2535.20 samples/sec. 100.978 ms/step.
Infer [40/40]. 2535.13 samples/sec. 100.981 ms/step.
Inference benchmark of resnet50.ram_in1k done. 2534.41 samples/sec, 100.98 ms/step
Model resnet50.ram_in1k created, param count: 25557032
Running train benchmark on resnet50.ram_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 772.18 samples/sec. 331.528 ms/step.
Train [16/40]. 772.16 samples/sec. 331.539 ms/step.
Train [24/40]. 772.16 samples/sec. 331.537 ms/step.
Train [32/40]. 772.16 samples/sec. 331.539 ms/step.
Train [40/40]. 772.15 samples/sec. 331.542 ms/step.
Train benchmark of resnet50.ram_in1k done. 769.79 samples/sec, 331.54 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnet50.tv2_in1k created, param count: 25557032
Running inference benchmark on resnet50.tv2_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3406.79 samples/sec. 75.144 ms/step.
Infer [16/40]. 3410.23 samples/sec. 75.068 ms/step.
Infer [24/40]. 3414.72 samples/sec. 74.970 ms/step.
Infer [32/40]. 3414.62 samples/sec. 74.972 ms/step.
Infer [40/40]. 3416.45 samples/sec. 74.932 ms/step.
Inference benchmark of resnet50.tv2_in1k done. 3415.25 samples/sec, 74.93 ms/step
Model resnet50.tv2_in1k created, param count: 25557032
Running train benchmark on resnet50.tv2_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1213.32 samples/sec. 210.992 ms/step.
Train [16/40]. 1215.02 samples/sec. 210.696 ms/step.
Train [24/40]. 1215.24 samples/sec. 210.658 ms/step.
Train [32/40]. 1214.89 samples/sec. 210.719 ms/step.
Train [40/40]. 1215.05 samples/sec. 210.691 ms/step.
Train benchmark of resnet50.tv2_in1k done. 1209.84 samples/sec, 210.69 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnet50.tv_in1k created, param count: 25557032
Running inference benchmark on resnet50.tv_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3450.49 samples/sec. 74.192 ms/step.
Infer [16/40]. 3444.62 samples/sec. 74.319 ms/step.
Infer [24/40]. 3447.99 samples/sec. 74.246 ms/step.
Infer [32/40]. 3455.10 samples/sec. 74.093 ms/step.
Infer [40/40]. 3459.64 samples/sec. 73.996 ms/step.
Inference benchmark of resnet50.tv_in1k done. 3458.42 samples/sec, 74.00 ms/step
Model resnet50.tv_in1k created, param count: 25557032
Running train benchmark on resnet50.tv_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1217.04 samples/sec. 210.346 ms/step.
Train [16/40]. 1214.53 samples/sec. 210.781 ms/step.
Train [24/40]. 1214.26 samples/sec. 210.827 ms/step.
Train [32/40]. 1214.14 samples/sec. 210.849 ms/step.
Train [40/40]. 1213.88 samples/sec. 210.894 ms/step.
Train benchmark of resnet50.tv_in1k done. 1208.69 samples/sec, 210.89 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnet50_gn.a1h_in1k created, param count: 25557032
Running inference benchmark on resnet50_gn.a1h_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 2285.22 samples/sec. 112.024 ms/step.
Infer [16/40]. 2285.02 samples/sec. 112.034 ms/step.
Infer [24/40]. 2284.84 samples/sec. 112.043 ms/step.
Infer [32/40]. 2284.79 samples/sec. 112.045 ms/step.
Infer [40/40]. 2284.81 samples/sec. 112.044 ms/step.
Inference benchmark of resnet50_gn.a1h_in1k done. 2284.22 samples/sec, 112.04 ms/step
Model resnet50_gn.a1h_in1k created, param count: 25557032
Running train benchmark on resnet50_gn.a1h_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 837.36 samples/sec. 305.721 ms/step.
Train [16/40]. 837.37 samples/sec. 305.719 ms/step.
Train [24/40]. 837.37 samples/sec. 305.718 ms/step.
Train [32/40]. 837.37 samples/sec. 305.720 ms/step.
Train [40/40]. 837.36 samples/sec. 305.723 ms/step.
Train benchmark of resnet50_gn.a1h_in1k done. 834.68 samples/sec, 305.72 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnet50c.gluon_in1k created, param count: 25576264
Running inference benchmark on resnet50c.gluon_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3283.25 samples/sec. 77.972 ms/step.
Infer [16/40]. 3292.38 samples/sec. 77.755 ms/step.
Infer [24/40]. 3300.38 samples/sec. 77.567 ms/step.
Infer [32/40]. 3294.42 samples/sec. 77.707 ms/step.
Infer [40/40]. 3291.80 samples/sec. 77.769 ms/step.
Inference benchmark of resnet50c.gluon_in1k done. 3290.67 samples/sec, 77.77 ms/step
Model resnet50c.gluon_in1k created, param count: 25576264
Running train benchmark on resnet50c.gluon_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1087.95 samples/sec. 235.305 ms/step.
Train [16/40]. 1086.76 samples/sec. 235.562 ms/step.
Train [24/40]. 1086.75 samples/sec. 235.564 ms/step.
Train [32/40]. 1087.01 samples/sec. 235.509 ms/step.
Train [40/40]. 1087.31 samples/sec. 235.443 ms/step.
Train benchmark of resnet50c.gluon_in1k done. 1082.99 samples/sec, 235.44 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnet50d.a1_in1k created, param count: 25576264
Running inference benchmark on resnet50d.a1_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 2376.98 samples/sec. 107.700 ms/step.
Infer [16/40]. 2375.90 samples/sec. 107.749 ms/step.
Infer [24/40]. 2375.40 samples/sec. 107.771 ms/step.
Infer [32/40]. 2375.05 samples/sec. 107.787 ms/step.
Infer [40/40]. 2374.91 samples/sec. 107.794 ms/step.
Inference benchmark of resnet50d.a1_in1k done. 2374.27 samples/sec, 107.79 ms/step
Model resnet50d.a1_in1k created, param count: 25576264
Running train benchmark on resnet50d.a1_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 686.89 samples/sec. 372.694 ms/step.
Train [16/40]. 686.89 samples/sec. 372.693 ms/step.
Train [24/40]. 686.88 samples/sec. 372.700 ms/step.
Train [32/40]. 686.89 samples/sec. 372.696 ms/step.
Train [40/40]. 686.89 samples/sec. 372.697 ms/step.
Train benchmark of resnet50d.a1_in1k done. 684.97 samples/sec, 372.70 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnet50d.a2_in1k created, param count: 25576264
Running inference benchmark on resnet50d.a2_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 2374.34 samples/sec. 107.819 ms/step.
Infer [16/40]. 2374.39 samples/sec. 107.817 ms/step.
Infer [24/40]. 2374.40 samples/sec. 107.817 ms/step.
Infer [32/40]. 2374.43 samples/sec. 107.815 ms/step.
Infer [40/40]. 2374.36 samples/sec. 107.819 ms/step.
Inference benchmark of resnet50d.a2_in1k done. 2373.73 samples/sec, 107.82 ms/step
Model resnet50d.a2_in1k created, param count: 25576264
Running train benchmark on resnet50d.a2_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 686.86 samples/sec. 372.709 ms/step.
Train [16/40]. 686.83 samples/sec. 372.729 ms/step.
Train [24/40]. 686.83 samples/sec. 372.727 ms/step.
Train [32/40]. 686.84 samples/sec. 372.720 ms/step.
Train [40/40]. 686.84 samples/sec. 372.719 ms/step.
Train benchmark of resnet50d.a2_in1k done. 684.93 samples/sec, 372.72 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnet50d.a3_in1k created, param count: 25576264
Running inference benchmark on resnet50d.a3_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3265.18 samples/sec. 78.403 ms/step.
Infer [16/40]. 3276.35 samples/sec. 78.136 ms/step.
Infer [24/40]. 3272.48 samples/sec. 78.228 ms/step.
Infer [32/40]. 3269.96 samples/sec. 78.288 ms/step.
Infer [40/40]. 3270.65 samples/sec. 78.272 ms/step.
Inference benchmark of resnet50d.a3_in1k done. 3269.52 samples/sec, 78.27 ms/step
Model resnet50d.a3_in1k created, param count: 25576264
Running train benchmark on resnet50d.a3_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1085.63 samples/sec. 235.807 ms/step.
Train [16/40]. 1084.21 samples/sec. 236.116 ms/step.
Train [24/40]. 1083.63 samples/sec. 236.244 ms/step.
Train [32/40]. 1084.00 samples/sec. 236.163 ms/step.
Train [40/40]. 1083.78 samples/sec. 236.210 ms/step.
Train benchmark of resnet50d.a3_in1k done. 1079.35 samples/sec, 236.21 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnet50d.gluon_in1k created, param count: 25576264
Running inference benchmark on resnet50d.gluon_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3291.65 samples/sec. 77.772 ms/step.
Infer [16/40]. 3279.13 samples/sec. 78.069 ms/step.
Infer [24/40]. 3276.97 samples/sec. 78.121 ms/step.
Infer [32/40]. 3274.43 samples/sec. 78.182 ms/step.
Infer [40/40]. 3270.38 samples/sec. 78.278 ms/step.
Inference benchmark of resnet50d.gluon_in1k done. 3269.25 samples/sec, 78.28 ms/step
Model resnet50d.gluon_in1k created, param count: 25576264
Running train benchmark on resnet50d.gluon_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1085.90 samples/sec. 235.749 ms/step.
Train [16/40]. 1084.27 samples/sec. 236.104 ms/step.
Train [24/40]. 1084.16 samples/sec. 236.127 ms/step.
Train [32/40]. 1084.17 samples/sec. 236.126 ms/step.
Train [40/40]. 1083.72 samples/sec. 236.224 ms/step.
Train benchmark of resnet50d.gluon_in1k done. 1079.38 samples/sec, 236.22 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnet50d.ra2_in1k created, param count: 25576264
Running inference benchmark on resnet50d.ra2_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 2375.63 samples/sec. 107.761 ms/step.
Infer [16/40]. 2375.04 samples/sec. 107.787 ms/step.
Infer [24/40]. 2374.91 samples/sec. 107.793 ms/step.
Infer [32/40]. 2374.83 samples/sec. 107.797 ms/step.
Infer [40/40]. 2374.72 samples/sec. 107.802 ms/step.
Inference benchmark of resnet50d.ra2_in1k done. 2374.11 samples/sec, 107.80 ms/step
Model resnet50d.ra2_in1k created, param count: 25576264
Running train benchmark on resnet50d.ra2_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 686.81 samples/sec. 372.737 ms/step.
Train [16/40]. 686.78 samples/sec. 372.751 ms/step.
Train [24/40]. 686.78 samples/sec. 372.755 ms/step.
Train [32/40]. 686.79 samples/sec. 372.750 ms/step.
Train [40/40]. 686.79 samples/sec. 372.747 ms/step.
Train benchmark of resnet50d.ra2_in1k done. 684.89 samples/sec, 372.75 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnet50s.gluon_in1k created, param count: 25680808
Running inference benchmark on resnet50s.gluon_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 2899.20 samples/sec. 88.300 ms/step.
Infer [16/40]. 2901.70 samples/sec. 88.224 ms/step.
Infer [24/40]. 2897.31 samples/sec. 88.358 ms/step.
Infer [32/40]. 2900.13 samples/sec. 88.272 ms/step.
Infer [40/40]. 2900.77 samples/sec. 88.252 ms/step.
Inference benchmark of resnet50s.gluon_in1k done. 2899.87 samples/sec, 88.25 ms/step
Model resnet50s.gluon_in1k created, param count: 25680808
Running train benchmark on resnet50s.gluon_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 967.44 samples/sec. 264.615 ms/step.
Train [16/40]. 966.22 samples/sec. 264.950 ms/step.
Train [24/40]. 965.51 samples/sec. 265.145 ms/step.
Train [32/40]. 965.19 samples/sec. 265.232 ms/step.
Train [40/40]. 964.84 samples/sec. 265.328 ms/step.
Train benchmark of resnet50s.gluon_in1k done. 961.38 samples/sec, 265.33 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnet51q.ra2_in1k created, param count: 35696920
Running inference benchmark on resnet51q.ra2_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 2284.01 samples/sec. 112.084 ms/step.
Infer [16/40]. 2283.89 samples/sec. 112.090 ms/step.
Infer [24/40]. 2283.74 samples/sec. 112.097 ms/step.
Infer [32/40]. 2283.72 samples/sec. 112.098 ms/step.
Infer [40/40]. 2283.74 samples/sec. 112.097 ms/step.
Inference benchmark of resnet51q.ra2_in1k done. 2283.14 samples/sec, 112.10 ms/step
Model resnet51q.ra2_in1k created, param count: 35696920
Running train benchmark on resnet51q.ra2_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 244.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 122.06 MiB is free. Including non-PyTorch memory, this process has 23.52 GiB memory in use. Of the allocated memory 22.24 GiB is allocated by PyTorch, and 54.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model resnet51q.ra2_in1k created, param count: 35696920
Running train benchmark on resnet51q.ra2_in1k for 40 steps w/ input size (3, 288, 288) and batch size 192.
Train [8/40]. 647.75 samples/sec. 296.411 ms/step.
Train [16/40]. 647.75 samples/sec. 296.412 ms/step.
Train [24/40]. 647.73 samples/sec. 296.418 ms/step.
Train [32/40]. 647.73 samples/sec. 296.421 ms/step.
Train [40/40]. 647.73 samples/sec. 296.420 ms/step.
Train benchmark of resnet51q.ra2_in1k done. 645.47 samples/sec, 296.42 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnet61q.ra2_in1k created, param count: 36846968
Running inference benchmark on resnet61q.ra2_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 2117.97 samples/sec. 120.870 ms/step.
Infer [16/40]. 2117.93 samples/sec. 120.873 ms/step.
Infer [24/40]. 2117.89 samples/sec. 120.875 ms/step.
Infer [32/40]. 2117.89 samples/sec. 120.875 ms/step.
Infer [40/40]. 2117.86 samples/sec. 120.877 ms/step.
Inference benchmark of resnet61q.ra2_in1k done. 2117.34 samples/sec, 120.88 ms/step
Model resnet61q.ra2_in1k created, param count: 36846968
Running train benchmark on resnet61q.ra2_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 244.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 236.06 MiB is free. Including non-PyTorch memory, this process has 23.41 GiB memory in use. Of the allocated memory 22.12 GiB is allocated by PyTorch, and 55.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model resnet61q.ra2_in1k created, param count: 36846968
Running train benchmark on resnet61q.ra2_in1k for 40 steps w/ input size (3, 288, 288) and batch size 192.
Train [8/40]. 585.78 samples/sec. 327.769 ms/step.
Train [16/40]. 585.65 samples/sec. 327.841 ms/step.
Train [24/40]. 585.63 samples/sec. 327.853 ms/step.
Train [32/40]. 585.62 samples/sec. 327.859 ms/step.
Train [40/40]. 585.60 samples/sec. 327.869 ms/step.
Train benchmark of resnet61q.ra2_in1k done. 583.51 samples/sec, 327.87 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnet101.a1_in1k created, param count: 44549160
Running inference benchmark on resnet101.a1_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 1650.65 samples/sec. 155.091 ms/step.
Infer [16/40]. 1650.14 samples/sec. 155.138 ms/step.
Infer [24/40]. 1649.90 samples/sec. 155.161 ms/step.
Infer [32/40]. 1649.79 samples/sec. 155.171 ms/step.
Infer [40/40]. 1649.75 samples/sec. 155.175 ms/step.
Inference benchmark of resnet101.a1_in1k done. 1649.39 samples/sec, 155.18 ms/step
Model resnet101.a1_in1k created, param count: 44549160
Running train benchmark on resnet101.a1_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 14.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.25 GiB is allocated by PyTorch, and 145.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model resnet101.a1_in1k created, param count: 44549160
Running train benchmark on resnet101.a1_in1k for 40 steps w/ input size (3, 288, 288) and batch size 192.
Train [8/40]. 512.98 samples/sec. 374.283 ms/step.
Train [16/40]. 512.98 samples/sec. 374.283 ms/step.
Train [24/40]. 512.98 samples/sec. 374.284 ms/step.
Train [32/40]. 512.98 samples/sec. 374.282 ms/step.
Train [40/40]. 512.99 samples/sec. 374.280 ms/step.
Train benchmark of resnet101.a1_in1k done. 510.87 samples/sec, 374.28 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnet101.a1h_in1k created, param count: 44549160
Running inference benchmark on resnet101.a1h_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 1205.49 samples/sec. 212.362 ms/step.
Infer [16/40]. 1206.82 samples/sec. 212.127 ms/step.
Infer [24/40]. 1207.77 samples/sec. 211.960 ms/step.
Infer [32/40]. 1207.67 samples/sec. 211.978 ms/step.
Infer [40/40]. 1208.07 samples/sec. 211.908 ms/step.
Inference benchmark of resnet101.a1h_in1k done. 1207.84 samples/sec, 211.91 ms/step
Model resnet101.a1h_in1k created, param count: 44549160
Running train benchmark on resnet101.a1h_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 14.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.25 GiB is allocated by PyTorch, and 145.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model resnet101.a1h_in1k created, param count: 44549160
Running train benchmark on resnet101.a1h_in1k for 40 steps w/ input size (3, 288, 288) and batch size 192.
Train [8/40]. 513.12 samples/sec. 374.179 ms/step.
Train [16/40]. 513.14 samples/sec. 374.163 ms/step.
Train [24/40]. 513.15 samples/sec. 374.157 ms/step.
Train [32/40]. 513.15 samples/sec. 374.157 ms/step.
Train [40/40]. 513.14 samples/sec. 374.164 ms/step.
Train benchmark of resnet101.a1h_in1k done. 511.07 samples/sec, 374.16 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnet101.a2_in1k created, param count: 44549160
Running inference benchmark on resnet101.a2_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 1207.27 samples/sec. 212.048 ms/step.
Infer [16/40]. 1206.97 samples/sec. 212.101 ms/step.
Infer [24/40]. 1206.67 samples/sec. 212.154 ms/step.
Infer [32/40]. 1207.30 samples/sec. 212.044 ms/step.
Infer [40/40]. 1206.94 samples/sec. 212.107 ms/step.
Inference benchmark of resnet101.a2_in1k done. 1206.72 samples/sec, 212.11 ms/step
Model resnet101.a2_in1k created, param count: 44549160
Running train benchmark on resnet101.a2_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 14.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.25 GiB is allocated by PyTorch, and 145.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model resnet101.a2_in1k created, param count: 44549160
Running train benchmark on resnet101.a2_in1k for 40 steps w/ input size (3, 288, 288) and batch size 192.
Train [8/40]. 513.04 samples/sec. 374.243 ms/step.
Train [16/40]. 513.04 samples/sec. 374.237 ms/step.
Train [24/40]. 513.03 samples/sec. 374.246 ms/step.
Train [32/40]. 513.02 samples/sec. 374.252 ms/step.
Train [40/40]. 513.02 samples/sec. 374.256 ms/step.
Train benchmark of resnet101.a2_in1k done. 510.94 samples/sec, 374.26 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnet101.a3_in1k created, param count: 44549160
Running inference benchmark on resnet101.a3_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1923.26 samples/sec. 133.107 ms/step.
Infer [16/40]. 1925.17 samples/sec. 132.975 ms/step.
Infer [24/40]. 1927.63 samples/sec. 132.806 ms/step.
Infer [32/40]. 1929.23 samples/sec. 132.695 ms/step.
Infer [40/40]. 1928.46 samples/sec. 132.749 ms/step.
Inference benchmark of resnet101.a3_in1k done. 1928.02 samples/sec, 132.75 ms/step
Model resnet101.a3_in1k created, param count: 44549160
Running train benchmark on resnet101.a3_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 753.41 samples/sec. 339.789 ms/step.
Train [16/40]. 753.56 samples/sec. 339.720 ms/step.
Train [24/40]. 753.60 samples/sec. 339.704 ms/step.
Train [32/40]. 753.50 samples/sec. 339.749 ms/step.
Train [40/40]. 753.46 samples/sec. 339.767 ms/step.
Train benchmark of resnet101.a3_in1k done. 750.32 samples/sec, 339.77 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnet101.gluon_in1k created, param count: 44549160
Running inference benchmark on resnet101.gluon_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1943.01 samples/sec. 131.755 ms/step.
Infer [16/40]. 1938.04 samples/sec. 132.092 ms/step.
Infer [24/40]. 1934.43 samples/sec. 132.339 ms/step.
Infer [32/40]. 1933.16 samples/sec. 132.426 ms/step.
Infer [40/40]. 1931.66 samples/sec. 132.529 ms/step.
Inference benchmark of resnet101.gluon_in1k done. 1931.22 samples/sec, 132.53 ms/step
Model resnet101.gluon_in1k created, param count: 44549160
Running train benchmark on resnet101.gluon_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 753.78 samples/sec. 339.621 ms/step.
Train [16/40]. 753.72 samples/sec. 339.647 ms/step.
Train [24/40]. 753.66 samples/sec. 339.677 ms/step.
Train [32/40]. 753.60 samples/sec. 339.704 ms/step.
Train [40/40]. 753.65 samples/sec. 339.681 ms/step.
Train benchmark of resnet101.gluon_in1k done. 750.55 samples/sec, 339.68 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnet101.tv2_in1k created, param count: 44549160
Running inference benchmark on resnet101.tv2_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1927.17 samples/sec. 132.838 ms/step.
Infer [16/40]. 1930.94 samples/sec. 132.578 ms/step.
Infer [24/40]. 1929.02 samples/sec. 132.710 ms/step.
Infer [32/40]. 1928.88 samples/sec. 132.719 ms/step.
Infer [40/40]. 1928.18 samples/sec. 132.768 ms/step.
Inference benchmark of resnet101.tv2_in1k done. 1927.73 samples/sec, 132.77 ms/step
Model resnet101.tv2_in1k created, param count: 44549160
Running train benchmark on resnet101.tv2_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 756.49 samples/sec. 338.406 ms/step.
Train [16/40]. 755.89 samples/sec. 338.672 ms/step.
Train [24/40]. 754.97 samples/sec. 339.088 ms/step.
Train [32/40]. 754.55 samples/sec. 339.273 ms/step.
Train [40/40]. 754.28 samples/sec. 339.399 ms/step.
Train benchmark of resnet101.tv2_in1k done. 751.10 samples/sec, 339.40 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnet101.tv_in1k created, param count: 44549160
Running inference benchmark on resnet101.tv_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1928.06 samples/sec. 132.776 ms/step.
Infer [16/40]. 1925.41 samples/sec. 132.958 ms/step.
Infer [24/40]. 1924.92 samples/sec. 132.993 ms/step.
Infer [32/40]. 1926.14 samples/sec. 132.909 ms/step.
Infer [40/40]. 1925.45 samples/sec. 132.956 ms/step.
Inference benchmark of resnet101.tv_in1k done. 1925.00 samples/sec, 132.96 ms/step
Model resnet101.tv_in1k created, param count: 44549160
Running train benchmark on resnet101.tv_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 753.10 samples/sec. 339.929 ms/step.
Train [16/40]. 753.71 samples/sec. 339.654 ms/step.
Train [24/40]. 753.79 samples/sec. 339.619 ms/step.
Train [32/40]. 753.65 samples/sec. 339.680 ms/step.
Train [40/40]. 753.51 samples/sec. 339.742 ms/step.
Train benchmark of resnet101.tv_in1k done. 750.33 samples/sec, 339.74 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnet101c.gluon_in1k created, param count: 44568392
Running inference benchmark on resnet101c.gluon_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1868.06 samples/sec. 137.040 ms/step.
Infer [16/40]. 1869.48 samples/sec. 136.937 ms/step.
Infer [24/40]. 1869.36 samples/sec. 136.945 ms/step.
Infer [32/40]. 1868.34 samples/sec. 137.020 ms/step.
Infer [40/40]. 1867.56 samples/sec. 137.077 ms/step.
Inference benchmark of resnet101c.gluon_in1k done. 1867.13 samples/sec, 137.08 ms/step
Model resnet101c.gluon_in1k created, param count: 44568392
Running train benchmark on resnet101c.gluon_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 703.09 samples/sec. 364.106 ms/step.
Train [16/40]. 703.47 samples/sec. 363.909 ms/step.
Train [24/40]. 703.37 samples/sec. 363.963 ms/step.
Train [32/40]. 703.31 samples/sec. 363.992 ms/step.
Train [40/40]. 703.33 samples/sec. 363.981 ms/step.
Train benchmark of resnet101c.gluon_in1k done. 700.46 samples/sec, 363.98 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnet101d.gluon_in1k created, param count: 44568392
Running inference benchmark on resnet101d.gluon_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1878.59 samples/sec. 136.272 ms/step.
Infer [16/40]. 1876.97 samples/sec. 136.390 ms/step.
Infer [24/40]. 1876.92 samples/sec. 136.394 ms/step.
Infer [32/40]. 1877.01 samples/sec. 136.387 ms/step.
Infer [40/40]. 1876.93 samples/sec. 136.393 ms/step.
Inference benchmark of resnet101d.gluon_in1k done. 1876.50 samples/sec, 136.39 ms/step
Model resnet101d.gluon_in1k created, param count: 44568392
Running train benchmark on resnet101d.gluon_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 703.29 samples/sec. 364.004 ms/step.
Train [16/40]. 703.59 samples/sec. 363.846 ms/step.
Train [24/40]. 703.55 samples/sec. 363.869 ms/step.
Train [32/40]. 703.56 samples/sec. 363.865 ms/step.
Train [40/40]. 703.59 samples/sec. 363.849 ms/step.
Train benchmark of resnet101d.gluon_in1k done. 700.73 samples/sec, 363.85 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnet101d.ra2_in1k created, param count: 44568392
Running inference benchmark on resnet101d.ra2_in1k for 40 steps w/ input size (3, 320, 320) and batch size 256.
Infer [8/40]. 1343.81 samples/sec. 190.503 ms/step.
Infer [16/40]. 1343.67 samples/sec. 190.523 ms/step.
Infer [24/40]. 1343.67 samples/sec. 190.523 ms/step.
Infer [32/40]. 1343.66 samples/sec. 190.524 ms/step.
Infer [40/40]. 1343.65 samples/sec. 190.526 ms/step.
Inference benchmark of resnet101d.ra2_in1k done. 1343.39 samples/sec, 190.53 ms/step
Model resnet101d.ra2_in1k created, param count: 44568392
Running train benchmark on resnet101d.ra2_in1k for 40 steps w/ input size (3, 320, 320) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 38.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 22.12 GiB is allocated by PyTorch, and 255.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model resnet101d.ra2_in1k created, param count: 44568392
Running train benchmark on resnet101d.ra2_in1k for 40 steps w/ input size (3, 320, 320) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 150.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 68.06 MiB is free. Including non-PyTorch memory, this process has 23.57 GiB memory in use. Of the allocated memory 22.20 GiB is allocated by PyTorch, and 141.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model resnet101d.ra2_in1k created, param count: 44568392
Running train benchmark on resnet101d.ra2_in1k for 40 steps w/ input size (3, 320, 320) and batch size 128.
Train [8/40]. 387.87 samples/sec. 330.009 ms/step.
Train [16/40]. 387.87 samples/sec. 330.009 ms/step.
Train [24/40]. 387.86 samples/sec. 330.016 ms/step.
Train [32/40]. 387.86 samples/sec. 330.015 ms/step.
Train [40/40]. 387.86 samples/sec. 330.018 ms/step.
Train benchmark of resnet101d.ra2_in1k done. 386.19 samples/sec, 330.02 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnet101s.gluon_in1k created, param count: 44672936
Running inference benchmark on resnet101s.gluon_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1737.99 samples/sec. 147.297 ms/step.
Infer [16/40]. 1736.55 samples/sec. 147.419 ms/step.
Infer [24/40]. 1737.15 samples/sec. 147.368 ms/step.
Infer [32/40]. 1737.17 samples/sec. 147.366 ms/step.
Infer [40/40]. 1737.43 samples/sec. 147.344 ms/step.
Inference benchmark of resnet101s.gluon_in1k done. 1737.05 samples/sec, 147.34 ms/step
Model resnet101s.gluon_in1k created, param count: 44672936
Running train benchmark on resnet101s.gluon_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 651.77 samples/sec. 392.775 ms/step.
Train [16/40]. 651.21 samples/sec. 393.113 ms/step.
Train [24/40]. 650.84 samples/sec. 393.340 ms/step.
Train [32/40]. 650.60 samples/sec. 393.485 ms/step.
Train [40/40]. 650.33 samples/sec. 393.644 ms/step.
Train benchmark of resnet101s.gluon_in1k done. 647.89 samples/sec, 393.64 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnet152.a1_in1k created, param count: 60192808
Running inference benchmark on resnet152.a1_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 831.79 samples/sec. 307.771 ms/step.
Infer [16/40]. 831.37 samples/sec. 307.924 ms/step.
Infer [24/40]. 831.09 samples/sec. 308.030 ms/step.
Infer [32/40]. 831.22 samples/sec. 307.981 ms/step.
Infer [40/40]. 831.36 samples/sec. 307.930 ms/step.
Inference benchmark of resnet152.a1_in1k done. 831.24 samples/sec, 307.93 ms/step
Model resnet152.a1_in1k created, param count: 60192808
Running train benchmark on resnet152.a1_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 32.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 22.29 GiB is allocated by PyTorch, and 89.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model resnet152.a1_in1k created, param count: 60192808
Running train benchmark on resnet152.a1_in1k for 40 steps w/ input size (3, 288, 288) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 32.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 255.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model resnet152.a1_in1k created, param count: 60192808
Running train benchmark on resnet152.a1_in1k for 40 steps w/ input size (3, 288, 288) and batch size 128.
Train [8/40]. 366.53 samples/sec. 349.224 ms/step.
Train [16/40]. 366.53 samples/sec. 349.218 ms/step.
Train [24/40]. 366.53 samples/sec. 349.221 ms/step.
Train [32/40]. 366.53 samples/sec. 349.217 ms/step.
Train [40/40]. 366.53 samples/sec. 349.223 ms/step.
Train benchmark of resnet152.a1_in1k done. 364.51 samples/sec, 349.22 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnet152.a1h_in1k created, param count: 60192808
Running inference benchmark on resnet152.a1h_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 831.93 samples/sec. 307.719 ms/step.
Infer [16/40]. 831.92 samples/sec. 307.721 ms/step.
Infer [24/40]. 831.86 samples/sec. 307.742 ms/step.
Infer [32/40]. 831.35 samples/sec. 307.934 ms/step.
Infer [40/40]. 831.03 samples/sec. 308.051 ms/step.
Inference benchmark of resnet152.a1h_in1k done. 830.91 samples/sec, 308.05 ms/step
Model resnet152.a1h_in1k created, param count: 60192808
Running train benchmark on resnet152.a1h_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 32.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 22.29 GiB is allocated by PyTorch, and 89.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model resnet152.a1h_in1k created, param count: 60192808
Running train benchmark on resnet152.a1h_in1k for 40 steps w/ input size (3, 288, 288) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 32.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 255.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model resnet152.a1h_in1k created, param count: 60192808
Running train benchmark on resnet152.a1h_in1k for 40 steps w/ input size (3, 288, 288) and batch size 128.
Train [8/40]. 366.50 samples/sec. 349.254 ms/step.
Train [16/40]. 366.50 samples/sec. 349.248 ms/step.
Train [24/40]. 366.50 samples/sec. 349.250 ms/step.
Train [32/40]. 366.50 samples/sec. 349.253 ms/step.
Train [40/40]. 366.50 samples/sec. 349.254 ms/step.
Train benchmark of resnet152.a1h_in1k done. 364.45 samples/sec, 349.25 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnet152.a2_in1k created, param count: 60192808
Running inference benchmark on resnet152.a2_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 830.32 samples/sec. 308.314 ms/step.
Infer [16/40]. 830.21 samples/sec. 308.357 ms/step.
Infer [24/40]. 831.00 samples/sec. 308.061 ms/step.
Infer [32/40]. 830.86 samples/sec. 308.113 ms/step.
Infer [40/40]. 830.73 samples/sec. 308.161 ms/step.
Inference benchmark of resnet152.a2_in1k done. 830.61 samples/sec, 308.16 ms/step
Model resnet152.a2_in1k created, param count: 60192808
Running train benchmark on resnet152.a2_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 32.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 22.29 GiB is allocated by PyTorch, and 89.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model resnet152.a2_in1k created, param count: 60192808
Running train benchmark on resnet152.a2_in1k for 40 steps w/ input size (3, 288, 288) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 32.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 255.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model resnet152.a2_in1k created, param count: 60192808
Running train benchmark on resnet152.a2_in1k for 40 steps w/ input size (3, 288, 288) and batch size 128.
Train [8/40]. 366.45 samples/sec. 349.294 ms/step.
Train [16/40]. 366.45 samples/sec. 349.298 ms/step.
Train [24/40]. 366.45 samples/sec. 349.298 ms/step.
Train [32/40]. 366.45 samples/sec. 349.296 ms/step.
Train [40/40]. 366.46 samples/sec. 349.292 ms/step.
Train benchmark of resnet152.a2_in1k done. 364.43 samples/sec, 349.29 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnet152.a3_in1k created, param count: 60192808
Running inference benchmark on resnet152.a3_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1354.22 samples/sec. 189.039 ms/step.
Infer [16/40]. 1353.67 samples/sec. 189.115 ms/step.
Infer [24/40]. 1352.90 samples/sec. 189.223 ms/step.
Infer [32/40]. 1352.60 samples/sec. 189.266 ms/step.
Infer [40/40]. 1352.44 samples/sec. 189.288 ms/step.
Inference benchmark of resnet152.a3_in1k done. 1352.18 samples/sec, 189.29 ms/step
Model resnet152.a3_in1k created, param count: 60192808
Running train benchmark on resnet152.a3_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 535.18 samples/sec. 478.348 ms/step.
Train [16/40]. 535.15 samples/sec. 478.371 ms/step.
Train [24/40]. 535.08 samples/sec. 478.435 ms/step.
Train [32/40]. 535.04 samples/sec. 478.466 ms/step.
Train [40/40]. 535.05 samples/sec. 478.460 ms/step.
Train benchmark of resnet152.a3_in1k done. 532.84 samples/sec, 478.46 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnet152.gluon_in1k created, param count: 60192808
Running inference benchmark on resnet152.gluon_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1350.70 samples/sec. 189.531 ms/step.
Infer [16/40]. 1350.77 samples/sec. 189.521 ms/step.
Infer [24/40]. 1351.14 samples/sec. 189.470 ms/step.
Infer [32/40]. 1351.55 samples/sec. 189.412 ms/step.
Infer [40/40]. 1351.78 samples/sec. 189.380 ms/step.
Inference benchmark of resnet152.gluon_in1k done. 1351.54 samples/sec, 189.38 ms/step
Model resnet152.gluon_in1k created, param count: 60192808
Running train benchmark on resnet152.gluon_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 535.77 samples/sec. 477.817 ms/step.
Train [16/40]. 535.46 samples/sec. 478.090 ms/step.
Train [24/40]. 535.20 samples/sec. 478.322 ms/step.
Train [32/40]. 535.12 samples/sec. 478.401 ms/step.
Train [40/40]. 535.09 samples/sec. 478.427 ms/step.
Train benchmark of resnet152.gluon_in1k done. 532.91 samples/sec, 478.43 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnet152.tv2_in1k created, param count: 60192808
Running inference benchmark on resnet152.tv2_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1356.18 samples/sec. 188.765 ms/step.
Infer [16/40]. 1355.84 samples/sec. 188.812 ms/step.
Infer [24/40]. 1354.13 samples/sec. 189.052 ms/step.
Infer [32/40]. 1353.59 samples/sec. 189.127 ms/step.
Infer [40/40]. 1353.61 samples/sec. 189.124 ms/step.
Inference benchmark of resnet152.tv2_in1k done. 1353.35 samples/sec, 189.12 ms/step
Model resnet152.tv2_in1k created, param count: 60192808
Running train benchmark on resnet152.tv2_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 535.08 samples/sec. 478.434 ms/step.
Train [16/40]. 535.05 samples/sec. 478.458 ms/step.
Train [24/40]. 535.08 samples/sec. 478.435 ms/step.
Train [32/40]. 535.11 samples/sec. 478.408 ms/step.
Train [40/40]. 535.10 samples/sec. 478.417 ms/step.
Train benchmark of resnet152.tv2_in1k done. 532.87 samples/sec, 478.42 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnet152.tv_in1k created, param count: 60192808
Running inference benchmark on resnet152.tv_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1350.02 samples/sec. 189.627 ms/step.
Infer [16/40]. 1351.66 samples/sec. 189.396 ms/step.
Infer [24/40]. 1351.23 samples/sec. 189.457 ms/step.
Infer [32/40]. 1351.83 samples/sec. 189.373 ms/step.
Infer [40/40]. 1351.49 samples/sec. 189.420 ms/step.
Inference benchmark of resnet152.tv_in1k done. 1351.23 samples/sec, 189.42 ms/step
Model resnet152.tv_in1k created, param count: 60192808
Running train benchmark on resnet152.tv_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 535.45 samples/sec. 478.102 ms/step.
Train [16/40]. 535.41 samples/sec. 478.141 ms/step.
Train [24/40]. 535.25 samples/sec. 478.279 ms/step.
Train [32/40]. 535.21 samples/sec. 478.316 ms/step.
Train [40/40]. 535.19 samples/sec. 478.338 ms/step.
Train benchmark of resnet152.tv_in1k done. 533.01 samples/sec, 478.34 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnet152c.gluon_in1k created, param count: 60212040
Running inference benchmark on resnet152c.gluon_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1325.43 samples/sec. 193.145 ms/step.
Infer [16/40]. 1327.23 samples/sec. 192.883 ms/step.
Infer [24/40]. 1326.02 samples/sec. 193.058 ms/step.
Infer [32/40]. 1326.32 samples/sec. 193.015 ms/step.
Infer [40/40]. 1325.62 samples/sec. 193.117 ms/step.
Inference benchmark of resnet152c.gluon_in1k done. 1325.37 samples/sec, 193.12 ms/step
Model resnet152c.gluon_in1k created, param count: 60212040
Running train benchmark on resnet152c.gluon_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 42.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 21.96 GiB is allocated by PyTorch, and 411.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model resnet152c.gluon_in1k created, param count: 60212040
Running train benchmark on resnet152c.gluon_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
Train [8/40]. 575.81 samples/sec. 333.446 ms/step.
Train [16/40]. 575.83 samples/sec. 333.432 ms/step.
Train [24/40]. 575.81 samples/sec. 333.441 ms/step.
Train [32/40]. 575.81 samples/sec. 333.441 ms/step.
Train [40/40]. 575.81 samples/sec. 333.445 ms/step.
Train benchmark of resnet152c.gluon_in1k done. 572.60 samples/sec, 333.44 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnet152d.gluon_in1k created, param count: 60212040
Running inference benchmark on resnet152d.gluon_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1330.32 samples/sec. 192.434 ms/step.
Infer [16/40]. 1328.32 samples/sec. 192.725 ms/step.
Infer [24/40]. 1327.57 samples/sec. 192.834 ms/step.
Infer [32/40]. 1327.18 samples/sec. 192.890 ms/step.
Infer [40/40]. 1327.71 samples/sec. 192.814 ms/step.
Inference benchmark of resnet152d.gluon_in1k done. 1327.44 samples/sec, 192.81 ms/step
Model resnet152d.gluon_in1k created, param count: 60212040
Running train benchmark on resnet152d.gluon_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 4.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 22.01 GiB is allocated by PyTorch, and 401.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model resnet152d.gluon_in1k created, param count: 60212040
Running train benchmark on resnet152d.gluon_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
Train [8/40]. 576.12 samples/sec. 333.262 ms/step.
Train [16/40]. 576.13 samples/sec. 333.258 ms/step.
Train [24/40]. 576.14 samples/sec. 333.254 ms/step.
Train [32/40]. 576.16 samples/sec. 333.243 ms/step.
Train [40/40]. 576.15 samples/sec. 333.246 ms/step.
Train benchmark of resnet152d.gluon_in1k done. 572.89 samples/sec, 333.25 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnet152d.ra2_in1k created, param count: 60212040
Running inference benchmark on resnet152d.ra2_in1k for 40 steps w/ input size (3, 320, 320) and batch size 256.
Infer [8/40]. 968.74 samples/sec. 264.261 ms/step.
Infer [16/40]. 968.70 samples/sec. 264.273 ms/step.
Infer [24/40]. 968.84 samples/sec. 264.233 ms/step.
Infer [32/40]. 968.78 samples/sec. 264.250 ms/step.
Infer [40/40]. 968.72 samples/sec. 264.265 ms/step.
Inference benchmark of resnet152d.ra2_in1k done. 968.54 samples/sec, 264.26 ms/step
Model resnet152d.ra2_in1k created, param count: 60212040
Running train benchmark on resnet152d.ra2_in1k for 40 steps w/ input size (3, 320, 320) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 400.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 76.06 MiB is free. Including non-PyTorch memory, this process has 23.57 GiB memory in use. Of the allocated memory 21.96 GiB is allocated by PyTorch, and 387.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model resnet152d.ra2_in1k created, param count: 60212040
Running train benchmark on resnet152d.ra2_in1k for 40 steps w/ input size (3, 320, 320) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 150.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 84.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 22.23 GiB is allocated by PyTorch, and 99.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model resnet152d.ra2_in1k created, param count: 60212040
Running train benchmark on resnet152d.ra2_in1k for 40 steps w/ input size (3, 320, 320) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 14.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.34 GiB is allocated by PyTorch, and 54.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model resnet152d.ra2_in1k created, param count: 60212040
Running train benchmark on resnet152d.ra2_in1k for 40 steps w/ input size (3, 320, 320) and batch size 96.
Train [8/40]. 284.12 samples/sec. 337.885 ms/step.
Train [16/40]. 284.12 samples/sec. 337.884 ms/step.
Train [24/40]. 284.13 samples/sec. 337.873 ms/step.
Train [32/40]. 284.13 samples/sec. 337.869 ms/step.
Train [40/40]. 284.13 samples/sec. 337.869 ms/step.
Train benchmark of resnet152d.ra2_in1k done. 282.57 samples/sec, 337.87 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnet152s.gluon_in1k created, param count: 60316584
Running inference benchmark on resnet152s.gluon_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1238.37 samples/sec. 206.724 ms/step.
Infer [16/40]. 1238.87 samples/sec. 206.641 ms/step.
Infer [24/40]. 1239.45 samples/sec. 206.543 ms/step.
Infer [32/40]. 1238.90 samples/sec. 206.635 ms/step.
Infer [40/40]. 1238.60 samples/sec. 206.685 ms/step.
Inference benchmark of resnet152s.gluon_in1k done. 1238.37 samples/sec, 206.69 ms/step
Model resnet152s.gluon_in1k created, param count: 60316584
Running train benchmark on resnet152s.gluon_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 8.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 326.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model resnet152s.gluon_in1k created, param count: 60316584
Running train benchmark on resnet152s.gluon_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
Train [8/40]. 540.46 samples/sec. 355.252 ms/step.
Train [16/40]. 540.46 samples/sec. 355.251 ms/step.
Train [24/40]. 540.46 samples/sec. 355.254 ms/step.
Train [32/40]. 540.45 samples/sec. 355.257 ms/step.
Train [40/40]. 540.45 samples/sec. 355.262 ms/step.
Train benchmark of resnet152s.gluon_in1k done. 537.49 samples/sec, 355.26 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnet200d.ra2_in1k created, param count: 64693064
Running inference benchmark on resnet200d.ra2_in1k for 40 steps w/ input size (3, 320, 320) and batch size 256.
Infer [8/40]. 696.74 samples/sec. 367.427 ms/step.
Infer [16/40]. 696.92 samples/sec. 367.330 ms/step.
Infer [24/40]. 696.79 samples/sec. 367.397 ms/step.
Infer [32/40]. 696.76 samples/sec. 367.415 ms/step.
Infer [40/40]. 696.79 samples/sec. 367.401 ms/step.
Inference benchmark of resnet200d.ra2_in1k done. 696.70 samples/sec, 367.40 ms/step
Model resnet200d.ra2_in1k created, param count: 64693064
Running train benchmark on resnet200d.ra2_in1k for 40 steps w/ input size (3, 320, 320) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 400.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 66.06 MiB is free. Including non-PyTorch memory, this process has 23.58 GiB memory in use. Of the allocated memory 21.96 GiB is allocated by PyTorch, and 388.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model resnet200d.ra2_in1k created, param count: 64693064
Running train benchmark on resnet200d.ra2_in1k for 40 steps w/ input size (3, 320, 320) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 300.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 26.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.10 GiB is allocated by PyTorch, and 294.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model resnet200d.ra2_in1k created, param count: 64693064
Running train benchmark on resnet200d.ra2_in1k for 40 steps w/ input size (3, 320, 320) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 92.06 MiB is free. Including non-PyTorch memory, this process has 23.55 GiB memory in use. Of the allocated memory 22.19 GiB is allocated by PyTorch, and 135.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model resnet200d.ra2_in1k created, param count: 64693064
Running train benchmark on resnet200d.ra2_in1k for 40 steps w/ input size (3, 320, 320) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 76.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 48.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 21.91 GiB is allocated by PyTorch, and 460.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model resnet200d.ra2_in1k created, param count: 64693064
Running train benchmark on resnet200d.ra2_in1k for 40 steps w/ input size (3, 320, 320) and batch size 64.
Train [8/40]. 216.47 samples/sec. 295.658 ms/step.
Train [16/40]. 216.46 samples/sec. 295.661 ms/step.
Train [24/40]. 216.47 samples/sec. 295.658 ms/step.
Train [32/40]. 216.46 samples/sec. 295.664 ms/step.
Train [40/40]. 216.46 samples/sec. 295.665 ms/step.
Train benchmark of resnet200d.ra2_in1k done. 214.84 samples/sec, 295.67 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnetaa50.a1h_in1k created, param count: 25557032
Running inference benchmark on resnetaa50.a1h_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 2132.66 samples/sec. 120.038 ms/step.
Infer [16/40]. 2132.89 samples/sec. 120.025 ms/step.
Infer [24/40]. 2133.16 samples/sec. 120.010 ms/step.
Infer [32/40]. 2132.91 samples/sec. 120.024 ms/step.
Infer [40/40]. 2133.23 samples/sec. 120.006 ms/step.
Inference benchmark of resnetaa50.a1h_in1k done. 2132.68 samples/sec, 120.01 ms/step
Model resnetaa50.a1h_in1k created, param count: 25557032
Running train benchmark on resnetaa50.a1h_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 683.43 samples/sec. 374.584 ms/step.
Train [16/40]. 683.39 samples/sec. 374.604 ms/step.
Train [24/40]. 683.41 samples/sec. 374.591 ms/step.
Train [32/40]. 683.41 samples/sec. 374.590 ms/step.
Train [40/40]. 683.40 samples/sec. 374.600 ms/step.
Train benchmark of resnetaa50.a1h_in1k done. 681.48 samples/sec, 374.60 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnetaa50d.d_in12k created, param count: 47748493
Running inference benchmark on resnetaa50d.d_in12k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 2017.19 samples/sec. 126.909 ms/step.
Infer [16/40]. 2016.32 samples/sec. 126.964 ms/step.
Infer [24/40]. 2016.19 samples/sec. 126.972 ms/step.
Infer [32/40]. 2016.05 samples/sec. 126.981 ms/step.
Infer [40/40]. 2015.99 samples/sec. 126.985 ms/step.
Inference benchmark of resnetaa50d.d_in12k done. 2015.52 samples/sec, 126.98 ms/step
Model resnetaa50d.d_in12k created, param count: 47748493
Running train benchmark on resnetaa50d.d_in12k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 614.88 samples/sec. 416.339 ms/step.
Train [16/40]. 614.90 samples/sec. 416.325 ms/step.
Train [24/40]. 614.91 samples/sec. 416.324 ms/step.
Train [32/40]. 614.91 samples/sec. 416.323 ms/step.
Train [40/40]. 614.89 samples/sec. 416.332 ms/step.
Train benchmark of resnetaa50d.d_in12k done. 613.36 samples/sec, 416.33 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnetaa50d.sw_in12k created, param count: 47748493
Running inference benchmark on resnetaa50d.sw_in12k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 2017.44 samples/sec. 126.893 ms/step.
Infer [16/40]. 2017.00 samples/sec. 126.921 ms/step.
Infer [24/40]. 2017.09 samples/sec. 126.915 ms/step.
Infer [32/40]. 2017.04 samples/sec. 126.918 ms/step.
Infer [40/40]. 2016.68 samples/sec. 126.941 ms/step.
Inference benchmark of resnetaa50d.sw_in12k done. 2016.20 samples/sec, 126.94 ms/step
Model resnetaa50d.sw_in12k created, param count: 47748493
Running train benchmark on resnetaa50d.sw_in12k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 614.92 samples/sec. 416.313 ms/step.
Train [16/40]. 614.89 samples/sec. 416.335 ms/step.
Train [24/40]. 614.88 samples/sec. 416.340 ms/step.
Train [32/40]. 614.88 samples/sec. 416.339 ms/step.
Train [40/40]. 614.89 samples/sec. 416.336 ms/step.
Train benchmark of resnetaa50d.sw_in12k done. 613.36 samples/sec, 416.34 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnetaa50d.sw_in12k_ft_in1k created, param count: 25576264
Running inference benchmark on resnetaa50d.sw_in12k_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 2018.62 samples/sec. 126.819 ms/step.
Infer [16/40]. 2018.25 samples/sec. 126.843 ms/step.
Infer [24/40]. 2017.96 samples/sec. 126.861 ms/step.
Infer [32/40]. 2018.01 samples/sec. 126.857 ms/step.
Infer [40/40]. 2018.01 samples/sec. 126.858 ms/step.
Inference benchmark of resnetaa50d.sw_in12k_ft_in1k done. 2017.55 samples/sec, 126.86 ms/step
Model resnetaa50d.sw_in12k_ft_in1k created, param count: 25576264
Running train benchmark on resnetaa50d.sw_in12k_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 616.10 samples/sec. 415.514 ms/step.
Train [16/40]. 616.09 samples/sec. 415.523 ms/step.
Train [24/40]. 616.09 samples/sec. 415.524 ms/step.
Train [32/40]. 616.09 samples/sec. 415.522 ms/step.
Train [40/40]. 616.09 samples/sec. 415.524 ms/step.
Train benchmark of resnetaa50d.sw_in12k_ft_in1k done. 614.54 samples/sec, 415.52 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnetaa101d.sw_in12k created, param count: 66740621
Running inference benchmark on resnetaa101d.sw_in12k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 1137.11 samples/sec. 225.132 ms/step.
Infer [16/40]. 1136.37 samples/sec. 225.280 ms/step.
Infer [24/40]. 1136.23 samples/sec. 225.307 ms/step.
Infer [32/40]. 1135.89 samples/sec. 225.375 ms/step.
Infer [40/40]. 1136.34 samples/sec. 225.285 ms/step.
Inference benchmark of resnetaa101d.sw_in12k done. 1136.14 samples/sec, 225.28 ms/step
Model resnetaa101d.sw_in12k created, param count: 66740621
Running train benchmark on resnetaa101d.sw_in12k for 40 steps w/ input size (3, 288, 288) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 162.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 154.06 MiB is free. Including non-PyTorch memory, this process has 23.49 GiB memory in use. Of the allocated memory 22.14 GiB is allocated by PyTorch, and 124.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model resnetaa101d.sw_in12k created, param count: 66740621
Running train benchmark on resnetaa101d.sw_in12k for 40 steps w/ input size (3, 288, 288) and batch size 192.
Train [8/40]. 403.13 samples/sec. 476.274 ms/step.
Train [16/40]. 403.16 samples/sec. 476.234 ms/step.
Train [24/40]. 403.14 samples/sec. 476.265 ms/step.
Train [32/40]. 403.13 samples/sec. 476.275 ms/step.
Train [40/40]. 403.12 samples/sec. 476.280 ms/step.
Train benchmark of resnetaa101d.sw_in12k done. 401.80 samples/sec, 476.28 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnetaa101d.sw_in12k_ft_in1k created, param count: 44568392
Running inference benchmark on resnetaa101d.sw_in12k_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 1138.10 samples/sec. 224.937 ms/step.
Infer [16/40]. 1138.06 samples/sec. 224.943 ms/step.
Infer [24/40]. 1137.74 samples/sec. 225.007 ms/step.
Infer [32/40]. 1137.65 samples/sec. 225.025 ms/step.
Infer [40/40]. 1137.13 samples/sec. 225.128 ms/step.
Inference benchmark of resnetaa101d.sw_in12k_ft_in1k done. 1136.93 samples/sec, 225.13 ms/step
Model resnetaa101d.sw_in12k_ft_in1k created, param count: 44568392
Running train benchmark on resnetaa101d.sw_in12k_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 162.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 32.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 22.25 GiB is allocated by PyTorch, and 128.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model resnetaa101d.sw_in12k_ft_in1k created, param count: 44568392
Running train benchmark on resnetaa101d.sw_in12k_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 192.
Train [8/40]. 403.92 samples/sec. 475.338 ms/step.
Train [16/40]. 403.90 samples/sec. 475.368 ms/step.
Train [24/40]. 403.89 samples/sec. 475.372 ms/step.
Train [32/40]. 403.90 samples/sec. 475.364 ms/step.
Train [40/40]. 403.90 samples/sec. 475.363 ms/step.
Train benchmark of resnetaa101d.sw_in12k_ft_in1k done. 402.58 samples/sec, 475.36 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnetblur50.bt_in1k created, param count: 25557032
Running inference benchmark on resnetblur50.bt_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 2003.51 samples/sec. 127.776 ms/step.
Infer [16/40]. 2003.63 samples/sec. 127.768 ms/step.
Infer [24/40]. 2003.67 samples/sec. 127.765 ms/step.
Infer [32/40]. 2003.60 samples/sec. 127.770 ms/step.
Infer [40/40]. 2003.59 samples/sec. 127.771 ms/step.
Inference benchmark of resnetblur50.bt_in1k done. 2003.11 samples/sec, 127.77 ms/step
Model resnetblur50.bt_in1k created, param count: 25557032
Running train benchmark on resnetblur50.bt_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 82.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 34.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 22.21 GiB is allocated by PyTorch, and 173.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model resnetblur50.bt_in1k created, param count: 25557032
Running train benchmark on resnetblur50.bt_in1k for 40 steps w/ input size (3, 288, 288) and batch size 192.
Train [8/40]. 651.60 samples/sec. 294.661 ms/step.
Train [16/40]. 651.60 samples/sec. 294.659 ms/step.
Train [24/40]. 651.60 samples/sec. 294.661 ms/step.
Train [32/40]. 651.60 samples/sec. 294.658 ms/step.
Train [40/40]. 651.60 samples/sec. 294.661 ms/step.
Train benchmark of resnetblur50.bt_in1k done. 649.37 samples/sec, 294.66 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnetrs50.tf_in1k created, param count: 35691912
Running inference benchmark on resnetrs50.tf_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 2802.78 samples/sec. 91.338 ms/step.
Infer [16/40]. 2803.55 samples/sec. 91.313 ms/step.
Infer [24/40]. 2802.12 samples/sec. 91.360 ms/step.
Infer [32/40]. 2801.41 samples/sec. 91.383 ms/step.
Infer [40/40]. 2802.05 samples/sec. 91.362 ms/step.
Inference benchmark of resnetrs50.tf_in1k done. 2801.23 samples/sec, 91.36 ms/step
Model resnetrs50.tf_in1k created, param count: 35691912
Running train benchmark on resnetrs50.tf_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 905.27 samples/sec. 282.788 ms/step.
Train [16/40]. 905.39 samples/sec. 282.750 ms/step.
Train [24/40]. 905.28 samples/sec. 282.786 ms/step.
Train [32/40]. 904.94 samples/sec. 282.892 ms/step.
Train [40/40]. 904.64 samples/sec. 282.986 ms/step.
Train benchmark of resnetrs50.tf_in1k done. 900.79 samples/sec, 282.99 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnetrs101.tf_in1k created, param count: 63618696
Running inference benchmark on resnetrs101.tf_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 1036.16 samples/sec. 247.066 ms/step.
Infer [16/40]. 1035.94 samples/sec. 247.118 ms/step.
Infer [24/40]. 1035.90 samples/sec. 247.127 ms/step.
Infer [32/40]. 1035.83 samples/sec. 247.145 ms/step.
Infer [40/40]. 1035.84 samples/sec. 247.144 ms/step.
Inference benchmark of resnetrs101.tf_in1k done. 1035.67 samples/sec, 247.14 ms/step
Model resnetrs101.tf_in1k created, param count: 63618696
Running train benchmark on resnetrs101.tf_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 2.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 22.34 GiB is allocated by PyTorch, and 66.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model resnetrs101.tf_in1k created, param count: 63618696
Running train benchmark on resnetrs101.tf_in1k for 40 steps w/ input size (3, 288, 288) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 122.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 90.06 MiB is free. Including non-PyTorch memory, this process has 23.55 GiB memory in use. Of the allocated memory 22.12 GiB is allocated by PyTorch, and 207.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model resnetrs101.tf_in1k created, param count: 63618696
Running train benchmark on resnetrs101.tf_in1k for 40 steps w/ input size (3, 288, 288) and batch size 128.
Train [8/40]. 389.02 samples/sec. 329.031 ms/step.
Train [16/40]. 389.01 samples/sec. 329.037 ms/step.
Train [24/40]. 389.01 samples/sec. 329.038 ms/step.
Train [32/40]. 389.01 samples/sec. 329.038 ms/step.
Train [40/40]. 389.01 samples/sec. 329.041 ms/step.
Train benchmark of resnetrs101.tf_in1k done. 386.77 samples/sec, 329.04 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnetrs152.tf_in1k created, param count: 86621576
Running inference benchmark on resnetrs152.tf_in1k for 40 steps w/ input size (3, 320, 320) and batch size 256.
Infer [8/40]. 821.48 samples/sec. 311.634 ms/step.
Infer [16/40]. 820.86 samples/sec. 311.867 ms/step.
Infer [24/40]. 820.67 samples/sec. 311.940 ms/step.
Infer [32/40]. 820.66 samples/sec. 311.945 ms/step.
Infer [40/40]. 820.56 samples/sec. 311.982 ms/step.
Inference benchmark of resnetrs152.tf_in1k done. 820.46 samples/sec, 311.98 ms/step
Model resnetrs152.tf_in1k created, param count: 86621576
Running train benchmark on resnetrs152.tf_in1k for 40 steps w/ input size (3, 320, 320) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 400.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 326.06 MiB is free. Including non-PyTorch memory, this process has 23.32 GiB memory in use. Of the allocated memory 21.81 GiB is allocated by PyTorch, and 283.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model resnetrs152.tf_in1k created, param count: 86621576
Running train benchmark on resnetrs152.tf_in1k for 40 steps w/ input size (3, 320, 320) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 150.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 146.06 MiB is free. Including non-PyTorch memory, this process has 23.50 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 138.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model resnetrs152.tf_in1k created, param count: 86621576
Running train benchmark on resnetrs152.tf_in1k for 40 steps w/ input size (3, 320, 320) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 88.06 MiB is free. Including non-PyTorch memory, this process has 23.55 GiB memory in use. Of the allocated memory 22.16 GiB is allocated by PyTorch, and 161.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model resnetrs152.tf_in1k created, param count: 86621576
Running train benchmark on resnetrs152.tf_in1k for 40 steps w/ input size (3, 320, 320) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 10.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.11 GiB is allocated by PyTorch, and 295.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model resnetrs152.tf_in1k created, param count: 86621576
Running train benchmark on resnetrs152.tf_in1k for 40 steps w/ input size (3, 320, 320) and batch size 64.
Train [8/40]. 236.41 samples/sec. 270.721 ms/step.
Train [16/40]. 236.41 samples/sec. 270.717 ms/step.
Train [24/40]. 236.41 samples/sec. 270.713 ms/step.
Train [32/40]. 236.41 samples/sec. 270.714 ms/step.
Train [40/40]. 236.41 samples/sec. 270.714 ms/step.
Train benchmark of resnetrs152.tf_in1k done. 234.36 samples/sec, 270.71 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnetrs200.tf_in1k created, param count: 93209992
Running inference benchmark on resnetrs200.tf_in1k for 40 steps w/ input size (3, 320, 320) and batch size 256.
Infer [8/40]. 585.56 samples/sec. 437.187 ms/step.
Infer [16/40]. 585.55 samples/sec. 437.197 ms/step.
Infer [24/40]. 585.48 samples/sec. 437.244 ms/step.
Infer [32/40]. 585.44 samples/sec. 437.278 ms/step.
Infer [40/40]. 585.52 samples/sec. 437.219 ms/step.
Inference benchmark of resnetrs200.tf_in1k done. 585.47 samples/sec, 437.22 ms/step
Model resnetrs200.tf_in1k created, param count: 93209992
Running train benchmark on resnetrs200.tf_in1k for 40 steps w/ input size (3, 320, 320) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 400.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 314.06 MiB is free. Including non-PyTorch memory, this process has 23.33 GiB memory in use. Of the allocated memory 21.83 GiB is allocated by PyTorch, and 283.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model resnetrs200.tf_in1k created, param count: 93209992
Running train benchmark on resnetrs200.tf_in1k for 40 steps w/ input size (3, 320, 320) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 76.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 20.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.15 GiB is allocated by PyTorch, and 248.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model resnetrs200.tf_in1k created, param count: 93209992
Running train benchmark on resnetrs200.tf_in1k for 40 steps w/ input size (3, 320, 320) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 42.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 21.80 GiB is allocated by PyTorch, and 577.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model resnetrs200.tf_in1k created, param count: 93209992
Running train benchmark on resnetrs200.tf_in1k for 40 steps w/ input size (3, 320, 320) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 76.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 16.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 21.96 GiB is allocated by PyTorch, and 443.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model resnetrs200.tf_in1k created, param count: 93209992
Running train benchmark on resnetrs200.tf_in1k for 40 steps w/ input size (3, 320, 320) and batch size 64.
Train [8/40]. 172.60 samples/sec. 370.802 ms/step.
Train [16/40]. 172.59 samples/sec. 370.822 ms/step.
Train [24/40]. 172.59 samples/sec. 370.815 ms/step.
Train [32/40]. 172.59 samples/sec. 370.812 ms/step.
Train [40/40]. 172.59 samples/sec. 370.817 ms/step.
Train benchmark of resnetrs200.tf_in1k done. 171.08 samples/sec, 370.82 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnetrs270.tf_in1k created, param count: 129861448
Running inference benchmark on resnetrs270.tf_in1k for 40 steps w/ input size (3, 352, 352) and batch size 256.
Infer [8/40]. 373.11 samples/sec. 686.129 ms/step.
Infer [16/40]. 373.11 samples/sec. 686.127 ms/step.
Infer [24/40]. 373.11 samples/sec. 686.130 ms/step.
Infer [32/40]. 373.10 samples/sec. 686.140 ms/step.
Infer [40/40]. 373.10 samples/sec. 686.142 ms/step.
Inference benchmark of resnetrs270.tf_in1k done. 373.08 samples/sec, 686.14 ms/step
Model resnetrs270.tf_in1k created, param count: 129861448
Running train benchmark on resnetrs270.tf_in1k for 40 steps w/ input size (3, 352, 352) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 484.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 428.06 MiB is free. Including non-PyTorch memory, this process has 23.22 GiB memory in use. Of the allocated memory 21.94 GiB is allocated by PyTorch, and 47.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model resnetrs270.tf_in1k created, param count: 129861448
Running train benchmark on resnetrs270.tf_in1k for 40 steps w/ input size (3, 352, 352) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 364.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 166.06 MiB is free. Including non-PyTorch memory, this process has 23.48 GiB memory in use. Of the allocated memory 22.04 GiB is allocated by PyTorch, and 211.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model resnetrs270.tf_in1k created, param count: 129861448
Running train benchmark on resnetrs270.tf_in1k for 40 steps w/ input size (3, 352, 352) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 242.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 82.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 22.10 GiB is allocated by PyTorch, and 237.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model resnetrs270.tf_in1k created, param count: 129861448
Running train benchmark on resnetrs270.tf_in1k for 40 steps w/ input size (3, 352, 352) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 182.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 32.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 21.66 GiB is allocated by PyTorch, and 729.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model resnetrs270.tf_in1k created, param count: 129861448
Running train benchmark on resnetrs270.tf_in1k for 40 steps w/ input size (3, 352, 352) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 62.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 16.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 21.88 GiB is allocated by PyTorch, and 522.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model resnetrs270.tf_in1k created, param count: 129861448
Running train benchmark on resnetrs270.tf_in1k for 40 steps w/ input size (3, 352, 352) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 46.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 28.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 22.12 GiB is allocated by PyTorch, and 264.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model resnetrs270.tf_in1k created, param count: 129861448
Running train benchmark on resnetrs270.tf_in1k for 40 steps w/ input size (3, 352, 352) and batch size 32.
Train [8/40]. 110.20 samples/sec. 290.385 ms/step.
Train [16/40]. 110.18 samples/sec. 290.439 ms/step.
Train [24/40]. 110.18 samples/sec. 290.440 ms/step.
Train [32/40]. 110.17 samples/sec. 290.456 ms/step.
Train [40/40]. 110.17 samples/sec. 290.467 ms/step.
Train benchmark of resnetrs270.tf_in1k done. 108.79 samples/sec, 290.47 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnetrs350.tf_in1k created, param count: 163956168
Running inference benchmark on resnetrs350.tf_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 251.60 samples/sec. 1017.495 ms/step.
Infer [16/40]. 251.56 samples/sec. 1017.650 ms/step.
Infer [24/40]. 251.54 samples/sec. 1017.725 ms/step.
Infer [32/40]. 251.53 samples/sec. 1017.756 ms/step.
Infer [40/40]. 251.53 samples/sec. 1017.778 ms/step.
Inference benchmark of resnetrs350.tf_in1k done. 251.52 samples/sec, 1017.78 ms/step
Model resnetrs350.tf_in1k created, param count: 163956168
Running train benchmark on resnetrs350.tf_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 1.12 GiB. GPU 0 has a total capacty of 23.65 GiB of which 610.06 MiB is free. Including non-PyTorch memory, this process has 23.04 GiB memory in use. Of the allocated memory 21.35 GiB is allocated by PyTorch, and 475.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model resnetrs350.tf_in1k created, param count: 163956168
Running train benchmark on resnetrs350.tf_in1k for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 432.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 82.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 22.00 GiB is allocated by PyTorch, and 338.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model resnetrs350.tf_in1k created, param count: 163956168
Running train benchmark on resnetrs350.tf_in1k for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 288.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 54.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 21.81 GiB is allocated by PyTorch, and 562.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model resnetrs350.tf_in1k created, param count: 163956168
Running train benchmark on resnetrs350.tf_in1k for 40 steps w/ input size (3, 384, 384) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 54.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 48.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 21.92 GiB is allocated by PyTorch, and 451.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model resnetrs350.tf_in1k created, param count: 163956168
Running train benchmark on resnetrs350.tf_in1k for 40 steps w/ input size (3, 384, 384) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 144.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 78.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 22.18 GiB is allocated by PyTorch, and 158.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model resnetrs350.tf_in1k created, param count: 163956168
Running train benchmark on resnetrs350.tf_in1k for 40 steps w/ input size (3, 384, 384) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 54.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 54.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 21.73 GiB is allocated by PyTorch, and 641.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model resnetrs350.tf_in1k created, param count: 163956168
Running train benchmark on resnetrs350.tf_in1k for 40 steps w/ input size (3, 384, 384) and batch size 32.
ERROR: "CUDA out of memory. Tried to allocate 36.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 6.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.31 GiB is allocated by PyTorch, and 98.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model resnetrs350.tf_in1k created, param count: 163956168
Running train benchmark on resnetrs350.tf_in1k for 40 steps w/ input size (3, 384, 384) and batch size 24.
Train [8/40]. 70.47 samples/sec. 340.591 ms/step.
Train [16/40]. 70.47 samples/sec. 340.572 ms/step.
Train [24/40]. 70.47 samples/sec. 340.579 ms/step.
Train [32/40]. 70.47 samples/sec. 340.579 ms/step.
Train [40/40]. 70.46 samples/sec. 340.598 ms/step.
Train benchmark of resnetrs350.tf_in1k done. 69.50 samples/sec, 340.60 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnetrs420.tf_in1k created, param count: 191891656
Running inference benchmark on resnetrs420.tf_in1k for 40 steps w/ input size (3, 416, 416) and batch size 256.
Infer [8/40]. 179.88 samples/sec. 1423.200 ms/step.
Infer [16/40]. 179.84 samples/sec. 1423.470 ms/step.
Infer [24/40]. 179.83 samples/sec. 1423.564 ms/step.
Infer [32/40]. 179.82 samples/sec. 1423.622 ms/step.
Infer [40/40]. 179.82 samples/sec. 1423.648 ms/step.
Inference benchmark of resnetrs420.tf_in1k done. 179.81 samples/sec, 1423.65 ms/step
Model resnetrs420.tf_in1k created, param count: 191891656
Running train benchmark on resnetrs420.tf_in1k for 40 steps w/ input size (3, 416, 416) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 1.32 GiB. GPU 0 has a total capacty of 23.65 GiB of which 932.06 MiB is free. Including non-PyTorch memory, this process has 22.73 GiB memory in use. Of the allocated memory 21.09 GiB is allocated by PyTorch, and 419.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model resnetrs420.tf_in1k created, param count: 191891656
Running train benchmark on resnetrs420.tf_in1k for 40 steps w/ input size (3, 416, 416) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 508.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 82.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 21.86 GiB is allocated by PyTorch, and 480.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model resnetrs420.tf_in1k created, param count: 191891656
Running train benchmark on resnetrs420.tf_in1k for 40 steps w/ input size (3, 416, 416) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 338.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 192.06 MiB is free. Including non-PyTorch memory, this process has 23.45 GiB memory in use. Of the allocated memory 21.96 GiB is allocated by PyTorch, and 264.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model resnetrs420.tf_in1k created, param count: 191891656
Running train benchmark on resnetrs420.tf_in1k for 40 steps w/ input size (3, 416, 416) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 254.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 214.06 MiB is free. Including non-PyTorch memory, this process has 23.43 GiB memory in use. Of the allocated memory 22.04 GiB is allocated by PyTorch, and 161.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model resnetrs420.tf_in1k created, param count: 191891656
Running train benchmark on resnetrs420.tf_in1k for 40 steps w/ input size (3, 416, 416) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 170.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 36.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 248.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model resnetrs420.tf_in1k created, param count: 191891656
Running train benchmark on resnetrs420.tf_in1k for 40 steps w/ input size (3, 416, 416) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 128.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 64.06 MiB is free. Including non-PyTorch memory, this process has 23.58 GiB memory in use. Of the allocated memory 22.09 GiB is allocated by PyTorch, and 257.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model resnetrs420.tf_in1k created, param count: 191891656
Running train benchmark on resnetrs420.tf_in1k for 40 steps w/ input size (3, 416, 416) and batch size 32.
ERROR: "CUDA out of memory. Tried to allocate 44.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 40.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 21.87 GiB is allocated by PyTorch, and 508.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model resnetrs420.tf_in1k created, param count: 191891656
Running train benchmark on resnetrs420.tf_in1k for 40 steps w/ input size (3, 416, 416) and batch size 24.
ERROR: "CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 16.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.19 GiB is allocated by PyTorch, and 207.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model resnetrs420.tf_in1k created, param count: 191891656
Running train benchmark on resnetrs420.tf_in1k for 40 steps w/ input size (3, 416, 416) and batch size 16.
Train [8/40]. 44.21 samples/sec. 361.923 ms/step.
Train [16/40]. 44.23 samples/sec. 361.779 ms/step.
Train [24/40]. 44.22 samples/sec. 361.792 ms/step.
Train [32/40]. 44.22 samples/sec. 361.810 ms/step.
Train [40/40]. 44.22 samples/sec. 361.834 ms/step.
Train benchmark of resnetrs420.tf_in1k done. 43.56 samples/sec, 361.83 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnetv2_50.a1h_in1k created, param count: 25549352
Running inference benchmark on resnetv2_50.a1h_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 2349.29 samples/sec. 108.969 ms/step.
Infer [16/40]. 2349.22 samples/sec. 108.972 ms/step.
Infer [24/40]. 2349.08 samples/sec. 108.979 ms/step.
Infer [32/40]. 2349.09 samples/sec. 108.979 ms/step.
Infer [40/40]. 2349.20 samples/sec. 108.973 ms/step.
Inference benchmark of resnetv2_50.a1h_in1k done. 2348.51 samples/sec, 108.97 ms/step
Model resnetv2_50.a1h_in1k created, param count: 25549352
Running train benchmark on resnetv2_50.a1h_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 789.25 samples/sec. 324.357 ms/step.
Train [16/40]. 789.29 samples/sec. 324.341 ms/step.
Train [24/40]. 789.28 samples/sec. 324.346 ms/step.
Train [32/40]. 789.28 samples/sec. 324.344 ms/step.
Train [40/40]. 789.29 samples/sec. 324.341 ms/step.
Train benchmark of resnetv2_50.a1h_in1k done. 786.79 samples/sec, 324.34 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnetv2_50d_evos.ah_in1k created, param count: 25591368
Running inference benchmark on resnetv2_50d_evos.ah_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 995.36 samples/sec. 257.194 ms/step.
Infer [16/40]. 995.38 samples/sec. 257.189 ms/step.
Infer [24/40]. 995.33 samples/sec. 257.200 ms/step.
Infer [32/40]. 995.29 samples/sec. 257.212 ms/step.
Infer [40/40]. 995.30 samples/sec. 257.209 ms/step.
Inference benchmark of resnetv2_50d_evos.ah_in1k done. 995.13 samples/sec, 257.21 ms/step
Model resnetv2_50d_evos.ah_in1k created, param count: 25591368
Running train benchmark on resnetv2_50d_evos.ah_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 648.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 546.06 MiB is free. Including non-PyTorch memory, this process has 23.11 GiB memory in use. Of the allocated memory 21.70 GiB is allocated by PyTorch, and 179.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model resnetv2_50d_evos.ah_in1k created, param count: 25591368
Running train benchmark on resnetv2_50d_evos.ah_in1k for 40 steps w/ input size (3, 288, 288) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 486.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 316.06 MiB is free. Including non-PyTorch memory, this process has 23.33 GiB memory in use. Of the allocated memory 22.01 GiB is allocated by PyTorch, and 87.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model resnetv2_50d_evos.ah_in1k created, param count: 25591368
Running train benchmark on resnetv2_50d_evos.ah_in1k for 40 steps w/ input size (3, 288, 288) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 26.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.27 GiB is allocated by PyTorch, and 116.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model resnetv2_50d_evos.ah_in1k created, param count: 25591368
Running train benchmark on resnetv2_50d_evos.ah_in1k for 40 steps w/ input size (3, 288, 288) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 122.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 46.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 22.16 GiB is allocated by PyTorch, and 211.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model resnetv2_50d_evos.ah_in1k created, param count: 25591368
Running train benchmark on resnetv2_50d_evos.ah_in1k for 40 steps w/ input size (3, 288, 288) and batch size 64.
Train [8/40]. 313.24 samples/sec. 204.318 ms/step.
Train [16/40]. 313.25 samples/sec. 204.312 ms/step.
Train [24/40]. 313.24 samples/sec. 204.314 ms/step.
Train [32/40]. 313.24 samples/sec. 204.315 ms/step.
Train [40/40]. 313.24 samples/sec. 204.313 ms/step.
Train benchmark of resnetv2_50d_evos.ah_in1k done. 311.21 samples/sec, 204.31 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnetv2_50d_gn.ah_in1k created, param count: 25568584
Running inference benchmark on resnetv2_50d_gn.ah_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 2038.90 samples/sec. 125.558 ms/step.
Infer [16/40]. 2037.94 samples/sec. 125.617 ms/step.
Infer [24/40]. 2038.34 samples/sec. 125.593 ms/step.
Infer [32/40]. 2038.32 samples/sec. 125.594 ms/step.
Infer [40/40]. 2038.25 samples/sec. 125.598 ms/step.
Inference benchmark of resnetv2_50d_gn.ah_in1k done. 2037.72 samples/sec, 125.60 ms/step
Model resnetv2_50d_gn.ah_in1k created, param count: 25568584
Running train benchmark on resnetv2_50d_gn.ah_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 774.62 samples/sec. 330.486 ms/step.
Train [16/40]. 774.67 samples/sec. 330.465 ms/step.
Train [24/40]. 774.56 samples/sec. 330.510 ms/step.
Train [32/40]. 774.47 samples/sec. 330.549 ms/step.
Train [40/40]. 774.47 samples/sec. 330.549 ms/step.
Train benchmark of resnetv2_50d_gn.ah_in1k done. 772.02 samples/sec, 330.55 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnetv2_50x1_bit.goog_distilled_in1k created, param count: 25549352
Running inference benchmark on resnetv2_50x1_bit.goog_distilled_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3122.47 samples/sec. 81.986 ms/step.
Infer [16/40]. 3132.76 samples/sec. 81.717 ms/step.
Infer [24/40]. 3140.68 samples/sec. 81.511 ms/step.
Infer [32/40]. 3136.55 samples/sec. 81.618 ms/step.
Infer [40/40]. 3144.49 samples/sec. 81.412 ms/step.
Inference benchmark of resnetv2_50x1_bit.goog_distilled_in1k done. 3143.42 samples/sec, 81.41 ms/step
Model resnetv2_50x1_bit.goog_distilled_in1k created, param count: 25549352
Running train benchmark on resnetv2_50x1_bit.goog_distilled_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1317.99 samples/sec. 194.235 ms/step.
Train [16/40]. 1317.85 samples/sec. 194.256 ms/step.
Train [24/40]. 1317.96 samples/sec. 194.239 ms/step.
Train [32/40]. 1318.10 samples/sec. 194.219 ms/step.
Train [40/40]. 1318.05 samples/sec. 194.226 ms/step.
Train benchmark of resnetv2_50x1_bit.goog_distilled_in1k done. 1311.62 samples/sec, 194.23 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnetv2_50x1_bit.goog_in21k_ft_in1k created, param count: 25549352
Running inference benchmark on resnetv2_50x1_bit.goog_in21k_ft_in1k for 40 steps w/ input size (3, 448, 448) and batch size 256.
Infer [8/40]. 1026.02 samples/sec. 249.508 ms/step.
Infer [16/40]. 1025.95 samples/sec. 249.524 ms/step.
Infer [24/40]. 1025.91 samples/sec. 249.535 ms/step.
Infer [32/40]. 1025.88 samples/sec. 249.542 ms/step.
Infer [40/40]. 1025.85 samples/sec. 249.549 ms/step.
Inference benchmark of resnetv2_50x1_bit.goog_in21k_ft_in1k done. 1025.67 samples/sec, 249.55 ms/step
Model resnetv2_50x1_bit.goog_in21k_ft_in1k created, param count: 25549352
Running train benchmark on resnetv2_50x1_bit.goog_in21k_ft_in1k for 40 steps w/ input size (3, 448, 448) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 108.06 MiB is free. Including non-PyTorch memory, this process has 23.54 GiB memory in use. Of the allocated memory 22.20 GiB is allocated by PyTorch, and 103.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model resnetv2_50x1_bit.goog_in21k_ft_in1k created, param count: 25549352
Running train benchmark on resnetv2_50x1_bit.goog_in21k_ft_in1k for 40 steps w/ input size (3, 448, 448) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 294.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 6.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.27 GiB is allocated by PyTorch, and 131.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model resnetv2_50x1_bit.goog_in21k_ft_in1k created, param count: 25549352
Running train benchmark on resnetv2_50x1_bit.goog_in21k_ft_in1k for 40 steps w/ input size (3, 448, 448) and batch size 128.
Train [8/40]. 379.73 samples/sec. 337.084 ms/step.
Train [16/40]. 379.72 samples/sec. 337.088 ms/step.
Train [24/40]. 379.72 samples/sec. 337.091 ms/step.
Train [32/40]. 379.69 samples/sec. 337.117 ms/step.
Train [40/40]. 379.66 samples/sec. 337.142 ms/step.
Train benchmark of resnetv2_50x1_bit.goog_in21k_ft_in1k done. 378.39 samples/sec, 337.14 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnetv2_50x3_bit.goog_in21k_ft_in1k created, param count: 217319080
Running inference benchmark on resnetv2_50x3_bit.goog_in21k_ft_in1k for 40 steps w/ input size (3, 448, 448) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 4.59 GiB. GPU 0 has a total capacty of 23.65 GiB of which 3.21 GiB is free. Including non-PyTorch memory, this process has 20.43 GiB memory in use. Of the allocated memory 15.64 GiB is allocated by PyTorch, and 3.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model resnetv2_50x3_bit.goog_in21k_ft_in1k created, param count: 217319080
Running inference benchmark on resnetv2_50x3_bit.goog_in21k_ft_in1k for 40 steps w/ input size (3, 448, 448) and batch size 192.
Infer [8/40]. 252.18 samples/sec. 761.353 ms/step.
Infer [16/40]. 251.95 samples/sec. 762.050 ms/step.
Infer [24/40]. 252.01 samples/sec. 761.874 ms/step.
Infer [32/40]. 251.87 samples/sec. 762.295 ms/step.
Infer [40/40]. 251.87 samples/sec. 762.296 ms/step.
Inference benchmark of resnetv2_50x3_bit.goog_in21k_ft_in1k done. 251.85 samples/sec, 762.30 ms/step
Model resnetv2_50x3_bit.goog_in21k_ft_in1k created, param count: 217319080
Running train benchmark on resnetv2_50x3_bit.goog_in21k_ft_in1k for 40 steps w/ input size (3, 448, 448) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 4.59 GiB. GPU 0 has a total capacty of 23.65 GiB of which 852.06 MiB is free. Including non-PyTorch memory, this process has 22.81 GiB memory in use. Of the allocated memory 21.46 GiB is allocated by PyTorch, and 114.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model resnetv2_50x3_bit.goog_in21k_ft_in1k created, param count: 217319080
Running train benchmark on resnetv2_50x3_bit.goog_in21k_ft_in1k for 40 steps w/ input size (3, 448, 448) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 3.45 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.40 GiB is free. Including non-PyTorch memory, this process has 22.24 GiB memory in use. Of the allocated memory 19.65 GiB is allocated by PyTorch, and 1.36 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model resnetv2_50x3_bit.goog_in21k_ft_in1k created, param count: 217319080
Running train benchmark on resnetv2_50x3_bit.goog_in21k_ft_in1k for 40 steps w/ input size (3, 448, 448) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 588.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 140.06 MiB is free. Including non-PyTorch memory, this process has 23.50 GiB memory in use. Of the allocated memory 21.86 GiB is allocated by PyTorch, and 423.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model resnetv2_50x3_bit.goog_in21k_ft_in1k created, param count: 217319080
Running train benchmark on resnetv2_50x3_bit.goog_in21k_ft_in1k for 40 steps w/ input size (3, 448, 448) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 882.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 280.06 MiB is free. Including non-PyTorch memory, this process has 23.37 GiB memory in use. Of the allocated memory 21.25 GiB is allocated by PyTorch, and 910.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model resnetv2_50x3_bit.goog_in21k_ft_in1k created, param count: 217319080
Running train benchmark on resnetv2_50x3_bit.goog_in21k_ft_in1k for 40 steps w/ input size (3, 448, 448) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 294.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 290.06 MiB is free. Including non-PyTorch memory, this process has 23.36 GiB memory in use. Of the allocated memory 21.80 GiB is allocated by PyTorch, and 328.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model resnetv2_50x3_bit.goog_in21k_ft_in1k created, param count: 217319080
Running train benchmark on resnetv2_50x3_bit.goog_in21k_ft_in1k for 40 steps w/ input size (3, 448, 448) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 88.06 MiB is free. Including non-PyTorch memory, this process has 23.55 GiB memory in use. Of the allocated memory 22.07 GiB is allocated by PyTorch, and 257.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model resnetv2_50x3_bit.goog_in21k_ft_in1k created, param count: 217319080
Running train benchmark on resnetv2_50x3_bit.goog_in21k_ft_in1k for 40 steps w/ input size (3, 448, 448) and batch size 32.
Train [8/40]. 86.54 samples/sec. 369.779 ms/step.
Train [16/40]. 86.54 samples/sec. 369.764 ms/step.
Train [24/40]. 86.54 samples/sec. 369.763 ms/step.
Train [32/40]. 86.54 samples/sec. 369.763 ms/step.
Train [40/40]. 86.54 samples/sec. 369.769 ms/step.
Train benchmark of resnetv2_50x3_bit.goog_in21k_ft_in1k done. 86.27 samples/sec, 369.77 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnetv2_101.a1h_in1k created, param count: 44541480
Running inference benchmark on resnetv2_101.a1h_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 1233.43 samples/sec. 207.551 ms/step.
Infer [16/40]. 1233.23 samples/sec. 207.585 ms/step.
Infer [24/40]. 1234.09 samples/sec. 207.440 ms/step.
Infer [32/40]. 1234.06 samples/sec. 207.446 ms/step.
Infer [40/40]. 1234.25 samples/sec. 207.413 ms/step.
Inference benchmark of resnetv2_101.a1h_in1k done. 1233.99 samples/sec, 207.41 ms/step
Model resnetv2_101.a1h_in1k created, param count: 44541480
Running train benchmark on resnetv2_101.a1h_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 162.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 116.06 MiB is free. Including non-PyTorch memory, this process has 23.53 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 164.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model resnetv2_101.a1h_in1k created, param count: 44541480
Running train benchmark on resnetv2_101.a1h_in1k for 40 steps w/ input size (3, 288, 288) and batch size 192.
Train [8/40]. 478.00 samples/sec. 401.673 ms/step.
Train [16/40]. 478.02 samples/sec. 401.658 ms/step.
Train [24/40]. 477.99 samples/sec. 401.679 ms/step.
Train [32/40]. 477.98 samples/sec. 401.691 ms/step.
Train [40/40]. 477.98 samples/sec. 401.686 ms/step.
Train benchmark of resnetv2_101.a1h_in1k done. 476.21 samples/sec, 401.69 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnetv2_101x1_bit.goog_in21k_ft_in1k created, param count: 44541480
Running inference benchmark on resnetv2_101x1_bit.goog_in21k_ft_in1k for 40 steps w/ input size (3, 448, 448) and batch size 256.
Infer [8/40]. 621.54 samples/sec. 411.881 ms/step.
Infer [16/40]. 621.73 samples/sec. 411.754 ms/step.
Infer [24/40]. 621.88 samples/sec. 411.653 ms/step.
Infer [32/40]. 621.89 samples/sec. 411.647 ms/step.
Infer [40/40]. 622.11 samples/sec. 411.505 ms/step.
Inference benchmark of resnetv2_101x1_bit.goog_in21k_ft_in1k done. 622.03 samples/sec, 411.50 ms/step
Model resnetv2_101x1_bit.goog_in21k_ft_in1k created, param count: 44541480
Running train benchmark on resnetv2_101x1_bit.goog_in21k_ft_in1k for 40 steps w/ input size (3, 448, 448) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 92.06 MiB is free. Including non-PyTorch memory, this process has 23.55 GiB memory in use. Of the allocated memory 22.24 GiB is allocated by PyTorch, and 83.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model resnetv2_101x1_bit.goog_in21k_ft_in1k created, param count: 44541480
Running train benchmark on resnetv2_101x1_bit.goog_in21k_ft_in1k for 40 steps w/ input size (3, 448, 448) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 294.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 136.06 MiB is free. Including non-PyTorch memory, this process has 23.51 GiB memory in use. Of the allocated memory 21.88 GiB is allocated by PyTorch, and 406.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model resnetv2_101x1_bit.goog_in21k_ft_in1k created, param count: 44541480
Running train benchmark on resnetv2_101x1_bit.goog_in21k_ft_in1k for 40 steps w/ input size (3, 448, 448) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 196.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 168.06 MiB is free. Including non-PyTorch memory, this process has 23.48 GiB memory in use. Of the allocated memory 21.83 GiB is allocated by PyTorch, and 421.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model resnetv2_101x1_bit.goog_in21k_ft_in1k created, param count: 44541480
Running train benchmark on resnetv2_101x1_bit.goog_in21k_ft_in1k for 40 steps w/ input size (3, 448, 448) and batch size 96.
Train [8/40]. 238.72 samples/sec. 402.148 ms/step.
Train [16/40]. 238.73 samples/sec. 402.133 ms/step.
Train [24/40]. 238.73 samples/sec. 402.135 ms/step.
Train [32/40]. 238.72 samples/sec. 402.142 ms/step.
Train [40/40]. 238.72 samples/sec. 402.148 ms/step.
Train benchmark of resnetv2_101x1_bit.goog_in21k_ft_in1k done. 237.75 samples/sec, 402.15 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnetv2_101x3_bit.goog_in21k_ft_in1k created, param count: 387934888
Running inference benchmark on resnetv2_101x3_bit.goog_in21k_ft_in1k for 40 steps w/ input size (3, 448, 448) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 4.59 GiB. GPU 0 has a total capacty of 23.65 GiB of which 2.84 GiB is free. Including non-PyTorch memory, this process has 20.80 GiB memory in use. Of the allocated memory 15.96 GiB is allocated by PyTorch, and 3.61 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model resnetv2_101x3_bit.goog_in21k_ft_in1k created, param count: 387934888
Running inference benchmark on resnetv2_101x3_bit.goog_in21k_ft_in1k for 40 steps w/ input size (3, 448, 448) and batch size 192.
Infer [8/40]. 151.22 samples/sec. 1269.635 ms/step.
Infer [16/40]. 151.11 samples/sec. 1270.562 ms/step.
Infer [24/40]. 151.11 samples/sec. 1270.633 ms/step.
Infer [32/40]. 151.11 samples/sec. 1270.599 ms/step.
Infer [40/40]. 151.09 samples/sec. 1270.728 ms/step.
Inference benchmark of resnetv2_101x3_bit.goog_in21k_ft_in1k done. 151.09 samples/sec, 1270.73 ms/step
Model resnetv2_101x3_bit.goog_in21k_ft_in1k created, param count: 387934888
Running train benchmark on resnetv2_101x3_bit.goog_in21k_ft_in1k for 40 steps w/ input size (3, 448, 448) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 4.59 GiB. GPU 0 has a total capacty of 23.65 GiB of which 556.06 MiB is free. Including non-PyTorch memory, this process has 23.10 GiB memory in use. Of the allocated memory 21.78 GiB is allocated by PyTorch, and 85.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model resnetv2_101x3_bit.goog_in21k_ft_in1k created, param count: 387934888
Running train benchmark on resnetv2_101x3_bit.goog_in21k_ft_in1k for 40 steps w/ input size (3, 448, 448) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 3.45 GiB. GPU 0 has a total capacty of 23.65 GiB of which 774.06 MiB is free. Including non-PyTorch memory, this process has 22.88 GiB memory in use. Of the allocated memory 19.98 GiB is allocated by PyTorch, and 1.67 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model resnetv2_101x3_bit.goog_in21k_ft_in1k created, param count: 387934888
Running train benchmark on resnetv2_101x3_bit.goog_in21k_ft_in1k for 40 steps w/ input size (3, 448, 448) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 588.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 450.06 MiB is free. Including non-PyTorch memory, this process has 23.20 GiB memory in use. Of the allocated memory 21.60 GiB is allocated by PyTorch, and 375.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model resnetv2_101x3_bit.goog_in21k_ft_in1k created, param count: 387934888
Running train benchmark on resnetv2_101x3_bit.goog_in21k_ft_in1k for 40 steps w/ input size (3, 448, 448) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 882.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 56.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 21.57 GiB is allocated by PyTorch, and 806.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model resnetv2_101x3_bit.goog_in21k_ft_in1k created, param count: 387934888
Running train benchmark on resnetv2_101x3_bit.goog_in21k_ft_in1k for 40 steps w/ input size (3, 448, 448) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 294.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 52.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 21.83 GiB is allocated by PyTorch, and 535.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model resnetv2_101x3_bit.goog_in21k_ft_in1k created, param count: 387934888
Running train benchmark on resnetv2_101x3_bit.goog_in21k_ft_in1k for 40 steps w/ input size (3, 448, 448) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 222.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 212.06 MiB is free. Including non-PyTorch memory, this process has 23.43 GiB memory in use. Of the allocated memory 22.00 GiB is allocated by PyTorch, and 203.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model resnetv2_101x3_bit.goog_in21k_ft_in1k created, param count: 387934888
Running train benchmark on resnetv2_101x3_bit.goog_in21k_ft_in1k for 40 steps w/ input size (3, 448, 448) and batch size 32.
ERROR: "CUDA out of memory. Tried to allocate 74.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 2.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 22.26 GiB is allocated by PyTorch, and 149.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model resnetv2_101x3_bit.goog_in21k_ft_in1k created, param count: 387934888
Running train benchmark on resnetv2_101x3_bit.goog_in21k_ft_in1k for 40 steps w/ input size (3, 448, 448) and batch size 24.
Train [8/40]. 49.66 samples/sec. 483.308 ms/step.
Train [16/40]. 49.66 samples/sec. 483.326 ms/step.
Train [24/40]. 49.66 samples/sec. 483.323 ms/step.
Train [32/40]. 49.66 samples/sec. 483.323 ms/step.
Train [40/40]. 49.65 samples/sec. 483.372 ms/step.
Train benchmark of resnetv2_101x3_bit.goog_in21k_ft_in1k done. 49.48 samples/sec, 483.37 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnetv2_152x2_bit.goog_in21k_ft_in1k created, param count: 236335208
Running inference benchmark on resnetv2_152x2_bit.goog_in21k_ft_in1k for 40 steps w/ input size (3, 448, 448) and batch size 256.
Infer [8/40]. 190.63 samples/sec. 1342.934 ms/step.
Infer [16/40]. 190.70 samples/sec. 1342.425 ms/step.
Infer [24/40]. 190.70 samples/sec. 1342.448 ms/step.
Infer [32/40]. 190.68 samples/sec. 1342.567 ms/step.
Infer [40/40]. 190.69 samples/sec. 1342.476 ms/step.
Inference benchmark of resnetv2_152x2_bit.goog_in21k_ft_in1k done. 190.68 samples/sec, 1342.48 ms/step
Model resnetv2_152x2_bit.goog_in21k_ft_in1k created, param count: 236335208
Running train benchmark on resnetv2_152x2_bit.goog_in21k_ft_in1k for 40 steps w/ input size (3, 448, 448) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 3.06 GiB. GPU 0 has a total capacty of 23.65 GiB of which 870.06 MiB is free. Including non-PyTorch memory, this process has 22.79 GiB memory in use. Of the allocated memory 20.71 GiB is allocated by PyTorch, and 872.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model resnetv2_152x2_bit.goog_in21k_ft_in1k created, param count: 236335208
Running train benchmark on resnetv2_152x2_bit.goog_in21k_ft_in1k for 40 steps w/ input size (3, 448, 448) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 588.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 492.06 MiB is free. Including non-PyTorch memory, this process has 23.16 GiB memory in use. Of the allocated memory 21.40 GiB is allocated by PyTorch, and 543.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model resnetv2_152x2_bit.goog_in21k_ft_in1k created, param count: 236335208
Running train benchmark on resnetv2_152x2_bit.goog_in21k_ft_in1k for 40 steps w/ input size (3, 448, 448) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 418.06 MiB is free. Including non-PyTorch memory, this process has 23.23 GiB memory in use. Of the allocated memory 21.69 GiB is allocated by PyTorch, and 320.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model resnetv2_152x2_bit.goog_in21k_ft_in1k created, param count: 236335208
Running train benchmark on resnetv2_152x2_bit.goog_in21k_ft_in1k for 40 steps w/ input size (3, 448, 448) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 588.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 64.06 MiB is free. Including non-PyTorch memory, this process has 23.58 GiB memory in use. Of the allocated memory 22.15 GiB is allocated by PyTorch, and 205.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model resnetv2_152x2_bit.goog_in21k_ft_in1k created, param count: 236335208
Running train benchmark on resnetv2_152x2_bit.goog_in21k_ft_in1k for 40 steps w/ input size (3, 448, 448) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 196.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 20.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.15 GiB is allocated by PyTorch, and 243.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model resnetv2_152x2_bit.goog_in21k_ft_in1k created, param count: 236335208
Running train benchmark on resnetv2_152x2_bit.goog_in21k_ft_in1k for 40 steps w/ input size (3, 448, 448) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 148.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 144.06 MiB is free. Including non-PyTorch memory, this process has 23.50 GiB memory in use. Of the allocated memory 22.06 GiB is allocated by PyTorch, and 210.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model resnetv2_152x2_bit.goog_in21k_ft_in1k created, param count: 236335208
Running train benchmark on resnetv2_152x2_bit.goog_in21k_ft_in1k for 40 steps w/ input size (3, 448, 448) and batch size 32.
Train [8/40]. 65.63 samples/sec. 487.573 ms/step.
Train [16/40]. 65.90 samples/sec. 485.613 ms/step.
Train [24/40]. 65.98 samples/sec. 484.972 ms/step.
Train [32/40]. 66.03 samples/sec. 484.652 ms/step.
Train [40/40]. 66.05 samples/sec. 484.461 ms/step.
Train benchmark of resnetv2_152x2_bit.goog_in21k_ft_in1k done. 65.76 samples/sec, 484.46 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnetv2_152x2_bit.goog_teacher_in21k_ft_in1k created, param count: 236335208
Running inference benchmark on resnetv2_152x2_bit.goog_teacher_in21k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 699.06 samples/sec. 366.207 ms/step.
Infer [16/40]. 699.06 samples/sec. 366.205 ms/step.
Infer [24/40]. 699.07 samples/sec. 366.202 ms/step.
Infer [32/40]. 699.07 samples/sec. 366.202 ms/step.
Infer [40/40]. 699.00 samples/sec. 366.238 ms/step.
Inference benchmark of resnetv2_152x2_bit.goog_teacher_in21k_ft_in1k done. 698.91 samples/sec, 366.24 ms/step
Model resnetv2_152x2_bit.goog_teacher_in21k_ft_in1k created, param count: 236335208
Running train benchmark on resnetv2_152x2_bit.goog_teacher_in21k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 196.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 82.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 21.97 GiB is allocated by PyTorch, and 366.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model resnetv2_152x2_bit.goog_teacher_in21k_ft_in1k created, param count: 236335208
Running train benchmark on resnetv2_152x2_bit.goog_teacher_in21k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 20.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.33 GiB is allocated by PyTorch, and 65.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model resnetv2_152x2_bit.goog_teacher_in21k_ft_in1k created, param count: 236335208
Running train benchmark on resnetv2_152x2_bit.goog_teacher_in21k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
Train [8/40]. 247.80 samples/sec. 516.545 ms/step.
Train [16/40]. 247.80 samples/sec. 516.555 ms/step.
Train [24/40]. 247.79 samples/sec. 516.557 ms/step.
Train [32/40]. 247.79 samples/sec. 516.556 ms/step.
Train [40/40]. 247.79 samples/sec. 516.561 ms/step.
Train benchmark of resnetv2_152x2_bit.goog_teacher_in21k_ft_in1k done. 246.73 samples/sec, 516.56 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnetv2_152x2_bit.goog_teacher_in21k_ft_in1k_384 created, param count: 236335208
Running inference benchmark on resnetv2_152x2_bit.goog_teacher_in21k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 263.61 samples/sec. 971.132 ms/step.
Infer [16/40]. 263.58 samples/sec. 971.251 ms/step.
Infer [24/40]. 263.57 samples/sec. 971.292 ms/step.
Infer [32/40]. 263.56 samples/sec. 971.334 ms/step.
Infer [40/40]. 263.55 samples/sec. 971.344 ms/step.
Inference benchmark of resnetv2_152x2_bit.goog_teacher_in21k_ft_in1k_384 done. 263.54 samples/sec, 971.34 ms/step
Model resnetv2_152x2_bit.goog_teacher_in21k_ft_in1k_384 created, param count: 236335208
Running train benchmark on resnetv2_152x2_bit.goog_teacher_in21k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 2.25 GiB. GPU 0 has a total capacty of 23.65 GiB of which 156.06 MiB is free. Including non-PyTorch memory, this process has 23.49 GiB memory in use. Of the allocated memory 22.09 GiB is allocated by PyTorch, and 167.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model resnetv2_152x2_bit.goog_teacher_in21k_ft_in1k_384 created, param count: 236335208
Running train benchmark on resnetv2_152x2_bit.goog_teacher_in21k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 864.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 158.06 MiB is free. Including non-PyTorch memory, this process has 23.49 GiB memory in use. Of the allocated memory 21.75 GiB is allocated by PyTorch, and 512.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model resnetv2_152x2_bit.goog_teacher_in21k_ft_in1k_384 created, param count: 236335208
Running train benchmark on resnetv2_152x2_bit.goog_teacher_in21k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 576.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 390.06 MiB is free. Including non-PyTorch memory, this process has 23.26 GiB memory in use. Of the allocated memory 21.70 GiB is allocated by PyTorch, and 336.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model resnetv2_152x2_bit.goog_teacher_in21k_ft_in1k_384 created, param count: 236335208
Running train benchmark on resnetv2_152x2_bit.goog_teacher_in21k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 216.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 136.06 MiB is free. Including non-PyTorch memory, this process has 23.51 GiB memory in use. Of the allocated memory 22.00 GiB is allocated by PyTorch, and 277.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model resnetv2_152x2_bit.goog_teacher_in21k_ft_in1k_384 created, param count: 236335208
Running train benchmark on resnetv2_152x2_bit.goog_teacher_in21k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 144.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 38.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 22.30 GiB is allocated by PyTorch, and 73.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model resnetv2_152x2_bit.goog_teacher_in21k_ft_in1k_384 created, param count: 236335208
Running train benchmark on resnetv2_152x2_bit.goog_teacher_in21k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 108.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 56.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 22.25 GiB is allocated by PyTorch, and 105.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model resnetv2_152x2_bit.goog_teacher_in21k_ft_in1k_384 created, param count: 236335208
Running train benchmark on resnetv2_152x2_bit.goog_teacher_in21k_ft_in1k_384 for 40 steps w/ input size (3, 384, 384) and batch size 32.
Train [8/40]. 85.88 samples/sec. 372.601 ms/step.
Train [16/40]. 85.88 samples/sec. 372.602 ms/step.
Train [24/40]. 85.88 samples/sec. 372.607 ms/step.
Train [32/40]. 85.88 samples/sec. 372.616 ms/step.
Train [40/40]. 85.87 samples/sec. 372.635 ms/step.
Train benchmark of resnetv2_152x2_bit.goog_teacher_in21k_ft_in1k_384 done. 85.41 samples/sec, 372.63 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnetv2_152x4_bit.goog_in21k_ft_in1k created, param count: 936533224
Running inference benchmark on resnetv2_152x4_bit.goog_in21k_ft_in1k for 40 steps w/ input size (3, 480, 480) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 7.03 GiB. GPU 0 has a total capacty of 23.65 GiB of which 6.08 GiB is free. Including non-PyTorch memory, this process has 17.56 GiB memory in use. Of the allocated memory 11.00 GiB is allocated by PyTorch, and 5.33 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model resnetv2_152x4_bit.goog_in21k_ft_in1k created, param count: 936533224
Running inference benchmark on resnetv2_152x4_bit.goog_in21k_ft_in1k for 40 steps w/ input size (3, 480, 480) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 5.27 GiB. GPU 0 has a total capacty of 23.65 GiB of which 3.39 GiB is free. Including non-PyTorch memory, this process has 20.25 GiB memory in use. Of the allocated memory 15.19 GiB is allocated by PyTorch, and 3.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model resnetv2_152x4_bit.goog_in21k_ft_in1k created, param count: 936533224
Running inference benchmark on resnetv2_152x4_bit.goog_in21k_ft_in1k for 40 steps w/ input size (3, 480, 480) and batch size 128.
Infer [8/40]. 57.52 samples/sec. 2225.145 ms/step.
Infer [16/40]. 57.53 samples/sec. 2224.868 ms/step.
Infer [24/40]. 57.55 samples/sec. 2224.301 ms/step.
Infer [32/40]. 57.53 samples/sec. 2224.957 ms/step.
Infer [40/40]. 57.52 samples/sec. 2225.399 ms/step.
Inference benchmark of resnetv2_152x4_bit.goog_in21k_ft_in1k done. 57.52 samples/sec, 2225.40 ms/step
Model resnetv2_152x4_bit.goog_in21k_ft_in1k created, param count: 936533224
Running train benchmark on resnetv2_152x4_bit.goog_in21k_ft_in1k for 40 steps w/ input size (3, 480, 480) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 7.03 GiB. GPU 0 has a total capacty of 23.65 GiB of which 6.08 GiB is free. Including non-PyTorch memory, this process has 17.56 GiB memory in use. Of the allocated memory 11.00 GiB is allocated by PyTorch, and 5.33 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model resnetv2_152x4_bit.goog_in21k_ft_in1k created, param count: 936533224
Running train benchmark on resnetv2_152x4_bit.goog_in21k_ft_in1k for 40 steps w/ input size (3, 480, 480) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 5.27 GiB. GPU 0 has a total capacty of 23.65 GiB of which 3.30 GiB is free. Including non-PyTorch memory, this process has 20.34 GiB memory in use. Of the allocated memory 15.28 GiB is allocated by PyTorch, and 3.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model resnetv2_152x4_bit.goog_in21k_ft_in1k created, param count: 936533224
Running train benchmark on resnetv2_152x4_bit.goog_in21k_ft_in1k for 40 steps w/ input size (3, 480, 480) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 3.52 GiB. GPU 0 has a total capacty of 23.65 GiB of which 3.40 GiB is free. Including non-PyTorch memory, this process has 20.24 GiB memory in use. Of the allocated memory 17.81 GiB is allocated by PyTorch, and 1.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model resnetv2_152x4_bit.goog_in21k_ft_in1k created, param count: 936533224
Running train benchmark on resnetv2_152x4_bit.goog_in21k_ft_in1k for 40 steps w/ input size (3, 480, 480) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 2.64 GiB. GPU 0 has a total capacty of 23.65 GiB of which 2.10 GiB is free. Including non-PyTorch memory, this process has 21.54 GiB memory in use. Of the allocated memory 19.08 GiB is allocated by PyTorch, and 1.23 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model resnetv2_152x4_bit.goog_in21k_ft_in1k created, param count: 936533224
Running train benchmark on resnetv2_152x4_bit.goog_in21k_ft_in1k for 40 steps w/ input size (3, 480, 480) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 1.76 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.26 GiB is free. Including non-PyTorch memory, this process has 22.38 GiB memory in use. Of the allocated memory 20.34 GiB is allocated by PyTorch, and 823.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model resnetv2_152x4_bit.goog_in21k_ft_in1k created, param count: 936533224
Running train benchmark on resnetv2_152x4_bit.goog_in21k_ft_in1k for 40 steps w/ input size (3, 480, 480) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 676.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 426.06 MiB is free. Including non-PyTorch memory, this process has 23.22 GiB memory in use. Of the allocated memory 21.33 GiB is allocated by PyTorch, and 677.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model resnetv2_152x4_bit.goog_in21k_ft_in1k created, param count: 936533224
Running train benchmark on resnetv2_152x4_bit.goog_in21k_ft_in1k for 40 steps w/ input size (3, 480, 480) and batch size 32.
ERROR: "CUDA out of memory. Tried to allocate 114.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 88.06 MiB is free. Including non-PyTorch memory, this process has 23.55 GiB memory in use. Of the allocated memory 22.00 GiB is allocated by PyTorch, and 324.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model resnetv2_152x4_bit.goog_in21k_ft_in1k created, param count: 936533224
Running train benchmark on resnetv2_152x4_bit.goog_in21k_ft_in1k for 40 steps w/ input size (3, 480, 480) and batch size 24.
ERROR: "CUDA out of memory. Tried to allocate 170.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 6.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.04 GiB is allocated by PyTorch, and 376.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model resnetv2_152x4_bit.goog_in21k_ft_in1k created, param count: 936533224
Running train benchmark on resnetv2_152x4_bit.goog_in21k_ft_in1k for 40 steps w/ input size (3, 480, 480) and batch size 16.
ERROR: "CUDA out of memory. Tried to allocate 114.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 82.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 21.94 GiB is allocated by PyTorch, and 398.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 12 for retry.
Model resnetv2_152x4_bit.goog_in21k_ft_in1k created, param count: 936533224
Running train benchmark on resnetv2_152x4_bit.goog_in21k_ft_in1k for 40 steps w/ input size (3, 480, 480) and batch size 12.
ERROR: "CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 16.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 21.85 GiB is allocated by PyTorch, and 553.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 8 for retry.
Model resnetv2_152x4_bit.goog_in21k_ft_in1k created, param count: 936533224
Running train benchmark on resnetv2_152x4_bit.goog_in21k_ft_in1k for 40 steps w/ input size (3, 480, 480) and batch size 8.
Train [8/40]. 18.21 samples/sec. 439.288 ms/step.
Train [16/40]. 18.21 samples/sec. 439.300 ms/step.
Train [24/40]. 18.21 samples/sec. 439.297 ms/step.
Train [32/40]. 18.21 samples/sec. 439.299 ms/step.
Train [40/40]. 18.21 samples/sec. 439.302 ms/step.
Train benchmark of resnetv2_152x4_bit.goog_in21k_ft_in1k done. 18.12 samples/sec, 439.30 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnext26ts.ra2_in1k created, param count: 10297952
Running inference benchmark on resnext26ts.ra2_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 3884.35 samples/sec. 65.905 ms/step.
Infer [16/40]. 3884.15 samples/sec. 65.909 ms/step.
Infer [24/40]. 3884.10 samples/sec. 65.910 ms/step.
Infer [32/40]. 3884.11 samples/sec. 65.910 ms/step.
Infer [40/40]. 3884.10 samples/sec. 65.910 ms/step.
Inference benchmark of resnext26ts.ra2_in1k done. 3882.49 samples/sec, 65.91 ms/step
Model resnext26ts.ra2_in1k created, param count: 10297952
Running train benchmark on resnext26ts.ra2_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 1005.98 samples/sec. 254.478 ms/step.
Train [16/40]. 1005.96 samples/sec. 254.483 ms/step.
Train [24/40]. 1005.98 samples/sec. 254.479 ms/step.
Train [32/40]. 1005.97 samples/sec. 254.481 ms/step.
Train [40/40]. 1005.97 samples/sec. 254.482 ms/step.
Train benchmark of resnext26ts.ra2_in1k done. 1003.10 samples/sec, 254.48 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnext50_32x4d.a1_in1k created, param count: 25028904
Running inference benchmark on resnext50_32x4d.a1_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 2021.71 samples/sec. 126.625 ms/step.
Infer [16/40]. 2021.73 samples/sec. 126.624 ms/step.
Infer [24/40]. 2021.72 samples/sec. 126.625 ms/step.
Infer [32/40]. 2021.71 samples/sec. 126.626 ms/step.
Infer [40/40]. 2021.68 samples/sec. 126.627 ms/step.
Inference benchmark of resnext50_32x4d.a1_in1k done. 2021.19 samples/sec, 126.63 ms/step
Model resnext50_32x4d.a1_in1k created, param count: 25028904
Running train benchmark on resnext50_32x4d.a1_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 18.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.30 GiB is allocated by PyTorch, and 90.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model resnext50_32x4d.a1_in1k created, param count: 25028904
Running train benchmark on resnext50_32x4d.a1_in1k for 40 steps w/ input size (3, 288, 288) and batch size 192.
Train [8/40]. 634.47 samples/sec. 302.616 ms/step.
Train [16/40]. 634.16 samples/sec. 302.762 ms/step.
Train [24/40]. 634.02 samples/sec. 302.830 ms/step.
Train [32/40]. 633.94 samples/sec. 302.870 ms/step.
Train [40/40]. 633.89 samples/sec. 302.890 ms/step.
Train benchmark of resnext50_32x4d.a1_in1k done. 631.84 samples/sec, 302.89 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnext50_32x4d.a1h_in1k created, param count: 25028904
Running inference benchmark on resnext50_32x4d.a1h_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 2013.77 samples/sec. 127.125 ms/step.
Infer [16/40]. 2013.28 samples/sec. 127.156 ms/step.
Infer [24/40]. 2013.19 samples/sec. 127.162 ms/step.
Infer [32/40]. 2013.10 samples/sec. 127.167 ms/step.
Infer [40/40]. 2013.00 samples/sec. 127.174 ms/step.
Inference benchmark of resnext50_32x4d.a1h_in1k done. 2012.48 samples/sec, 127.17 ms/step
Model resnext50_32x4d.a1h_in1k created, param count: 25028904
Running train benchmark on resnext50_32x4d.a1h_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 18.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.30 GiB is allocated by PyTorch, and 90.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model resnext50_32x4d.a1h_in1k created, param count: 25028904
Running train benchmark on resnext50_32x4d.a1h_in1k for 40 steps w/ input size (3, 288, 288) and batch size 192.
Train [8/40]. 634.09 samples/sec. 302.798 ms/step.
Train [16/40]. 634.05 samples/sec. 302.814 ms/step.
Train [24/40]. 634.05 samples/sec. 302.815 ms/step.
Train [32/40]. 634.04 samples/sec. 302.820 ms/step.
Train [40/40]. 634.04 samples/sec. 302.819 ms/step.
Train benchmark of resnext50_32x4d.a1h_in1k done. 631.98 samples/sec, 302.82 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnext50_32x4d.a2_in1k created, param count: 25028904
Running inference benchmark on resnext50_32x4d.a2_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 2012.64 samples/sec. 127.196 ms/step.
Infer [16/40]. 2012.79 samples/sec. 127.187 ms/step.
Infer [24/40]. 2012.72 samples/sec. 127.191 ms/step.
Infer [32/40]. 2012.76 samples/sec. 127.188 ms/step.
Infer [40/40]. 2012.78 samples/sec. 127.187 ms/step.
Inference benchmark of resnext50_32x4d.a2_in1k done. 2012.28 samples/sec, 127.19 ms/step
Model resnext50_32x4d.a2_in1k created, param count: 25028904
Running train benchmark on resnext50_32x4d.a2_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 18.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.30 GiB is allocated by PyTorch, and 90.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model resnext50_32x4d.a2_in1k created, param count: 25028904
Running train benchmark on resnext50_32x4d.a2_in1k for 40 steps w/ input size (3, 288, 288) and batch size 192.
Train [8/40]. 634.03 samples/sec. 302.825 ms/step.
Train [16/40]. 634.04 samples/sec. 302.820 ms/step.
Train [24/40]. 634.04 samples/sec. 302.819 ms/step.
Train [32/40]. 634.03 samples/sec. 302.823 ms/step.
Train [40/40]. 634.04 samples/sec. 302.821 ms/step.
Train benchmark of resnext50_32x4d.a2_in1k done. 631.97 samples/sec, 302.82 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnext50_32x4d.a3_in1k created, param count: 25028904
Running inference benchmark on resnext50_32x4d.a3_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3525.01 samples/sec. 72.624 ms/step.
Infer [16/40]. 3523.35 samples/sec. 72.658 ms/step.
Infer [24/40]. 3523.60 samples/sec. 72.653 ms/step.
Infer [32/40]. 3522.52 samples/sec. 72.675 ms/step.
Infer [40/40]. 3522.37 samples/sec. 72.678 ms/step.
Inference benchmark of resnext50_32x4d.a3_in1k done. 3521.08 samples/sec, 72.68 ms/step
Model resnext50_32x4d.a3_in1k created, param count: 25028904
Running train benchmark on resnext50_32x4d.a3_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1066.38 samples/sec. 240.065 ms/step.
Train [16/40]. 1066.34 samples/sec. 240.073 ms/step.
Train [24/40]. 1066.22 samples/sec. 240.100 ms/step.
Train [32/40]. 1066.23 samples/sec. 240.099 ms/step.
Train [40/40]. 1066.21 samples/sec. 240.103 ms/step.
Train benchmark of resnext50_32x4d.a3_in1k done. 1062.09 samples/sec, 240.10 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnext50_32x4d.fb_ssl_yfcc100m_ft_in1k created, param count: 25028904
Running inference benchmark on resnext50_32x4d.fb_ssl_yfcc100m_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3523.69 samples/sec. 72.651 ms/step.
Infer [16/40]. 3521.42 samples/sec. 72.698 ms/step.
Infer [24/40]. 3522.19 samples/sec. 72.682 ms/step.
Infer [32/40]. 3522.17 samples/sec. 72.682 ms/step.
Infer [40/40]. 3522.06 samples/sec. 72.685 ms/step.
Inference benchmark of resnext50_32x4d.fb_ssl_yfcc100m_ft_in1k done. 3520.75 samples/sec, 72.69 ms/step
Model resnext50_32x4d.fb_ssl_yfcc100m_ft_in1k created, param count: 25028904
Running train benchmark on resnext50_32x4d.fb_ssl_yfcc100m_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1066.00 samples/sec. 240.150 ms/step.
Train [16/40]. 1066.18 samples/sec. 240.110 ms/step.
Train [24/40]. 1066.14 samples/sec. 240.118 ms/step.
Train [32/40]. 1066.23 samples/sec. 240.099 ms/step.
Train [40/40]. 1066.26 samples/sec. 240.092 ms/step.
Train benchmark of resnext50_32x4d.fb_ssl_yfcc100m_ft_in1k done. 1062.23 samples/sec, 240.09 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnext50_32x4d.fb_swsl_ig1b_ft_in1k created, param count: 25028904
Running inference benchmark on resnext50_32x4d.fb_swsl_ig1b_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3521.81 samples/sec. 72.690 ms/step.
Infer [16/40]. 3519.19 samples/sec. 72.744 ms/step.
Infer [24/40]. 3520.16 samples/sec. 72.724 ms/step.
Infer [32/40]. 3520.52 samples/sec. 72.717 ms/step.
Infer [40/40]. 3520.31 samples/sec. 72.721 ms/step.
Inference benchmark of resnext50_32x4d.fb_swsl_ig1b_ft_in1k done. 3519.01 samples/sec, 72.72 ms/step
Model resnext50_32x4d.fb_swsl_ig1b_ft_in1k created, param count: 25028904
Running train benchmark on resnext50_32x4d.fb_swsl_ig1b_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1066.31 samples/sec. 240.080 ms/step.
Train [16/40]. 1066.25 samples/sec. 240.093 ms/step.
Train [24/40]. 1066.23 samples/sec. 240.097 ms/step.
Train [32/40]. 1066.23 samples/sec. 240.098 ms/step.
Train [40/40]. 1066.26 samples/sec. 240.092 ms/step.
Train benchmark of resnext50_32x4d.fb_swsl_ig1b_ft_in1k done. 1062.29 samples/sec, 240.09 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnext50_32x4d.gluon_in1k created, param count: 25028904
Running inference benchmark on resnext50_32x4d.gluon_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3521.24 samples/sec. 72.702 ms/step.
Infer [16/40]. 3519.32 samples/sec. 72.741 ms/step.
Infer [24/40]. 3518.60 samples/sec. 72.756 ms/step.
Infer [32/40]. 3519.95 samples/sec. 72.728 ms/step.
Infer [40/40]. 3520.40 samples/sec. 72.719 ms/step.
Inference benchmark of resnext50_32x4d.gluon_in1k done. 3519.09 samples/sec, 72.72 ms/step
Model resnext50_32x4d.gluon_in1k created, param count: 25028904
Running train benchmark on resnext50_32x4d.gluon_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1065.99 samples/sec. 240.152 ms/step.
Train [16/40]. 1066.14 samples/sec. 240.118 ms/step.
Train [24/40]. 1066.16 samples/sec. 240.114 ms/step.
Train [32/40]. 1066.27 samples/sec. 240.090 ms/step.
Train [40/40]. 1066.33 samples/sec. 240.075 ms/step.
Train benchmark of resnext50_32x4d.gluon_in1k done. 1062.28 samples/sec, 240.07 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnext50_32x4d.ra_in1k created, param count: 25028904
Running inference benchmark on resnext50_32x4d.ra_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 2013.94 samples/sec. 127.114 ms/step.
Infer [16/40]. 2013.97 samples/sec. 127.112 ms/step.
Infer [24/40]. 2014.01 samples/sec. 127.109 ms/step.
Infer [32/40]. 2013.97 samples/sec. 127.112 ms/step.
Infer [40/40]. 2013.82 samples/sec. 127.122 ms/step.
Inference benchmark of resnext50_32x4d.ra_in1k done. 2013.32 samples/sec, 127.12 ms/step
Model resnext50_32x4d.ra_in1k created, param count: 25028904
Running train benchmark on resnext50_32x4d.ra_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 18.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.30 GiB is allocated by PyTorch, and 90.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model resnext50_32x4d.ra_in1k created, param count: 25028904
Running train benchmark on resnext50_32x4d.ra_in1k for 40 steps w/ input size (3, 288, 288) and batch size 192.
Train [8/40]. 634.07 samples/sec. 302.807 ms/step.
Train [16/40]. 634.05 samples/sec. 302.816 ms/step.
Train [24/40]. 634.06 samples/sec. 302.813 ms/step.
Train [32/40]. 634.06 samples/sec. 302.809 ms/step.
Train [40/40]. 634.05 samples/sec. 302.814 ms/step.
Train benchmark of resnext50_32x4d.ra_in1k done. 631.99 samples/sec, 302.81 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnext50_32x4d.tv2_in1k created, param count: 25028904
Running inference benchmark on resnext50_32x4d.tv2_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3520.61 samples/sec. 72.715 ms/step.
Infer [16/40]. 3520.26 samples/sec. 72.722 ms/step.
Infer [24/40]. 3519.12 samples/sec. 72.745 ms/step.
Infer [32/40]. 3518.65 samples/sec. 72.755 ms/step.
Infer [40/40]. 3518.30 samples/sec. 72.762 ms/step.
Inference benchmark of resnext50_32x4d.tv2_in1k done. 3516.98 samples/sec, 72.76 ms/step
Model resnext50_32x4d.tv2_in1k created, param count: 25028904
Running train benchmark on resnext50_32x4d.tv2_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1066.10 samples/sec. 240.128 ms/step.
Train [16/40]. 1066.22 samples/sec. 240.100 ms/step.
Train [24/40]. 1066.14 samples/sec. 240.119 ms/step.
Train [32/40]. 1066.22 samples/sec. 240.101 ms/step.
Train [40/40]. 1066.24 samples/sec. 240.097 ms/step.
Train benchmark of resnext50_32x4d.tv2_in1k done. 1062.12 samples/sec, 240.10 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnext50_32x4d.tv_in1k created, param count: 25028904
Running inference benchmark on resnext50_32x4d.tv_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3519.80 samples/sec. 72.731 ms/step.
Infer [16/40]. 3518.50 samples/sec. 72.758 ms/step.
Infer [24/40]. 3518.80 samples/sec. 72.752 ms/step.
Infer [32/40]. 3518.40 samples/sec. 72.760 ms/step.
Infer [40/40]. 3518.15 samples/sec. 72.766 ms/step.
Inference benchmark of resnext50_32x4d.tv_in1k done. 3516.83 samples/sec, 72.77 ms/step
Model resnext50_32x4d.tv_in1k created, param count: 25028904
Running train benchmark on resnext50_32x4d.tv_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1066.06 samples/sec. 240.136 ms/step.
Train [16/40]. 1066.07 samples/sec. 240.135 ms/step.
Train [24/40]. 1066.12 samples/sec. 240.123 ms/step.
Train [32/40]. 1066.11 samples/sec. 240.126 ms/step.
Train [40/40]. 1066.10 samples/sec. 240.127 ms/step.
Train benchmark of resnext50_32x4d.tv_in1k done. 1061.93 samples/sec, 240.13 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnext50d_32x4d.bt_in1k created, param count: 25048136
Running inference benchmark on resnext50d_32x4d.bt_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 1911.20 samples/sec. 133.947 ms/step.
Infer [16/40]. 1910.88 samples/sec. 133.970 ms/step.
Infer [24/40]. 1910.65 samples/sec. 133.986 ms/step.
Infer [32/40]. 1910.58 samples/sec. 133.991 ms/step.
Infer [40/40]. 1910.60 samples/sec. 133.989 ms/step.
Inference benchmark of resnext50d_32x4d.bt_in1k done. 1910.13 samples/sec, 133.99 ms/step
Model resnext50d_32x4d.bt_in1k created, param count: 25048136
Running train benchmark on resnext50d_32x4d.bt_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 82.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 18.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.26 GiB is allocated by PyTorch, and 135.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model resnext50d_32x4d.bt_in1k created, param count: 25048136
Running train benchmark on resnext50d_32x4d.bt_in1k for 40 steps w/ input size (3, 288, 288) and batch size 192.
Train [8/40]. 575.75 samples/sec. 333.479 ms/step.
Train [16/40]. 575.72 samples/sec. 333.498 ms/step.
Train [24/40]. 575.72 samples/sec. 333.494 ms/step.
Train [32/40]. 575.73 samples/sec. 333.488 ms/step.
Train [40/40]. 575.74 samples/sec. 333.486 ms/step.
Train benchmark of resnext50d_32x4d.bt_in1k done. 573.94 samples/sec, 333.49 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnext101_32x4d.fb_ssl_yfcc100m_ft_in1k created, param count: 44177704
Running inference benchmark on resnext101_32x4d.fb_ssl_yfcc100m_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 2284.80 samples/sec. 112.045 ms/step.
Infer [16/40]. 2284.25 samples/sec. 112.072 ms/step.
Infer [24/40]. 2284.03 samples/sec. 112.083 ms/step.
Infer [32/40]. 2283.96 samples/sec. 112.086 ms/step.
Infer [40/40]. 2284.09 samples/sec. 112.080 ms/step.
Inference benchmark of resnext101_32x4d.fb_ssl_yfcc100m_ft_in1k done. 2283.50 samples/sec, 112.08 ms/step
Model resnext101_32x4d.fb_ssl_yfcc100m_ft_in1k created, param count: 44177704
Running train benchmark on resnext101_32x4d.fb_ssl_yfcc100m_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 691.55 samples/sec. 370.184 ms/step.
Train [16/40]. 691.68 samples/sec. 370.114 ms/step.
Train [24/40]. 691.73 samples/sec. 370.089 ms/step.
Train [32/40]. 691.71 samples/sec. 370.096 ms/step.
Train [40/40]. 691.72 samples/sec. 370.093 ms/step.
Train benchmark of resnext101_32x4d.fb_ssl_yfcc100m_ft_in1k done. 688.99 samples/sec, 370.09 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnext101_32x4d.fb_swsl_ig1b_ft_in1k created, param count: 44177704
Running inference benchmark on resnext101_32x4d.fb_swsl_ig1b_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 2283.72 samples/sec. 112.098 ms/step.
Infer [16/40]. 2283.91 samples/sec. 112.088 ms/step.
Infer [24/40]. 2283.98 samples/sec. 112.085 ms/step.
Infer [32/40]. 2283.97 samples/sec. 112.085 ms/step.
Infer [40/40]. 2283.94 samples/sec. 112.087 ms/step.
Inference benchmark of resnext101_32x4d.fb_swsl_ig1b_ft_in1k done. 2283.34 samples/sec, 112.09 ms/step
Model resnext101_32x4d.fb_swsl_ig1b_ft_in1k created, param count: 44177704
Running train benchmark on resnext101_32x4d.fb_swsl_ig1b_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 691.77 samples/sec. 370.064 ms/step.
Train [16/40]. 691.78 samples/sec. 370.061 ms/step.
Train [24/40]. 691.80 samples/sec. 370.051 ms/step.
Train [32/40]. 691.77 samples/sec. 370.068 ms/step.
Train [40/40]. 691.77 samples/sec. 370.066 ms/step.
Train benchmark of resnext101_32x4d.fb_swsl_ig1b_ft_in1k done. 689.16 samples/sec, 370.07 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnext101_32x4d.gluon_in1k created, param count: 44177704
Running inference benchmark on resnext101_32x4d.gluon_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 2285.65 samples/sec. 112.003 ms/step.
Infer [16/40]. 2285.00 samples/sec. 112.035 ms/step.
Infer [24/40]. 2284.61 samples/sec. 112.054 ms/step.
Infer [32/40]. 2284.49 samples/sec. 112.060 ms/step.
Infer [40/40]. 2284.34 samples/sec. 112.068 ms/step.
Inference benchmark of resnext101_32x4d.gluon_in1k done. 2283.75 samples/sec, 112.07 ms/step
Model resnext101_32x4d.gluon_in1k created, param count: 44177704
Running train benchmark on resnext101_32x4d.gluon_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 691.85 samples/sec. 370.025 ms/step.
Train [16/40]. 691.76 samples/sec. 370.070 ms/step.
Train [24/40]. 691.72 samples/sec. 370.089 ms/step.
Train [32/40]. 691.72 samples/sec. 370.090 ms/step.
Train [40/40]. 691.73 samples/sec. 370.088 ms/step.
Train benchmark of resnext101_32x4d.gluon_in1k done. 689.05 samples/sec, 370.09 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnext101_32x8d.fb_ssl_yfcc100m_ft_in1k created, param count: 88791336
Running inference benchmark on resnext101_32x8d.fb_ssl_yfcc100m_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1423.10 samples/sec. 179.888 ms/step.
Infer [16/40]. 1423.57 samples/sec. 179.830 ms/step.
Infer [24/40]. 1423.64 samples/sec. 179.821 ms/step.
Infer [32/40]. 1423.63 samples/sec. 179.822 ms/step.
Infer [40/40]. 1423.46 samples/sec. 179.843 ms/step.
Inference benchmark of resnext101_32x8d.fb_ssl_yfcc100m_ft_in1k done. 1423.17 samples/sec, 179.84 ms/step
Model resnext101_32x8d.fb_ssl_yfcc100m_ft_in1k created, param count: 88791336
Running train benchmark on resnext101_32x8d.fb_ssl_yfcc100m_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 88.06 MiB is free. Including non-PyTorch memory, this process has 23.55 GiB memory in use. Of the allocated memory 21.69 GiB is allocated by PyTorch, and 646.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model resnext101_32x8d.fb_ssl_yfcc100m_ft_in1k created, param count: 88791336
Running train benchmark on resnext101_32x8d.fb_ssl_yfcc100m_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 28.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 22.28 GiB is allocated by PyTorch, and 101.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model resnext101_32x8d.fb_ssl_yfcc100m_ft_in1k created, param count: 88791336
Running train benchmark on resnext101_32x8d.fb_ssl_yfcc100m_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
Train [8/40]. 438.44 samples/sec. 291.945 ms/step.
Train [16/40]. 437.95 samples/sec. 292.271 ms/step.
Train [24/40]. 437.52 samples/sec. 292.559 ms/step.
Train [32/40]. 437.27 samples/sec. 292.727 ms/step.
Train [40/40]. 437.15 samples/sec. 292.803 ms/step.
Train benchmark of resnext101_32x8d.fb_ssl_yfcc100m_ft_in1k done. 435.07 samples/sec, 292.80 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnext101_32x8d.fb_swsl_ig1b_ft_in1k created, param count: 88791336
Running inference benchmark on resnext101_32x8d.fb_swsl_ig1b_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1423.85 samples/sec. 179.794 ms/step.
Infer [16/40]. 1423.72 samples/sec. 179.811 ms/step.
Infer [24/40]. 1423.61 samples/sec. 179.825 ms/step.
Infer [32/40]. 1423.50 samples/sec. 179.838 ms/step.
Infer [40/40]. 1423.42 samples/sec. 179.849 ms/step.
Inference benchmark of resnext101_32x8d.fb_swsl_ig1b_ft_in1k done. 1423.12 samples/sec, 179.85 ms/step
Model resnext101_32x8d.fb_swsl_ig1b_ft_in1k created, param count: 88791336
Running train benchmark on resnext101_32x8d.fb_swsl_ig1b_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 88.06 MiB is free. Including non-PyTorch memory, this process has 23.55 GiB memory in use. Of the allocated memory 21.69 GiB is allocated by PyTorch, and 646.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model resnext101_32x8d.fb_swsl_ig1b_ft_in1k created, param count: 88791336
Running train benchmark on resnext101_32x8d.fb_swsl_ig1b_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 24.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.28 GiB is allocated by PyTorch, and 106.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model resnext101_32x8d.fb_swsl_ig1b_ft_in1k created, param count: 88791336
Running train benchmark on resnext101_32x8d.fb_swsl_ig1b_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
Train [8/40]. 436.26 samples/sec. 293.400 ms/step.
Train [16/40]. 436.33 samples/sec. 293.358 ms/step.
Train [24/40]. 436.34 samples/sec. 293.347 ms/step.
Train [32/40]. 436.32 samples/sec. 293.365 ms/step.
Train [40/40]. 436.48 samples/sec. 293.258 ms/step.
Train benchmark of resnext101_32x8d.fb_swsl_ig1b_ft_in1k done. 434.34 samples/sec, 293.26 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnext101_32x8d.fb_wsl_ig1b_ft_in1k created, param count: 88791336
Running inference benchmark on resnext101_32x8d.fb_wsl_ig1b_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1423.98 samples/sec. 179.778 ms/step.
Infer [16/40]. 1423.69 samples/sec. 179.814 ms/step.
Infer [24/40]. 1423.51 samples/sec. 179.837 ms/step.
Infer [32/40]. 1423.50 samples/sec. 179.839 ms/step.
Infer [40/40]. 1423.43 samples/sec. 179.848 ms/step.
Inference benchmark of resnext101_32x8d.fb_wsl_ig1b_ft_in1k done. 1423.14 samples/sec, 179.85 ms/step
Model resnext101_32x8d.fb_wsl_ig1b_ft_in1k created, param count: 88791336
Running train benchmark on resnext101_32x8d.fb_wsl_ig1b_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 88.06 MiB is free. Including non-PyTorch memory, this process has 23.55 GiB memory in use. Of the allocated memory 21.69 GiB is allocated by PyTorch, and 646.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model resnext101_32x8d.fb_wsl_ig1b_ft_in1k created, param count: 88791336
Running train benchmark on resnext101_32x8d.fb_wsl_ig1b_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 24.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.28 GiB is allocated by PyTorch, and 106.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model resnext101_32x8d.fb_wsl_ig1b_ft_in1k created, param count: 88791336
Running train benchmark on resnext101_32x8d.fb_wsl_ig1b_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
Train [8/40]. 436.33 samples/sec. 293.359 ms/step.
Train [16/40]. 436.55 samples/sec. 293.207 ms/step.
Train [24/40]. 436.55 samples/sec. 293.208 ms/step.
Train [32/40]. 436.52 samples/sec. 293.231 ms/step.
Train [40/40]. 436.58 samples/sec. 293.191 ms/step.
Train benchmark of resnext101_32x8d.fb_wsl_ig1b_ft_in1k done. 434.52 samples/sec, 293.19 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnext101_32x8d.tv2_in1k created, param count: 88791336
Running inference benchmark on resnext101_32x8d.tv2_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1423.12 samples/sec. 179.886 ms/step.
Infer [16/40]. 1423.11 samples/sec. 179.888 ms/step.
Infer [24/40]. 1423.13 samples/sec. 179.885 ms/step.
Infer [32/40]. 1423.24 samples/sec. 179.871 ms/step.
Infer [40/40]. 1423.20 samples/sec. 179.876 ms/step.
Inference benchmark of resnext101_32x8d.tv2_in1k done. 1422.92 samples/sec, 179.88 ms/step
Model resnext101_32x8d.tv2_in1k created, param count: 88791336
Running train benchmark on resnext101_32x8d.tv2_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 88.06 MiB is free. Including non-PyTorch memory, this process has 23.55 GiB memory in use. Of the allocated memory 21.69 GiB is allocated by PyTorch, and 646.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model resnext101_32x8d.tv2_in1k created, param count: 88791336
Running train benchmark on resnext101_32x8d.tv2_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 24.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.28 GiB is allocated by PyTorch, and 106.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model resnext101_32x8d.tv2_in1k created, param count: 88791336
Running train benchmark on resnext101_32x8d.tv2_in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
Train [8/40]. 436.69 samples/sec. 293.115 ms/step.
Train [16/40]. 436.55 samples/sec. 293.206 ms/step.
Train [24/40]. 436.66 samples/sec. 293.136 ms/step.
Train [32/40]. 436.55 samples/sec. 293.207 ms/step.
Train [40/40]. 436.60 samples/sec. 293.175 ms/step.
Train benchmark of resnext101_32x8d.tv2_in1k done. 434.57 samples/sec, 293.18 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnext101_32x8d.tv_in1k created, param count: 88791336
Running inference benchmark on resnext101_32x8d.tv_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1424.10 samples/sec. 179.763 ms/step.
Infer [16/40]. 1424.22 samples/sec. 179.747 ms/step.
Infer [24/40]. 1423.82 samples/sec. 179.798 ms/step.
Infer [32/40]. 1423.64 samples/sec. 179.821 ms/step.
Infer [40/40]. 1423.55 samples/sec. 179.832 ms/step.
Inference benchmark of resnext101_32x8d.tv_in1k done. 1423.27 samples/sec, 179.83 ms/step
Model resnext101_32x8d.tv_in1k created, param count: 88791336
Running train benchmark on resnext101_32x8d.tv_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 88.06 MiB is free. Including non-PyTorch memory, this process has 23.55 GiB memory in use. Of the allocated memory 21.69 GiB is allocated by PyTorch, and 646.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model resnext101_32x8d.tv_in1k created, param count: 88791336
Running train benchmark on resnext101_32x8d.tv_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 24.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.28 GiB is allocated by PyTorch, and 106.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model resnext101_32x8d.tv_in1k created, param count: 88791336
Running train benchmark on resnext101_32x8d.tv_in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
Train [8/40]. 436.41 samples/sec. 293.304 ms/step.
Train [16/40]. 436.49 samples/sec. 293.248 ms/step.
Train [24/40]. 436.50 samples/sec. 293.243 ms/step.
Train [32/40]. 436.56 samples/sec. 293.198 ms/step.
Train [40/40]. 436.53 samples/sec. 293.221 ms/step.
Train benchmark of resnext101_32x8d.tv_in1k done. 434.51 samples/sec, 293.22 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnext101_32x16d.fb_ssl_yfcc100m_ft_in1k created, param count: 194026792
Running inference benchmark on resnext101_32x16d.fb_ssl_yfcc100m_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 804.67 samples/sec. 318.144 ms/step.
Infer [16/40]. 804.44 samples/sec. 318.232 ms/step.
Infer [24/40]. 804.37 samples/sec. 318.261 ms/step.
Infer [32/40]. 804.34 samples/sec. 318.272 ms/step.
Infer [40/40]. 804.33 samples/sec. 318.276 ms/step.
Inference benchmark of resnext101_32x16d.fb_ssl_yfcc100m_ft_in1k done. 804.22 samples/sec, 318.28 ms/step
Model resnext101_32x16d.fb_ssl_yfcc100m_ft_in1k created, param count: 194026792
Running train benchmark on resnext101_32x16d.fb_ssl_yfcc100m_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 392.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 132.06 MiB is free. Including non-PyTorch memory, this process has 23.51 GiB memory in use. Of the allocated memory 22.18 GiB is allocated by PyTorch, and 106.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model resnext101_32x16d.fb_ssl_yfcc100m_ft_in1k created, param count: 194026792
Running train benchmark on resnext101_32x16d.fb_ssl_yfcc100m_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 148.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 80.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 22.12 GiB is allocated by PyTorch, and 216.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model resnext101_32x16d.fb_ssl_yfcc100m_ft_in1k created, param count: 194026792
Running train benchmark on resnext101_32x16d.fb_ssl_yfcc100m_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 50.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 22.24 GiB is allocated by PyTorch, and 124.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model resnext101_32x16d.fb_ssl_yfcc100m_ft_in1k created, param count: 194026792
Running train benchmark on resnext101_32x16d.fb_ssl_yfcc100m_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 96.
Train [8/40]. 260.11 samples/sec. 369.080 ms/step.
Train [16/40]. 260.10 samples/sec. 369.089 ms/step.
Train [24/40]. 260.10 samples/sec. 369.084 ms/step.
Train [32/40]. 260.09 samples/sec. 369.097 ms/step.
Train [40/40]. 260.09 samples/sec. 369.099 ms/step.
Train benchmark of resnext101_32x16d.fb_ssl_yfcc100m_ft_in1k done. 259.11 samples/sec, 369.10 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnext101_32x16d.fb_swsl_ig1b_ft_in1k created, param count: 194026792
Running inference benchmark on resnext101_32x16d.fb_swsl_ig1b_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 804.51 samples/sec. 318.205 ms/step.
Infer [16/40]. 804.51 samples/sec. 318.206 ms/step.
Infer [24/40]. 804.51 samples/sec. 318.206 ms/step.
Infer [32/40]. 804.49 samples/sec. 318.215 ms/step.
Infer [40/40]. 804.48 samples/sec. 318.217 ms/step.
Inference benchmark of resnext101_32x16d.fb_swsl_ig1b_ft_in1k done. 804.37 samples/sec, 318.22 ms/step
Model resnext101_32x16d.fb_swsl_ig1b_ft_in1k created, param count: 194026792
Running train benchmark on resnext101_32x16d.fb_swsl_ig1b_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 392.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 132.06 MiB is free. Including non-PyTorch memory, this process has 23.51 GiB memory in use. Of the allocated memory 22.18 GiB is allocated by PyTorch, and 106.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model resnext101_32x16d.fb_swsl_ig1b_ft_in1k created, param count: 194026792
Running train benchmark on resnext101_32x16d.fb_swsl_ig1b_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 148.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 106.06 MiB is free. Including non-PyTorch memory, this process has 23.54 GiB memory in use. Of the allocated memory 22.12 GiB is allocated by PyTorch, and 185.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model resnext101_32x16d.fb_swsl_ig1b_ft_in1k created, param count: 194026792
Running train benchmark on resnext101_32x16d.fb_swsl_ig1b_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 56.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 22.27 GiB is allocated by PyTorch, and 82.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model resnext101_32x16d.fb_swsl_ig1b_ft_in1k created, param count: 194026792
Running train benchmark on resnext101_32x16d.fb_swsl_ig1b_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 96.
Train [8/40]. 260.04 samples/sec. 369.171 ms/step.
Train [16/40]. 260.04 samples/sec. 369.173 ms/step.
Train [24/40]. 260.04 samples/sec. 369.177 ms/step.
Train [32/40]. 260.05 samples/sec. 369.160 ms/step.
Train [40/40]. 260.06 samples/sec. 369.149 ms/step.
Train benchmark of resnext101_32x16d.fb_swsl_ig1b_ft_in1k done. 259.07 samples/sec, 369.15 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnext101_32x16d.fb_wsl_ig1b_ft_in1k created, param count: 194026792
Running inference benchmark on resnext101_32x16d.fb_wsl_ig1b_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 804.50 samples/sec. 318.208 ms/step.
Infer [16/40]. 804.49 samples/sec. 318.213 ms/step.
Infer [24/40]. 804.51 samples/sec. 318.206 ms/step.
Infer [32/40]. 804.51 samples/sec. 318.208 ms/step.
Infer [40/40]. 804.51 samples/sec. 318.205 ms/step.
Inference benchmark of resnext101_32x16d.fb_wsl_ig1b_ft_in1k done. 804.40 samples/sec, 318.20 ms/step
Model resnext101_32x16d.fb_wsl_ig1b_ft_in1k created, param count: 194026792
Running train benchmark on resnext101_32x16d.fb_wsl_ig1b_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 392.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 132.06 MiB is free. Including non-PyTorch memory, this process has 23.51 GiB memory in use. Of the allocated memory 22.18 GiB is allocated by PyTorch, and 106.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model resnext101_32x16d.fb_wsl_ig1b_ft_in1k created, param count: 194026792
Running train benchmark on resnext101_32x16d.fb_wsl_ig1b_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 148.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 106.06 MiB is free. Including non-PyTorch memory, this process has 23.54 GiB memory in use. Of the allocated memory 22.12 GiB is allocated by PyTorch, and 185.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model resnext101_32x16d.fb_wsl_ig1b_ft_in1k created, param count: 194026792
Running train benchmark on resnext101_32x16d.fb_wsl_ig1b_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 56.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 22.27 GiB is allocated by PyTorch, and 82.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model resnext101_32x16d.fb_wsl_ig1b_ft_in1k created, param count: 194026792
Running train benchmark on resnext101_32x16d.fb_wsl_ig1b_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 96.
Train [8/40]. 260.09 samples/sec. 369.108 ms/step.
Train [16/40]. 260.09 samples/sec. 369.106 ms/step.
Train [24/40]. 260.09 samples/sec. 369.099 ms/step.
Train [32/40]. 260.09 samples/sec. 369.104 ms/step.
Train [40/40]. 260.09 samples/sec. 369.105 ms/step.
Train benchmark of resnext101_32x16d.fb_wsl_ig1b_ft_in1k done. 259.14 samples/sec, 369.11 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnext101_32x32d.fb_wsl_ig1b_ft_in1k created, param count: 468530472
Running inference benchmark on resnext101_32x32d.fb_wsl_ig1b_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 412.28 samples/sec. 620.937 ms/step.
Infer [16/40]. 412.28 samples/sec. 620.930 ms/step.
Infer [24/40]. 412.21 samples/sec. 621.041 ms/step.
Infer [32/40]. 412.17 samples/sec. 621.103 ms/step.
Infer [40/40]. 412.14 samples/sec. 621.144 ms/step.
Inference benchmark of resnext101_32x32d.fb_wsl_ig1b_ft_in1k done. 412.11 samples/sec, 621.14 ms/step
Model resnext101_32x32d.fb_wsl_ig1b_ft_in1k created, param count: 468530472
Running train benchmark on resnext101_32x32d.fb_wsl_ig1b_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 1.53 GiB. GPU 0 has a total capacty of 23.65 GiB of which 266.06 MiB is free. Including non-PyTorch memory, this process has 23.38 GiB memory in use. Of the allocated memory 20.96 GiB is allocated by PyTorch, and 1.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model resnext101_32x32d.fb_wsl_ig1b_ft_in1k created, param count: 468530472
Running train benchmark on resnext101_32x32d.fb_wsl_ig1b_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 2.30 GiB. GPU 0 has a total capacty of 23.65 GiB of which 772.06 MiB is free. Including non-PyTorch memory, this process has 22.89 GiB memory in use. Of the allocated memory 19.96 GiB is allocated by PyTorch, and 1.69 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model resnext101_32x32d.fb_wsl_ig1b_ft_in1k created, param count: 468530472
Running train benchmark on resnext101_32x32d.fb_wsl_ig1b_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 510.06 MiB is free. Including non-PyTorch memory, this process has 23.14 GiB memory in use. Of the allocated memory 21.37 GiB is allocated by PyTorch, and 553.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model resnext101_32x32d.fb_wsl_ig1b_ft_in1k created, param count: 468530472
Running train benchmark on resnext101_32x32d.fb_wsl_ig1b_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 148.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 60.06 MiB is free. Including non-PyTorch memory, this process has 23.58 GiB memory in use. Of the allocated memory 22.12 GiB is allocated by PyTorch, and 233.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model resnext101_32x32d.fb_wsl_ig1b_ft_in1k created, param count: 468530472
Running train benchmark on resnext101_32x32d.fb_wsl_ig1b_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 196.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 194.06 MiB is free. Including non-PyTorch memory, this process has 23.45 GiB memory in use. Of the allocated memory 21.83 GiB is allocated by PyTorch, and 397.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model resnext101_32x32d.fb_wsl_ig1b_ft_in1k created, param count: 468530472
Running train benchmark on resnext101_32x32d.fb_wsl_ig1b_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 48.
Train [8/40]. 133.85 samples/sec. 358.621 ms/step.
Train [16/40]. 133.86 samples/sec. 358.585 ms/step.
Train [24/40]. 133.86 samples/sec. 358.579 ms/step.
Train [32/40]. 133.86 samples/sec. 358.574 ms/step.
Train [40/40]. 133.87 samples/sec. 358.565 ms/step.
Train benchmark of resnext101_32x32d.fb_wsl_ig1b_ft_in1k done. 133.36 samples/sec, 358.56 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnext101_64x4d.c1_in1k created, param count: 83455272
Running inference benchmark on resnext101_64x4d.c1_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 836.05 samples/sec. 306.201 ms/step.
Infer [16/40]. 835.82 samples/sec. 306.287 ms/step.
Infer [24/40]. 835.70 samples/sec. 306.330 ms/step.
Infer [32/40]. 835.66 samples/sec. 306.346 ms/step.
Infer [40/40]. 835.64 samples/sec. 306.350 ms/step.
Inference benchmark of resnext101_64x4d.c1_in1k done. 835.51 samples/sec, 306.35 ms/step
Model resnext101_64x4d.c1_in1k created, param count: 83455272
Running train benchmark on resnext101_64x4d.c1_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 324.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 52.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 22.28 GiB is allocated by PyTorch, and 77.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model resnext101_64x4d.c1_in1k created, param count: 83455272
Running train benchmark on resnext101_64x4d.c1_in1k for 40 steps w/ input size (3, 288, 288) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 122.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 64.06 MiB is free. Including non-PyTorch memory, this process has 23.58 GiB memory in use. Of the allocated memory 22.12 GiB is allocated by PyTorch, and 233.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model resnext101_64x4d.c1_in1k created, param count: 83455272
Running train benchmark on resnext101_64x4d.c1_in1k for 40 steps w/ input size (3, 288, 288) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 82.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 16.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 20.83 GiB is allocated by PyTorch, and 1.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model resnext101_64x4d.c1_in1k created, param count: 83455272
Running train benchmark on resnext101_64x4d.c1_in1k for 40 steps w/ input size (3, 288, 288) and batch size 96.
Train [8/40]. 263.68 samples/sec. 364.073 ms/step.
Train [16/40]. 263.65 samples/sec. 364.113 ms/step.
Train [24/40]. 263.66 samples/sec. 364.111 ms/step.
Train [32/40]. 263.66 samples/sec. 364.104 ms/step.
Train [40/40]. 263.66 samples/sec. 364.105 ms/step.
Train benchmark of resnext101_64x4d.c1_in1k done. 262.64 samples/sec, 364.11 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnext101_64x4d.gluon_in1k created, param count: 83455272
Running inference benchmark on resnext101_64x4d.gluon_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1438.11 samples/sec. 178.012 ms/step.
Infer [16/40]. 1438.00 samples/sec. 178.025 ms/step.
Infer [24/40]. 1437.76 samples/sec. 178.055 ms/step.
Infer [32/40]. 1437.66 samples/sec. 178.067 ms/step.
Infer [40/40]. 1437.58 samples/sec. 178.077 ms/step.
Inference benchmark of resnext101_64x4d.gluon_in1k done. 1437.27 samples/sec, 178.08 ms/step
Model resnext101_64x4d.gluon_in1k created, param count: 83455272
Running train benchmark on resnext101_64x4d.gluon_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 66.06 MiB is free. Including non-PyTorch memory, this process has 23.58 GiB memory in use. Of the allocated memory 21.68 GiB is allocated by PyTorch, and 677.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model resnext101_64x4d.gluon_in1k created, param count: 83455272
Running train benchmark on resnext101_64x4d.gluon_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 8.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.31 GiB is allocated by PyTorch, and 95.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model resnext101_64x4d.gluon_in1k created, param count: 83455272
Running train benchmark on resnext101_64x4d.gluon_in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
Train [8/40]. 441.62 samples/sec. 289.841 ms/step.
Train [16/40]. 441.66 samples/sec. 289.815 ms/step.
Train [24/40]. 441.71 samples/sec. 289.780 ms/step.
Train [32/40]. 441.74 samples/sec. 289.765 ms/step.
Train [40/40]. 441.69 samples/sec. 289.795 ms/step.
Train benchmark of resnext101_64x4d.gluon_in1k done. 439.63 samples/sec, 289.80 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model resnext101_64x4d.tv_in1k created, param count: 83455272
Running inference benchmark on resnext101_64x4d.tv_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1437.95 samples/sec. 178.031 ms/step.
Infer [16/40]. 1437.90 samples/sec. 178.037 ms/step.
Infer [24/40]. 1437.72 samples/sec. 178.059 ms/step.
Infer [32/40]. 1437.62 samples/sec. 178.072 ms/step.
Infer [40/40]. 1437.55 samples/sec. 178.080 ms/step.
Inference benchmark of resnext101_64x4d.tv_in1k done. 1437.25 samples/sec, 178.08 ms/step
Model resnext101_64x4d.tv_in1k created, param count: 83455272
Running train benchmark on resnext101_64x4d.tv_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 66.06 MiB is free. Including non-PyTorch memory, this process has 23.58 GiB memory in use. Of the allocated memory 21.68 GiB is allocated by PyTorch, and 677.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model resnext101_64x4d.tv_in1k created, param count: 83455272
Running train benchmark on resnext101_64x4d.tv_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 8.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.31 GiB is allocated by PyTorch, and 95.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model resnext101_64x4d.tv_in1k created, param count: 83455272
Running train benchmark on resnext101_64x4d.tv_in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
Train [8/40]. 441.54 samples/sec. 289.897 ms/step.
Train [16/40]. 441.45 samples/sec. 289.952 ms/step.
Train [24/40]. 441.58 samples/sec. 289.869 ms/step.
Train [32/40]. 441.57 samples/sec. 289.874 ms/step.
Train [40/40]. 441.55 samples/sec. 289.889 ms/step.
Train benchmark of resnext101_64x4d.tv_in1k done. 439.51 samples/sec, 289.89 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model rexnet_100.nav_in1k created, param count: 4796873
Running inference benchmark on rexnet_100.nav_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 9048.57 samples/sec. 28.292 ms/step.
Infer [16/40]. 9048.46 samples/sec. 28.292 ms/step.
Infer [24/40]. 9048.13 samples/sec. 28.293 ms/step.
Infer [32/40]. 9047.79 samples/sec. 28.294 ms/step.
Infer [40/40]. 9047.80 samples/sec. 28.294 ms/step.
Inference benchmark of rexnet_100.nav_in1k done. 9040.11 samples/sec, 28.29 ms/step
Model rexnet_100.nav_in1k created, param count: 4796873
Running train benchmark on rexnet_100.nav_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1990.60 samples/sec. 128.604 ms/step.
Train [16/40]. 1989.69 samples/sec. 128.663 ms/step.
Train [24/40]. 1988.95 samples/sec. 128.711 ms/step.
Train [32/40]. 1988.57 samples/sec. 128.736 ms/step.
Train [40/40]. 1988.34 samples/sec. 128.751 ms/step.
Train benchmark of rexnet_100.nav_in1k done. 1975.21 samples/sec, 128.75 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model rexnet_130.nav_in1k created, param count: 7557091
Running inference benchmark on rexnet_130.nav_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 6791.93 samples/sec. 37.692 ms/step.
Infer [16/40]. 6791.49 samples/sec. 37.694 ms/step.
Infer [24/40]. 6791.46 samples/sec. 37.694 ms/step.
Infer [32/40]. 6791.13 samples/sec. 37.696 ms/step.
Infer [40/40]. 6790.88 samples/sec. 37.698 ms/step.
Inference benchmark of rexnet_130.nav_in1k done. 6786.47 samples/sec, 37.70 ms/step
Model rexnet_130.nav_in1k created, param count: 7557091
Running train benchmark on rexnet_130.nav_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1560.60 samples/sec. 164.039 ms/step.
Train [16/40]. 1560.70 samples/sec. 164.029 ms/step.
Train [24/40]. 1560.77 samples/sec. 164.022 ms/step.
Train [32/40]. 1560.74 samples/sec. 164.024 ms/step.
Train [40/40]. 1560.78 samples/sec. 164.020 ms/step.
Train benchmark of rexnet_130.nav_in1k done. 1551.99 samples/sec, 164.02 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model rexnet_150.nav_in1k created, param count: 9728593
Running inference benchmark on rexnet_150.nav_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 5766.29 samples/sec. 44.396 ms/step.
Infer [16/40]. 5766.12 samples/sec. 44.397 ms/step.
Infer [24/40]. 5766.28 samples/sec. 44.396 ms/step.
Infer [32/40]. 5766.14 samples/sec. 44.397 ms/step.
Infer [40/40]. 5766.07 samples/sec. 44.398 ms/step.
Inference benchmark of rexnet_150.nav_in1k done. 5762.90 samples/sec, 44.40 ms/step
Model rexnet_150.nav_in1k created, param count: 9728593
Running train benchmark on rexnet_150.nav_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1371.35 samples/sec. 186.677 ms/step.
Train [16/40]. 1371.54 samples/sec. 186.651 ms/step.
Train [24/40]. 1371.54 samples/sec. 186.651 ms/step.
Train [32/40]. 1371.51 samples/sec. 186.656 ms/step.
Train [40/40]. 1371.53 samples/sec. 186.653 ms/step.
Train benchmark of rexnet_150.nav_in1k done. 1364.31 samples/sec, 186.65 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model rexnet_200.nav_in1k created, param count: 16366620
Running inference benchmark on rexnet_200.nav_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 4338.16 samples/sec. 59.011 ms/step.
Infer [16/40]. 4337.19 samples/sec. 59.024 ms/step.
Infer [24/40]. 4336.81 samples/sec. 59.030 ms/step.
Infer [32/40]. 4336.61 samples/sec. 59.032 ms/step.
Infer [40/40]. 4336.57 samples/sec. 59.033 ms/step.
Inference benchmark of rexnet_200.nav_in1k done. 4334.71 samples/sec, 59.03 ms/step
Model rexnet_200.nav_in1k created, param count: 16366620
Running train benchmark on rexnet_200.nav_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 62.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 6.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.04 GiB is allocated by PyTorch, and 366.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model rexnet_200.nav_in1k created, param count: 16366620
Running train benchmark on rexnet_200.nav_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
Train [8/40]. 1069.40 samples/sec. 179.540 ms/step.
Train [16/40]. 1069.46 samples/sec. 179.530 ms/step.
Train [24/40]. 1069.51 samples/sec. 179.521 ms/step.
Train [32/40]. 1069.54 samples/sec. 179.516 ms/step.
Train [40/40]. 1069.54 samples/sec. 179.517 ms/step.
Train benchmark of rexnet_200.nav_in1k done. 1063.68 samples/sec, 179.52 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model rexnet_300.nav_in1k created, param count: 34708792
Running inference benchmark on rexnet_300.nav_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 2670.76 samples/sec. 95.853 ms/step.
Infer [16/40]. 2670.24 samples/sec. 95.871 ms/step.
Infer [24/40]. 2669.92 samples/sec. 95.883 ms/step.
Infer [32/40]. 2669.70 samples/sec. 95.891 ms/step.
Infer [40/40]. 2669.57 samples/sec. 95.895 ms/step.
Inference benchmark of rexnet_300.nav_in1k done. 2668.81 samples/sec, 95.89 ms/step
Model rexnet_300.nav_in1k created, param count: 34708792
Running train benchmark on rexnet_300.nav_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 344.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 250.06 MiB is free. Including non-PyTorch memory, this process has 23.40 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 241.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model rexnet_300.nav_in1k created, param count: 34708792
Running train benchmark on rexnet_300.nav_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 152.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 96.06 MiB is free. Including non-PyTorch memory, this process has 23.55 GiB memory in use. Of the allocated memory 22.17 GiB is allocated by PyTorch, and 151.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model rexnet_300.nav_in1k created, param count: 34708792
Running train benchmark on rexnet_300.nav_in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
Train [8/40]. 691.10 samples/sec. 185.213 ms/step.
Train [16/40]. 691.15 samples/sec. 185.198 ms/step.
Train [24/40]. 691.16 samples/sec. 185.197 ms/step.
Train [32/40]. 691.14 samples/sec. 185.201 ms/step.
Train [40/40]. 691.14 samples/sec. 185.201 ms/step.
Train benchmark of rexnet_300.nav_in1k done. 687.38 samples/sec, 185.20 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model rexnetr_200.sw_in12k created, param count: 44235013
Running inference benchmark on rexnetr_200.sw_in12k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 2576.81 samples/sec. 99.348 ms/step.
Infer [16/40]. 2576.75 samples/sec. 99.350 ms/step.
Infer [24/40]. 2576.78 samples/sec. 99.349 ms/step.
Infer [32/40]. 2576.78 samples/sec. 99.349 ms/step.
Infer [40/40]. 2576.75 samples/sec. 99.350 ms/step.
Inference benchmark of rexnetr_200.sw_in12k done. 2576.04 samples/sec, 99.35 ms/step
Model rexnetr_200.sw_in12k created, param count: 44235013
Running train benchmark on rexnetr_200.sw_in12k for 40 steps w/ input size (3, 288, 288) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 66.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 64.06 MiB is free. Including non-PyTorch memory, this process has 23.58 GiB memory in use. Of the allocated memory 22.24 GiB is allocated by PyTorch, and 111.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model rexnetr_200.sw_in12k created, param count: 44235013
Running train benchmark on rexnetr_200.sw_in12k for 40 steps w/ input size (3, 288, 288) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 138.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 42.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 239.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model rexnetr_200.sw_in12k created, param count: 44235013
Running train benchmark on rexnetr_200.sw_in12k for 40 steps w/ input size (3, 288, 288) and batch size 128.
Train [8/40]. 648.25 samples/sec. 197.455 ms/step.
Train [16/40]. 648.25 samples/sec. 197.454 ms/step.
Train [24/40]. 648.22 samples/sec. 197.464 ms/step.
Train [32/40]. 648.23 samples/sec. 197.462 ms/step.
Train [40/40]. 648.23 samples/sec. 197.462 ms/step.
Train benchmark of rexnetr_200.sw_in12k done. 644.79 samples/sec, 197.46 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model rexnetr_200.sw_in12k_ft_in1k created, param count: 16522432
Running inference benchmark on rexnetr_200.sw_in12k_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 2581.31 samples/sec. 99.174 ms/step.
Infer [16/40]. 2581.22 samples/sec. 99.178 ms/step.
Infer [24/40]. 2581.12 samples/sec. 99.182 ms/step.
Infer [32/40]. 2581.20 samples/sec. 99.179 ms/step.
Infer [40/40]. 2581.24 samples/sec. 99.177 ms/step.
Inference benchmark of rexnetr_200.sw_in12k_ft_in1k done. 2580.53 samples/sec, 99.18 ms/step
Model rexnetr_200.sw_in12k_ft_in1k created, param count: 16522432
Running train benchmark on rexnetr_200.sw_in12k_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 396.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 74.06 MiB is free. Including non-PyTorch memory, this process has 23.57 GiB memory in use. Of the allocated memory 22.25 GiB is allocated by PyTorch, and 87.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model rexnetr_200.sw_in12k_ft_in1k created, param count: 16522432
Running train benchmark on rexnetr_200.sw_in12k_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 138.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 104.06 MiB is free. Including non-PyTorch memory, this process has 23.54 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 230.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model rexnetr_200.sw_in12k_ft_in1k created, param count: 16522432
Running train benchmark on rexnetr_200.sw_in12k_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 128.
Train [8/40]. 651.73 samples/sec. 196.401 ms/step.
Train [16/40]. 651.69 samples/sec. 196.414 ms/step.
Train [24/40]. 651.68 samples/sec. 196.417 ms/step.
Train [32/40]. 651.72 samples/sec. 196.404 ms/step.
Train [40/40]. 651.73 samples/sec. 196.400 ms/step.
Train benchmark of rexnetr_200.sw_in12k_ft_in1k done. 648.17 samples/sec, 196.40 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model rexnetr_300.sw_in12k created, param count: 76373469
Running inference benchmark on rexnetr_300.sw_in12k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 1664.77 samples/sec. 153.775 ms/step.
Infer [16/40]. 1664.60 samples/sec. 153.790 ms/step.
Infer [24/40]. 1664.55 samples/sec. 153.795 ms/step.
Infer [32/40]. 1664.54 samples/sec. 153.796 ms/step.
Infer [40/40]. 1664.49 samples/sec. 153.801 ms/step.
Inference benchmark of rexnetr_300.sw_in12k done. 1664.12 samples/sec, 153.80 ms/step
Model rexnetr_300.sw_in12k created, param count: 76373469
Running train benchmark on rexnetr_300.sw_in12k for 40 steps w/ input size (3, 288, 288) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 1.19 GiB. GPU 0 has a total capacty of 23.65 GiB of which 782.06 MiB is free. Including non-PyTorch memory, this process has 22.88 GiB memory in use. Of the allocated memory 21.56 GiB is allocated by PyTorch, and 84.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model rexnetr_300.sw_in12k created, param count: 76373469
Running train benchmark on rexnetr_300.sw_in12k for 40 steps w/ input size (3, 288, 288) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 1.25 GiB. GPU 0 has a total capacty of 23.65 GiB of which 378.06 MiB is free. Including non-PyTorch memory, this process has 23.27 GiB memory in use. Of the allocated memory 21.80 GiB is allocated by PyTorch, and 248.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model rexnetr_300.sw_in12k created, param count: 76373469
Running train benchmark on rexnetr_300.sw_in12k for 40 steps w/ input size (3, 288, 288) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 138.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 126.06 MiB is free. Including non-PyTorch memory, this process has 23.52 GiB memory in use. Of the allocated memory 22.10 GiB is allocated by PyTorch, and 184.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model rexnetr_300.sw_in12k created, param count: 76373469
Running train benchmark on rexnetr_300.sw_in12k for 40 steps w/ input size (3, 288, 288) and batch size 96.
Train [8/40]. 432.74 samples/sec. 221.840 ms/step.
Train [16/40]. 432.75 samples/sec. 221.836 ms/step.
Train [24/40]. 432.76 samples/sec. 221.830 ms/step.
Train [32/40]. 432.74 samples/sec. 221.840 ms/step.
Train [40/40]. 432.74 samples/sec. 221.841 ms/step.
Train benchmark of rexnetr_300.sw_in12k done. 430.53 samples/sec, 221.84 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model rexnetr_300.sw_in12k_ft_in1k created, param count: 34810008
Running inference benchmark on rexnetr_300.sw_in12k_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 1667.68 samples/sec. 153.506 ms/step.
Infer [16/40]. 1667.62 samples/sec. 153.512 ms/step.
Infer [24/40]. 1667.60 samples/sec. 153.514 ms/step.
Infer [32/40]. 1667.56 samples/sec. 153.518 ms/step.
Infer [40/40]. 1667.46 samples/sec. 153.527 ms/step.
Inference benchmark of rexnetr_300.sw_in12k_ft_in1k done. 1667.09 samples/sec, 153.53 ms/step
Model rexnetr_300.sw_in12k_ft_in1k created, param count: 34810008
Running train benchmark on rexnetr_300.sw_in12k_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 1.19 GiB. GPU 0 has a total capacty of 23.65 GiB of which 782.06 MiB is free. Including non-PyTorch memory, this process has 22.88 GiB memory in use. Of the allocated memory 21.48 GiB is allocated by PyTorch, and 164.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model rexnetr_300.sw_in12k_ft_in1k created, param count: 34810008
Running train benchmark on rexnetr_300.sw_in12k_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 1.25 GiB. GPU 0 has a total capacty of 23.65 GiB of which 530.06 MiB is free. Including non-PyTorch memory, this process has 23.12 GiB memory in use. Of the allocated memory 21.72 GiB is allocated by PyTorch, and 175.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model rexnetr_300.sw_in12k_ft_in1k created, param count: 34810008
Running train benchmark on rexnetr_300.sw_in12k_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 138.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 104.06 MiB is free. Including non-PyTorch memory, this process has 23.54 GiB memory in use. Of the allocated memory 22.03 GiB is allocated by PyTorch, and 286.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model rexnetr_300.sw_in12k_ft_in1k created, param count: 34810008
Running train benchmark on rexnetr_300.sw_in12k_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 96.
Train [8/40]. 435.52 samples/sec. 220.428 ms/step.
Train [16/40]. 435.49 samples/sec. 220.440 ms/step.
Train [24/40]. 435.41 samples/sec. 220.481 ms/step.
Train [32/40]. 435.36 samples/sec. 220.507 ms/step.
Train [40/40]. 435.34 samples/sec. 220.517 ms/step.
Train benchmark of rexnetr_300.sw_in12k_ft_in1k done. 433.13 samples/sec, 220.52 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model samvit_base_patch16.sa1b created, param count: 89670912
Running inference benchmark on samvit_base_patch16.sa1b for 40 steps w/ input size (3, 1024, 1024) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 5.50 GiB. GPU 0 has a total capacty of 23.65 GiB of which 4.23 GiB is free. Including non-PyTorch memory, this process has 19.41 GiB memory in use. Of the allocated memory 17.54 GiB is allocated by PyTorch, and 650.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model samvit_base_patch16.sa1b created, param count: 89670912
Running inference benchmark on samvit_base_patch16.sa1b for 40 steps w/ input size (3, 1024, 1024) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 4.12 GiB. GPU 0 has a total capacty of 23.65 GiB of which 4.07 GiB is free. Including non-PyTorch memory, this process has 19.57 GiB memory in use. Of the allocated memory 17.91 GiB is allocated by PyTorch, and 438.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model samvit_base_patch16.sa1b created, param count: 89670912
Running inference benchmark on samvit_base_patch16.sa1b for 40 steps w/ input size (3, 1024, 1024) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 48.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 10.40 GiB is free. Including non-PyTorch memory, this process has 13.24 GiB memory in use. Of the allocated memory 8.43 GiB is allocated by PyTorch, and 3.57 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model samvit_base_patch16.sa1b created, param count: 89670912
Running inference benchmark on samvit_base_patch16.sa1b for 40 steps w/ input size (3, 1024, 1024) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 36.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 14.25 GiB is free. Including non-PyTorch memory, this process has 9.39 GiB memory in use. Of the allocated memory 6.37 GiB is allocated by PyTorch, and 1.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model samvit_base_patch16.sa1b created, param count: 89670912
Running inference benchmark on samvit_base_patch16.sa1b for 40 steps w/ input size (3, 1024, 1024) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 24.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 17.40 GiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Of the allocated memory 4.31 GiB is allocated by PyTorch, and 719.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model samvit_base_patch16.sa1b created, param count: 89670912
Running inference benchmark on samvit_base_patch16.sa1b for 40 steps w/ input size (3, 1024, 1024) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 288.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 110.06 MiB is free. Including non-PyTorch memory, this process has 23.53 GiB memory in use. Of the allocated memory 21.56 GiB is allocated by PyTorch, and 759.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model samvit_base_patch16.sa1b created, param count: 89670912
Running inference benchmark on samvit_base_patch16.sa1b for 40 steps w/ input size (3, 1024, 1024) and batch size 32.
ERROR: "CUDA out of memory. Tried to allocate 12.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 6.73 GiB is free. Including non-PyTorch memory, this process has 16.91 GiB memory in use. Of the allocated memory 14.62 GiB is allocated by PyTorch, and 1.05 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model samvit_base_patch16.sa1b created, param count: 89670912
Running inference benchmark on samvit_base_patch16.sa1b for 40 steps w/ input size (3, 1024, 1024) and batch size 24.
ERROR: "CUDA out of memory. Tried to allocate 9.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.93 GiB is free. Including non-PyTorch memory, this process has 21.71 GiB memory in use. Of the allocated memory 20.01 GiB is allocated by PyTorch, and 479.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model samvit_base_patch16.sa1b created, param count: 89670912
Running inference benchmark on samvit_base_patch16.sa1b for 40 steps w/ input size (3, 1024, 1024) and batch size 16.
Infer [8/40]. 35.99 samples/sec. 444.546 ms/step.
Infer [16/40]. 35.99 samples/sec. 444.522 ms/step.
Infer [24/40]. 35.99 samples/sec. 444.517 ms/step.
Infer [32/40]. 35.99 samples/sec. 444.517 ms/step.
Infer [40/40]. 35.99 samples/sec. 444.520 ms/step.
Inference benchmark of samvit_base_patch16.sa1b done. 35.99 samples/sec, 444.52 ms/step
Model samvit_base_patch16.sa1b created, param count: 89670912
Running train benchmark on samvit_base_patch16.sa1b for 40 steps w/ input size (3, 1024, 1024) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 5.50 GiB. GPU 0 has a total capacty of 23.65 GiB of which 4.23 GiB is free. Including non-PyTorch memory, this process has 19.41 GiB memory in use. Of the allocated memory 17.55 GiB is allocated by PyTorch, and 642.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model samvit_base_patch16.sa1b created, param count: 89670912
Running train benchmark on samvit_base_patch16.sa1b for 40 steps w/ input size (3, 1024, 1024) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 4.12 GiB. GPU 0 has a total capacty of 23.65 GiB of which 2.72 GiB is free. Including non-PyTorch memory, this process has 20.92 GiB memory in use. Of the allocated memory 19.26 GiB is allocated by PyTorch, and 432.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model samvit_base_patch16.sa1b created, param count: 89670912
Running train benchmark on samvit_base_patch16.sa1b for 40 steps w/ input size (3, 1024, 1024) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 920.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 60.06 MiB is free. Including non-PyTorch memory, this process has 23.58 GiB memory in use. Of the allocated memory 20.62 GiB is allocated by PyTorch, and 1.73 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model samvit_base_patch16.sa1b created, param count: 89670912
Running train benchmark on samvit_base_patch16.sa1b for 40 steps w/ input size (3, 1024, 1024) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 2.02 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.44 GiB is free. Including non-PyTorch memory, this process has 22.21 GiB memory in use. Of the allocated memory 16.97 GiB is allocated by PyTorch, and 4.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model samvit_base_patch16.sa1b created, param count: 89670912
Running train benchmark on samvit_base_patch16.sa1b for 40 steps w/ input size (3, 1024, 1024) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 24.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 44.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 21.22 GiB is allocated by PyTorch, and 1.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model samvit_base_patch16.sa1b created, param count: 89670912
Running train benchmark on samvit_base_patch16.sa1b for 40 steps w/ input size (3, 1024, 1024) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 18.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 4.83 GiB is free. Including non-PyTorch memory, this process has 18.81 GiB memory in use. Of the allocated memory 15.96 GiB is allocated by PyTorch, and 1.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model samvit_base_patch16.sa1b created, param count: 89670912
Running train benchmark on samvit_base_patch16.sa1b for 40 steps w/ input size (3, 1024, 1024) and batch size 32.
ERROR: "CUDA out of memory. Tried to allocate 12.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 10.94 GiB is free. Including non-PyTorch memory, this process has 12.70 GiB memory in use. Of the allocated memory 10.71 GiB is allocated by PyTorch, and 777.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model samvit_base_patch16.sa1b created, param count: 89670912
Running train benchmark on samvit_base_patch16.sa1b for 40 steps w/ input size (3, 1024, 1024) and batch size 24.
ERROR: "CUDA out of memory. Tried to allocate 9.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 4.34 GiB is free. Including non-PyTorch memory, this process has 19.30 GiB memory in use. Of the allocated memory 17.50 GiB is allocated by PyTorch, and 583.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model samvit_base_patch16.sa1b created, param count: 89670912
Running train benchmark on samvit_base_patch16.sa1b for 40 steps w/ input size (3, 1024, 1024) and batch size 16.
ERROR: "CUDA out of memory. Tried to allocate 6.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 4.30 GiB is free. Including non-PyTorch memory, this process has 19.34 GiB memory in use. Of the allocated memory 17.73 GiB is allocated by PyTorch, and 390.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 12 for retry.
Model samvit_base_patch16.sa1b created, param count: 89670912
Running train benchmark on samvit_base_patch16.sa1b for 40 steps w/ input size (3, 1024, 1024) and batch size 12.
ERROR: "CUDA out of memory. Tried to allocate 4.50 GiB. GPU 0 has a total capacty of 23.65 GiB of which 4.05 GiB is free. Including non-PyTorch memory, this process has 19.59 GiB memory in use. Of the allocated memory 17.85 GiB is allocated by PyTorch, and 524.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 8 for retry.
Model samvit_base_patch16.sa1b created, param count: 89670912
Running train benchmark on samvit_base_patch16.sa1b for 40 steps w/ input size (3, 1024, 1024) and batch size 8.
ERROR: "CUDA out of memory. Tried to allocate 3.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.09 GiB is free. Including non-PyTorch memory, this process has 22.55 GiB memory in use. Of the allocated memory 20.97 GiB is allocated by PyTorch, and 360.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 6 for retry.
Model samvit_base_patch16.sa1b created, param count: 89670912
Running train benchmark on samvit_base_patch16.sa1b for 40 steps w/ input size (3, 1024, 1024) and batch size 6.
ERROR: "CUDA out of memory. Tried to allocate 2.25 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.77 GiB is free. Including non-PyTorch memory, this process has 21.87 GiB memory in use. Of the allocated memory 20.28 GiB is allocated by PyTorch, and 369.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 4 for retry.
Model samvit_base_patch16.sa1b created, param count: 89670912
Running train benchmark on samvit_base_patch16.sa1b for 40 steps w/ input size (3, 1024, 1024) and batch size 4.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 3 for retry.
Model samvit_base_patch16.sa1b created, param count: 89670912
Running train benchmark on samvit_base_patch16.sa1b for 40 steps w/ input size (3, 1024, 1024) and batch size 3.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 2 for retry.
Model samvit_base_patch16.sa1b created, param count: 89670912
Running train benchmark on samvit_base_patch16.sa1b for 40 steps w/ input size (3, 1024, 1024) and batch size 2.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 1 for retry.
Model samvit_base_patch16.sa1b created, param count: 89670912
Running train benchmark on samvit_base_patch16.sa1b for 40 steps w/ input size (3, 1024, 1024) and batch size 1.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 0 for retry.
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model samvit_huge_patch16.sa1b created, param count: 637026048
Running inference benchmark on samvit_huge_patch16.sa1b for 40 steps w/ input size (3, 1024, 1024) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 8.97 GiB. GPU 0 has a total capacty of 23.65 GiB of which 5.20 GiB is free. Including non-PyTorch memory, this process has 18.44 GiB memory in use. Of the allocated memory 17.17 GiB is allocated by PyTorch, and 44.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model samvit_huge_patch16.sa1b created, param count: 637026048
Running inference benchmark on samvit_huge_patch16.sa1b for 40 steps w/ input size (3, 1024, 1024) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 2.24 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.18 GiB is free. Including non-PyTorch memory, this process has 22.46 GiB memory in use. Of the allocated memory 19.90 GiB is allocated by PyTorch, and 1.32 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model samvit_huge_patch16.sa1b created, param count: 637026048
Running inference benchmark on samvit_huge_patch16.sa1b for 40 steps w/ input size (3, 1024, 1024) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 3.66 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.66 GiB is free. Including non-PyTorch memory, this process has 21.98 GiB memory in use. Of the allocated memory 19.36 GiB is allocated by PyTorch, and 1.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model samvit_huge_patch16.sa1b created, param count: 637026048
Running inference benchmark on samvit_huge_patch16.sa1b for 40 steps w/ input size (3, 1024, 1024) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 2.75 GiB. GPU 0 has a total capacty of 23.65 GiB of which 788.06 MiB is free. Including non-PyTorch memory, this process has 22.87 GiB memory in use. Of the allocated memory 15.75 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model samvit_huge_patch16.sa1b created, param count: 637026048
Running inference benchmark on samvit_huge_patch16.sa1b for 40 steps w/ input size (3, 1024, 1024) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 12.92 GiB is free. Including non-PyTorch memory, this process has 10.72 GiB memory in use. Of the allocated memory 7.83 GiB is allocated by PyTorch, and 1.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model samvit_huge_patch16.sa1b created, param count: 637026048
Running inference benchmark on samvit_huge_patch16.sa1b for 40 steps w/ input size (3, 1024, 1024) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 24.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 15.19 GiB is free. Including non-PyTorch memory, this process has 8.45 GiB memory in use. Of the allocated memory 6.17 GiB is allocated by PyTorch, and 1.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model samvit_huge_patch16.sa1b created, param count: 637026048
Running inference benchmark on samvit_huge_patch16.sa1b for 40 steps w/ input size (3, 1024, 1024) and batch size 32.
ERROR: "CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 782.06 MiB is free. Including non-PyTorch memory, this process has 22.88 GiB memory in use. Of the allocated memory 21.02 GiB is allocated by PyTorch, and 641.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model samvit_huge_patch16.sa1b created, param count: 637026048
Running inference benchmark on samvit_huge_patch16.sa1b for 40 steps w/ input size (3, 1024, 1024) and batch size 24.
ERROR: "CUDA out of memory. Tried to allocate 12.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 5.15 GiB is free. Including non-PyTorch memory, this process has 18.49 GiB memory in use. Of the allocated memory 16.07 GiB is allocated by PyTorch, and 1.19 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model samvit_huge_patch16.sa1b created, param count: 637026048
Running inference benchmark on samvit_huge_patch16.sa1b for 40 steps w/ input size (3, 1024, 1024) and batch size 16.
ERROR: "CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 2.24 GiB is free. Including non-PyTorch memory, this process has 21.40 GiB memory in use. Of the allocated memory 19.11 GiB is allocated by PyTorch, and 1.06 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 12 for retry.
Model samvit_huge_patch16.sa1b created, param count: 637026048
Running inference benchmark on samvit_huge_patch16.sa1b for 40 steps w/ input size (3, 1024, 1024) and batch size 12.
Infer [8/40]. 10.75 samples/sec. 1116.298 ms/step.
Infer [16/40]. 10.75 samples/sec. 1116.275 ms/step.
Infer [24/40]. 10.75 samples/sec. 1116.264 ms/step.
Infer [32/40]. 10.75 samples/sec. 1116.264 ms/step.
Infer [40/40]. 10.75 samples/sec. 1116.480 ms/step.
Inference benchmark of samvit_huge_patch16.sa1b done. 10.75 samples/sec, 1116.48 ms/step
Model samvit_huge_patch16.sa1b created, param count: 637026048
Running train benchmark on samvit_huge_patch16.sa1b for 40 steps w/ input size (3, 1024, 1024) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 8.97 GiB. GPU 0 has a total capacty of 23.65 GiB of which 5.18 GiB is free. Including non-PyTorch memory, this process has 18.46 GiB memory in use. Of the allocated memory 17.18 GiB is allocated by PyTorch, and 56.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model samvit_huge_patch16.sa1b created, param count: 637026048
Running train benchmark on samvit_huge_patch16.sa1b for 40 steps w/ input size (3, 1024, 1024) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 2.24 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.18 GiB is free. Including non-PyTorch memory, this process has 22.46 GiB memory in use. Of the allocated memory 19.91 GiB is allocated by PyTorch, and 1.31 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model samvit_huge_patch16.sa1b created, param count: 637026048
Running train benchmark on samvit_huge_patch16.sa1b for 40 steps w/ input size (3, 1024, 1024) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 3.66 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.39 GiB is free. Including non-PyTorch memory, this process has 22.25 GiB memory in use. Of the allocated memory 20.86 GiB is allocated by PyTorch, and 161.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model samvit_huge_patch16.sa1b created, param count: 637026048
Running train benchmark on samvit_huge_patch16.sa1b for 40 steps w/ input size (3, 1024, 1024) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 2.75 GiB. GPU 0 has a total capacty of 23.65 GiB of which 2.66 GiB is free. Including non-PyTorch memory, this process has 20.98 GiB memory in use. Of the allocated memory 18.69 GiB is allocated by PyTorch, and 1.06 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model samvit_huge_patch16.sa1b created, param count: 637026048
Running train benchmark on samvit_huge_patch16.sa1b for 40 steps w/ input size (3, 1024, 1024) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 2.24 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.55 GiB is free. Including non-PyTorch memory, this process has 22.09 GiB memory in use. Of the allocated memory 16.90 GiB is allocated by PyTorch, and 3.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model samvit_huge_patch16.sa1b created, param count: 637026048
Running train benchmark on samvit_huge_patch16.sa1b for 40 steps w/ input size (3, 1024, 1024) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 1.38 GiB. GPU 0 has a total capacty of 23.65 GiB of which 74.06 MiB is free. Including non-PyTorch memory, this process has 23.57 GiB memory in use. Of the allocated memory 20.41 GiB is allocated by PyTorch, and 1.93 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model samvit_huge_patch16.sa1b created, param count: 637026048
Running train benchmark on samvit_huge_patch16.sa1b for 40 steps w/ input size (3, 1024, 1024) and batch size 32.
ERROR: "CUDA out of memory. Tried to allocate 938.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 750.06 MiB is free. Including non-PyTorch memory, this process has 22.91 GiB memory in use. Of the allocated memory 20.06 GiB is allocated by PyTorch, and 1.61 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model samvit_huge_patch16.sa1b created, param count: 637026048
Running train benchmark on samvit_huge_patch16.sa1b for 40 steps w/ input size (3, 1024, 1024) and batch size 24.
ERROR: "CUDA out of memory. Tried to allocate 704.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 2.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 20.59 GiB is allocated by PyTorch, and 1.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model samvit_huge_patch16.sa1b created, param count: 637026048
Running train benchmark on samvit_huge_patch16.sa1b for 40 steps w/ input size (3, 1024, 1024) and batch size 16.
ERROR: "CUDA out of memory. Tried to allocate 470.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 212.06 MiB is free. Including non-PyTorch memory, this process has 23.43 GiB memory in use. Of the allocated memory 20.38 GiB is allocated by PyTorch, and 1.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 12 for retry.
Model samvit_huge_patch16.sa1b created, param count: 637026048
Running train benchmark on samvit_huge_patch16.sa1b for 40 steps w/ input size (3, 1024, 1024) and batch size 12.
ERROR: "CUDA out of memory. Tried to allocate 6.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 144.06 MiB is free. Including non-PyTorch memory, this process has 23.50 GiB memory in use. Of the allocated memory 20.63 GiB is allocated by PyTorch, and 1.63 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 8 for retry.
Model samvit_huge_patch16.sa1b created, param count: 637026048
Running train benchmark on samvit_huge_patch16.sa1b for 40 steps w/ input size (3, 1024, 1024) and batch size 8.
ERROR: "CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 3.01 GiB is free. Including non-PyTorch memory, this process has 20.63 GiB memory in use. Of the allocated memory 18.37 GiB is allocated by PyTorch, and 1.03 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 6 for retry.
Model samvit_huge_patch16.sa1b created, param count: 637026048
Running train benchmark on samvit_huge_patch16.sa1b for 40 steps w/ input size (3, 1024, 1024) and batch size 6.
ERROR: "CUDA out of memory. Tried to allocate 216.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 8.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 21.33 GiB is allocated by PyTorch, and 1.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 4 for retry.
Model samvit_huge_patch16.sa1b created, param count: 637026048
Running train benchmark on samvit_huge_patch16.sa1b for 40 steps w/ input size (3, 1024, 1024) and batch size 4.
ERROR: "CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1012.06 MiB is free. Including non-PyTorch memory, this process has 22.65 GiB memory in use. Of the allocated memory 20.60 GiB is allocated by PyTorch, and 837.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 3 for retry.
Model samvit_huge_patch16.sa1b created, param count: 637026048
Running train benchmark on samvit_huge_patch16.sa1b for 40 steps w/ input size (3, 1024, 1024) and batch size 3.
ERROR: "CUDA out of memory. Tried to allocate 1.50 GiB. GPU 0 has a total capacty of 23.65 GiB of which 758.06 MiB is free. Including non-PyTorch memory, this process has 22.90 GiB memory in use. Of the allocated memory 20.85 GiB is allocated by PyTorch, and 832.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 2 for retry.
Model samvit_huge_patch16.sa1b created, param count: 637026048
Running train benchmark on samvit_huge_patch16.sa1b for 40 steps w/ input size (3, 1024, 1024) and batch size 2.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 1 for retry.
Model samvit_huge_patch16.sa1b created, param count: 637026048
Running train benchmark on samvit_huge_patch16.sa1b for 40 steps w/ input size (3, 1024, 1024) and batch size 1.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 0 for retry.
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model samvit_large_patch16.sa1b created, param count: 308278272
Running inference benchmark on samvit_large_patch16.sa1b for 40 steps w/ input size (3, 1024, 1024) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 2.39 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.34 GiB is free. Including non-PyTorch memory, this process has 22.30 GiB memory in use. Of the allocated memory 20.84 GiB is allocated by PyTorch, and 231.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model samvit_large_patch16.sa1b created, param count: 308278272
Running inference benchmark on samvit_large_patch16.sa1b for 40 steps w/ input size (3, 1024, 1024) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 5.50 GiB. GPU 0 has a total capacty of 23.65 GiB of which 3.86 GiB is free. Including non-PyTorch memory, this process has 19.78 GiB memory in use. Of the allocated memory 17.57 GiB is allocated by PyTorch, and 1000.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model samvit_large_patch16.sa1b created, param count: 308278272
Running inference benchmark on samvit_large_patch16.sa1b for 40 steps w/ input size (3, 1024, 1024) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 3.66 GiB. GPU 0 has a total capacty of 23.65 GiB of which 2.09 GiB is free. Including non-PyTorch memory, this process has 21.55 GiB memory in use. Of the allocated memory 19.77 GiB is allocated by PyTorch, and 565.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model samvit_large_patch16.sa1b created, param count: 308278272
Running inference benchmark on samvit_large_patch16.sa1b for 40 steps w/ input size (3, 1024, 1024) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 2.75 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.41 GiB is free. Including non-PyTorch memory, this process has 22.23 GiB memory in use. Of the allocated memory 15.72 GiB is allocated by PyTorch, and 5.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model samvit_large_patch16.sa1b created, param count: 308278272
Running inference benchmark on samvit_large_patch16.sa1b for 40 steps w/ input size (3, 1024, 1024) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 15.30 GiB is free. Including non-PyTorch memory, this process has 8.34 GiB memory in use. Of the allocated memory 5.97 GiB is allocated by PyTorch, and 1.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model samvit_large_patch16.sa1b created, param count: 308278272
Running inference benchmark on samvit_large_patch16.sa1b for 40 steps w/ input size (3, 1024, 1024) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 24.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 16.85 GiB is free. Including non-PyTorch memory, this process has 6.79 GiB memory in use. Of the allocated memory 4.62 GiB is allocated by PyTorch, and 960.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model samvit_large_patch16.sa1b created, param count: 308278272
Running inference benchmark on samvit_large_patch16.sa1b for 40 steps w/ input size (3, 1024, 1024) and batch size 32.
ERROR: "CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.95 GiB is free. Including non-PyTorch memory, this process has 21.70 GiB memory in use. Of the allocated memory 19.78 GiB is allocated by PyTorch, and 698.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model samvit_large_patch16.sa1b created, param count: 308278272
Running inference benchmark on samvit_large_patch16.sa1b for 40 steps w/ input size (3, 1024, 1024) and batch size 24.
ERROR: "CUDA out of memory. Tried to allocate 12.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 5.27 GiB is free. Including non-PyTorch memory, this process has 18.37 GiB memory in use. Of the allocated memory 14.98 GiB is allocated by PyTorch, and 2.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model samvit_large_patch16.sa1b created, param count: 308278272
Running inference benchmark on samvit_large_patch16.sa1b for 40 steps w/ input size (3, 1024, 1024) and batch size 16.
ERROR: "CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 2.63 GiB is free. Including non-PyTorch memory, this process has 21.02 GiB memory in use. Of the allocated memory 18.19 GiB is allocated by PyTorch, and 1.60 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 12 for retry.
Model samvit_large_patch16.sa1b created, param count: 308278272
Running inference benchmark on samvit_large_patch16.sa1b for 40 steps w/ input size (3, 1024, 1024) and batch size 12.
Infer [8/40]. 16.50 samples/sec. 727.341 ms/step.
Infer [16/40]. 16.50 samples/sec. 727.336 ms/step.
Infer [24/40]. 16.50 samples/sec. 727.346 ms/step.
Infer [32/40]. 16.50 samples/sec. 727.339 ms/step.
Infer [40/40]. 16.50 samples/sec. 727.345 ms/step.
Inference benchmark of samvit_large_patch16.sa1b done. 16.50 samples/sec, 727.35 ms/step
Model samvit_large_patch16.sa1b created, param count: 308278272
Running train benchmark on samvit_large_patch16.sa1b for 40 steps w/ input size (3, 1024, 1024) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 2.39 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.34 GiB is free. Including non-PyTorch memory, this process has 22.30 GiB memory in use. Of the allocated memory 20.85 GiB is allocated by PyTorch, and 223.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model samvit_large_patch16.sa1b created, param count: 308278272
Running train benchmark on samvit_large_patch16.sa1b for 40 steps w/ input size (3, 1024, 1024) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 5.50 GiB. GPU 0 has a total capacty of 23.65 GiB of which 3.86 GiB is free. Including non-PyTorch memory, this process has 19.78 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 992.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model samvit_large_patch16.sa1b created, param count: 308278272
Running train benchmark on samvit_large_patch16.sa1b for 40 steps w/ input size (3, 1024, 1024) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 3.66 GiB. GPU 0 has a total capacty of 23.65 GiB of which 914.06 MiB is free. Including non-PyTorch memory, this process has 22.75 GiB memory in use. Of the allocated memory 20.97 GiB is allocated by PyTorch, and 561.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model samvit_large_patch16.sa1b created, param count: 308278272
Running train benchmark on samvit_large_patch16.sa1b for 40 steps w/ input size (3, 1024, 1024) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 920.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 110.06 MiB is free. Including non-PyTorch memory, this process has 23.53 GiB memory in use. Of the allocated memory 20.84 GiB is allocated by PyTorch, and 1.46 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model samvit_large_patch16.sa1b created, param count: 308278272
Running train benchmark on samvit_large_patch16.sa1b for 40 steps w/ input size (3, 1024, 1024) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 1.83 GiB. GPU 0 has a total capacty of 23.65 GiB of which 898.06 MiB is free. Including non-PyTorch memory, this process has 22.76 GiB memory in use. Of the allocated memory 20.47 GiB is allocated by PyTorch, and 1.06 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model samvit_large_patch16.sa1b created, param count: 308278272
Running train benchmark on samvit_large_patch16.sa1b for 40 steps w/ input size (3, 1024, 1024) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 1.35 GiB. GPU 0 has a total capacty of 23.65 GiB of which 868.06 MiB is free. Including non-PyTorch memory, this process has 22.79 GiB memory in use. Of the allocated memory 20.33 GiB is allocated by PyTorch, and 1.23 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model samvit_large_patch16.sa1b created, param count: 308278272
Running train benchmark on samvit_large_patch16.sa1b for 40 steps w/ input size (3, 1024, 1024) and batch size 32.
ERROR: "CUDA out of memory. Tried to allocate 938.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 360.06 MiB is free. Including non-PyTorch memory, this process has 23.29 GiB memory in use. Of the allocated memory 20.71 GiB is allocated by PyTorch, and 1.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model samvit_large_patch16.sa1b created, param count: 308278272
Running train benchmark on samvit_large_patch16.sa1b for 40 steps w/ input size (3, 1024, 1024) and batch size 24.
ERROR: "CUDA out of memory. Tried to allocate 704.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 340.06 MiB is free. Including non-PyTorch memory, this process has 23.31 GiB memory in use. Of the allocated memory 21.02 GiB is allocated by PyTorch, and 1.06 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model samvit_large_patch16.sa1b created, param count: 308278272
Running train benchmark on samvit_large_patch16.sa1b for 40 steps w/ input size (3, 1024, 1024) and batch size 16.
ERROR: "CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 4.93 GiB is free. Including non-PyTorch memory, this process has 18.71 GiB memory in use. Of the allocated memory 16.22 GiB is allocated by PyTorch, and 1.26 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 12 for retry.
Model samvit_large_patch16.sa1b created, param count: 308278272
Running train benchmark on samvit_large_patch16.sa1b for 40 steps w/ input size (3, 1024, 1024) and batch size 12.
ERROR: "CUDA out of memory. Tried to allocate 6.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 3.17 GiB is free. Including non-PyTorch memory, this process has 20.47 GiB memory in use. Of the allocated memory 18.59 GiB is allocated by PyTorch, and 659.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 8 for retry.
Model samvit_large_patch16.sa1b created, param count: 308278272
Running train benchmark on samvit_large_patch16.sa1b for 40 steps w/ input size (3, 1024, 1024) and batch size 8.
ERROR: "CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1018.06 MiB is free. Including non-PyTorch memory, this process has 22.65 GiB memory in use. Of the allocated memory 20.73 GiB is allocated by PyTorch, and 697.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 6 for retry.
Model samvit_large_patch16.sa1b created, param count: 308278272
Running train benchmark on samvit_large_patch16.sa1b for 40 steps w/ input size (3, 1024, 1024) and batch size 6.
ERROR: "CUDA out of memory. Tried to allocate 3.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 80.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 21.84 GiB is allocated by PyTorch, and 502.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 4 for retry.
Model samvit_large_patch16.sa1b created, param count: 308278272
Running train benchmark on samvit_large_patch16.sa1b for 40 steps w/ input size (3, 1024, 1024) and batch size 4.
ERROR: "CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 968.06 MiB is free. Including non-PyTorch memory, this process has 22.70 GiB memory in use. Of the allocated memory 20.92 GiB is allocated by PyTorch, and 551.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 3 for retry.
Model samvit_large_patch16.sa1b created, param count: 308278272
Running train benchmark on samvit_large_patch16.sa1b for 40 steps w/ input size (3, 1024, 1024) and batch size 3.
ERROR: "CUDA out of memory. Tried to allocate 1.50 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.49 GiB is free. Including non-PyTorch memory, this process has 22.15 GiB memory in use. Of the allocated memory 20.46 GiB is allocated by PyTorch, and 467.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 2 for retry.
Model samvit_large_patch16.sa1b created, param count: 308278272
Running train benchmark on samvit_large_patch16.sa1b for 40 steps w/ input size (3, 1024, 1024) and batch size 2.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 1 for retry.
Model samvit_large_patch16.sa1b created, param count: 308278272
Running train benchmark on samvit_large_patch16.sa1b for 40 steps w/ input size (3, 1024, 1024) and batch size 1.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 0 for retry.
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model sebotnet33ts_256.a1h_in1k created, param count: 13701984
Running inference benchmark on sebotnet33ts_256.a1h_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
Infer [8/40]. 2752.40 samples/sec. 93.010 ms/step.
Infer [16/40]. 2752.64 samples/sec. 93.002 ms/step.
Infer [24/40]. 2752.69 samples/sec. 93.000 ms/step.
Infer [32/40]. 2752.72 samples/sec. 92.999 ms/step.
Infer [40/40]. 2752.71 samples/sec. 92.999 ms/step.
Inference benchmark of sebotnet33ts_256.a1h_in1k done. 2751.89 samples/sec, 93.00 ms/step
Model sebotnet33ts_256.a1h_in1k created, param count: 13701984
Running train benchmark on sebotnet33ts_256.a1h_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.55 GiB is free. Including non-PyTorch memory, this process has 22.09 GiB memory in use. Of the allocated memory 17.07 GiB is allocated by PyTorch, and 3.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model sebotnet33ts_256.a1h_in1k created, param count: 13701984
Running train benchmark on sebotnet33ts_256.a1h_in1k for 40 steps w/ input size (3, 256, 256) and batch size 192.
Train [8/40]. 869.96 samples/sec. 220.699 ms/step.
Train [16/40]. 870.01 samples/sec. 220.687 ms/step.
Train [24/40]. 870.04 samples/sec. 220.681 ms/step.
Train [32/40]. 870.05 samples/sec. 220.677 ms/step.
Train [40/40]. 870.03 samples/sec. 220.682 ms/step.
Train benchmark of sebotnet33ts_256.a1h_in1k done. 866.21 samples/sec, 220.68 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model sehalonet33ts.ra2_in1k created, param count: 13691712
Running inference benchmark on sehalonet33ts.ra2_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
Infer [8/40]. 3179.89 samples/sec. 80.506 ms/step.
Infer [16/40]. 3179.78 samples/sec. 80.509 ms/step.
Infer [24/40]. 3179.70 samples/sec. 80.511 ms/step.
Infer [32/40]. 3179.68 samples/sec. 80.511 ms/step.
Infer [40/40]. 3179.69 samples/sec. 80.511 ms/step.
Inference benchmark of sehalonet33ts.ra2_in1k done. 3178.53 samples/sec, 80.51 ms/step
Model sehalonet33ts.ra2_in1k created, param count: 13691712
Running train benchmark on sehalonet33ts.ra2_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
Train [8/40]. 924.56 samples/sec. 276.888 ms/step.
Train [16/40]. 924.54 samples/sec. 276.896 ms/step.
Train [24/40]. 924.52 samples/sec. 276.900 ms/step.
Train [32/40]. 924.53 samples/sec. 276.897 ms/step.
Train [40/40]. 924.39 samples/sec. 276.938 ms/step.
Train benchmark of sehalonet33ts.ra2_in1k done. 920.78 samples/sec, 276.94 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model SelecSls42b.in1k created, param count: 32458248
Running inference benchmark on SelecSls42b.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 7865.22 samples/sec. 32.548 ms/step.
Infer [16/40]. 7861.72 samples/sec. 32.563 ms/step.
Infer [24/40]. 7858.47 samples/sec. 32.576 ms/step.
Infer [32/40]. 7854.61 samples/sec. 32.592 ms/step.
Infer [40/40]. 7861.08 samples/sec. 32.566 ms/step.
Inference benchmark of SelecSls42b.in1k done. 7854.56 samples/sec, 32.57 ms/step
Model SelecSls42b.in1k created, param count: 32458248
Running train benchmark on SelecSls42b.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 2127.68 samples/sec. 120.319 ms/step.
Train [16/40]. 2127.75 samples/sec. 120.315 ms/step.
Train [24/40]. 2123.30 samples/sec. 120.567 ms/step.
Train [32/40]. 2122.34 samples/sec. 120.621 ms/step.
Train [40/40]. 2121.20 samples/sec. 120.687 ms/step.
Train benchmark of SelecSls42b.in1k done. 2111.19 samples/sec, 120.69 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model SelecSls60.in1k created, param count: 30670768
Running inference benchmark on SelecSls60.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 7018.12 samples/sec. 36.477 ms/step.
Infer [16/40]. 7070.17 samples/sec. 36.208 ms/step.
Infer [24/40]. 7081.36 samples/sec. 36.151 ms/step.
Infer [32/40]. 7098.66 samples/sec. 36.063 ms/step.
Infer [40/40]. 7099.35 samples/sec. 36.060 ms/step.
Inference benchmark of SelecSls60.in1k done. 7094.46 samples/sec, 36.06 ms/step
Model SelecSls60.in1k created, param count: 30670768
Running train benchmark on SelecSls60.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1870.28 samples/sec. 136.878 ms/step.
Train [16/40]. 1871.58 samples/sec. 136.783 ms/step.
Train [24/40]. 1871.12 samples/sec. 136.817 ms/step.
Train [32/40]. 1871.47 samples/sec. 136.791 ms/step.
Train [40/40]. 1870.41 samples/sec. 136.868 ms/step.
Train benchmark of SelecSls60.in1k done. 1860.29 samples/sec, 136.87 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model SelecSls60b.in1k created, param count: 32774064
Running inference benchmark on SelecSls60b.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 7001.09 samples/sec. 36.566 ms/step.
Infer [16/40]. 7001.10 samples/sec. 36.566 ms/step.
Infer [24/40]. 6991.20 samples/sec. 36.617 ms/step.
Infer [32/40]. 6986.64 samples/sec. 36.641 ms/step.
Infer [40/40]. 6984.39 samples/sec. 36.653 ms/step.
Inference benchmark of SelecSls60b.in1k done. 6979.50 samples/sec, 36.65 ms/step
Model SelecSls60b.in1k created, param count: 32774064
Running train benchmark on SelecSls60b.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1858.02 samples/sec. 137.781 ms/step.
Train [16/40]. 1858.14 samples/sec. 137.772 ms/step.
Train [24/40]. 1858.57 samples/sec. 137.740 ms/step.
Train [32/40]. 1859.48 samples/sec. 137.673 ms/step.
Train [40/40]. 1859.62 samples/sec. 137.663 ms/step.
Train benchmark of SelecSls60b.in1k done. 1849.90 samples/sec, 137.66 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model semnasnet_075.rmsp_in1k created, param count: 2912278
Running inference benchmark on semnasnet_075.rmsp_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 13663.50 samples/sec. 18.736 ms/step.
Infer [16/40]. 13660.97 samples/sec. 18.740 ms/step.
Infer [24/40]. 13660.50 samples/sec. 18.740 ms/step.
Infer [32/40]. 13658.86 samples/sec. 18.742 ms/step.
Infer [40/40]. 13658.08 samples/sec. 18.743 ms/step.
Inference benchmark of semnasnet_075.rmsp_in1k done. 13640.45 samples/sec, 18.74 ms/step
Model semnasnet_075.rmsp_in1k created, param count: 2912278
Running train benchmark on semnasnet_075.rmsp_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 2722.76 samples/sec. 94.022 ms/step.
Train [16/40]. 2722.97 samples/sec. 94.015 ms/step.
Train [24/40]. 2723.01 samples/sec. 94.014 ms/step.
Train [32/40]. 2722.83 samples/sec. 94.020 ms/step.
Train [40/40]. 2722.85 samples/sec. 94.019 ms/step.
Train benchmark of semnasnet_075.rmsp_in1k done. 2703.75 samples/sec, 94.02 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model semnasnet_100.rmsp_in1k created, param count: 3887038
Running inference benchmark on semnasnet_100.rmsp_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 11999.69 samples/sec. 21.334 ms/step.
Infer [16/40]. 11998.54 samples/sec. 21.336 ms/step.
Infer [24/40]. 11996.40 samples/sec. 21.340 ms/step.
Infer [32/40]. 11995.69 samples/sec. 21.341 ms/step.
Infer [40/40]. 11995.22 samples/sec. 21.342 ms/step.
Inference benchmark of semnasnet_100.rmsp_in1k done. 11981.15 samples/sec, 21.34 ms/step
Model semnasnet_100.rmsp_in1k created, param count: 3887038
Running train benchmark on semnasnet_100.rmsp_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 2490.89 samples/sec. 102.775 ms/step.
Train [16/40]. 2490.99 samples/sec. 102.770 ms/step.
Train [24/40]. 2491.21 samples/sec. 102.761 ms/step.
Train [32/40]. 2491.04 samples/sec. 102.768 ms/step.
Train [40/40]. 2491.05 samples/sec. 102.768 ms/step.
Train benchmark of semnasnet_100.rmsp_in1k done. 2474.53 samples/sec, 102.77 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model senet154.gluon_in1k created, param count: 115088984
Running inference benchmark on senet154.gluon_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1033.67 samples/sec. 247.662 ms/step.
Infer [16/40]. 1033.17 samples/sec. 247.780 ms/step.
Infer [24/40]. 1032.99 samples/sec. 247.825 ms/step.
Infer [32/40]. 1032.90 samples/sec. 247.846 ms/step.
Infer [40/40]. 1032.84 samples/sec. 247.861 ms/step.
Inference benchmark of senet154.gluon_in1k done. 1032.70 samples/sec, 247.86 ms/step
Model senet154.gluon_in1k created, param count: 115088984
Running train benchmark on senet154.gluon_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 68.06 MiB is free. Including non-PyTorch memory, this process has 23.57 GiB memory in use. Of the allocated memory 22.03 GiB is allocated by PyTorch, and 316.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model senet154.gluon_in1k created, param count: 115088984
Running train benchmark on senet154.gluon_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 74.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 40.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 22.27 GiB is allocated by PyTorch, and 104.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model senet154.gluon_in1k created, param count: 115088984
Running train benchmark on senet154.gluon_in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 20.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.07 GiB is allocated by PyTorch, and 330.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model senet154.gluon_in1k created, param count: 115088984
Running train benchmark on senet154.gluon_in1k for 40 steps w/ input size (3, 224, 224) and batch size 96.
Train [8/40]. 313.74 samples/sec. 305.985 ms/step.
Train [16/40]. 313.73 samples/sec. 305.993 ms/step.
Train [24/40]. 313.73 samples/sec. 305.991 ms/step.
Train [32/40]. 313.73 samples/sec. 305.995 ms/step.
Train [40/40]. 313.72 samples/sec. 306.009 ms/step.
Train benchmark of senet154.gluon_in1k done. 311.24 samples/sec, 306.01 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model sequencer2d_l.in1k created, param count: 54298216
Running inference benchmark on sequencer2d_l.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1301.54 samples/sec. 196.691 ms/step.
Infer [16/40]. 1301.50 samples/sec. 196.697 ms/step.
Infer [24/40]. 1301.45 samples/sec. 196.704 ms/step.
Infer [32/40]. 1301.41 samples/sec. 196.709 ms/step.
Infer [40/40]. 1301.41 samples/sec. 196.710 ms/step.
Inference benchmark of sequencer2d_l.in1k done. 1301.15 samples/sec, 196.71 ms/step
Model sequencer2d_l.in1k created, param count: 54298216
Running train benchmark on sequencer2d_l.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 122.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 4.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 22.32 GiB is allocated by PyTorch, and 82.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model sequencer2d_l.in1k created, param count: 54298216
Running train benchmark on sequencer2d_l.in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 294.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 90.06 MiB is free. Including non-PyTorch memory, this process has 23.55 GiB memory in use. Of the allocated memory 22.15 GiB is allocated by PyTorch, and 176.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model sequencer2d_l.in1k created, param count: 54298216
Running train benchmark on sequencer2d_l.in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
Train [8/40]. 417.72 samples/sec. 306.423 ms/step.
Train [16/40]. 417.64 samples/sec. 306.486 ms/step.
Train [24/40]. 417.57 samples/sec. 306.535 ms/step.
Train [32/40]. 417.52 samples/sec. 306.571 ms/step.
Train [40/40]. 417.50 samples/sec. 306.590 ms/step.
Train benchmark of sequencer2d_l.in1k done. 413.72 samples/sec, 306.59 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model sequencer2d_m.in1k created, param count: 38307688
Running inference benchmark on sequencer2d_m.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 2017.90 samples/sec. 126.865 ms/step.
Infer [16/40]. 2017.88 samples/sec. 126.866 ms/step.
Infer [24/40]. 2017.84 samples/sec. 126.868 ms/step.
Infer [32/40]. 2017.73 samples/sec. 126.875 ms/step.
Infer [40/40]. 2017.77 samples/sec. 126.873 ms/step.
Inference benchmark of sequencer2d_m.in1k done. 2017.27 samples/sec, 126.87 ms/step
Model sequencer2d_m.in1k created, param count: 38307688
Running train benchmark on sequencer2d_m.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 122.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 50.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 22.22 GiB is allocated by PyTorch, and 143.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model sequencer2d_m.in1k created, param count: 38307688
Running train benchmark on sequencer2d_m.in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
Train [8/40]. 638.11 samples/sec. 300.887 ms/step.
Train [16/40]. 638.01 samples/sec. 300.936 ms/step.
Train [24/40]. 638.20 samples/sec. 300.847 ms/step.
Train [32/40]. 638.05 samples/sec. 300.915 ms/step.
Train [40/40]. 638.08 samples/sec. 300.904 ms/step.
Train benchmark of sequencer2d_m.in1k done. 633.80 samples/sec, 300.90 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model sequencer2d_s.in1k created, param count: 27651688
Running inference benchmark on sequencer2d_s.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 2573.14 samples/sec. 99.490 ms/step.
Infer [16/40]. 2572.97 samples/sec. 99.496 ms/step.
Infer [24/40]. 2572.89 samples/sec. 99.499 ms/step.
Infer [32/40]. 2572.82 samples/sec. 99.502 ms/step.
Infer [40/40]. 2572.82 samples/sec. 99.502 ms/step.
Inference benchmark of sequencer2d_s.in1k done. 2572.06 samples/sec, 99.50 ms/step
Model sequencer2d_s.in1k created, param count: 27651688
Running train benchmark on sequencer2d_s.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 799.67 samples/sec. 320.133 ms/step.
Train [16/40]. 799.75 samples/sec. 320.099 ms/step.
Train [24/40]. 799.40 samples/sec. 320.238 ms/step.
Train [32/40]. 799.47 samples/sec. 320.212 ms/step.
Train [40/40]. 799.41 samples/sec. 320.237 ms/step.
Train benchmark of sequencer2d_s.in1k done. 795.08 samples/sec, 320.24 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model seresnet33ts.ra2_in1k created, param count: 19779200
Running inference benchmark on seresnet33ts.ra2_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 3112.55 samples/sec. 82.248 ms/step.
Infer [16/40]. 3112.38 samples/sec. 82.252 ms/step.
Infer [24/40]. 3112.32 samples/sec. 82.254 ms/step.
Infer [32/40]. 3112.29 samples/sec. 82.255 ms/step.
Infer [40/40]. 3112.27 samples/sec. 82.255 ms/step.
Inference benchmark of seresnet33ts.ra2_in1k done. 3111.25 samples/sec, 82.25 ms/step
Model seresnet33ts.ra2_in1k created, param count: 19779200
Running train benchmark on seresnet33ts.ra2_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 840.87 samples/sec. 304.448 ms/step.
Train [16/40]. 840.83 samples/sec. 304.459 ms/step.
Train [24/40]. 840.85 samples/sec. 304.455 ms/step.
Train [32/40]. 840.84 samples/sec. 304.458 ms/step.
Train [40/40]. 840.84 samples/sec. 304.457 ms/step.
Train benchmark of seresnet33ts.ra2_in1k done. 838.22 samples/sec, 304.46 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model seresnet50.a1_in1k created, param count: 28088024
Running inference benchmark on seresnet50.a1_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 1976.41 samples/sec. 129.528 ms/step.
Infer [16/40]. 1976.22 samples/sec. 129.540 ms/step.
Infer [24/40]. 1975.67 samples/sec. 129.576 ms/step.
Infer [32/40]. 1975.35 samples/sec. 129.597 ms/step.
Infer [40/40]. 1975.14 samples/sec. 129.611 ms/step.
Inference benchmark of seresnet50.a1_in1k done. 1974.66 samples/sec, 129.61 ms/step
Model seresnet50.a1_in1k created, param count: 28088024
Running train benchmark on seresnet50.a1_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 616.88 samples/sec. 414.994 ms/step.
Train [16/40]. 616.87 samples/sec. 414.998 ms/step.
Train [24/40]. 616.88 samples/sec. 414.990 ms/step.
Train [32/40]. 616.88 samples/sec. 414.988 ms/step.
Train [40/40]. 616.90 samples/sec. 414.981 ms/step.
Train benchmark of seresnet50.a1_in1k done. 615.08 samples/sec, 414.98 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model seresnet50.a2_in1k created, param count: 28088024
Running inference benchmark on seresnet50.a2_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 1974.46 samples/sec. 129.656 ms/step.
Infer [16/40]. 1974.42 samples/sec. 129.658 ms/step.
Infer [24/40]. 1974.38 samples/sec. 129.661 ms/step.
Infer [32/40]. 1974.53 samples/sec. 129.651 ms/step.
Infer [40/40]. 1974.48 samples/sec. 129.654 ms/step.
Inference benchmark of seresnet50.a2_in1k done. 1974.01 samples/sec, 129.65 ms/step
Model seresnet50.a2_in1k created, param count: 28088024
Running train benchmark on seresnet50.a2_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 616.88 samples/sec. 414.989 ms/step.
Train [16/40]. 616.89 samples/sec. 414.987 ms/step.
Train [24/40]. 616.89 samples/sec. 414.988 ms/step.
Train [32/40]. 616.87 samples/sec. 414.995 ms/step.
Train [40/40]. 616.87 samples/sec. 414.997 ms/step.
Train benchmark of seresnet50.a2_in1k done. 615.05 samples/sec, 415.00 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model seresnet50.a3_in1k created, param count: 28088024
Running inference benchmark on seresnet50.a3_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 2916.40 samples/sec. 87.780 ms/step.
Infer [16/40]. 2914.09 samples/sec. 87.849 ms/step.
Infer [24/40]. 2918.48 samples/sec. 87.717 ms/step.
Infer [32/40]. 2916.35 samples/sec. 87.781 ms/step.
Infer [40/40]. 2915.23 samples/sec. 87.815 ms/step.
Inference benchmark of seresnet50.a3_in1k done. 2914.28 samples/sec, 87.81 ms/step
Model seresnet50.a3_in1k created, param count: 28088024
Running train benchmark on seresnet50.a3_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 998.25 samples/sec. 256.448 ms/step.
Train [16/40]. 999.29 samples/sec. 256.183 ms/step.
Train [24/40]. 999.36 samples/sec. 256.164 ms/step.
Train [32/40]. 999.54 samples/sec. 256.119 ms/step.
Train [40/40]. 999.42 samples/sec. 256.149 ms/step.
Train benchmark of seresnet50.a3_in1k done. 995.02 samples/sec, 256.15 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model seresnet50.ra2_in1k created, param count: 28088024
Running inference benchmark on seresnet50.ra2_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 1974.84 samples/sec. 129.631 ms/step.
Infer [16/40]. 1974.49 samples/sec. 129.653 ms/step.
Infer [24/40]. 1974.32 samples/sec. 129.665 ms/step.
Infer [32/40]. 1974.21 samples/sec. 129.672 ms/step.
Infer [40/40]. 1974.26 samples/sec. 129.669 ms/step.
Inference benchmark of seresnet50.ra2_in1k done. 1973.75 samples/sec, 129.67 ms/step
Model seresnet50.ra2_in1k created, param count: 28088024
Running train benchmark on seresnet50.ra2_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 617.01 samples/sec. 414.905 ms/step.
Train [16/40]. 617.00 samples/sec. 414.912 ms/step.
Train [24/40]. 617.01 samples/sec. 414.903 ms/step.
Train [32/40]. 617.06 samples/sec. 414.873 ms/step.
Train [40/40]. 617.04 samples/sec. 414.885 ms/step.
Train benchmark of seresnet50.ra2_in1k done. 615.21 samples/sec, 414.88 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model seresnet152d.ra2_in1k created, param count: 66841080
Running inference benchmark on seresnet152d.ra2_in1k for 40 steps w/ input size (3, 320, 320) and batch size 256.
Infer [8/40]. 826.50 samples/sec. 309.739 ms/step.
Infer [16/40]. 826.52 samples/sec. 309.731 ms/step.
Infer [24/40]. 826.60 samples/sec. 309.704 ms/step.
Infer [32/40]. 826.73 samples/sec. 309.653 ms/step.
Infer [40/40]. 826.69 samples/sec. 309.670 ms/step.
Inference benchmark of seresnet152d.ra2_in1k done. 826.58 samples/sec, 309.67 ms/step
Model seresnet152d.ra2_in1k created, param count: 66841080
Running train benchmark on seresnet152d.ra2_in1k for 40 steps w/ input size (3, 320, 320) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 52.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 22.17 GiB is allocated by PyTorch, and 195.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model seresnet152d.ra2_in1k created, param count: 66841080
Running train benchmark on seresnet152d.ra2_in1k for 40 steps w/ input size (3, 320, 320) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 150.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 2.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 22.24 GiB is allocated by PyTorch, and 171.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model seresnet152d.ra2_in1k created, param count: 66841080
Running train benchmark on seresnet152d.ra2_in1k for 40 steps w/ input size (3, 320, 320) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 62.06 MiB is free. Including non-PyTorch memory, this process has 23.58 GiB memory in use. Of the allocated memory 22.22 GiB is allocated by PyTorch, and 127.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model seresnet152d.ra2_in1k created, param count: 66841080
Running train benchmark on seresnet152d.ra2_in1k for 40 steps w/ input size (3, 320, 320) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 10.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 21.63 GiB is allocated by PyTorch, and 787.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model seresnet152d.ra2_in1k created, param count: 66841080
Running train benchmark on seresnet152d.ra2_in1k for 40 steps w/ input size (3, 320, 320) and batch size 64.
Train [8/40]. 238.77 samples/sec. 268.042 ms/step.
Train [16/40]. 238.76 samples/sec. 268.051 ms/step.
Train [24/40]. 238.76 samples/sec. 268.051 ms/step.
Train [32/40]. 238.76 samples/sec. 268.053 ms/step.
Train [40/40]. 238.76 samples/sec. 268.054 ms/step.
Train benchmark of seresnet152d.ra2_in1k done. 236.67 samples/sec, 268.05 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model seresnext26d_32x4d.bt_in1k created, param count: 16809512
Running inference benchmark on seresnext26d_32x4d.bt_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 2738.90 samples/sec. 93.468 ms/step.
Infer [16/40]. 2738.80 samples/sec. 93.471 ms/step.
Infer [24/40]. 2738.89 samples/sec. 93.469 ms/step.
Infer [32/40]. 2738.94 samples/sec. 93.467 ms/step.
Infer [40/40]. 2738.86 samples/sec. 93.470 ms/step.
Inference benchmark of seresnext26d_32x4d.bt_in1k done. 2737.97 samples/sec, 93.47 ms/step
Model seresnext26d_32x4d.bt_in1k created, param count: 16809512
Running train benchmark on seresnext26d_32x4d.bt_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 765.30 samples/sec. 334.511 ms/step.
Train [16/40]. 765.31 samples/sec. 334.506 ms/step.
Train [24/40]. 765.31 samples/sec. 334.506 ms/step.
Train [32/40]. 765.31 samples/sec. 334.505 ms/step.
Train [40/40]. 765.32 samples/sec. 334.502 ms/step.
Train benchmark of seresnext26d_32x4d.bt_in1k done. 763.19 samples/sec, 334.50 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model seresnext26t_32x4d.bt_in1k created, param count: 16806976
Running inference benchmark on seresnext26t_32x4d.bt_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 2760.75 samples/sec. 92.728 ms/step.
Infer [16/40]. 2760.47 samples/sec. 92.738 ms/step.
Infer [24/40]. 2760.61 samples/sec. 92.733 ms/step.
Infer [32/40]. 2760.59 samples/sec. 92.734 ms/step.
Infer [40/40]. 2760.50 samples/sec. 92.737 ms/step.
Inference benchmark of seresnext26t_32x4d.bt_in1k done. 2759.63 samples/sec, 92.74 ms/step
Model seresnext26t_32x4d.bt_in1k created, param count: 16806976
Running train benchmark on seresnext26t_32x4d.bt_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 772.98 samples/sec. 331.185 ms/step.
Train [16/40]. 772.97 samples/sec. 331.189 ms/step.
Train [24/40]. 772.96 samples/sec. 331.192 ms/step.
Train [32/40]. 772.96 samples/sec. 331.193 ms/step.
Train [40/40]. 772.95 samples/sec. 331.199 ms/step.
Train benchmark of seresnext26t_32x4d.bt_in1k done. 770.82 samples/sec, 331.20 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model seresnext26ts.ch_in1k created, param count: 10388064
Running inference benchmark on seresnext26ts.ch_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 3772.23 samples/sec. 67.864 ms/step.
Infer [16/40]. 3771.73 samples/sec. 67.873 ms/step.
Infer [24/40]. 3771.39 samples/sec. 67.880 ms/step.
Infer [32/40]. 3771.22 samples/sec. 67.883 ms/step.
Infer [40/40]. 3771.14 samples/sec. 67.884 ms/step.
Inference benchmark of seresnext26ts.ch_in1k done. 3769.68 samples/sec, 67.88 ms/step
Model seresnext26ts.ch_in1k created, param count: 10388064
Running train benchmark on seresnext26ts.ch_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Train [8/40]. 971.18 samples/sec. 263.596 ms/step.
Train [16/40]. 971.15 samples/sec. 263.606 ms/step.
Train [24/40]. 971.16 samples/sec. 263.603 ms/step.
Train [32/40]. 971.16 samples/sec. 263.603 ms/step.
Train [40/40]. 971.16 samples/sec. 263.601 ms/step.
Train benchmark of seresnext26ts.ch_in1k done. 968.08 samples/sec, 263.60 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model seresnext50_32x4d.gluon_in1k created, param count: 27559896
Running inference benchmark on seresnext50_32x4d.gluon_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3109.16 samples/sec. 82.337 ms/step.
Infer [16/40]. 3108.62 samples/sec. 82.352 ms/step.
Infer [24/40]. 3108.61 samples/sec. 82.352 ms/step.
Infer [32/40]. 3109.47 samples/sec. 82.329 ms/step.
Infer [40/40]. 3110.58 samples/sec. 82.300 ms/step.
Inference benchmark of seresnext50_32x4d.gluon_in1k done. 3109.55 samples/sec, 82.30 ms/step
Model seresnext50_32x4d.gluon_in1k created, param count: 27559896
Running train benchmark on seresnext50_32x4d.gluon_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 911.30 samples/sec. 280.918 ms/step.
Train [16/40]. 911.28 samples/sec. 280.922 ms/step.
Train [24/40]. 911.33 samples/sec. 280.909 ms/step.
Train [32/40]. 911.32 samples/sec. 280.910 ms/step.
Train [40/40]. 911.37 samples/sec. 280.897 ms/step.
Train benchmark of seresnext50_32x4d.gluon_in1k done. 907.59 samples/sec, 280.90 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model seresnext50_32x4d.racm_in1k created, param count: 27559896
Running inference benchmark on seresnext50_32x4d.racm_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 1792.05 samples/sec. 142.853 ms/step.
Infer [16/40]. 1791.97 samples/sec. 142.859 ms/step.
Infer [24/40]. 1791.87 samples/sec. 142.867 ms/step.
Infer [32/40]. 1791.83 samples/sec. 142.871 ms/step.
Infer [40/40]. 1791.79 samples/sec. 142.874 ms/step.
Inference benchmark of seresnext50_32x4d.racm_in1k done. 1791.35 samples/sec, 142.87 ms/step
Model seresnext50_32x4d.racm_in1k created, param count: 27559896
Running train benchmark on seresnext50_32x4d.racm_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 82.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 28.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 22.26 GiB is allocated by PyTorch, and 123.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model seresnext50_32x4d.racm_in1k created, param count: 27559896
Running train benchmark on seresnext50_32x4d.racm_in1k for 40 steps w/ input size (3, 288, 288) and batch size 192.
Train [8/40]. 543.04 samples/sec. 353.564 ms/step.
Train [16/40]. 543.04 samples/sec. 353.565 ms/step.
Train [24/40]. 543.03 samples/sec. 353.575 ms/step.
Train [32/40]. 543.02 samples/sec. 353.577 ms/step.
Train [40/40]. 543.02 samples/sec. 353.576 ms/step.
Train benchmark of seresnext50_32x4d.racm_in1k done. 541.12 samples/sec, 353.58 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model seresnext101_32x4d.gluon_in1k created, param count: 48955416
Running inference benchmark on seresnext101_32x4d.gluon_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 2002.33 samples/sec. 127.851 ms/step.
Infer [16/40]. 2002.07 samples/sec. 127.868 ms/step.
Infer [24/40]. 2002.75 samples/sec. 127.824 ms/step.
Infer [32/40]. 2002.69 samples/sec. 127.828 ms/step.
Infer [40/40]. 2002.76 samples/sec. 127.824 ms/step.
Inference benchmark of seresnext101_32x4d.gluon_in1k done. 2002.27 samples/sec, 127.82 ms/step
Model seresnext101_32x4d.gluon_in1k created, param count: 48955416
Running train benchmark on seresnext101_32x4d.gluon_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 2.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 22.25 GiB is allocated by PyTorch, and 159.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model seresnext101_32x4d.gluon_in1k created, param count: 48955416
Running train benchmark on seresnext101_32x4d.gluon_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
Train [8/40]. 599.66 samples/sec. 320.182 ms/step.
Train [16/40]. 599.67 samples/sec. 320.175 ms/step.
Train [24/40]. 599.67 samples/sec. 320.178 ms/step.
Train [32/40]. 599.67 samples/sec. 320.178 ms/step.
Train [40/40]. 599.66 samples/sec. 320.180 ms/step.
Train benchmark of seresnext101_32x4d.gluon_in1k done. 596.20 samples/sec, 320.18 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model seresnext101_32x8d.ah_in1k created, param count: 93569048
Running inference benchmark on seresnext101_32x8d.ah_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 762.74 samples/sec. 335.631 ms/step.
Infer [16/40]. 762.73 samples/sec. 335.636 ms/step.
Infer [24/40]. 762.74 samples/sec. 335.630 ms/step.
Infer [32/40]. 762.74 samples/sec. 335.632 ms/step.
Infer [40/40]. 762.73 samples/sec. 335.637 ms/step.
Inference benchmark of seresnext101_32x8d.ah_in1k done. 762.62 samples/sec, 335.64 ms/step
Model seresnext101_32x8d.ah_in1k created, param count: 93569048
Running train benchmark on seresnext101_32x8d.ah_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 324.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 38.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 22.30 GiB is allocated by PyTorch, and 72.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model seresnext101_32x8d.ah_in1k created, param count: 93569048
Running train benchmark on seresnext101_32x8d.ah_in1k for 40 steps w/ input size (3, 288, 288) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 122.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 46.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 234.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model seresnext101_32x8d.ah_in1k created, param count: 93569048
Running train benchmark on seresnext101_32x8d.ah_in1k for 40 steps w/ input size (3, 288, 288) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 82.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 50.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 22.23 GiB is allocated by PyTorch, and 129.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model seresnext101_32x8d.ah_in1k created, param count: 93569048
Running train benchmark on seresnext101_32x8d.ah_in1k for 40 steps w/ input size (3, 288, 288) and batch size 96.
Train [8/40]. 234.92 samples/sec. 408.657 ms/step.
Train [16/40]. 234.93 samples/sec. 408.640 ms/step.
Train [24/40]. 234.92 samples/sec. 408.649 ms/step.
Train [32/40]. 234.92 samples/sec. 408.656 ms/step.
Train [40/40]. 234.92 samples/sec. 408.648 ms/step.
Train benchmark of seresnext101_32x8d.ah_in1k done. 233.89 samples/sec, 408.65 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model seresnext101_64x4d.gluon_in1k created, param count: 88232984
Running inference benchmark on seresnext101_64x4d.gluon_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1320.79 samples/sec. 193.823 ms/step.
Infer [16/40]. 1320.41 samples/sec. 193.879 ms/step.
Infer [24/40]. 1320.28 samples/sec. 193.899 ms/step.
Infer [32/40]. 1320.16 samples/sec. 193.916 ms/step.
Infer [40/40]. 1320.17 samples/sec. 193.914 ms/step.
Inference benchmark of seresnext101_64x4d.gluon_in1k done. 1319.94 samples/sec, 193.91 ms/step
Model seresnext101_64x4d.gluon_in1k created, param count: 88232984
Running train benchmark on seresnext101_64x4d.gluon_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 98.06 MiB is free. Including non-PyTorch memory, this process has 23.54 GiB memory in use. Of the allocated memory 21.89 GiB is allocated by PyTorch, and 429.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model seresnext101_64x4d.gluon_in1k created, param count: 88232984
Running train benchmark on seresnext101_64x4d.gluon_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 74.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 74.06 MiB is free. Including non-PyTorch memory, this process has 23.57 GiB memory in use. Of the allocated memory 22.22 GiB is allocated by PyTorch, and 118.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model seresnext101_64x4d.gluon_in1k created, param count: 88232984
Running train benchmark on seresnext101_64x4d.gluon_in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
Train [8/40]. 389.40 samples/sec. 328.714 ms/step.
Train [16/40]. 389.48 samples/sec. 328.645 ms/step.
Train [24/40]. 389.55 samples/sec. 328.584 ms/step.
Train [32/40]. 389.52 samples/sec. 328.607 ms/step.
Train [40/40]. 389.48 samples/sec. 328.646 ms/step.
Train benchmark of seresnext101_64x4d.gluon_in1k done. 387.36 samples/sec, 328.65 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model seresnext101d_32x8d.ah_in1k created, param count: 93588280
Running inference benchmark on seresnext101d_32x8d.ah_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 747.76 samples/sec. 342.358 ms/step.
Infer [16/40]. 747.65 samples/sec. 342.406 ms/step.
Infer [24/40]. 747.60 samples/sec. 342.429 ms/step.
Infer [32/40]. 747.58 samples/sec. 342.437 ms/step.
Infer [40/40]. 747.57 samples/sec. 342.442 ms/step.
Inference benchmark of seresnext101d_32x8d.ah_in1k done. 747.47 samples/sec, 342.44 ms/step
Model seresnext101d_32x8d.ah_in1k created, param count: 93588280
Running train benchmark on seresnext101d_32x8d.ah_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 324.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 200.06 MiB is free. Including non-PyTorch memory, this process has 23.45 GiB memory in use. Of the allocated memory 22.14 GiB is allocated by PyTorch, and 71.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model seresnext101d_32x8d.ah_in1k created, param count: 93588280
Running train benchmark on seresnext101d_32x8d.ah_in1k for 40 steps w/ input size (3, 288, 288) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 486.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 406.06 MiB is free. Including non-PyTorch memory, this process has 23.24 GiB memory in use. Of the allocated memory 21.66 GiB is allocated by PyTorch, and 360.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model seresnext101d_32x8d.ah_in1k created, param count: 93588280
Running train benchmark on seresnext101d_32x8d.ah_in1k for 40 steps w/ input size (3, 288, 288) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 82.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 8.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.27 GiB is allocated by PyTorch, and 135.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model seresnext101d_32x8d.ah_in1k created, param count: 93588280
Running train benchmark on seresnext101d_32x8d.ah_in1k for 40 steps w/ input size (3, 288, 288) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 32.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 21.88 GiB is allocated by PyTorch, and 513.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model seresnext101d_32x8d.ah_in1k created, param count: 93588280
Running train benchmark on seresnext101d_32x8d.ah_in1k for 40 steps w/ input size (3, 288, 288) and batch size 64.
Train [8/40]. 236.11 samples/sec. 271.063 ms/step.
Train [16/40]. 236.04 samples/sec. 271.136 ms/step.
Train [24/40]. 236.02 samples/sec. 271.159 ms/step.
Train [32/40]. 236.01 samples/sec. 271.170 ms/step.
Train [40/40]. 236.00 samples/sec. 271.183 ms/step.
Train benchmark of seresnext101d_32x8d.ah_in1k done. 234.45 samples/sec, 271.18 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model seresnextaa101d_32x8d.ah_in1k created, param count: 93588280
Running inference benchmark on seresnextaa101d_32x8d.ah_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 707.56 samples/sec. 361.806 ms/step.
Infer [16/40]. 707.38 samples/sec. 361.898 ms/step.
Infer [24/40]. 707.33 samples/sec. 361.926 ms/step.
Infer [32/40]. 707.29 samples/sec. 361.943 ms/step.
Infer [40/40]. 707.27 samples/sec. 361.955 ms/step.
Inference benchmark of seresnextaa101d_32x8d.ah_in1k done. 707.18 samples/sec, 361.95 ms/step
Model seresnextaa101d_32x8d.ah_in1k created, param count: 93588280
Running train benchmark on seresnextaa101d_32x8d.ah_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 1.27 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1010.06 MiB is free. Including non-PyTorch memory, this process has 22.65 GiB memory in use. Of the allocated memory 20.72 GiB is allocated by PyTorch, and 720.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model seresnextaa101d_32x8d.ah_in1k created, param count: 93588280
Running train benchmark on seresnextaa101d_32x8d.ah_in1k for 40 steps w/ input size (3, 288, 288) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 244.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 54.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 21.89 GiB is allocated by PyTorch, and 475.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model seresnextaa101d_32x8d.ah_in1k created, param count: 93588280
Running train benchmark on seresnextaa101d_32x8d.ah_in1k for 40 steps w/ input size (3, 288, 288) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 82.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 36.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 22.26 GiB is allocated by PyTorch, and 120.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model seresnextaa101d_32x8d.ah_in1k created, param count: 93588280
Running train benchmark on seresnextaa101d_32x8d.ah_in1k for 40 steps w/ input size (3, 288, 288) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 122.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 54.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 21.96 GiB is allocated by PyTorch, and 407.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model seresnextaa101d_32x8d.ah_in1k created, param count: 93588280
Running train benchmark on seresnextaa101d_32x8d.ah_in1k for 40 steps w/ input size (3, 288, 288) and batch size 64.
Train [8/40]. 221.57 samples/sec. 288.848 ms/step.
Train [16/40]. 221.57 samples/sec. 288.850 ms/step.
Train [24/40]. 221.57 samples/sec. 288.850 ms/step.
Train [32/40]. 221.56 samples/sec. 288.856 ms/step.
Train [40/40]. 221.56 samples/sec. 288.855 ms/step.
Train benchmark of seresnextaa101d_32x8d.ah_in1k done. 220.15 samples/sec, 288.86 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model seresnextaa101d_32x8d.sw_in12k created, param count: 115760509
Running inference benchmark on seresnextaa101d_32x8d.sw_in12k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 705.45 samples/sec. 362.889 ms/step.
Infer [16/40]. 705.45 samples/sec. 362.887 ms/step.
Infer [24/40]. 705.45 samples/sec. 362.891 ms/step.
Infer [32/40]. 705.44 samples/sec. 362.893 ms/step.
Infer [40/40]. 705.43 samples/sec. 362.899 ms/step.
Inference benchmark of seresnextaa101d_32x8d.sw_in12k done. 705.34 samples/sec, 362.90 ms/step
Model seresnextaa101d_32x8d.sw_in12k created, param count: 115760509
Running train benchmark on seresnextaa101d_32x8d.sw_in12k for 40 steps w/ input size (3, 288, 288) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 1.27 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1010.06 MiB is free. Including non-PyTorch memory, this process has 22.65 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 678.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model seresnextaa101d_32x8d.sw_in12k created, param count: 115760509
Running train benchmark on seresnextaa101d_32x8d.sw_in12k for 40 steps w/ input size (3, 288, 288) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 244.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 54.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 21.94 GiB is allocated by PyTorch, and 429.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model seresnextaa101d_32x8d.sw_in12k created, param count: 115760509
Running train benchmark on seresnextaa101d_32x8d.sw_in12k for 40 steps w/ input size (3, 288, 288) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 82.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 72.06 MiB is free. Including non-PyTorch memory, this process has 23.57 GiB memory in use. Of the allocated memory 22.23 GiB is allocated by PyTorch, and 107.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model seresnextaa101d_32x8d.sw_in12k created, param count: 115760509
Running train benchmark on seresnextaa101d_32x8d.sw_in12k for 40 steps w/ input size (3, 288, 288) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 122.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 48.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 22.00 GiB is allocated by PyTorch, and 371.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model seresnextaa101d_32x8d.sw_in12k created, param count: 115760509
Running train benchmark on seresnextaa101d_32x8d.sw_in12k for 40 steps w/ input size (3, 288, 288) and batch size 64.
Train [8/40]. 221.01 samples/sec. 289.574 ms/step.
Train [16/40]. 221.01 samples/sec. 289.585 ms/step.
Train [24/40]. 221.01 samples/sec. 289.585 ms/step.
Train [32/40]. 221.01 samples/sec. 289.584 ms/step.
Train [40/40]. 221.01 samples/sec. 289.586 ms/step.
Train benchmark of seresnextaa101d_32x8d.sw_in12k done. 219.59 samples/sec, 289.59 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model seresnextaa101d_32x8d.sw_in12k_ft_in1k created, param count: 93588280
Running inference benchmark on seresnextaa101d_32x8d.sw_in12k_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
Infer [8/40]. 705.58 samples/sec. 362.820 ms/step.
Infer [16/40]. 705.62 samples/sec. 362.801 ms/step.
Infer [24/40]. 705.60 samples/sec. 362.811 ms/step.
Infer [32/40]. 705.61 samples/sec. 362.808 ms/step.
Infer [40/40]. 705.60 samples/sec. 362.809 ms/step.
Inference benchmark of seresnextaa101d_32x8d.sw_in12k_ft_in1k done. 705.51 samples/sec, 362.81 ms/step
Model seresnextaa101d_32x8d.sw_in12k_ft_in1k created, param count: 93588280
Running train benchmark on seresnextaa101d_32x8d.sw_in12k_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 1.27 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1010.06 MiB is free. Including non-PyTorch memory, this process has 22.65 GiB memory in use. Of the allocated memory 20.72 GiB is allocated by PyTorch, and 720.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model seresnextaa101d_32x8d.sw_in12k_ft_in1k created, param count: 93588280
Running train benchmark on seresnextaa101d_32x8d.sw_in12k_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 244.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 54.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 21.89 GiB is allocated by PyTorch, and 471.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model seresnextaa101d_32x8d.sw_in12k_ft_in1k created, param count: 93588280
Running train benchmark on seresnextaa101d_32x8d.sw_in12k_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 82.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 72.06 MiB is free. Including non-PyTorch memory, this process has 23.57 GiB memory in use. Of the allocated memory 22.27 GiB is allocated by PyTorch, and 68.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model seresnextaa101d_32x8d.sw_in12k_ft_in1k created, param count: 93588280
Running train benchmark on seresnextaa101d_32x8d.sw_in12k_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 122.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 78.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 261.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model seresnextaa101d_32x8d.sw_in12k_ft_in1k created, param count: 93588280
Running train benchmark on seresnextaa101d_32x8d.sw_in12k_ft_in1k for 40 steps w/ input size (3, 288, 288) and batch size 64.
Train [8/40]. 221.49 samples/sec. 288.951 ms/step.
Train [16/40]. 221.48 samples/sec. 288.971 ms/step.
Train [24/40]. 221.48 samples/sec. 288.971 ms/step.
Train [32/40]. 221.47 samples/sec. 288.972 ms/step.
Train [40/40]. 221.48 samples/sec. 288.969 ms/step.
Train benchmark of seresnextaa101d_32x8d.sw_in12k_ft_in1k done. 220.10 samples/sec, 288.97 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model seresnextaa101d_32x8d.sw_in12k_ft_in1k_288 created, param count: 93588280
Running inference benchmark on seresnextaa101d_32x8d.sw_in12k_ft_in1k_288 for 40 steps w/ input size (3, 320, 320) and batch size 256.
Infer [8/40]. 612.30 samples/sec. 418.094 ms/step.
Infer [16/40]. 612.25 samples/sec. 418.127 ms/step.
Infer [24/40]. 612.24 samples/sec. 418.139 ms/step.
Infer [32/40]. 612.18 samples/sec. 418.179 ms/step.
Infer [40/40]. 612.10 samples/sec. 418.233 ms/step.
Inference benchmark of seresnextaa101d_32x8d.sw_in12k_ft_in1k_288 done. 612.03 samples/sec, 418.23 ms/step
Model seresnextaa101d_32x8d.sw_in12k_ft_in1k_288 created, param count: 93588280
Running train benchmark on seresnextaa101d_32x8d.sw_in12k_ft_in1k_288 for 40 steps w/ input size (3, 320, 320) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 1.56 GiB. GPU 0 has a total capacty of 23.65 GiB of which 876.06 MiB is free. Including non-PyTorch memory, this process has 22.79 GiB memory in use. Of the allocated memory 20.85 GiB is allocated by PyTorch, and 723.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model seresnextaa101d_32x8d.sw_in12k_ft_in1k_288 created, param count: 93588280
Running train benchmark on seresnextaa101d_32x8d.sw_in12k_ft_in1k_288 for 40 steps w/ input size (3, 320, 320) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 300.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 164.06 MiB is free. Including non-PyTorch memory, this process has 23.48 GiB memory in use. Of the allocated memory 21.98 GiB is allocated by PyTorch, and 273.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model seresnextaa101d_32x8d.sw_in12k_ft_in1k_288 created, param count: 93588280
Running train benchmark on seresnextaa101d_32x8d.sw_in12k_ft_in1k_288 for 40 steps w/ input size (3, 320, 320) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 98.06 MiB is free. Including non-PyTorch memory, this process has 23.54 GiB memory in use. Of the allocated memory 22.19 GiB is allocated by PyTorch, and 124.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model seresnextaa101d_32x8d.sw_in12k_ft_in1k_288 created, param count: 93588280
Running train benchmark on seresnextaa101d_32x8d.sw_in12k_ft_in1k_288 for 40 steps w/ input size (3, 320, 320) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 76.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 26.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.15 GiB is allocated by PyTorch, and 237.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model seresnextaa101d_32x8d.sw_in12k_ft_in1k_288 created, param count: 93588280
Running train benchmark on seresnextaa101d_32x8d.sw_in12k_ft_in1k_288 for 40 steps w/ input size (3, 320, 320) and batch size 64.
Train [8/40]. 176.64 samples/sec. 362.318 ms/step.
Train [16/40]. 176.61 samples/sec. 362.388 ms/step.
Train [24/40]. 176.58 samples/sec. 362.441 ms/step.
Train [32/40]. 176.56 samples/sec. 362.473 ms/step.
Train [40/40]. 176.56 samples/sec. 362.488 ms/step.
Train benchmark of seresnextaa101d_32x8d.sw_in12k_ft_in1k_288 done. 175.62 samples/sec, 362.49 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model skresnet18.ra_in1k created, param count: 11958056
Running inference benchmark on skresnet18.ra_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 6969.24 samples/sec. 36.733 ms/step.
Infer [16/40]. 6965.96 samples/sec. 36.750 ms/step.
Infer [24/40]. 6943.47 samples/sec. 36.869 ms/step.
Infer [32/40]. 6932.83 samples/sec. 36.926 ms/step.
Infer [40/40]. 6927.19 samples/sec. 36.956 ms/step.
Inference benchmark of skresnet18.ra_in1k done. 6922.29 samples/sec, 36.96 ms/step
Model skresnet18.ra_in1k created, param count: 11958056
Running train benchmark on skresnet18.ra_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 2492.07 samples/sec. 102.726 ms/step.
Train [16/40]. 2493.56 samples/sec. 102.664 ms/step.
Train [24/40]. 2494.44 samples/sec. 102.628 ms/step.
Train [32/40]. 2495.62 samples/sec. 102.580 ms/step.
Train [40/40]. 2495.97 samples/sec. 102.565 ms/step.
Train benchmark of skresnet18.ra_in1k done. 2481.50 samples/sec, 102.56 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model skresnet34.ra_in1k created, param count: 22282376
Running inference benchmark on skresnet34.ra_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3982.42 samples/sec. 64.283 ms/step.
Infer [16/40]. 3992.58 samples/sec. 64.119 ms/step.
Infer [24/40]. 3997.60 samples/sec. 64.038 ms/step.
Infer [32/40]. 4003.03 samples/sec. 63.951 ms/step.
Infer [40/40]. 4001.49 samples/sec. 63.976 ms/step.
Inference benchmark of skresnet34.ra_in1k done. 3999.81 samples/sec, 63.98 ms/step
Model skresnet34.ra_in1k created, param count: 22282376
Running train benchmark on skresnet34.ra_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1481.65 samples/sec. 172.780 ms/step.
Train [16/40]. 1481.69 samples/sec. 172.775 ms/step.
Train [24/40]. 1482.30 samples/sec. 172.704 ms/step.
Train [32/40]. 1481.87 samples/sec. 172.754 ms/step.
Train [40/40]. 1481.97 samples/sec. 172.743 ms/step.
Train benchmark of skresnet34.ra_in1k done. 1473.69 samples/sec, 172.74 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model skresnext50_32x4d.ra_in1k created, param count: 27479784
Running inference benchmark on skresnext50_32x4d.ra_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 2277.80 samples/sec. 112.389 ms/step.
Infer [16/40]. 2277.07 samples/sec. 112.425 ms/step.
Infer [24/40]. 2276.88 samples/sec. 112.434 ms/step.
Infer [32/40]. 2276.96 samples/sec. 112.431 ms/step.
Infer [40/40]. 2276.83 samples/sec. 112.437 ms/step.
Inference benchmark of skresnext50_32x4d.ra_in1k done. 2276.24 samples/sec, 112.44 ms/step
Model skresnext50_32x4d.ra_in1k created, param count: 27479784
Running train benchmark on skresnext50_32x4d.ra_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 723.03 samples/sec. 354.063 ms/step.
Train [16/40]. 722.93 samples/sec. 354.114 ms/step.
Train [24/40]. 722.83 samples/sec. 354.162 ms/step.
Train [32/40]. 722.88 samples/sec. 354.138 ms/step.
Train [40/40]. 722.88 samples/sec. 354.139 ms/step.
Train benchmark of skresnext50_32x4d.ra_in1k done. 720.08 samples/sec, 354.14 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model spnasnet_100.rmsp_in1k created, param count: 4421616
Running inference benchmark on spnasnet_100.rmsp_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 12897.51 samples/sec. 19.849 ms/step.
Infer [16/40]. 12896.58 samples/sec. 19.850 ms/step.
Infer [24/40]. 12896.30 samples/sec. 19.851 ms/step.
Infer [32/40]. 12896.46 samples/sec. 19.850 ms/step.
Infer [40/40]. 12896.44 samples/sec. 19.850 ms/step.
Inference benchmark of spnasnet_100.rmsp_in1k done. 12880.46 samples/sec, 19.85 ms/step
Model spnasnet_100.rmsp_in1k created, param count: 4421616
Running train benchmark on spnasnet_100.rmsp_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 2484.87 samples/sec. 103.024 ms/step.
Train [16/40]. 2484.99 samples/sec. 103.018 ms/step.
Train [24/40]. 2485.13 samples/sec. 103.013 ms/step.
Train [32/40]. 2485.11 samples/sec. 103.014 ms/step.
Train [40/40]. 2485.05 samples/sec. 103.016 ms/step.
Train benchmark of spnasnet_100.rmsp_in1k done. 2468.06 samples/sec, 103.02 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model swin_base_patch4_window7_224.ms_in1k created, param count: 87768224
Running inference benchmark on swin_base_patch4_window7_224.ms_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1723.35 samples/sec. 148.548 ms/step.
Infer [16/40]. 1723.45 samples/sec. 148.539 ms/step.
Infer [24/40]. 1723.49 samples/sec. 148.536 ms/step.
Infer [32/40]. 1723.51 samples/sec. 148.534 ms/step.
Infer [40/40]. 1723.48 samples/sec. 148.537 ms/step.
Inference benchmark of swin_base_patch4_window7_224.ms_in1k done. 1723.10 samples/sec, 148.54 ms/step
Model swin_base_patch4_window7_224.ms_in1k created, param count: 87768224
Running train benchmark on swin_base_patch4_window7_224.ms_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 4.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 21.97 GiB is allocated by PyTorch, and 448.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model swin_base_patch4_window7_224.ms_in1k created, param count: 87768224
Running train benchmark on swin_base_patch4_window7_224.ms_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
Train [8/40]. 598.66 samples/sec. 320.715 ms/step.
Train [16/40]. 598.57 samples/sec. 320.763 ms/step.
Train [24/40]. 598.59 samples/sec. 320.754 ms/step.
Train [32/40]. 598.60 samples/sec. 320.750 ms/step.
Train [40/40]. 598.58 samples/sec. 320.758 ms/step.
Train benchmark of swin_base_patch4_window7_224.ms_in1k done. 594.74 samples/sec, 320.76 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model swin_base_patch4_window7_224.ms_in22k created, param count: 109130249
Running inference benchmark on swin_base_patch4_window7_224.ms_in22k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1723.31 samples/sec. 148.552 ms/step.
Infer [16/40]. 1723.31 samples/sec. 148.551 ms/step.
Infer [24/40]. 1723.17 samples/sec. 148.564 ms/step.
Infer [32/40]. 1723.06 samples/sec. 148.573 ms/step.
Infer [40/40]. 1723.01 samples/sec. 148.578 ms/step.
Inference benchmark of swin_base_patch4_window7_224.ms_in22k done. 1722.63 samples/sec, 148.58 ms/step
Model swin_base_patch4_window7_224.ms_in22k created, param count: 109130249
Running train benchmark on swin_base_patch4_window7_224.ms_in22k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 76.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 52.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 21.91 GiB is allocated by PyTorch, and 459.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model swin_base_patch4_window7_224.ms_in22k created, param count: 109130249
Running train benchmark on swin_base_patch4_window7_224.ms_in22k for 40 steps w/ input size (3, 224, 224) and batch size 192.
Train [8/40]. 596.59 samples/sec. 321.827 ms/step.
Train [16/40]. 596.59 samples/sec. 321.831 ms/step.
Train [24/40]. 596.58 samples/sec. 321.833 ms/step.
Train [32/40]. 596.56 samples/sec. 321.843 ms/step.
Train [40/40]. 596.57 samples/sec. 321.842 ms/step.
Train benchmark of swin_base_patch4_window7_224.ms_in22k done. 592.62 samples/sec, 321.84 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model swin_base_patch4_window7_224.ms_in22k_ft_in1k created, param count: 87768224
Running inference benchmark on swin_base_patch4_window7_224.ms_in22k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1723.76 samples/sec. 148.513 ms/step.
Infer [16/40]. 1723.50 samples/sec. 148.535 ms/step.
Infer [24/40]. 1723.52 samples/sec. 148.533 ms/step.
Infer [32/40]. 1723.51 samples/sec. 148.534 ms/step.
Infer [40/40]. 1723.53 samples/sec. 148.532 ms/step.
Inference benchmark of swin_base_patch4_window7_224.ms_in22k_ft_in1k done. 1723.16 samples/sec, 148.53 ms/step
Model swin_base_patch4_window7_224.ms_in22k_ft_in1k created, param count: 87768224
Running train benchmark on swin_base_patch4_window7_224.ms_in22k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 4.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 21.97 GiB is allocated by PyTorch, and 448.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model swin_base_patch4_window7_224.ms_in22k_ft_in1k created, param count: 87768224
Running train benchmark on swin_base_patch4_window7_224.ms_in22k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
Train [8/40]. 598.62 samples/sec. 320.736 ms/step.
Train [16/40]. 598.68 samples/sec. 320.704 ms/step.
Train [24/40]. 598.67 samples/sec. 320.710 ms/step.
Train [32/40]. 598.67 samples/sec. 320.710 ms/step.
Train [40/40]. 598.67 samples/sec. 320.712 ms/step.
Train benchmark of swin_base_patch4_window7_224.ms_in22k_ft_in1k done. 594.68 samples/sec, 320.71 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model swin_base_patch4_window12_384.ms_in1k created, param count: 87903584
Running inference benchmark on swin_base_patch4_window12_384.ms_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 447.42 samples/sec. 572.171 ms/step.
Infer [16/40]. 447.41 samples/sec. 572.178 ms/step.
Infer [24/40]. 447.41 samples/sec. 572.188 ms/step.
Infer [32/40]. 447.38 samples/sec. 572.214 ms/step.
Infer [40/40]. 447.36 samples/sec. 572.249 ms/step.
Inference benchmark of swin_base_patch4_window12_384.ms_in1k done. 447.32 samples/sec, 572.25 ms/step
Model swin_base_patch4_window12_384.ms_in1k created, param count: 87903584
Running train benchmark on swin_base_patch4_window12_384.ms_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 2.53 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.04 GiB is free. Including non-PyTorch memory, this process has 22.60 GiB memory in use. Of the allocated memory 20.72 GiB is allocated by PyTorch, and 662.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model swin_base_patch4_window12_384.ms_in1k created, param count: 87903584
Running train benchmark on swin_base_patch4_window12_384.ms_in1k for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 972.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 918.06 MiB is free. Including non-PyTorch memory, this process has 22.74 GiB memory in use. Of the allocated memory 21.30 GiB is allocated by PyTorch, and 214.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model swin_base_patch4_window12_384.ms_in1k created, param count: 87903584
Running train benchmark on swin_base_patch4_window12_384.ms_in1k for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 216.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 198.06 MiB is free. Including non-PyTorch memory, this process has 23.45 GiB memory in use. Of the allocated memory 22.01 GiB is allocated by PyTorch, and 210.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model swin_base_patch4_window12_384.ms_in1k created, param count: 87903584
Running train benchmark on swin_base_patch4_window12_384.ms_in1k for 40 steps w/ input size (3, 384, 384) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 162.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 136.06 MiB is free. Including non-PyTorch memory, this process has 23.51 GiB memory in use. Of the allocated memory 22.02 GiB is allocated by PyTorch, and 258.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model swin_base_patch4_window12_384.ms_in1k created, param count: 87903584
Running train benchmark on swin_base_patch4_window12_384.ms_in1k for 40 steps w/ input size (3, 384, 384) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 108.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 78.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 21.95 GiB is allocated by PyTorch, and 388.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model swin_base_patch4_window12_384.ms_in1k created, param count: 87903584
Running train benchmark on swin_base_patch4_window12_384.ms_in1k for 40 steps w/ input size (3, 384, 384) and batch size 48.
Train [8/40]. 167.09 samples/sec. 287.276 ms/step.
Train [16/40]. 167.08 samples/sec. 287.287 ms/step.
Train [24/40]. 167.09 samples/sec. 287.277 ms/step.
Train [32/40]. 167.08 samples/sec. 287.284 ms/step.
Train [40/40]. 167.09 samples/sec. 287.279 ms/step.
Train benchmark of swin_base_patch4_window12_384.ms_in1k done. 165.94 samples/sec, 287.28 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model swin_base_patch4_window12_384.ms_in22k created, param count: 109265609
Running inference benchmark on swin_base_patch4_window12_384.ms_in22k for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 447.17 samples/sec. 572.492 ms/step.
Infer [16/40]. 447.17 samples/sec. 572.495 ms/step.
Infer [24/40]. 447.16 samples/sec. 572.501 ms/step.
Infer [32/40]. 447.16 samples/sec. 572.504 ms/step.
Infer [40/40]. 447.16 samples/sec. 572.503 ms/step.
Inference benchmark of swin_base_patch4_window12_384.ms_in22k done. 447.12 samples/sec, 572.50 ms/step
Model swin_base_patch4_window12_384.ms_in22k created, param count: 109265609
Running train benchmark on swin_base_patch4_window12_384.ms_in22k for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 2.53 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.04 GiB is free. Including non-PyTorch memory, this process has 22.60 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 621.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model swin_base_patch4_window12_384.ms_in22k created, param count: 109265609
Running train benchmark on swin_base_patch4_window12_384.ms_in22k for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 972.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 918.06 MiB is free. Including non-PyTorch memory, this process has 22.74 GiB memory in use. Of the allocated memory 21.34 GiB is allocated by PyTorch, and 173.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model swin_base_patch4_window12_384.ms_in22k created, param count: 109265609
Running train benchmark on swin_base_patch4_window12_384.ms_in22k for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 216.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 198.06 MiB is free. Including non-PyTorch memory, this process has 23.45 GiB memory in use. Of the allocated memory 22.05 GiB is allocated by PyTorch, and 170.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model swin_base_patch4_window12_384.ms_in22k created, param count: 109265609
Running train benchmark on swin_base_patch4_window12_384.ms_in22k for 40 steps w/ input size (3, 384, 384) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 162.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 82.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 22.06 GiB is allocated by PyTorch, and 271.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model swin_base_patch4_window12_384.ms_in22k created, param count: 109265609
Running train benchmark on swin_base_patch4_window12_384.ms_in22k for 40 steps w/ input size (3, 384, 384) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 108.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 42.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 21.99 GiB is allocated by PyTorch, and 383.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model swin_base_patch4_window12_384.ms_in22k created, param count: 109265609
Running train benchmark on swin_base_patch4_window12_384.ms_in22k for 40 steps w/ input size (3, 384, 384) and batch size 48.
Train [8/40]. 166.79 samples/sec. 287.783 ms/step.
Train [16/40]. 166.79 samples/sec. 287.782 ms/step.
Train [24/40]. 166.79 samples/sec. 287.793 ms/step.
Train [32/40]. 166.79 samples/sec. 287.795 ms/step.
Train [40/40]. 166.79 samples/sec. 287.793 ms/step.
Train benchmark of swin_base_patch4_window12_384.ms_in22k done. 165.65 samples/sec, 287.79 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model swin_base_patch4_window12_384.ms_in22k_ft_in1k created, param count: 87903584
Running inference benchmark on swin_base_patch4_window12_384.ms_in22k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 447.28 samples/sec. 572.352 ms/step.
Infer [16/40]. 447.28 samples/sec. 572.354 ms/step.
Infer [24/40]. 447.27 samples/sec. 572.355 ms/step.
Infer [32/40]. 447.27 samples/sec. 572.356 ms/step.
Infer [40/40]. 447.28 samples/sec. 572.353 ms/step.
Inference benchmark of swin_base_patch4_window12_384.ms_in22k_ft_in1k done. 447.24 samples/sec, 572.35 ms/step
Model swin_base_patch4_window12_384.ms_in22k_ft_in1k created, param count: 87903584
Running train benchmark on swin_base_patch4_window12_384.ms_in22k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 2.53 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.04 GiB is free. Including non-PyTorch memory, this process has 22.60 GiB memory in use. Of the allocated memory 20.72 GiB is allocated by PyTorch, and 662.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model swin_base_patch4_window12_384.ms_in22k_ft_in1k created, param count: 87903584
Running train benchmark on swin_base_patch4_window12_384.ms_in22k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 972.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 918.06 MiB is free. Including non-PyTorch memory, this process has 22.74 GiB memory in use. Of the allocated memory 21.30 GiB is allocated by PyTorch, and 214.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model swin_base_patch4_window12_384.ms_in22k_ft_in1k created, param count: 87903584
Running train benchmark on swin_base_patch4_window12_384.ms_in22k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 216.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 198.06 MiB is free. Including non-PyTorch memory, this process has 23.45 GiB memory in use. Of the allocated memory 22.01 GiB is allocated by PyTorch, and 210.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model swin_base_patch4_window12_384.ms_in22k_ft_in1k created, param count: 87903584
Running train benchmark on swin_base_patch4_window12_384.ms_in22k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 162.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 136.06 MiB is free. Including non-PyTorch memory, this process has 23.51 GiB memory in use. Of the allocated memory 22.02 GiB is allocated by PyTorch, and 258.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model swin_base_patch4_window12_384.ms_in22k_ft_in1k created, param count: 87903584
Running train benchmark on swin_base_patch4_window12_384.ms_in22k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 108.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 78.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 21.95 GiB is allocated by PyTorch, and 388.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model swin_base_patch4_window12_384.ms_in22k_ft_in1k created, param count: 87903584
Running train benchmark on swin_base_patch4_window12_384.ms_in22k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 48.
Train [8/40]. 167.08 samples/sec. 287.295 ms/step.
Train [16/40]. 167.07 samples/sec. 287.307 ms/step.
Train [24/40]. 167.06 samples/sec. 287.314 ms/step.
Train [32/40]. 167.06 samples/sec. 287.320 ms/step.
Train [40/40]. 167.06 samples/sec. 287.318 ms/step.
Train benchmark of swin_base_patch4_window12_384.ms_in22k_ft_in1k done. 165.89 samples/sec, 287.32 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model swin_large_patch4_window7_224.ms_in22k created, param count: 228565093
Running inference benchmark on swin_large_patch4_window7_224.ms_in22k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1016.42 samples/sec. 251.865 ms/step.
Infer [16/40]. 1016.45 samples/sec. 251.857 ms/step.
Infer [24/40]. 1016.44 samples/sec. 251.859 ms/step.
Infer [32/40]. 1016.45 samples/sec. 251.858 ms/step.
Infer [40/40]. 1016.45 samples/sec. 251.858 ms/step.
Inference benchmark of swin_large_patch4_window7_224.ms_in22k done. 1016.27 samples/sec, 251.86 ms/step
Model swin_large_patch4_window7_224.ms_in22k created, param count: 228565093
Running train benchmark on swin_large_patch4_window7_224.ms_in22k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 294.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 262.06 MiB is free. Including non-PyTorch memory, this process has 23.38 GiB memory in use. Of the allocated memory 21.86 GiB is allocated by PyTorch, and 303.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model swin_large_patch4_window7_224.ms_in22k created, param count: 228565093
Running train benchmark on swin_large_patch4_window7_224.ms_in22k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 86.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 52.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 21.90 GiB is allocated by PyTorch, and 469.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model swin_large_patch4_window7_224.ms_in22k created, param count: 228565093
Running train benchmark on swin_large_patch4_window7_224.ms_in22k for 40 steps w/ input size (3, 224, 224) and batch size 128.
Train [8/40]. 347.92 samples/sec. 367.898 ms/step.
Train [16/40]. 347.89 samples/sec. 367.933 ms/step.
Train [24/40]. 347.86 samples/sec. 367.963 ms/step.
Train [32/40]. 347.84 samples/sec. 367.981 ms/step.
Train [40/40]. 347.83 samples/sec. 367.991 ms/step.
Train benchmark of swin_large_patch4_window7_224.ms_in22k done. 345.75 samples/sec, 367.99 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model swin_large_patch4_window7_224.ms_in22k_ft_in1k created, param count: 196532476
Running inference benchmark on swin_large_patch4_window7_224.ms_in22k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1017.11 samples/sec. 251.692 ms/step.
Infer [16/40]. 1017.09 samples/sec. 251.698 ms/step.
Infer [24/40]. 1017.07 samples/sec. 251.704 ms/step.
Infer [32/40]. 1017.08 samples/sec. 251.701 ms/step.
Infer [40/40]. 1017.08 samples/sec. 251.700 ms/step.
Inference benchmark of swin_large_patch4_window7_224.ms_in22k_ft_in1k done. 1016.91 samples/sec, 251.70 ms/step
Model swin_large_patch4_window7_224.ms_in22k_ft_in1k created, param count: 196532476
Running train benchmark on swin_large_patch4_window7_224.ms_in22k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 74.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 74.06 MiB is free. Including non-PyTorch memory, this process has 23.57 GiB memory in use. Of the allocated memory 22.09 GiB is allocated by PyTorch, and 256.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model swin_large_patch4_window7_224.ms_in22k_ft_in1k created, param count: 196532476
Running train benchmark on swin_large_patch4_window7_224.ms_in22k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 86.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 78.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 21.84 GiB is allocated by PyTorch, and 504.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model swin_large_patch4_window7_224.ms_in22k_ft_in1k created, param count: 196532476
Running train benchmark on swin_large_patch4_window7_224.ms_in22k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
Train [8/40]. 348.90 samples/sec. 366.862 ms/step.
Train [16/40]. 348.89 samples/sec. 366.879 ms/step.
Train [24/40]. 348.89 samples/sec. 366.882 ms/step.
Train [32/40]. 348.88 samples/sec. 366.885 ms/step.
Train [40/40]. 348.88 samples/sec. 366.886 ms/step.
Train benchmark of swin_large_patch4_window7_224.ms_in22k_ft_in1k done. 346.82 samples/sec, 366.89 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model swin_large_patch4_window12_384.ms_in22k created, param count: 228768133
Running inference benchmark on swin_large_patch4_window12_384.ms_in22k for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 272.08 samples/sec. 940.915 ms/step.
Infer [16/40]. 272.08 samples/sec. 940.909 ms/step.
Infer [24/40]. 272.02 samples/sec. 941.102 ms/step.
Infer [32/40]. 271.98 samples/sec. 941.241 ms/step.
Infer [40/40]. 271.96 samples/sec. 941.324 ms/step.
Inference benchmark of swin_large_patch4_window12_384.ms_in22k done. 271.94 samples/sec, 941.32 ms/step
Model swin_large_patch4_window12_384.ms_in22k created, param count: 228768133
Running train benchmark on swin_large_patch4_window12_384.ms_in22k for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 864.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 500.06 MiB is free. Including non-PyTorch memory, this process has 23.15 GiB memory in use. Of the allocated memory 21.39 GiB is allocated by PyTorch, and 545.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model swin_large_patch4_window12_384.ms_in22k created, param count: 228768133
Running train benchmark on swin_large_patch4_window12_384.ms_in22k for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 2.85 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.04 GiB is free. Including non-PyTorch memory, this process has 22.60 GiB memory in use. Of the allocated memory 20.60 GiB is allocated by PyTorch, and 783.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model swin_large_patch4_window12_384.ms_in22k created, param count: 228768133
Running train benchmark on swin_large_patch4_window12_384.ms_in22k for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 972.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 814.06 MiB is free. Including non-PyTorch memory, this process has 22.85 GiB memory in use. Of the allocated memory 21.49 GiB is allocated by PyTorch, and 123.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model swin_large_patch4_window12_384.ms_in22k created, param count: 228768133
Running train benchmark on swin_large_patch4_window12_384.ms_in22k for 40 steps w/ input size (3, 384, 384) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 366.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 114.06 MiB is free. Including non-PyTorch memory, this process has 23.53 GiB memory in use. Of the allocated memory 22.17 GiB is allocated by PyTorch, and 126.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model swin_large_patch4_window12_384.ms_in22k created, param count: 228768133
Running train benchmark on swin_large_patch4_window12_384.ms_in22k for 40 steps w/ input size (3, 384, 384) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 216.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 142.06 MiB is free. Including non-PyTorch memory, this process has 23.50 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 348.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model swin_large_patch4_window12_384.ms_in22k created, param count: 228768133
Running train benchmark on swin_large_patch4_window12_384.ms_in22k for 40 steps w/ input size (3, 384, 384) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 184.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 66.06 MiB is free. Including non-PyTorch memory, this process has 23.58 GiB memory in use. Of the allocated memory 21.91 GiB is allocated by PyTorch, and 448.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model swin_large_patch4_window12_384.ms_in22k created, param count: 228768133
Running train benchmark on swin_large_patch4_window12_384.ms_in22k for 40 steps w/ input size (3, 384, 384) and batch size 32.
Train [8/40]. 97.92 samples/sec. 326.785 ms/step.
Train [16/40]. 97.93 samples/sec. 326.775 ms/step.
Train [24/40]. 97.93 samples/sec. 326.769 ms/step.
Train [32/40]. 97.93 samples/sec. 326.768 ms/step.
Train [40/40]. 97.93 samples/sec. 326.766 ms/step.
Train benchmark of swin_large_patch4_window12_384.ms_in22k done. 97.32 samples/sec, 326.77 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model swin_large_patch4_window12_384.ms_in22k_ft_in1k created, param count: 196735516
Running inference benchmark on swin_large_patch4_window12_384.ms_in22k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 272.14 samples/sec. 940.707 ms/step.
Infer [16/40]. 272.13 samples/sec. 940.742 ms/step.
Infer [24/40]. 272.12 samples/sec. 940.758 ms/step.
Infer [32/40]. 272.12 samples/sec. 940.756 ms/step.
Infer [40/40]. 272.12 samples/sec. 940.752 ms/step.
Inference benchmark of swin_large_patch4_window12_384.ms_in22k_ft_in1k done. 272.11 samples/sec, 940.75 ms/step
Model swin_large_patch4_window12_384.ms_in22k_ft_in1k created, param count: 196735516
Running train benchmark on swin_large_patch4_window12_384.ms_in22k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 864.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 500.06 MiB is free. Including non-PyTorch memory, this process has 23.15 GiB memory in use. Of the allocated memory 21.33 GiB is allocated by PyTorch, and 607.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model swin_large_patch4_window12_384.ms_in22k_ft_in1k created, param count: 196735516
Running train benchmark on swin_large_patch4_window12_384.ms_in22k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 2.85 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.16 GiB is free. Including non-PyTorch memory, this process has 22.48 GiB memory in use. Of the allocated memory 20.54 GiB is allocated by PyTorch, and 728.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model swin_large_patch4_window12_384.ms_in22k_ft_in1k created, param count: 196735516
Running train benchmark on swin_large_patch4_window12_384.ms_in22k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 972.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 828.06 MiB is free. Including non-PyTorch memory, this process has 22.83 GiB memory in use. Of the allocated memory 21.43 GiB is allocated by PyTorch, and 172.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model swin_large_patch4_window12_384.ms_in22k_ft_in1k created, param count: 196735516
Running train benchmark on swin_large_patch4_window12_384.ms_in22k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 366.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 134.06 MiB is free. Including non-PyTorch memory, this process has 23.51 GiB memory in use. Of the allocated memory 22.11 GiB is allocated by PyTorch, and 168.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model swin_large_patch4_window12_384.ms_in22k_ft_in1k created, param count: 196735516
Running train benchmark on swin_large_patch4_window12_384.ms_in22k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 54.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 16.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 265.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model swin_large_patch4_window12_384.ms_in22k_ft_in1k created, param count: 196735516
Running train benchmark on swin_large_patch4_window12_384.ms_in22k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 184.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 132.06 MiB is free. Including non-PyTorch memory, this process has 23.51 GiB memory in use. Of the allocated memory 21.85 GiB is allocated by PyTorch, and 444.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model swin_large_patch4_window12_384.ms_in22k_ft_in1k created, param count: 196735516
Running train benchmark on swin_large_patch4_window12_384.ms_in22k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 32.
Train [8/40]. 98.28 samples/sec. 325.601 ms/step.
Train [16/40]. 98.28 samples/sec. 325.584 ms/step.
Train [24/40]. 98.28 samples/sec. 325.590 ms/step.
Train [32/40]. 98.28 samples/sec. 325.587 ms/step.
Train [40/40]. 98.28 samples/sec. 325.588 ms/step.
Train benchmark of swin_large_patch4_window12_384.ms_in22k_ft_in1k done. 97.65 samples/sec, 325.59 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model swin_s3_base_224.ms_in1k created, param count: 71125762
Running inference benchmark on swin_s3_base_224.ms_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1505.73 samples/sec. 170.017 ms/step.
Infer [16/40]. 1505.70 samples/sec. 170.021 ms/step.
Infer [24/40]. 1505.68 samples/sec. 170.023 ms/step.
Infer [32/40]. 1505.67 samples/sec. 170.024 ms/step.
Infer [40/40]. 1505.65 samples/sec. 170.026 ms/step.
Inference benchmark of swin_s3_base_224.ms_in1k done. 1505.35 samples/sec, 170.03 ms/step
Model swin_s3_base_224.ms_in1k created, param count: 71125762
Running train benchmark on swin_s3_base_224.ms_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 148.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 118.06 MiB is free. Including non-PyTorch memory, this process has 23.53 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 368.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model swin_s3_base_224.ms_in1k created, param count: 71125762
Running train benchmark on swin_s3_base_224.ms_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 12.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 324.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model swin_s3_base_224.ms_in1k created, param count: 71125762
Running train benchmark on swin_s3_base_224.ms_in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
Train [8/40]. 511.64 samples/sec. 250.178 ms/step.
Train [16/40]. 511.63 samples/sec. 250.179 ms/step.
Train [24/40]. 511.62 samples/sec. 250.186 ms/step.
Train [32/40]. 511.62 samples/sec. 250.187 ms/step.
Train [40/40]. 511.62 samples/sec. 250.188 ms/step.
Train benchmark of swin_s3_base_224.ms_in1k done. 506.02 samples/sec, 250.19 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model swin_s3_small_224.ms_in1k created, param count: 49737298
Running inference benchmark on swin_s3_small_224.ms_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1863.44 samples/sec. 137.380 ms/step.
Infer [16/40]. 1863.39 samples/sec. 137.384 ms/step.
Infer [24/40]. 1863.23 samples/sec. 137.396 ms/step.
Infer [32/40]. 1863.23 samples/sec. 137.396 ms/step.
Infer [40/40]. 1863.21 samples/sec. 137.397 ms/step.
Inference benchmark of swin_s3_small_224.ms_in1k done. 1862.80 samples/sec, 137.40 ms/step
Model swin_s3_small_224.ms_in1k created, param count: 49737298
Running train benchmark on swin_s3_small_224.ms_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 226.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 134.06 MiB is free. Including non-PyTorch memory, this process has 23.51 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 198.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model swin_s3_small_224.ms_in1k created, param count: 49737298
Running train benchmark on swin_s3_small_224.ms_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
Train [8/40]. 641.19 samples/sec. 299.444 ms/step.
Train [16/40]. 641.21 samples/sec. 299.435 ms/step.
Train [24/40]. 641.19 samples/sec. 299.442 ms/step.
Train [32/40]. 641.21 samples/sec. 299.435 ms/step.
Train [40/40]. 641.20 samples/sec. 299.439 ms/step.
Train benchmark of swin_s3_small_224.ms_in1k done. 636.88 samples/sec, 299.44 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model swin_s3_tiny_224.ms_in1k created, param count: 28328674
Running inference benchmark on swin_s3_tiny_224.ms_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3706.63 samples/sec. 69.065 ms/step.
Infer [16/40]. 3706.53 samples/sec. 69.067 ms/step.
Infer [24/40]. 3706.66 samples/sec. 69.065 ms/step.
Infer [32/40]. 3706.77 samples/sec. 69.063 ms/step.
Infer [40/40]. 3706.84 samples/sec. 69.062 ms/step.
Inference benchmark of swin_s3_tiny_224.ms_in1k done. 3705.42 samples/sec, 69.06 ms/step
Model swin_s3_tiny_224.ms_in1k created, param count: 28328674
Running train benchmark on swin_s3_tiny_224.ms_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1209.24 samples/sec. 211.704 ms/step.
Train [16/40]. 1209.23 samples/sec. 211.706 ms/step.
Train [24/40]. 1209.15 samples/sec. 211.720 ms/step.
Train [32/40]. 1209.11 samples/sec. 211.727 ms/step.
Train [40/40]. 1209.07 samples/sec. 211.733 ms/step.
Train benchmark of swin_s3_tiny_224.ms_in1k done. 1202.47 samples/sec, 211.73 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model swin_small_patch4_window7_224.ms_in1k created, param count: 49606258
Running inference benchmark on swin_small_patch4_window7_224.ms_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 2513.02 samples/sec. 101.869 ms/step.
Infer [16/40]. 2512.85 samples/sec. 101.877 ms/step.
Infer [24/40]. 2512.66 samples/sec. 101.884 ms/step.
Infer [32/40]. 2512.58 samples/sec. 101.887 ms/step.
Infer [40/40]. 2512.62 samples/sec. 101.886 ms/step.
Inference benchmark of swin_small_patch4_window7_224.ms_in1k done. 2511.88 samples/sec, 101.89 ms/step
Model swin_small_patch4_window7_224.ms_in1k created, param count: 49606258
Running train benchmark on swin_small_patch4_window7_224.ms_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 831.07 samples/sec. 308.038 ms/step.
Train [16/40]. 831.10 samples/sec. 308.025 ms/step.
Train [24/40]. 831.13 samples/sec. 308.016 ms/step.
Train [32/40]. 831.12 samples/sec. 308.017 ms/step.
Train [40/40]. 831.13 samples/sec. 308.014 ms/step.
Train benchmark of swin_small_patch4_window7_224.ms_in1k done. 825.63 samples/sec, 308.01 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model swin_small_patch4_window7_224.ms_in22k created, param count: 65632987
Running inference benchmark on swin_small_patch4_window7_224.ms_in22k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 2511.00 samples/sec. 101.951 ms/step.
Infer [16/40]. 2510.86 samples/sec. 101.957 ms/step.
Infer [24/40]. 2510.55 samples/sec. 101.970 ms/step.
Infer [32/40]. 2510.46 samples/sec. 101.973 ms/step.
Infer [40/40]. 2510.38 samples/sec. 101.977 ms/step.
Inference benchmark of swin_small_patch4_window7_224.ms_in22k done. 2509.64 samples/sec, 101.98 ms/step
Model swin_small_patch4_window7_224.ms_in22k created, param count: 65632987
Running train benchmark on swin_small_patch4_window7_224.ms_in22k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 829.20 samples/sec. 308.731 ms/step.
Train [16/40]. 829.24 samples/sec. 308.718 ms/step.
Train [24/40]. 829.23 samples/sec. 308.721 ms/step.
Train [32/40]. 829.23 samples/sec. 308.721 ms/step.
Train [40/40]. 829.23 samples/sec. 308.722 ms/step.
Train benchmark of swin_small_patch4_window7_224.ms_in22k done. 823.63 samples/sec, 308.72 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model swin_small_patch4_window7_224.ms_in22k_ft_in1k created, param count: 49606258
Running inference benchmark on swin_small_patch4_window7_224.ms_in22k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 2513.70 samples/sec. 101.842 ms/step.
Infer [16/40]. 2513.68 samples/sec. 101.843 ms/step.
Infer [24/40]. 2513.71 samples/sec. 101.841 ms/step.
Infer [32/40]. 2513.44 samples/sec. 101.852 ms/step.
Infer [40/40]. 2513.27 samples/sec. 101.859 ms/step.
Inference benchmark of swin_small_patch4_window7_224.ms_in22k_ft_in1k done. 2512.56 samples/sec, 101.86 ms/step
Model swin_small_patch4_window7_224.ms_in22k_ft_in1k created, param count: 49606258
Running train benchmark on swin_small_patch4_window7_224.ms_in22k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 831.17 samples/sec. 308.000 ms/step.
Train [16/40]. 831.13 samples/sec. 308.015 ms/step.
Train [24/40]. 831.12 samples/sec. 308.017 ms/step.
Train [32/40]. 831.13 samples/sec. 308.014 ms/step.
Train [40/40]. 831.14 samples/sec. 308.011 ms/step.
Train benchmark of swin_small_patch4_window7_224.ms_in22k_ft_in1k done. 825.45 samples/sec, 308.01 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model swin_tiny_patch4_window7_224.ms_in1k created, param count: 28288354
Running inference benchmark on swin_tiny_patch4_window7_224.ms_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 4051.40 samples/sec. 63.188 ms/step.
Infer [16/40]. 4051.14 samples/sec. 63.192 ms/step.
Infer [24/40]. 4051.07 samples/sec. 63.193 ms/step.
Infer [32/40]. 4051.14 samples/sec. 63.192 ms/step.
Infer [40/40]. 4051.16 samples/sec. 63.192 ms/step.
Inference benchmark of swin_tiny_patch4_window7_224.ms_in1k done. 4049.50 samples/sec, 63.19 ms/step
Model swin_tiny_patch4_window7_224.ms_in1k created, param count: 28288354
Running train benchmark on swin_tiny_patch4_window7_224.ms_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1319.13 samples/sec. 194.067 ms/step.
Train [16/40]. 1319.11 samples/sec. 194.070 ms/step.
Train [24/40]. 1319.07 samples/sec. 194.076 ms/step.
Train [32/40]. 1319.08 samples/sec. 194.074 ms/step.
Train [40/40]. 1319.09 samples/sec. 194.074 ms/step.
Train benchmark of swin_tiny_patch4_window7_224.ms_in1k done. 1311.35 samples/sec, 194.07 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model swin_tiny_patch4_window7_224.ms_in22k created, param count: 44315083
Running inference benchmark on swin_tiny_patch4_window7_224.ms_in22k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 4045.54 samples/sec. 63.280 ms/step.
Infer [16/40]. 4045.50 samples/sec. 63.280 ms/step.
Infer [24/40]. 4045.51 samples/sec. 63.280 ms/step.
Infer [32/40]. 4045.45 samples/sec. 63.281 ms/step.
Infer [40/40]. 4045.40 samples/sec. 63.282 ms/step.
Inference benchmark of swin_tiny_patch4_window7_224.ms_in22k done. 4043.70 samples/sec, 63.28 ms/step
Model swin_tiny_patch4_window7_224.ms_in22k created, param count: 44315083
Running train benchmark on swin_tiny_patch4_window7_224.ms_in22k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1314.01 samples/sec. 194.824 ms/step.
Train [16/40]. 1314.01 samples/sec. 194.823 ms/step.
Train [24/40]. 1314.03 samples/sec. 194.821 ms/step.
Train [32/40]. 1314.03 samples/sec. 194.820 ms/step.
Train [40/40]. 1314.05 samples/sec. 194.817 ms/step.
Train benchmark of swin_tiny_patch4_window7_224.ms_in22k done. 1306.10 samples/sec, 194.82 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model swin_tiny_patch4_window7_224.ms_in22k_ft_in1k created, param count: 28288354
Running inference benchmark on swin_tiny_patch4_window7_224.ms_in22k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 4052.17 samples/sec. 63.176 ms/step.
Infer [16/40]. 4052.13 samples/sec. 63.177 ms/step.
Infer [24/40]. 4052.03 samples/sec. 63.178 ms/step.
Infer [32/40]. 4051.97 samples/sec. 63.179 ms/step.
Infer [40/40]. 4051.95 samples/sec. 63.179 ms/step.
Inference benchmark of swin_tiny_patch4_window7_224.ms_in22k_ft_in1k done. 4050.26 samples/sec, 63.18 ms/step
Model swin_tiny_patch4_window7_224.ms_in22k_ft_in1k created, param count: 28288354
Running train benchmark on swin_tiny_patch4_window7_224.ms_in22k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1319.05 samples/sec. 194.079 ms/step.
Train [16/40]. 1319.08 samples/sec. 194.074 ms/step.
Train [24/40]. 1319.07 samples/sec. 194.076 ms/step.
Train [32/40]. 1319.06 samples/sec. 194.077 ms/step.
Train [40/40]. 1319.04 samples/sec. 194.080 ms/step.
Train benchmark of swin_tiny_patch4_window7_224.ms_in22k_ft_in1k done. 1311.35 samples/sec, 194.08 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model swinv2_base_window8_256.ms_in1k created, param count: 87918816
Running inference benchmark on swinv2_base_window8_256.ms_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
Infer [8/40]. 1144.42 samples/sec. 223.695 ms/step.
Infer [16/40]. 1144.37 samples/sec. 223.703 ms/step.
Infer [24/40]. 1144.47 samples/sec. 223.685 ms/step.
Infer [32/40]. 1144.49 samples/sec. 223.680 ms/step.
Infer [40/40]. 1144.51 samples/sec. 223.676 ms/step.
Inference benchmark of swinv2_base_window8_256.ms_in1k done. 1144.32 samples/sec, 223.68 ms/step
Model swinv2_base_window8_256.ms_in1k created, param count: 87918816
Running train benchmark on swinv2_base_window8_256.ms_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 128.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 68.06 MiB is free. Including non-PyTorch memory, this process has 23.57 GiB memory in use. Of the allocated memory 22.32 GiB is allocated by PyTorch, and 20.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model swinv2_base_window8_256.ms_in1k created, param count: 87918816
Running train benchmark on swinv2_base_window8_256.ms_in1k for 40 steps w/ input size (3, 256, 256) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 24.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.36 GiB is allocated by PyTorch, and 29.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model swinv2_base_window8_256.ms_in1k created, param count: 87918816
Running train benchmark on swinv2_base_window8_256.ms_in1k for 40 steps w/ input size (3, 256, 256) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 128.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 112.06 MiB is free. Including non-PyTorch memory, this process has 23.53 GiB memory in use. Of the allocated memory 22.29 GiB is allocated by PyTorch, and 5.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model swinv2_base_window8_256.ms_in1k created, param count: 87918816
Running train benchmark on swinv2_base_window8_256.ms_in1k for 40 steps w/ input size (3, 256, 256) and batch size 96.
Train [8/40]. 370.45 samples/sec. 259.141 ms/step.
Train [16/40]. 370.45 samples/sec. 259.144 ms/step.
Train [24/40]. 370.45 samples/sec. 259.146 ms/step.
Train [32/40]. 370.44 samples/sec. 259.151 ms/step.
Train [40/40]. 370.43 samples/sec. 259.158 ms/step.
Train benchmark of swinv2_base_window8_256.ms_in1k done. 367.12 samples/sec, 259.16 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model swinv2_base_window12_192.ms_in22k created, param count: 109280841
Running inference benchmark on swinv2_base_window12_192.ms_in22k for 40 steps w/ input size (3, 192, 192) and batch size 256.
Infer [8/40]. 1753.52 samples/sec. 145.992 ms/step.
Infer [16/40]. 1753.40 samples/sec. 146.002 ms/step.
Infer [24/40]. 1753.42 samples/sec. 146.000 ms/step.
Infer [32/40]. 1753.41 samples/sec. 146.001 ms/step.
Infer [40/40]. 1753.43 samples/sec. 145.999 ms/step.
Inference benchmark of swinv2_base_window12_192.ms_in22k done. 1753.07 samples/sec, 146.00 ms/step
Model swinv2_base_window12_192.ms_in22k created, param count: 109280841
Running train benchmark on swinv2_base_window12_192.ms_in22k for 40 steps w/ input size (3, 192, 192) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 162.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 144.06 MiB is free. Including non-PyTorch memory, this process has 23.50 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 141.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model swinv2_base_window12_192.ms_in22k created, param count: 109280841
Running train benchmark on swinv2_base_window12_192.ms_in22k for 40 steps w/ input size (3, 192, 192) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 122.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 98.06 MiB is free. Including non-PyTorch memory, this process has 23.54 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 240.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model swinv2_base_window12_192.ms_in22k created, param count: 109280841
Running train benchmark on swinv2_base_window12_192.ms_in22k for 40 steps w/ input size (3, 192, 192) and batch size 128.
Train [8/40]. 543.57 samples/sec. 235.481 ms/step.
Train [16/40]. 543.58 samples/sec. 235.478 ms/step.
Train [24/40]. 543.55 samples/sec. 235.489 ms/step.
Train [32/40]. 543.55 samples/sec. 235.490 ms/step.
Train [40/40]. 543.53 samples/sec. 235.498 ms/step.
Train benchmark of swinv2_base_window12_192.ms_in22k done. 537.95 samples/sec, 235.50 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model swinv2_base_window12to16_192to256.ms_in22k_ft_in1k created, param count: 87918816
Running inference benchmark on swinv2_base_window12to16_192to256.ms_in22k_ft_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
Infer [8/40]. 782.49 samples/sec. 327.163 ms/step.
Infer [16/40]. 782.53 samples/sec. 327.142 ms/step.
Infer [24/40]. 782.57 samples/sec. 327.127 ms/step.
Infer [32/40]. 782.57 samples/sec. 327.126 ms/step.
Infer [40/40]. 782.55 samples/sec. 327.136 ms/step.
Inference benchmark of swinv2_base_window12to16_192to256.ms_in22k_ft_in1k done. 782.45 samples/sec, 327.14 ms/step
Model swinv2_base_window12to16_192to256.ms_in22k_ft_in1k created, param count: 87918816
Running train benchmark on swinv2_base_window12to16_192to256.ms_in22k_ft_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 404.06 MiB is free. Including non-PyTorch memory, this process has 23.25 GiB memory in use. Of the allocated memory 21.91 GiB is allocated by PyTorch, and 103.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model swinv2_base_window12to16_192to256.ms_in22k_ft_in1k created, param count: 87918816
Running train benchmark on swinv2_base_window12to16_192to256.ms_in22k_ft_in1k for 40 steps w/ input size (3, 256, 256) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 384.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 350.06 MiB is free. Including non-PyTorch memory, this process has 23.30 GiB memory in use. Of the allocated memory 22.00 GiB is allocated by PyTorch, and 66.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model swinv2_base_window12to16_192to256.ms_in22k_ft_in1k created, param count: 87918816
Running train benchmark on swinv2_base_window12to16_192to256.ms_in22k_ft_in1k for 40 steps w/ input size (3, 256, 256) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 206.06 MiB is free. Including non-PyTorch memory, this process has 23.44 GiB memory in use. Of the allocated memory 22.19 GiB is allocated by PyTorch, and 20.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model swinv2_base_window12to16_192to256.ms_in22k_ft_in1k created, param count: 87918816
Running train benchmark on swinv2_base_window12to16_192to256.ms_in22k_ft_in1k for 40 steps w/ input size (3, 256, 256) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 40.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 22.34 GiB is allocated by PyTorch, and 27.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model swinv2_base_window12to16_192to256.ms_in22k_ft_in1k created, param count: 87918816
Running train benchmark on swinv2_base_window12to16_192to256.ms_in22k_ft_in1k for 40 steps w/ input size (3, 256, 256) and batch size 64.
Train [8/40]. 249.63 samples/sec. 256.378 ms/step.
Train [16/40]. 249.63 samples/sec. 256.378 ms/step.
Train [24/40]. 249.64 samples/sec. 256.373 ms/step.
Train [32/40]. 249.64 samples/sec. 256.371 ms/step.
Train [40/40]. 249.64 samples/sec. 256.372 ms/step.
Train benchmark of swinv2_base_window12to16_192to256.ms_in22k_ft_in1k done. 247.27 samples/sec, 256.37 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model swinv2_base_window12to24_192to384.ms_in22k_ft_in1k created, param count: 87918816
Running inference benchmark on swinv2_base_window12to24_192to384.ms_in22k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 10.12 GiB. GPU 0 has a total capacty of 23.65 GiB of which 8.64 GiB is free. Including non-PyTorch memory, this process has 15.00 GiB memory in use. Of the allocated memory 13.40 GiB is allocated by PyTorch, and 380.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model swinv2_base_window12to24_192to384.ms_in22k_ft_in1k created, param count: 87918816
Running inference benchmark on swinv2_base_window12to24_192to384.ms_in22k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 192.
Infer [8/40]. 219.58 samples/sec. 874.405 ms/step.
Infer [16/40]. 219.57 samples/sec. 874.437 ms/step.
Infer [24/40]. 219.57 samples/sec. 874.447 ms/step.
Infer [32/40]. 219.57 samples/sec. 874.447 ms/step.
Infer [40/40]. 219.56 samples/sec. 874.458 ms/step.
Inference benchmark of swinv2_base_window12to24_192to384.ms_in22k_ft_in1k done. 219.55 samples/sec, 874.46 ms/step
Model swinv2_base_window12to24_192to384.ms_in22k_ft_in1k created, param count: 87918816
Running train benchmark on swinv2_base_window12to24_192to384.ms_in22k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 10.12 GiB. GPU 0 has a total capacty of 23.65 GiB of which 6.95 GiB is free. Including non-PyTorch memory, this process has 16.69 GiB memory in use. Of the allocated memory 15.17 GiB is allocated by PyTorch, and 290.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model swinv2_base_window12to24_192to384.ms_in22k_ft_in1k created, param count: 87918816
Running train benchmark on swinv2_base_window12to24_192to384.ms_in22k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 7.59 GiB. GPU 0 has a total capacty of 23.65 GiB of which 3.01 GiB is free. Including non-PyTorch memory, this process has 20.63 GiB memory in use. Of the allocated memory 19.04 GiB is allocated by PyTorch, and 364.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model swinv2_base_window12to24_192to384.ms_in22k_ft_in1k created, param count: 87918816
Running train benchmark on swinv2_base_window12to24_192to384.ms_in22k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 5.06 GiB. GPU 0 has a total capacty of 23.65 GiB of which 3.01 GiB is free. Including non-PyTorch memory, this process has 20.63 GiB memory in use. Of the allocated memory 19.30 GiB is allocated by PyTorch, and 100.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model swinv2_base_window12to24_192to384.ms_in22k_ft_in1k created, param count: 87918816
Running train benchmark on swinv2_base_window12to24_192to384.ms_in22k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 3.80 GiB. GPU 0 has a total capacty of 23.65 GiB of which 272.06 MiB is free. Including non-PyTorch memory, this process has 23.38 GiB memory in use. Of the allocated memory 21.72 GiB is allocated by PyTorch, and 436.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model swinv2_base_window12to24_192to384.ms_in22k_ft_in1k created, param count: 87918816
Running train benchmark on swinv2_base_window12to24_192to384.ms_in22k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 1.27 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.18 GiB is free. Including non-PyTorch memory, this process has 22.46 GiB memory in use. Of the allocated memory 21.07 GiB is allocated by PyTorch, and 164.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model swinv2_base_window12to24_192to384.ms_in22k_ft_in1k created, param count: 87918816
Running train benchmark on swinv2_base_window12to24_192to384.ms_in22k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 486.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 208.06 MiB is free. Including non-PyTorch memory, this process has 23.44 GiB memory in use. Of the allocated memory 21.84 GiB is allocated by PyTorch, and 368.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model swinv2_base_window12to24_192to384.ms_in22k_ft_in1k created, param count: 87918816
Running train benchmark on swinv2_base_window12to24_192to384.ms_in22k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 32.
ERROR: "CUDA out of memory. Tried to allocate 324.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 120.06 MiB is free. Including non-PyTorch memory, this process has 23.52 GiB memory in use. Of the allocated memory 22.23 GiB is allocated by PyTorch, and 60.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model swinv2_base_window12to24_192to384.ms_in22k_ft_in1k created, param count: 87918816
Running train benchmark on swinv2_base_window12to24_192to384.ms_in22k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 24.
ERROR: "CUDA out of memory. Tried to allocate 244.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 144.06 MiB is free. Including non-PyTorch memory, this process has 23.50 GiB memory in use. Of the allocated memory 22.21 GiB is allocated by PyTorch, and 59.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model swinv2_base_window12to24_192to384.ms_in22k_ft_in1k created, param count: 87918816
Running train benchmark on swinv2_base_window12to24_192to384.ms_in22k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 16.
Train [8/40]. 69.31 samples/sec. 230.861 ms/step.
Train [16/40]. 69.31 samples/sec. 230.850 ms/step.
Train [24/40]. 69.30 samples/sec. 230.867 ms/step.
Train [32/40]. 69.30 samples/sec. 230.873 ms/step.
Train [40/40]. 69.30 samples/sec. 230.874 ms/step.
Train benchmark of swinv2_base_window12to24_192to384.ms_in22k_ft_in1k done. 68.61 samples/sec, 230.87 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model swinv2_base_window16_256.ms_in1k created, param count: 87918816
Running inference benchmark on swinv2_base_window16_256.ms_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
Infer [8/40]. 782.30 samples/sec. 327.238 ms/step.
Infer [16/40]. 782.40 samples/sec. 327.198 ms/step.
Infer [24/40]. 782.40 samples/sec. 327.197 ms/step.
Infer [32/40]. 782.40 samples/sec. 327.197 ms/step.
Infer [40/40]. 782.42 samples/sec. 327.191 ms/step.
Inference benchmark of swinv2_base_window16_256.ms_in1k done. 782.31 samples/sec, 327.19 ms/step
Model swinv2_base_window16_256.ms_in1k created, param count: 87918816
Running train benchmark on swinv2_base_window16_256.ms_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 404.06 MiB is free. Including non-PyTorch memory, this process has 23.25 GiB memory in use. Of the allocated memory 21.91 GiB is allocated by PyTorch, and 103.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model swinv2_base_window16_256.ms_in1k created, param count: 87918816
Running train benchmark on swinv2_base_window16_256.ms_in1k for 40 steps w/ input size (3, 256, 256) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 384.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 350.06 MiB is free. Including non-PyTorch memory, this process has 23.30 GiB memory in use. Of the allocated memory 22.00 GiB is allocated by PyTorch, and 66.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model swinv2_base_window16_256.ms_in1k created, param count: 87918816
Running train benchmark on swinv2_base_window16_256.ms_in1k for 40 steps w/ input size (3, 256, 256) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 206.06 MiB is free. Including non-PyTorch memory, this process has 23.44 GiB memory in use. Of the allocated memory 22.19 GiB is allocated by PyTorch, and 20.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model swinv2_base_window16_256.ms_in1k created, param count: 87918816
Running train benchmark on swinv2_base_window16_256.ms_in1k for 40 steps w/ input size (3, 256, 256) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 40.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 22.34 GiB is allocated by PyTorch, and 27.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model swinv2_base_window16_256.ms_in1k created, param count: 87918816
Running train benchmark on swinv2_base_window16_256.ms_in1k for 40 steps w/ input size (3, 256, 256) and batch size 64.
Train [8/40]. 249.58 samples/sec. 256.427 ms/step.
Train [16/40]. 249.57 samples/sec. 256.443 ms/step.
Train [24/40]. 249.56 samples/sec. 256.452 ms/step.
Train [32/40]. 249.56 samples/sec. 256.453 ms/step.
Train [40/40]. 249.56 samples/sec. 256.449 ms/step.
Train benchmark of swinv2_base_window16_256.ms_in1k done. 247.30 samples/sec, 256.45 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model swinv2_cr_small_224.sw_in1k created, param count: 49695100
Running inference benchmark on swinv2_cr_small_224.sw_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 2225.03 samples/sec. 115.055 ms/step.
Infer [16/40]. 2225.02 samples/sec. 115.055 ms/step.
Infer [24/40]. 2224.92 samples/sec. 115.060 ms/step.
Infer [32/40]. 2224.92 samples/sec. 115.060 ms/step.
Infer [40/40]. 2224.93 samples/sec. 115.060 ms/step.
Inference benchmark of swinv2_cr_small_224.sw_in1k done. 2224.39 samples/sec, 115.06 ms/step
Model swinv2_cr_small_224.sw_in1k created, param count: 49695100
Running train benchmark on swinv2_cr_small_224.sw_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 64.06 MiB is free. Including non-PyTorch memory, this process has 23.58 GiB memory in use. Of the allocated memory 21.94 GiB is allocated by PyTorch, and 420.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model swinv2_cr_small_224.sw_in1k created, param count: 49695100
Running train benchmark on swinv2_cr_small_224.sw_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
Train [8/40]. 696.41 samples/sec. 275.700 ms/step.
Train [16/40]. 696.42 samples/sec. 275.695 ms/step.
Train [24/40]. 696.42 samples/sec. 275.697 ms/step.
Train [32/40]. 696.41 samples/sec. 275.701 ms/step.
Train [40/40]. 696.41 samples/sec. 275.698 ms/step.
Train benchmark of swinv2_cr_small_224.sw_in1k done. 690.36 samples/sec, 275.70 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model swinv2_cr_small_ns_224.sw_in1k created, param count: 49696444
Running inference benchmark on swinv2_cr_small_ns_224.sw_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 2199.40 samples/sec. 116.395 ms/step.
Infer [16/40]. 2199.37 samples/sec. 116.397 ms/step.
Infer [24/40]. 2199.37 samples/sec. 116.397 ms/step.
Infer [32/40]. 2199.37 samples/sec. 116.397 ms/step.
Infer [40/40]. 2199.36 samples/sec. 116.398 ms/step.
Inference benchmark of swinv2_cr_small_ns_224.sw_in1k done. 2198.83 samples/sec, 116.40 ms/step
Model swinv2_cr_small_ns_224.sw_in1k created, param count: 49696444
Running train benchmark on swinv2_cr_small_ns_224.sw_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 148.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 120.06 MiB is free. Including non-PyTorch memory, this process has 23.52 GiB memory in use. Of the allocated memory 21.80 GiB is allocated by PyTorch, and 504.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model swinv2_cr_small_ns_224.sw_in1k created, param count: 49696444
Running train benchmark on swinv2_cr_small_ns_224.sw_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
Train [8/40]. 685.87 samples/sec. 279.937 ms/step.
Train [16/40]. 685.83 samples/sec. 279.953 ms/step.
Train [24/40]. 685.79 samples/sec. 279.967 ms/step.
Train [32/40]. 685.80 samples/sec. 279.964 ms/step.
Train [40/40]. 685.80 samples/sec. 279.967 ms/step.
Train benchmark of swinv2_cr_small_ns_224.sw_in1k done. 679.78 samples/sec, 279.97 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model swinv2_cr_tiny_ns_224.sw_in1k created, param count: 28333468
Running inference benchmark on swinv2_cr_tiny_ns_224.sw_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3527.30 samples/sec. 72.577 ms/step.
Infer [16/40]. 3527.23 samples/sec. 72.578 ms/step.
Infer [24/40]. 3527.31 samples/sec. 72.577 ms/step.
Infer [32/40]. 3527.15 samples/sec. 72.580 ms/step.
Infer [40/40]. 3526.89 samples/sec. 72.585 ms/step.
Inference benchmark of swinv2_cr_tiny_ns_224.sw_in1k done. 3525.63 samples/sec, 72.58 ms/step
Model swinv2_cr_tiny_ns_224.sw_in1k created, param count: 28333468
Running train benchmark on swinv2_cr_tiny_ns_224.sw_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1049.01 samples/sec. 244.041 ms/step.
Train [16/40]. 1048.97 samples/sec. 244.050 ms/step.
Train [24/40]. 1049.00 samples/sec. 244.041 ms/step.
Train [32/40]. 1049.00 samples/sec. 244.043 ms/step.
Train [40/40]. 1049.01 samples/sec. 244.040 ms/step.
Train benchmark of swinv2_cr_tiny_ns_224.sw_in1k done. 1042.62 samples/sec, 244.04 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model swinv2_large_window12_192.ms_in22k created, param count: 228772549
Running inference benchmark on swinv2_large_window12_192.ms_in22k for 40 steps w/ input size (3, 192, 192) and batch size 256.
Infer [8/40]. 1048.00 samples/sec. 244.275 ms/step.
Infer [16/40]. 1048.01 samples/sec. 244.273 ms/step.
Infer [24/40]. 1048.00 samples/sec. 244.274 ms/step.
Infer [32/40]. 1048.01 samples/sec. 244.274 ms/step.
Infer [40/40]. 1048.01 samples/sec. 244.272 ms/step.
Inference benchmark of swinv2_large_window12_192.ms_in22k done. 1047.83 samples/sec, 244.27 ms/step
Model swinv2_large_window12_192.ms_in22k created, param count: 228772549
Running train benchmark on swinv2_large_window12_192.ms_in22k for 40 steps w/ input size (3, 192, 192) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 216.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 210.06 MiB is free. Including non-PyTorch memory, this process has 23.44 GiB memory in use. Of the allocated memory 22.04 GiB is allocated by PyTorch, and 165.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model swinv2_large_window12_192.ms_in22k created, param count: 228772549
Running train benchmark on swinv2_large_window12_192.ms_in22k for 40 steps w/ input size (3, 192, 192) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 162.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 26.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 310.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model swinv2_large_window12_192.ms_in22k created, param count: 228772549
Running train benchmark on swinv2_large_window12_192.ms_in22k for 40 steps w/ input size (3, 192, 192) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 122.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 48.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 22.10 GiB is allocated by PyTorch, and 269.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model swinv2_large_window12_192.ms_in22k created, param count: 228772549
Running train benchmark on swinv2_large_window12_192.ms_in22k for 40 steps w/ input size (3, 192, 192) and batch size 96.
Train [8/40]. 324.83 samples/sec. 295.541 ms/step.
Train [16/40]. 324.85 samples/sec. 295.524 ms/step.
Train [24/40]. 324.84 samples/sec. 295.529 ms/step.
Train [32/40]. 324.84 samples/sec. 295.529 ms/step.
Train [40/40]. 324.84 samples/sec. 295.530 ms/step.
Train benchmark of swinv2_large_window12_192.ms_in22k done. 322.13 samples/sec, 295.53 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model swinv2_large_window12to16_192to256.ms_in22k_ft_in1k created, param count: 196739932
Running inference benchmark on swinv2_large_window12to16_192to256.ms_in22k_ft_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
Infer [8/40]. 483.95 samples/sec. 528.986 ms/step.
Infer [16/40]. 483.89 samples/sec. 529.049 ms/step.
Infer [24/40]. 483.88 samples/sec. 529.056 ms/step.
Infer [32/40]. 483.87 samples/sec. 529.067 ms/step.
Infer [40/40]. 483.87 samples/sec. 529.072 ms/step.
Inference benchmark of swinv2_large_window12to16_192to256.ms_in22k_ft_in1k done. 483.82 samples/sec, 529.07 ms/step
Model swinv2_large_window12to16_192to256.ms_in22k_ft_in1k created, param count: 196739932
Running train benchmark on swinv2_large_window12to16_192to256.ms_in22k_ft_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 3.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.70 GiB is free. Including non-PyTorch memory, this process has 21.94 GiB memory in use. Of the allocated memory 20.48 GiB is allocated by PyTorch, and 229.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model swinv2_large_window12to16_192to256.ms_in22k_ft_in1k created, param count: 196739932
Running train benchmark on swinv2_large_window12to16_192to256.ms_in22k_ft_in1k for 40 steps w/ input size (3, 256, 256) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 20.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.38 GiB is allocated by PyTorch, and 5.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model swinv2_large_window12to16_192to256.ms_in22k_ft_in1k created, param count: 196739932
Running train benchmark on swinv2_large_window12to16_192to256.ms_in22k_ft_in1k for 40 steps w/ input size (3, 256, 256) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 384.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 158.06 MiB is free. Including non-PyTorch memory, this process has 23.49 GiB memory in use. Of the allocated memory 22.17 GiB is allocated by PyTorch, and 86.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model swinv2_large_window12to16_192to256.ms_in22k_ft_in1k created, param count: 196739932
Running train benchmark on swinv2_large_window12to16_192to256.ms_in22k_ft_in1k for 40 steps w/ input size (3, 256, 256) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 36.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 26.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.36 GiB is allocated by PyTorch, and 25.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model swinv2_large_window12to16_192to256.ms_in22k_ft_in1k created, param count: 196739932
Running train benchmark on swinv2_large_window12to16_192to256.ms_in22k_ft_in1k for 40 steps w/ input size (3, 256, 256) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 2.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 22.37 GiB is allocated by PyTorch, and 34.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model swinv2_large_window12to16_192to256.ms_in22k_ft_in1k created, param count: 196739932
Running train benchmark on swinv2_large_window12to16_192to256.ms_in22k_ft_in1k for 40 steps w/ input size (3, 256, 256) and batch size 48.
Train [8/40]. 150.98 samples/sec. 317.920 ms/step.
Train [16/40]. 150.98 samples/sec. 317.918 ms/step.
Train [24/40]. 150.98 samples/sec. 317.916 ms/step.
Train [32/40]. 150.98 samples/sec. 317.925 ms/step.
Train [40/40]. 150.98 samples/sec. 317.927 ms/step.
Train benchmark of swinv2_large_window12to16_192to256.ms_in22k_ft_in1k done. 149.82 samples/sec, 317.93 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model swinv2_large_window12to24_192to384.ms_in22k_ft_in1k created, param count: 196739932
Running inference benchmark on swinv2_large_window12to24_192to384.ms_in22k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 15.19 GiB. GPU 0 has a total capacty of 23.65 GiB of which 13.99 GiB is free. Including non-PyTorch memory, this process has 9.65 GiB memory in use. Of the allocated memory 8.25 GiB is allocated by PyTorch, and 165.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model swinv2_large_window12to24_192to384.ms_in22k_ft_in1k created, param count: 196739932
Running inference benchmark on swinv2_large_window12to24_192to384.ms_in22k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 11.39 GiB. GPU 0 has a total capacty of 23.65 GiB of which 6.74 GiB is free. Including non-PyTorch memory, this process has 16.90 GiB memory in use. Of the allocated memory 15.17 GiB is allocated by PyTorch, and 516.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model swinv2_large_window12to24_192to384.ms_in22k_ft_in1k created, param count: 196739932
Running inference benchmark on swinv2_large_window12to24_192to384.ms_in22k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 128.
Infer [8/40]. 139.46 samples/sec. 917.849 ms/step.
Infer [16/40]. 139.45 samples/sec. 917.859 ms/step.
Infer [24/40]. 139.45 samples/sec. 917.860 ms/step.
Infer [32/40]. 139.46 samples/sec. 917.858 ms/step.
Infer [40/40]. 139.45 samples/sec. 917.881 ms/step.
Inference benchmark of swinv2_large_window12to24_192to384.ms_in22k_ft_in1k done. 139.44 samples/sec, 917.88 ms/step
Model swinv2_large_window12to24_192to384.ms_in22k_ft_in1k created, param count: 196739932
Running train benchmark on swinv2_large_window12to24_192to384.ms_in22k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 15.19 GiB. GPU 0 has a total capacty of 23.65 GiB of which 13.15 GiB is free. Including non-PyTorch memory, this process has 10.49 GiB memory in use. Of the allocated memory 9.22 GiB is allocated by PyTorch, and 39.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model swinv2_large_window12to24_192to384.ms_in22k_ft_in1k created, param count: 196739932
Running train benchmark on swinv2_large_window12to24_192to384.ms_in22k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 11.39 GiB. GPU 0 has a total capacty of 23.65 GiB of which 4.84 GiB is free. Including non-PyTorch memory, this process has 18.80 GiB memory in use. Of the allocated memory 17.16 GiB is allocated by PyTorch, and 421.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model swinv2_large_window12to24_192to384.ms_in22k_ft_in1k created, param count: 196739932
Running train benchmark on swinv2_large_window12to24_192to384.ms_in22k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 7.59 GiB. GPU 0 has a total capacty of 23.65 GiB of which 2.92 GiB is free. Including non-PyTorch memory, this process has 20.72 GiB memory in use. Of the allocated memory 19.19 GiB is allocated by PyTorch, and 303.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model swinv2_large_window12to24_192to384.ms_in22k_ft_in1k created, param count: 196739932
Running train benchmark on swinv2_large_window12to24_192to384.ms_in22k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 5.70 GiB. GPU 0 has a total capacty of 23.65 GiB of which 536.06 MiB is free. Including non-PyTorch memory, this process has 23.12 GiB memory in use. Of the allocated memory 21.84 GiB is allocated by PyTorch, and 48.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model swinv2_large_window12to24_192to384.ms_in22k_ft_in1k created, param count: 196739932
Running train benchmark on swinv2_large_window12to24_192to384.ms_in22k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 3.80 GiB. GPU 0 has a total capacty of 23.65 GiB of which 160.06 MiB is free. Including non-PyTorch memory, this process has 23.48 GiB memory in use. Of the allocated memory 21.89 GiB is allocated by PyTorch, and 367.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model swinv2_large_window12to24_192to384.ms_in22k_ft_in1k created, param count: 196739932
Running train benchmark on swinv2_large_window12to24_192to384.ms_in22k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 1.42 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.20 GiB is free. Including non-PyTorch memory, this process has 22.44 GiB memory in use. Of the allocated memory 20.66 GiB is allocated by PyTorch, and 558.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model swinv2_large_window12to24_192to384.ms_in22k_ft_in1k created, param count: 196739932
Running train benchmark on swinv2_large_window12to24_192to384.ms_in22k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 32.
ERROR: "CUDA out of memory. Tried to allocate 486.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 272.06 MiB is free. Including non-PyTorch memory, this process has 23.38 GiB memory in use. Of the allocated memory 22.05 GiB is allocated by PyTorch, and 95.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model swinv2_large_window12to24_192to384.ms_in22k_ft_in1k created, param count: 196739932
Running train benchmark on swinv2_large_window12to24_192to384.ms_in22k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 24.
ERROR: "CUDA out of memory. Tried to allocate 366.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 58.06 MiB is free. Including non-PyTorch memory, this process has 23.58 GiB memory in use. Of the allocated memory 22.22 GiB is allocated by PyTorch, and 139.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model swinv2_large_window12to24_192to384.ms_in22k_ft_in1k created, param count: 196739932
Running train benchmark on swinv2_large_window12to24_192to384.ms_in22k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 16.
ERROR: "CUDA out of memory. Tried to allocate 244.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 112.06 MiB is free. Including non-PyTorch memory, this process has 23.53 GiB memory in use. Of the allocated memory 22.02 GiB is allocated by PyTorch, and 282.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 12 for retry.
Model swinv2_large_window12to24_192to384.ms_in22k_ft_in1k created, param count: 196739932
Running train benchmark on swinv2_large_window12to24_192to384.ms_in22k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 12.
Train [8/40]. 43.49 samples/sec. 275.955 ms/step.
Train [16/40]. 43.47 samples/sec. 276.022 ms/step.
Train [24/40]. 43.47 samples/sec. 276.048 ms/step.
Train [32/40]. 43.47 samples/sec. 276.056 ms/step.
Train [40/40]. 43.47 samples/sec. 276.059 ms/step.
Train benchmark of swinv2_large_window12to24_192to384.ms_in22k_ft_in1k done. 43.09 samples/sec, 276.06 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model swinv2_small_window8_256.ms_in1k created, param count: 49728418
Running inference benchmark on swinv2_small_window8_256.ms_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
Infer [8/40]. 1597.25 samples/sec. 160.275 ms/step.
Infer [16/40]. 1597.18 samples/sec. 160.283 ms/step.
Infer [24/40]. 1597.20 samples/sec. 160.280 ms/step.
Infer [32/40]. 1597.27 samples/sec. 160.274 ms/step.
Infer [40/40]. 1597.30 samples/sec. 160.270 ms/step.
Inference benchmark of swinv2_small_window8_256.ms_in1k done. 1596.98 samples/sec, 160.27 ms/step
Model swinv2_small_window8_256.ms_in1k created, param count: 49728418
Running train benchmark on swinv2_small_window8_256.ms_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 68.06 MiB is free. Including non-PyTorch memory, this process has 23.57 GiB memory in use. Of the allocated memory 22.32 GiB is allocated by PyTorch, and 21.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model swinv2_small_window8_256.ms_in1k created, param count: 49728418
Running train benchmark on swinv2_small_window8_256.ms_in1k for 40 steps w/ input size (3, 256, 256) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 72.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 42.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 22.36 GiB is allocated by PyTorch, and 2.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model swinv2_small_window8_256.ms_in1k created, param count: 49728418
Running train benchmark on swinv2_small_window8_256.ms_in1k for 40 steps w/ input size (3, 256, 256) and batch size 128.
Train [8/40]. 511.36 samples/sec. 250.313 ms/step.
Train [16/40]. 511.33 samples/sec. 250.326 ms/step.
Train [24/40]. 511.36 samples/sec. 250.313 ms/step.
Train [32/40]. 511.36 samples/sec. 250.313 ms/step.
Train [40/40]. 511.37 samples/sec. 250.310 ms/step.
Train benchmark of swinv2_small_window8_256.ms_in1k done. 506.39 samples/sec, 250.31 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model swinv2_small_window16_256.ms_in1k created, param count: 49728418
Running inference benchmark on swinv2_small_window16_256.ms_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
Infer [8/40]. 1076.72 samples/sec. 237.759 ms/step.
Infer [16/40]. 1076.74 samples/sec. 237.754 ms/step.
Infer [24/40]. 1076.75 samples/sec. 237.753 ms/step.
Infer [32/40]. 1076.75 samples/sec. 237.752 ms/step.
Infer [40/40]. 1076.76 samples/sec. 237.751 ms/step.
Inference benchmark of swinv2_small_window16_256.ms_in1k done. 1076.58 samples/sec, 237.75 ms/step
Model swinv2_small_window16_256.ms_in1k created, param count: 49728418
Running train benchmark on swinv2_small_window16_256.ms_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 12.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.34 GiB is allocated by PyTorch, and 60.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model swinv2_small_window16_256.ms_in1k created, param count: 49728418
Running train benchmark on swinv2_small_window16_256.ms_in1k for 40 steps w/ input size (3, 256, 256) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 288.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 198.06 MiB is free. Including non-PyTorch memory, this process has 23.45 GiB memory in use. Of the allocated memory 22.20 GiB is allocated by PyTorch, and 15.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model swinv2_small_window16_256.ms_in1k created, param count: 49728418
Running train benchmark on swinv2_small_window16_256.ms_in1k for 40 steps w/ input size (3, 256, 256) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 100.06 MiB is free. Including non-PyTorch memory, this process has 23.54 GiB memory in use. Of the allocated memory 22.28 GiB is allocated by PyTorch, and 28.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model swinv2_small_window16_256.ms_in1k created, param count: 49728418
Running train benchmark on swinv2_small_window16_256.ms_in1k for 40 steps w/ input size (3, 256, 256) and batch size 96.
Train [8/40]. 336.96 samples/sec. 284.901 ms/step.
Train [16/40]. 336.95 samples/sec. 284.907 ms/step.
Train [24/40]. 336.93 samples/sec. 284.923 ms/step.
Train [32/40]. 336.94 samples/sec. 284.919 ms/step.
Train [40/40]. 336.94 samples/sec. 284.916 ms/step.
Train benchmark of swinv2_small_window16_256.ms_in1k done. 334.11 samples/sec, 284.92 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model swinv2_tiny_window8_256.ms_in1k created, param count: 28347154
Running inference benchmark on swinv2_tiny_window8_256.ms_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
Infer [8/40]. 2605.72 samples/sec. 98.245 ms/step.
Infer [16/40]. 2605.80 samples/sec. 98.242 ms/step.
Infer [24/40]. 2605.77 samples/sec. 98.243 ms/step.
Infer [32/40]. 2605.78 samples/sec. 98.243 ms/step.
Infer [40/40]. 2605.80 samples/sec. 98.243 ms/step.
Inference benchmark of swinv2_tiny_window8_256.ms_in1k done. 2605.09 samples/sec, 98.24 ms/step
Model swinv2_tiny_window8_256.ms_in1k created, param count: 28347154
Running train benchmark on swinv2_tiny_window8_256.ms_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
Train [8/40]. 772.20 samples/sec. 331.520 ms/step.
Train [16/40]. 772.21 samples/sec. 331.516 ms/step.
Train [24/40]. 772.23 samples/sec. 331.508 ms/step.
Train [32/40]. 772.23 samples/sec. 331.508 ms/step.
Train [40/40]. 772.23 samples/sec. 331.508 ms/step.
Train benchmark of swinv2_tiny_window8_256.ms_in1k done. 768.66 samples/sec, 331.51 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model swinv2_tiny_window16_256.ms_in1k created, param count: 28347154
Running inference benchmark on swinv2_tiny_window16_256.ms_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
Infer [8/40]. 1716.23 samples/sec. 149.164 ms/step.
Infer [16/40]. 1716.28 samples/sec. 149.160 ms/step.
Infer [24/40]. 1716.33 samples/sec. 149.156 ms/step.
Infer [32/40]. 1716.36 samples/sec. 149.153 ms/step.
Infer [40/40]. 1716.32 samples/sec. 149.156 ms/step.
Inference benchmark of swinv2_tiny_window16_256.ms_in1k done. 1715.95 samples/sec, 149.16 ms/step
Model swinv2_tiny_window16_256.ms_in1k created, param count: 28347154
Running train benchmark on swinv2_tiny_window16_256.ms_in1k for 40 steps w/ input size (3, 256, 256) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 4.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 22.29 GiB is allocated by PyTorch, and 115.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model swinv2_tiny_window16_256.ms_in1k created, param count: 28347154
Running train benchmark on swinv2_tiny_window16_256.ms_in1k for 40 steps w/ input size (3, 256, 256) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 288.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 256.06 MiB is free. Including non-PyTorch memory, this process has 23.39 GiB memory in use. Of the allocated memory 22.15 GiB is allocated by PyTorch, and 4.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model swinv2_tiny_window16_256.ms_in1k created, param count: 28347154
Running train benchmark on swinv2_tiny_window16_256.ms_in1k for 40 steps w/ input size (3, 256, 256) and batch size 128.
Train [8/40]. 536.18 samples/sec. 238.724 ms/step.
Train [16/40]. 536.18 samples/sec. 238.726 ms/step.
Train [24/40]. 536.17 samples/sec. 238.731 ms/step.
Train [32/40]. 536.17 samples/sec. 238.732 ms/step.
Train [40/40]. 536.15 samples/sec. 238.737 ms/step.
Train benchmark of swinv2_tiny_window16_256.ms_in1k done. 533.07 samples/sec, 238.74 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tf_efficientnet_b0.aa_in1k created, param count: 5288548
Running inference benchmark on tf_efficientnet_b0.aa_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 8348.69 samples/sec. 30.663 ms/step.
Infer [16/40]. 8348.37 samples/sec. 30.665 ms/step.
Infer [24/40]. 8348.35 samples/sec. 30.665 ms/step.
Infer [32/40]. 8348.21 samples/sec. 30.665 ms/step.
Infer [40/40]. 8346.96 samples/sec. 30.670 ms/step.
Inference benchmark of tf_efficientnet_b0.aa_in1k done. 8340.35 samples/sec, 30.67 ms/step
Model tf_efficientnet_b0.aa_in1k created, param count: 5288548
Running train benchmark on tf_efficientnet_b0.aa_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1870.87 samples/sec. 136.835 ms/step.
Train [16/40]. 1870.78 samples/sec. 136.842 ms/step.
Train [24/40]. 1870.89 samples/sec. 136.833 ms/step.
Train [32/40]. 1870.84 samples/sec. 136.837 ms/step.
Train [40/40]. 1870.86 samples/sec. 136.836 ms/step.
Train benchmark of tf_efficientnet_b0.aa_in1k done. 1859.38 samples/sec, 136.84 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tf_efficientnet_b0.ap_in1k created, param count: 5288548
Running inference benchmark on tf_efficientnet_b0.ap_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 8348.73 samples/sec. 30.663 ms/step.
Infer [16/40]. 8347.57 samples/sec. 30.668 ms/step.
Infer [24/40]. 8345.12 samples/sec. 30.677 ms/step.
Infer [32/40]. 8343.77 samples/sec. 30.682 ms/step.
Infer [40/40]. 8342.92 samples/sec. 30.685 ms/step.
Inference benchmark of tf_efficientnet_b0.ap_in1k done. 8336.42 samples/sec, 30.68 ms/step
Model tf_efficientnet_b0.ap_in1k created, param count: 5288548
Running train benchmark on tf_efficientnet_b0.ap_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1870.85 samples/sec. 136.836 ms/step.
Train [16/40]. 1870.73 samples/sec. 136.845 ms/step.
Train [24/40]. 1870.70 samples/sec. 136.847 ms/step.
Train [32/40]. 1870.72 samples/sec. 136.846 ms/step.
Train [40/40]. 1870.63 samples/sec. 136.852 ms/step.
Train benchmark of tf_efficientnet_b0.ap_in1k done. 1859.28 samples/sec, 136.85 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tf_efficientnet_b0.in1k created, param count: 5288548
Running inference benchmark on tf_efficientnet_b0.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 8349.01 samples/sec. 30.662 ms/step.
Infer [16/40]. 8348.96 samples/sec. 30.663 ms/step.
Infer [24/40]. 8348.45 samples/sec. 30.664 ms/step.
Infer [32/40]. 8348.21 samples/sec. 30.665 ms/step.
Infer [40/40]. 8348.35 samples/sec. 30.665 ms/step.
Inference benchmark of tf_efficientnet_b0.in1k done. 8341.74 samples/sec, 30.66 ms/step
Model tf_efficientnet_b0.in1k created, param count: 5288548
Running train benchmark on tf_efficientnet_b0.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1873.93 samples/sec. 136.611 ms/step.
Train [16/40]. 1873.77 samples/sec. 136.623 ms/step.
Train [24/40]. 1873.82 samples/sec. 136.619 ms/step.
Train [32/40]. 1873.77 samples/sec. 136.623 ms/step.
Train [40/40]. 1873.78 samples/sec. 136.622 ms/step.
Train benchmark of tf_efficientnet_b0.in1k done. 1862.34 samples/sec, 136.62 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tf_efficientnet_b0.ns_jft_in1k created, param count: 5288548
Running inference benchmark on tf_efficientnet_b0.ns_jft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 8346.31 samples/sec. 30.672 ms/step.
Infer [16/40]. 8346.39 samples/sec. 30.672 ms/step.
Infer [24/40]. 8346.49 samples/sec. 30.672 ms/step.
Infer [32/40]. 8346.76 samples/sec. 30.671 ms/step.
Infer [40/40]. 8346.45 samples/sec. 30.672 ms/step.
Inference benchmark of tf_efficientnet_b0.ns_jft_in1k done. 8339.83 samples/sec, 30.67 ms/step
Model tf_efficientnet_b0.ns_jft_in1k created, param count: 5288548
Running train benchmark on tf_efficientnet_b0.ns_jft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1872.45 samples/sec. 136.720 ms/step.
Train [16/40]. 1872.45 samples/sec. 136.719 ms/step.
Train [24/40]. 1872.36 samples/sec. 136.726 ms/step.
Train [32/40]. 1872.36 samples/sec. 136.726 ms/step.
Train [40/40]. 1872.41 samples/sec. 136.723 ms/step.
Train benchmark of tf_efficientnet_b0.ns_jft_in1k done. 1860.63 samples/sec, 136.72 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tf_efficientnet_b1.aa_in1k created, param count: 7794184
Running inference benchmark on tf_efficientnet_b1.aa_in1k for 40 steps w/ input size (3, 240, 240) and batch size 256.
Infer [8/40]. 5309.54 samples/sec. 48.215 ms/step.
Infer [16/40]. 5310.16 samples/sec. 48.209 ms/step.
Infer [24/40]. 5310.20 samples/sec. 48.209 ms/step.
Infer [32/40]. 5310.17 samples/sec. 48.209 ms/step.
Infer [40/40]. 5309.30 samples/sec. 48.217 ms/step.
Inference benchmark of tf_efficientnet_b1.aa_in1k done. 5306.48 samples/sec, 48.22 ms/step
Model tf_efficientnet_b1.aa_in1k created, param count: 7794184
Running train benchmark on tf_efficientnet_b1.aa_in1k for 40 steps w/ input size (3, 240, 240) and batch size 256.
Train [8/40]. 1139.89 samples/sec. 224.582 ms/step.
Train [16/40]. 1139.80 samples/sec. 224.602 ms/step.
Train [24/40]. 1139.81 samples/sec. 224.599 ms/step.
Train [32/40]. 1139.83 samples/sec. 224.594 ms/step.
Train [40/40]. 1139.85 samples/sec. 224.592 ms/step.
Train benchmark of tf_efficientnet_b1.aa_in1k done. 1133.15 samples/sec, 224.59 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tf_efficientnet_b1.ap_in1k created, param count: 7794184
Running inference benchmark on tf_efficientnet_b1.ap_in1k for 40 steps w/ input size (3, 240, 240) and batch size 256.
Infer [8/40]. 5311.42 samples/sec. 48.198 ms/step.
Infer [16/40]. 5311.34 samples/sec. 48.199 ms/step.
Infer [24/40]. 5311.12 samples/sec. 48.201 ms/step.
Infer [32/40]. 5311.02 samples/sec. 48.202 ms/step.
Infer [40/40]. 5311.04 samples/sec. 48.201 ms/step.
Inference benchmark of tf_efficientnet_b1.ap_in1k done. 5308.23 samples/sec, 48.20 ms/step
Model tf_efficientnet_b1.ap_in1k created, param count: 7794184
Running train benchmark on tf_efficientnet_b1.ap_in1k for 40 steps w/ input size (3, 240, 240) and batch size 256.
Train [8/40]. 1139.66 samples/sec. 224.629 ms/step.
Train [16/40]. 1139.70 samples/sec. 224.621 ms/step.
Train [24/40]. 1139.68 samples/sec. 224.625 ms/step.
Train [32/40]. 1139.67 samples/sec. 224.626 ms/step.
Train [40/40]. 1139.67 samples/sec. 224.627 ms/step.
Train benchmark of tf_efficientnet_b1.ap_in1k done. 1133.07 samples/sec, 224.63 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tf_efficientnet_b1.in1k created, param count: 7794184
Running inference benchmark on tf_efficientnet_b1.in1k for 40 steps w/ input size (3, 240, 240) and batch size 256.
Infer [8/40]. 5311.86 samples/sec. 48.194 ms/step.
Infer [16/40]. 5311.55 samples/sec. 48.197 ms/step.
Infer [24/40]. 5311.24 samples/sec. 48.200 ms/step.
Infer [32/40]. 5311.22 samples/sec. 48.200 ms/step.
Infer [40/40]. 5311.15 samples/sec. 48.200 ms/step.
Inference benchmark of tf_efficientnet_b1.in1k done. 5308.33 samples/sec, 48.20 ms/step
Model tf_efficientnet_b1.in1k created, param count: 7794184
Running train benchmark on tf_efficientnet_b1.in1k for 40 steps w/ input size (3, 240, 240) and batch size 256.
Train [8/40]. 1139.77 samples/sec. 224.607 ms/step.
Train [16/40]. 1139.74 samples/sec. 224.612 ms/step.
Train [24/40]. 1139.70 samples/sec. 224.621 ms/step.
Train [32/40]. 1139.70 samples/sec. 224.620 ms/step.
Train [40/40]. 1139.69 samples/sec. 224.623 ms/step.
Train benchmark of tf_efficientnet_b1.in1k done. 1133.23 samples/sec, 224.62 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tf_efficientnet_b1.ns_jft_in1k created, param count: 7794184
Running inference benchmark on tf_efficientnet_b1.ns_jft_in1k for 40 steps w/ input size (3, 240, 240) and batch size 256.
Infer [8/40]. 5314.58 samples/sec. 48.169 ms/step.
Infer [16/40]. 5314.79 samples/sec. 48.167 ms/step.
Infer [24/40]. 5314.74 samples/sec. 48.168 ms/step.
Infer [32/40]. 5314.78 samples/sec. 48.168 ms/step.
Infer [40/40]. 5314.75 samples/sec. 48.168 ms/step.
Inference benchmark of tf_efficientnet_b1.ns_jft_in1k done. 5311.97 samples/sec, 48.17 ms/step
Model tf_efficientnet_b1.ns_jft_in1k created, param count: 7794184
Running train benchmark on tf_efficientnet_b1.ns_jft_in1k for 40 steps w/ input size (3, 240, 240) and batch size 256.
Train [8/40]. 1140.65 samples/sec. 224.433 ms/step.
Train [16/40]. 1140.69 samples/sec. 224.426 ms/step.
Train [24/40]. 1140.68 samples/sec. 224.428 ms/step.
Train [32/40]. 1140.66 samples/sec. 224.430 ms/step.
Train [40/40]. 1140.68 samples/sec. 224.428 ms/step.
Train benchmark of tf_efficientnet_b1.ns_jft_in1k done. 1134.35 samples/sec, 224.43 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tf_efficientnet_b2.aa_in1k created, param count: 9109994
Running inference benchmark on tf_efficientnet_b2.aa_in1k for 40 steps w/ input size (3, 260, 260) and batch size 256.
Infer [8/40]. 3970.68 samples/sec. 64.473 ms/step.
Infer [16/40]. 3970.63 samples/sec. 64.473 ms/step.
Infer [24/40]. 3970.63 samples/sec. 64.473 ms/step.
Infer [32/40]. 3970.60 samples/sec. 64.474 ms/step.
Infer [40/40]. 3970.62 samples/sec. 64.474 ms/step.
Inference benchmark of tf_efficientnet_b2.aa_in1k done. 3968.97 samples/sec, 64.47 ms/step
Model tf_efficientnet_b2.aa_in1k created, param count: 9109994
Running train benchmark on tf_efficientnet_b2.aa_in1k for 40 steps w/ input size (3, 260, 260) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 84.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 82.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 21.60 GiB is allocated by PyTorch, and 745.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model tf_efficientnet_b2.aa_in1k created, param count: 9109994
Running train benchmark on tf_efficientnet_b2.aa_in1k for 40 steps w/ input size (3, 260, 260) and batch size 192.
Train [8/40]. 878.68 samples/sec. 218.509 ms/step.
Train [16/40]. 878.57 samples/sec. 218.538 ms/step.
Train [24/40]. 878.55 samples/sec. 218.541 ms/step.
Train [32/40]. 878.54 samples/sec. 218.544 ms/step.
Train [40/40]. 878.52 samples/sec. 218.549 ms/step.
Train benchmark of tf_efficientnet_b2.aa_in1k done. 873.37 samples/sec, 218.55 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tf_efficientnet_b2.ap_in1k created, param count: 9109994
Running inference benchmark on tf_efficientnet_b2.ap_in1k for 40 steps w/ input size (3, 260, 260) and batch size 256.
Infer [8/40]. 3970.90 samples/sec. 64.469 ms/step.
Infer [16/40]. 3970.94 samples/sec. 64.468 ms/step.
Infer [24/40]. 3970.86 samples/sec. 64.470 ms/step.
Infer [32/40]. 3970.77 samples/sec. 64.471 ms/step.
Infer [40/40]. 3970.80 samples/sec. 64.471 ms/step.
Inference benchmark of tf_efficientnet_b2.ap_in1k done. 3969.25 samples/sec, 64.47 ms/step
Model tf_efficientnet_b2.ap_in1k created, param count: 9109994
Running train benchmark on tf_efficientnet_b2.ap_in1k for 40 steps w/ input size (3, 260, 260) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 84.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 60.06 MiB is free. Including non-PyTorch memory, this process has 23.58 GiB memory in use. Of the allocated memory 21.68 GiB is allocated by PyTorch, and 681.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model tf_efficientnet_b2.ap_in1k created, param count: 9109994
Running train benchmark on tf_efficientnet_b2.ap_in1k for 40 steps w/ input size (3, 260, 260) and batch size 192.
Train [8/40]. 878.85 samples/sec. 218.467 ms/step.
Train [16/40]. 878.87 samples/sec. 218.462 ms/step.
Train [24/40]. 878.82 samples/sec. 218.475 ms/step.
Train [32/40]. 878.80 samples/sec. 218.481 ms/step.
Train [40/40]. 878.82 samples/sec. 218.476 ms/step.
Train benchmark of tf_efficientnet_b2.ap_in1k done. 873.62 samples/sec, 218.48 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tf_efficientnet_b2.in1k created, param count: 9109994
Running inference benchmark on tf_efficientnet_b2.in1k for 40 steps w/ input size (3, 260, 260) and batch size 256.
Infer [8/40]. 3970.63 samples/sec. 64.473 ms/step.
Infer [16/40]. 3970.64 samples/sec. 64.473 ms/step.
Infer [24/40]. 3970.66 samples/sec. 64.473 ms/step.
Infer [32/40]. 3970.65 samples/sec. 64.473 ms/step.
Infer [40/40]. 3970.64 samples/sec. 64.473 ms/step.
Inference benchmark of tf_efficientnet_b2.in1k done. 3969.06 samples/sec, 64.47 ms/step
Model tf_efficientnet_b2.in1k created, param count: 9109994
Running train benchmark on tf_efficientnet_b2.in1k for 40 steps w/ input size (3, 260, 260) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 84.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 60.06 MiB is free. Including non-PyTorch memory, this process has 23.58 GiB memory in use. Of the allocated memory 21.68 GiB is allocated by PyTorch, and 681.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model tf_efficientnet_b2.in1k created, param count: 9109994
Running train benchmark on tf_efficientnet_b2.in1k for 40 steps w/ input size (3, 260, 260) and batch size 192.
Train [8/40]. 878.04 samples/sec. 218.670 ms/step.
Train [16/40]. 878.06 samples/sec. 218.664 ms/step.
Train [24/40]. 878.08 samples/sec. 218.658 ms/step.
Train [32/40]. 878.08 samples/sec. 218.658 ms/step.
Train [40/40]. 878.08 samples/sec. 218.658 ms/step.
Train benchmark of tf_efficientnet_b2.in1k done. 872.99 samples/sec, 218.66 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tf_efficientnet_b2.ns_jft_in1k created, param count: 9109994
Running inference benchmark on tf_efficientnet_b2.ns_jft_in1k for 40 steps w/ input size (3, 260, 260) and batch size 256.
Infer [8/40]. 3968.99 samples/sec. 64.500 ms/step.
Infer [16/40]. 3969.00 samples/sec. 64.500 ms/step.
Infer [24/40]. 3968.86 samples/sec. 64.502 ms/step.
Infer [32/40]. 3968.81 samples/sec. 64.503 ms/step.
Infer [40/40]. 3968.73 samples/sec. 64.504 ms/step.
Inference benchmark of tf_efficientnet_b2.ns_jft_in1k done. 3967.17 samples/sec, 64.50 ms/step
Model tf_efficientnet_b2.ns_jft_in1k created, param count: 9109994
Running train benchmark on tf_efficientnet_b2.ns_jft_in1k for 40 steps w/ input size (3, 260, 260) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 84.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 60.06 MiB is free. Including non-PyTorch memory, this process has 23.58 GiB memory in use. Of the allocated memory 21.68 GiB is allocated by PyTorch, and 681.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model tf_efficientnet_b2.ns_jft_in1k created, param count: 9109994
Running train benchmark on tf_efficientnet_b2.ns_jft_in1k for 40 steps w/ input size (3, 260, 260) and batch size 192.
Train [8/40]. 878.10 samples/sec. 218.655 ms/step.
Train [16/40]. 878.08 samples/sec. 218.659 ms/step.
Train [24/40]. 878.05 samples/sec. 218.667 ms/step.
Train [32/40]. 878.05 samples/sec. 218.667 ms/step.
Train [40/40]. 878.05 samples/sec. 218.666 ms/step.
Train benchmark of tf_efficientnet_b2.ns_jft_in1k done. 872.99 samples/sec, 218.67 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tf_efficientnet_b3.aa_in1k created, param count: 12233232
Running inference benchmark on tf_efficientnet_b3.aa_in1k for 40 steps w/ input size (3, 300, 300) and batch size 256.
Infer [8/40]. 2284.48 samples/sec. 112.061 ms/step.
Infer [16/40]. 2284.60 samples/sec. 112.055 ms/step.
Infer [24/40]. 2284.56 samples/sec. 112.057 ms/step.
Infer [32/40]. 2284.57 samples/sec. 112.056 ms/step.
Infer [40/40]. 2284.59 samples/sec. 112.055 ms/step.
Inference benchmark of tf_efficientnet_b3.aa_in1k done. 2284.03 samples/sec, 112.06 ms/step
Model tf_efficientnet_b3.aa_in1k created, param count: 12233232
Running train benchmark on tf_efficientnet_b3.aa_in1k for 40 steps w/ input size (3, 300, 300) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 204.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 16.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.10 GiB is allocated by PyTorch, and 299.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model tf_efficientnet_b3.aa_in1k created, param count: 12233232
Running train benchmark on tf_efficientnet_b3.aa_in1k for 40 steps w/ input size (3, 300, 300) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 108.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 12.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.21 GiB is allocated by PyTorch, and 189.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model tf_efficientnet_b3.aa_in1k created, param count: 12233232
Running train benchmark on tf_efficientnet_b3.aa_in1k for 40 steps w/ input size (3, 300, 300) and batch size 128.
Train [8/40]. 556.73 samples/sec. 229.913 ms/step.
Train [16/40]. 556.70 samples/sec. 229.927 ms/step.
Train [24/40]. 556.72 samples/sec. 229.916 ms/step.
Train [32/40]. 556.73 samples/sec. 229.914 ms/step.
Train [40/40]. 556.73 samples/sec. 229.913 ms/step.
Train benchmark of tf_efficientnet_b3.aa_in1k done. 553.39 samples/sec, 229.91 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tf_efficientnet_b3.ap_in1k created, param count: 12233232
Running inference benchmark on tf_efficientnet_b3.ap_in1k for 40 steps w/ input size (3, 300, 300) and batch size 256.
Infer [8/40]. 2285.98 samples/sec. 111.987 ms/step.
Infer [16/40]. 2285.92 samples/sec. 111.990 ms/step.
Infer [24/40]. 2285.86 samples/sec. 111.993 ms/step.
Infer [32/40]. 2285.88 samples/sec. 111.992 ms/step.
Infer [40/40]. 2285.87 samples/sec. 111.993 ms/step.
Inference benchmark of tf_efficientnet_b3.ap_in1k done. 2285.30 samples/sec, 111.99 ms/step
Model tf_efficientnet_b3.ap_in1k created, param count: 12233232
Running train benchmark on tf_efficientnet_b3.ap_in1k for 40 steps w/ input size (3, 300, 300) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 204.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 16.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.10 GiB is allocated by PyTorch, and 299.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model tf_efficientnet_b3.ap_in1k created, param count: 12233232
Running train benchmark on tf_efficientnet_b3.ap_in1k for 40 steps w/ input size (3, 300, 300) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 108.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 8.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.21 GiB is allocated by PyTorch, and 193.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model tf_efficientnet_b3.ap_in1k created, param count: 12233232
Running train benchmark on tf_efficientnet_b3.ap_in1k for 40 steps w/ input size (3, 300, 300) and batch size 128.
Train [8/40]. 556.32 samples/sec. 230.083 ms/step.
Train [16/40]. 556.31 samples/sec. 230.087 ms/step.
Train [24/40]. 556.34 samples/sec. 230.077 ms/step.
Train [32/40]. 556.36 samples/sec. 230.067 ms/step.
Train [40/40]. 556.35 samples/sec. 230.071 ms/step.
Train benchmark of tf_efficientnet_b3.ap_in1k done. 552.98 samples/sec, 230.07 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tf_efficientnet_b3.in1k created, param count: 12233232
Running inference benchmark on tf_efficientnet_b3.in1k for 40 steps w/ input size (3, 300, 300) and batch size 256.
Infer [8/40]. 2285.04 samples/sec. 112.033 ms/step.
Infer [16/40]. 2285.02 samples/sec. 112.034 ms/step.
Infer [24/40]. 2285.00 samples/sec. 112.035 ms/step.
Infer [32/40]. 2284.99 samples/sec. 112.036 ms/step.
Infer [40/40]. 2284.96 samples/sec. 112.037 ms/step.
Inference benchmark of tf_efficientnet_b3.in1k done. 2284.39 samples/sec, 112.04 ms/step
Model tf_efficientnet_b3.in1k created, param count: 12233232
Running train benchmark on tf_efficientnet_b3.in1k for 40 steps w/ input size (3, 300, 300) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 204.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 16.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.10 GiB is allocated by PyTorch, and 299.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model tf_efficientnet_b3.in1k created, param count: 12233232
Running train benchmark on tf_efficientnet_b3.in1k for 40 steps w/ input size (3, 300, 300) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 108.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 8.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.21 GiB is allocated by PyTorch, and 193.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model tf_efficientnet_b3.in1k created, param count: 12233232
Running train benchmark on tf_efficientnet_b3.in1k for 40 steps w/ input size (3, 300, 300) and batch size 128.
Train [8/40]. 555.88 samples/sec. 230.265 ms/step.
Train [16/40]. 555.90 samples/sec. 230.259 ms/step.
Train [24/40]. 555.90 samples/sec. 230.255 ms/step.
Train [32/40]. 555.89 samples/sec. 230.261 ms/step.
Train [40/40]. 555.88 samples/sec. 230.265 ms/step.
Train benchmark of tf_efficientnet_b3.in1k done. 552.43 samples/sec, 230.26 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tf_efficientnet_b3.ns_jft_in1k created, param count: 12233232
Running inference benchmark on tf_efficientnet_b3.ns_jft_in1k for 40 steps w/ input size (3, 300, 300) and batch size 256.
Infer [8/40]. 2284.90 samples/sec. 112.040 ms/step.
Infer [16/40]. 2284.89 samples/sec. 112.040 ms/step.
Infer [24/40]. 2284.81 samples/sec. 112.044 ms/step.
Infer [32/40]. 2284.80 samples/sec. 112.045 ms/step.
Infer [40/40]. 2284.77 samples/sec. 112.046 ms/step.
Inference benchmark of tf_efficientnet_b3.ns_jft_in1k done. 2284.17 samples/sec, 112.05 ms/step
Model tf_efficientnet_b3.ns_jft_in1k created, param count: 12233232
Running train benchmark on tf_efficientnet_b3.ns_jft_in1k for 40 steps w/ input size (3, 300, 300) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 204.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 16.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.10 GiB is allocated by PyTorch, and 299.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model tf_efficientnet_b3.ns_jft_in1k created, param count: 12233232
Running train benchmark on tf_efficientnet_b3.ns_jft_in1k for 40 steps w/ input size (3, 300, 300) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 108.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 8.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.21 GiB is allocated by PyTorch, and 193.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model tf_efficientnet_b3.ns_jft_in1k created, param count: 12233232
Running train benchmark on tf_efficientnet_b3.ns_jft_in1k for 40 steps w/ input size (3, 300, 300) and batch size 128.
Train [8/40]. 555.83 samples/sec. 230.285 ms/step.
Train [16/40]. 555.90 samples/sec. 230.259 ms/step.
Train [24/40]. 555.88 samples/sec. 230.266 ms/step.
Train [32/40]. 555.87 samples/sec. 230.270 ms/step.
Train [40/40]. 555.87 samples/sec. 230.271 ms/step.
Train benchmark of tf_efficientnet_b3.ns_jft_in1k done. 552.44 samples/sec, 230.27 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tf_efficientnet_b4.aa_in1k created, param count: 19341616
Running inference benchmark on tf_efficientnet_b4.aa_in1k for 40 steps w/ input size (3, 380, 380) and batch size 256.
Infer [8/40]. 1123.86 samples/sec. 227.787 ms/step.
Infer [16/40]. 1123.86 samples/sec. 227.786 ms/step.
Infer [24/40]. 1123.86 samples/sec. 227.786 ms/step.
Infer [32/40]. 1123.87 samples/sec. 227.784 ms/step.
Infer [40/40]. 1123.86 samples/sec. 227.786 ms/step.
Inference benchmark of tf_efficientnet_b4.aa_in1k done. 1123.67 samples/sec, 227.79 ms/step
Model tf_efficientnet_b4.aa_in1k created, param count: 19341616
Running train benchmark on tf_efficientnet_b4.aa_in1k for 40 steps w/ input size (3, 380, 380) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 848.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 238.06 MiB is free. Including non-PyTorch memory, this process has 23.41 GiB memory in use. Of the allocated memory 21.43 GiB is allocated by PyTorch, and 759.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model tf_efficientnet_b4.aa_in1k created, param count: 19341616
Running train benchmark on tf_efficientnet_b4.aa_in1k for 40 steps w/ input size (3, 380, 380) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 636.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 294.06 MiB is free. Including non-PyTorch memory, this process has 23.35 GiB memory in use. Of the allocated memory 21.87 GiB is allocated by PyTorch, and 255.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model tf_efficientnet_b4.aa_in1k created, param count: 19341616
Running train benchmark on tf_efficientnet_b4.aa_in1k for 40 steps w/ input size (3, 380, 380) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 190.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 136.06 MiB is free. Including non-PyTorch memory, this process has 23.51 GiB memory in use. Of the allocated memory 22.15 GiB is allocated by PyTorch, and 124.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model tf_efficientnet_b4.aa_in1k created, param count: 19341616
Running train benchmark on tf_efficientnet_b4.aa_in1k for 40 steps w/ input size (3, 380, 380) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 18.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 18.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.28 GiB is allocated by PyTorch, and 111.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model tf_efficientnet_b4.aa_in1k created, param count: 19341616
Running train benchmark on tf_efficientnet_b4.aa_in1k for 40 steps w/ input size (3, 380, 380) and batch size 64.
Train [8/40]. 282.72 samples/sec. 226.371 ms/step.
Train [16/40]. 282.71 samples/sec. 226.378 ms/step.
Train [24/40]. 282.72 samples/sec. 226.376 ms/step.
Train [32/40]. 282.71 samples/sec. 226.379 ms/step.
Train [40/40]. 282.68 samples/sec. 226.406 ms/step.
Train benchmark of tf_efficientnet_b4.aa_in1k done. 280.69 samples/sec, 226.41 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tf_efficientnet_b4.ap_in1k created, param count: 19341616
Running inference benchmark on tf_efficientnet_b4.ap_in1k for 40 steps w/ input size (3, 380, 380) and batch size 256.
Infer [8/40]. 1123.90 samples/sec. 227.778 ms/step.
Infer [16/40]. 1123.92 samples/sec. 227.775 ms/step.
Infer [24/40]. 1123.93 samples/sec. 227.772 ms/step.
Infer [32/40]. 1123.91 samples/sec. 227.776 ms/step.
Infer [40/40]. 1123.91 samples/sec. 227.776 ms/step.
Inference benchmark of tf_efficientnet_b4.ap_in1k done. 1123.70 samples/sec, 227.78 ms/step
Model tf_efficientnet_b4.ap_in1k created, param count: 19341616
Running train benchmark on tf_efficientnet_b4.ap_in1k for 40 steps w/ input size (3, 380, 380) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 848.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 238.06 MiB is free. Including non-PyTorch memory, this process has 23.41 GiB memory in use. Of the allocated memory 21.43 GiB is allocated by PyTorch, and 759.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model tf_efficientnet_b4.ap_in1k created, param count: 19341616
Running train benchmark on tf_efficientnet_b4.ap_in1k for 40 steps w/ input size (3, 380, 380) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 636.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 612.06 MiB is free. Including non-PyTorch memory, this process has 23.04 GiB memory in use. Of the allocated memory 21.25 GiB is allocated by PyTorch, and 572.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model tf_efficientnet_b4.ap_in1k created, param count: 19341616
Running train benchmark on tf_efficientnet_b4.ap_in1k for 40 steps w/ input size (3, 380, 380) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 190.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 68.06 MiB is free. Including non-PyTorch memory, this process has 23.57 GiB memory in use. Of the allocated memory 22.15 GiB is allocated by PyTorch, and 191.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model tf_efficientnet_b4.ap_in1k created, param count: 19341616
Running train benchmark on tf_efficientnet_b4.ap_in1k for 40 steps w/ input size (3, 380, 380) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 102.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 2.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 22.30 GiB is allocated by PyTorch, and 110.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model tf_efficientnet_b4.ap_in1k created, param count: 19341616
Running train benchmark on tf_efficientnet_b4.ap_in1k for 40 steps w/ input size (3, 380, 380) and batch size 64.
Train [8/40]. 282.47 samples/sec. 226.575 ms/step.
Train [16/40]. 282.46 samples/sec. 226.579 ms/step.
Train [24/40]. 282.45 samples/sec. 226.587 ms/step.
Train [32/40]. 282.45 samples/sec. 226.588 ms/step.
Train [40/40]. 282.45 samples/sec. 226.592 ms/step.
Train benchmark of tf_efficientnet_b4.ap_in1k done. 280.48 samples/sec, 226.59 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tf_efficientnet_b4.in1k created, param count: 19341616
Running inference benchmark on tf_efficientnet_b4.in1k for 40 steps w/ input size (3, 380, 380) and batch size 256.
Infer [8/40]. 1123.94 samples/sec. 227.771 ms/step.
Infer [16/40]. 1123.90 samples/sec. 227.778 ms/step.
Infer [24/40]. 1123.88 samples/sec. 227.782 ms/step.
Infer [32/40]. 1123.89 samples/sec. 227.780 ms/step.
Infer [40/40]. 1123.89 samples/sec. 227.779 ms/step.
Inference benchmark of tf_efficientnet_b4.in1k done. 1123.68 samples/sec, 227.78 ms/step
Model tf_efficientnet_b4.in1k created, param count: 19341616
Running train benchmark on tf_efficientnet_b4.in1k for 40 steps w/ input size (3, 380, 380) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 848.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 238.06 MiB is free. Including non-PyTorch memory, this process has 23.41 GiB memory in use. Of the allocated memory 21.43 GiB is allocated by PyTorch, and 759.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model tf_efficientnet_b4.in1k created, param count: 19341616
Running train benchmark on tf_efficientnet_b4.in1k for 40 steps w/ input size (3, 380, 380) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 636.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 612.06 MiB is free. Including non-PyTorch memory, this process has 23.04 GiB memory in use. Of the allocated memory 21.25 GiB is allocated by PyTorch, and 572.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model tf_efficientnet_b4.in1k created, param count: 19341616
Running train benchmark on tf_efficientnet_b4.in1k for 40 steps w/ input size (3, 380, 380) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 190.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 68.06 MiB is free. Including non-PyTorch memory, this process has 23.57 GiB memory in use. Of the allocated memory 22.15 GiB is allocated by PyTorch, and 191.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model tf_efficientnet_b4.in1k created, param count: 19341616
Running train benchmark on tf_efficientnet_b4.in1k for 40 steps w/ input size (3, 380, 380) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 102.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 2.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 22.30 GiB is allocated by PyTorch, and 110.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model tf_efficientnet_b4.in1k created, param count: 19341616
Running train benchmark on tf_efficientnet_b4.in1k for 40 steps w/ input size (3, 380, 380) and batch size 64.
Train [8/40]. 282.43 samples/sec. 226.603 ms/step.
Train [16/40]. 282.42 samples/sec. 226.612 ms/step.
Train [24/40]. 282.42 samples/sec. 226.612 ms/step.
Train [32/40]. 282.41 samples/sec. 226.617 ms/step.
Train [40/40]. 282.41 samples/sec. 226.620 ms/step.
Train benchmark of tf_efficientnet_b4.in1k done. 280.40 samples/sec, 226.62 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tf_efficientnet_b4.ns_jft_in1k created, param count: 19341616
Running inference benchmark on tf_efficientnet_b4.ns_jft_in1k for 40 steps w/ input size (3, 380, 380) and batch size 256.
Infer [8/40]. 1123.96 samples/sec. 227.765 ms/step.
Infer [16/40]. 1123.94 samples/sec. 227.770 ms/step.
Infer [24/40]. 1123.93 samples/sec. 227.771 ms/step.
Infer [32/40]. 1123.89 samples/sec. 227.780 ms/step.
Infer [40/40]. 1123.89 samples/sec. 227.780 ms/step.
Inference benchmark of tf_efficientnet_b4.ns_jft_in1k done. 1123.68 samples/sec, 227.78 ms/step
Model tf_efficientnet_b4.ns_jft_in1k created, param count: 19341616
Running train benchmark on tf_efficientnet_b4.ns_jft_in1k for 40 steps w/ input size (3, 380, 380) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 848.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 238.06 MiB is free. Including non-PyTorch memory, this process has 23.41 GiB memory in use. Of the allocated memory 21.43 GiB is allocated by PyTorch, and 759.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model tf_efficientnet_b4.ns_jft_in1k created, param count: 19341616
Running train benchmark on tf_efficientnet_b4.ns_jft_in1k for 40 steps w/ input size (3, 380, 380) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 636.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 612.06 MiB is free. Including non-PyTorch memory, this process has 23.04 GiB memory in use. Of the allocated memory 21.25 GiB is allocated by PyTorch, and 572.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model tf_efficientnet_b4.ns_jft_in1k created, param count: 19341616
Running train benchmark on tf_efficientnet_b4.ns_jft_in1k for 40 steps w/ input size (3, 380, 380) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 190.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 68.06 MiB is free. Including non-PyTorch memory, this process has 23.57 GiB memory in use. Of the allocated memory 22.15 GiB is allocated by PyTorch, and 191.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model tf_efficientnet_b4.ns_jft_in1k created, param count: 19341616
Running train benchmark on tf_efficientnet_b4.ns_jft_in1k for 40 steps w/ input size (3, 380, 380) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 102.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 2.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 22.30 GiB is allocated by PyTorch, and 110.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model tf_efficientnet_b4.ns_jft_in1k created, param count: 19341616
Running train benchmark on tf_efficientnet_b4.ns_jft_in1k for 40 steps w/ input size (3, 380, 380) and batch size 64.
Train [8/40]. 282.45 samples/sec. 226.590 ms/step.
Train [16/40]. 282.47 samples/sec. 226.572 ms/step.
Train [24/40]. 282.46 samples/sec. 226.578 ms/step.
Train [32/40]. 282.46 samples/sec. 226.577 ms/step.
Train [40/40]. 282.47 samples/sec. 226.574 ms/step.
Train benchmark of tf_efficientnet_b4.ns_jft_in1k done. 280.55 samples/sec, 226.57 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tf_efficientnet_b5.aa_in1k created, param count: 30389784
Running inference benchmark on tf_efficientnet_b5.aa_in1k for 40 steps w/ input size (3, 456, 456) and batch size 256.
Infer [8/40]. 561.40 samples/sec. 455.999 ms/step.
Infer [16/40]. 561.36 samples/sec. 456.035 ms/step.
Infer [24/40]. 561.33 samples/sec. 456.061 ms/step.
Infer [32/40]. 561.31 samples/sec. 456.076 ms/step.
Infer [40/40]. 561.30 samples/sec. 456.085 ms/step.
Inference benchmark of tf_efficientnet_b5.aa_in1k done. 561.24 samples/sec, 456.08 ms/step
Model tf_efficientnet_b5.aa_in1k created, param count: 30389784
Running train benchmark on tf_efficientnet_b5.aa_in1k for 40 steps w/ input size (3, 456, 456) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 3.57 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.32 GiB is free. Including non-PyTorch memory, this process has 22.32 GiB memory in use. Of the allocated memory 20.91 GiB is allocated by PyTorch, and 180.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model tf_efficientnet_b5.aa_in1k created, param count: 30389784
Running train benchmark on tf_efficientnet_b5.aa_in1k for 40 steps w/ input size (3, 456, 456) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 2.70 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.02 GiB is free. Including non-PyTorch memory, this process has 22.62 GiB memory in use. Of the allocated memory 21.05 GiB is allocated by PyTorch, and 343.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model tf_efficientnet_b5.aa_in1k created, param count: 30389784
Running train benchmark on tf_efficientnet_b5.aa_in1k for 40 steps w/ input size (3, 456, 456) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 762.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 588.06 MiB is free. Including non-PyTorch memory, this process has 23.07 GiB memory in use. Of the allocated memory 21.57 GiB is allocated by PyTorch, and 272.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model tf_efficientnet_b5.aa_in1k created, param count: 30389784
Running train benchmark on tf_efficientnet_b5.aa_in1k for 40 steps w/ input size (3, 456, 456) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 572.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 222.06 MiB is free. Including non-PyTorch memory, this process has 23.42 GiB memory in use. Of the allocated memory 21.97 GiB is allocated by PyTorch, and 231.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model tf_efficientnet_b5.aa_in1k created, param count: 30389784
Running train benchmark on tf_efficientnet_b5.aa_in1k for 40 steps w/ input size (3, 456, 456) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 154.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 120.06 MiB is free. Including non-PyTorch memory, this process has 23.52 GiB memory in use. Of the allocated memory 22.10 GiB is allocated by PyTorch, and 200.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model tf_efficientnet_b5.aa_in1k created, param count: 30389784
Running train benchmark on tf_efficientnet_b5.aa_in1k for 40 steps w/ input size (3, 456, 456) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 10.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.18 GiB is allocated by PyTorch, and 222.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model tf_efficientnet_b5.aa_in1k created, param count: 30389784
Running train benchmark on tf_efficientnet_b5.aa_in1k for 40 steps w/ input size (3, 456, 456) and batch size 32.
Train [8/40]. 140.49 samples/sec. 227.774 ms/step.
Train [16/40]. 140.48 samples/sec. 227.784 ms/step.
Train [24/40]. 140.49 samples/sec. 227.782 ms/step.
Train [32/40]. 140.48 samples/sec. 227.785 ms/step.
Train [40/40]. 140.48 samples/sec. 227.788 ms/step.
Train benchmark of tf_efficientnet_b5.aa_in1k done. 139.39 samples/sec, 227.79 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tf_efficientnet_b5.ap_in1k created, param count: 30389784
Running inference benchmark on tf_efficientnet_b5.ap_in1k for 40 steps w/ input size (3, 456, 456) and batch size 256.
Infer [8/40]. 561.43 samples/sec. 455.976 ms/step.
Infer [16/40]. 561.40 samples/sec. 456.001 ms/step.
Infer [24/40]. 561.38 samples/sec. 456.016 ms/step.
Infer [32/40]. 561.37 samples/sec. 456.024 ms/step.
Infer [40/40]. 561.36 samples/sec. 456.037 ms/step.
Inference benchmark of tf_efficientnet_b5.ap_in1k done. 561.30 samples/sec, 456.04 ms/step
Model tf_efficientnet_b5.ap_in1k created, param count: 30389784
Running train benchmark on tf_efficientnet_b5.ap_in1k for 40 steps w/ input size (3, 456, 456) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 3.57 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.32 GiB is free. Including non-PyTorch memory, this process has 22.32 GiB memory in use. Of the allocated memory 20.91 GiB is allocated by PyTorch, and 180.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model tf_efficientnet_b5.ap_in1k created, param count: 30389784
Running train benchmark on tf_efficientnet_b5.ap_in1k for 40 steps w/ input size (3, 456, 456) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 2.70 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.02 GiB is free. Including non-PyTorch memory, this process has 22.62 GiB memory in use. Of the allocated memory 21.05 GiB is allocated by PyTorch, and 343.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model tf_efficientnet_b5.ap_in1k created, param count: 30389784
Running train benchmark on tf_efficientnet_b5.ap_in1k for 40 steps w/ input size (3, 456, 456) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 762.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 410.06 MiB is free. Including non-PyTorch memory, this process has 23.24 GiB memory in use. Of the allocated memory 21.57 GiB is allocated by PyTorch, and 450.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model tf_efficientnet_b5.ap_in1k created, param count: 30389784
Running train benchmark on tf_efficientnet_b5.ap_in1k for 40 steps w/ input size (3, 456, 456) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 572.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 88.06 MiB is free. Including non-PyTorch memory, this process has 23.55 GiB memory in use. Of the allocated memory 21.96 GiB is allocated by PyTorch, and 365.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model tf_efficientnet_b5.ap_in1k created, param count: 30389784
Running train benchmark on tf_efficientnet_b5.ap_in1k for 40 steps w/ input size (3, 456, 456) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 154.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 56.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 22.09 GiB is allocated by PyTorch, and 265.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model tf_efficientnet_b5.ap_in1k created, param count: 30389784
Running train benchmark on tf_efficientnet_b5.ap_in1k for 40 steps w/ input size (3, 456, 456) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 4.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 22.18 GiB is allocated by PyTorch, and 233.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model tf_efficientnet_b5.ap_in1k created, param count: 30389784
Running train benchmark on tf_efficientnet_b5.ap_in1k for 40 steps w/ input size (3, 456, 456) and batch size 32.
Train [8/40]. 140.40 samples/sec. 227.922 ms/step.
Train [16/40]. 140.40 samples/sec. 227.919 ms/step.
Train [24/40]. 140.40 samples/sec. 227.917 ms/step.
Train [32/40]. 140.40 samples/sec. 227.921 ms/step.
Train [40/40]. 140.40 samples/sec. 227.919 ms/step.
Train benchmark of tf_efficientnet_b5.ap_in1k done. 139.28 samples/sec, 227.92 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tf_efficientnet_b5.in1k created, param count: 30389784
Running inference benchmark on tf_efficientnet_b5.in1k for 40 steps w/ input size (3, 456, 456) and batch size 256.
Infer [8/40]. 561.40 samples/sec. 456.005 ms/step.
Infer [16/40]. 561.36 samples/sec. 456.033 ms/step.
Infer [24/40]. 561.34 samples/sec. 456.052 ms/step.
Infer [32/40]. 561.33 samples/sec. 456.062 ms/step.
Infer [40/40]. 561.31 samples/sec. 456.072 ms/step.
Inference benchmark of tf_efficientnet_b5.in1k done. 561.26 samples/sec, 456.07 ms/step
Model tf_efficientnet_b5.in1k created, param count: 30389784
Running train benchmark on tf_efficientnet_b5.in1k for 40 steps w/ input size (3, 456, 456) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 3.57 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.32 GiB is free. Including non-PyTorch memory, this process has 22.32 GiB memory in use. Of the allocated memory 20.91 GiB is allocated by PyTorch, and 180.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model tf_efficientnet_b5.in1k created, param count: 30389784
Running train benchmark on tf_efficientnet_b5.in1k for 40 steps w/ input size (3, 456, 456) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 2.70 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.02 GiB is free. Including non-PyTorch memory, this process has 22.62 GiB memory in use. Of the allocated memory 21.05 GiB is allocated by PyTorch, and 343.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model tf_efficientnet_b5.in1k created, param count: 30389784
Running train benchmark on tf_efficientnet_b5.in1k for 40 steps w/ input size (3, 456, 456) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 762.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 410.06 MiB is free. Including non-PyTorch memory, this process has 23.24 GiB memory in use. Of the allocated memory 21.57 GiB is allocated by PyTorch, and 450.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model tf_efficientnet_b5.in1k created, param count: 30389784
Running train benchmark on tf_efficientnet_b5.in1k for 40 steps w/ input size (3, 456, 456) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 572.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 88.06 MiB is free. Including non-PyTorch memory, this process has 23.55 GiB memory in use. Of the allocated memory 21.96 GiB is allocated by PyTorch, and 365.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model tf_efficientnet_b5.in1k created, param count: 30389784
Running train benchmark on tf_efficientnet_b5.in1k for 40 steps w/ input size (3, 456, 456) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 154.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 56.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 22.09 GiB is allocated by PyTorch, and 265.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model tf_efficientnet_b5.in1k created, param count: 30389784
Running train benchmark on tf_efficientnet_b5.in1k for 40 steps w/ input size (3, 456, 456) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 4.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 22.18 GiB is allocated by PyTorch, and 233.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model tf_efficientnet_b5.in1k created, param count: 30389784
Running train benchmark on tf_efficientnet_b5.in1k for 40 steps w/ input size (3, 456, 456) and batch size 32.
Train [8/40]. 140.41 samples/sec. 227.903 ms/step.
Train [16/40]. 140.41 samples/sec. 227.901 ms/step.
Train [24/40]. 140.41 samples/sec. 227.908 ms/step.
Train [32/40]. 140.41 samples/sec. 227.904 ms/step.
Train [40/40]. 140.41 samples/sec. 227.910 ms/step.
Train benchmark of tf_efficientnet_b5.in1k done. 139.29 samples/sec, 227.91 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tf_efficientnet_b5.ns_jft_in1k created, param count: 30389784
Running inference benchmark on tf_efficientnet_b5.ns_jft_in1k for 40 steps w/ input size (3, 456, 456) and batch size 256.
Infer [8/40]. 561.39 samples/sec. 456.010 ms/step.
Infer [16/40]. 561.36 samples/sec. 456.033 ms/step.
Infer [24/40]. 561.36 samples/sec. 456.039 ms/step.
Infer [32/40]. 561.34 samples/sec. 456.052 ms/step.
Infer [40/40]. 561.34 samples/sec. 456.056 ms/step.
Inference benchmark of tf_efficientnet_b5.ns_jft_in1k done. 561.28 samples/sec, 456.06 ms/step
Model tf_efficientnet_b5.ns_jft_in1k created, param count: 30389784
Running train benchmark on tf_efficientnet_b5.ns_jft_in1k for 40 steps w/ input size (3, 456, 456) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 3.57 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.32 GiB is free. Including non-PyTorch memory, this process has 22.32 GiB memory in use. Of the allocated memory 20.91 GiB is allocated by PyTorch, and 180.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model tf_efficientnet_b5.ns_jft_in1k created, param count: 30389784
Running train benchmark on tf_efficientnet_b5.ns_jft_in1k for 40 steps w/ input size (3, 456, 456) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 2.70 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.02 GiB is free. Including non-PyTorch memory, this process has 22.62 GiB memory in use. Of the allocated memory 21.05 GiB is allocated by PyTorch, and 343.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model tf_efficientnet_b5.ns_jft_in1k created, param count: 30389784
Running train benchmark on tf_efficientnet_b5.ns_jft_in1k for 40 steps w/ input size (3, 456, 456) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 762.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 410.06 MiB is free. Including non-PyTorch memory, this process has 23.24 GiB memory in use. Of the allocated memory 21.57 GiB is allocated by PyTorch, and 450.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model tf_efficientnet_b5.ns_jft_in1k created, param count: 30389784
Running train benchmark on tf_efficientnet_b5.ns_jft_in1k for 40 steps w/ input size (3, 456, 456) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 572.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 88.06 MiB is free. Including non-PyTorch memory, this process has 23.55 GiB memory in use. Of the allocated memory 21.96 GiB is allocated by PyTorch, and 365.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model tf_efficientnet_b5.ns_jft_in1k created, param count: 30389784
Running train benchmark on tf_efficientnet_b5.ns_jft_in1k for 40 steps w/ input size (3, 456, 456) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 154.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 56.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 22.09 GiB is allocated by PyTorch, and 265.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model tf_efficientnet_b5.ns_jft_in1k created, param count: 30389784
Running train benchmark on tf_efficientnet_b5.ns_jft_in1k for 40 steps w/ input size (3, 456, 456) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 4.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 22.18 GiB is allocated by PyTorch, and 233.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model tf_efficientnet_b5.ns_jft_in1k created, param count: 30389784
Running train benchmark on tf_efficientnet_b5.ns_jft_in1k for 40 steps w/ input size (3, 456, 456) and batch size 32.
Train [8/40]. 140.40 samples/sec. 227.922 ms/step.
Train [16/40]. 140.40 samples/sec. 227.916 ms/step.
Train [24/40]. 140.41 samples/sec. 227.912 ms/step.
Train [32/40]. 140.40 samples/sec. 227.917 ms/step.
Train [40/40]. 140.40 samples/sec. 227.913 ms/step.
Train benchmark of tf_efficientnet_b5.ns_jft_in1k done. 139.26 samples/sec, 227.91 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tf_efficientnet_b5.ra_in1k created, param count: 30389784
Running inference benchmark on tf_efficientnet_b5.ra_in1k for 40 steps w/ input size (3, 456, 456) and batch size 256.
Infer [8/40]. 561.38 samples/sec. 456.021 ms/step.
Infer [16/40]. 561.36 samples/sec. 456.037 ms/step.
Infer [24/40]. 561.34 samples/sec. 456.049 ms/step.
Infer [32/40]. 561.34 samples/sec. 456.054 ms/step.
Infer [40/40]. 561.32 samples/sec. 456.065 ms/step.
Inference benchmark of tf_efficientnet_b5.ra_in1k done. 561.27 samples/sec, 456.06 ms/step
Model tf_efficientnet_b5.ra_in1k created, param count: 30389784
Running train benchmark on tf_efficientnet_b5.ra_in1k for 40 steps w/ input size (3, 456, 456) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 3.57 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.32 GiB is free. Including non-PyTorch memory, this process has 22.32 GiB memory in use. Of the allocated memory 20.91 GiB is allocated by PyTorch, and 180.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model tf_efficientnet_b5.ra_in1k created, param count: 30389784
Running train benchmark on tf_efficientnet_b5.ra_in1k for 40 steps w/ input size (3, 456, 456) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 2.70 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.02 GiB is free. Including non-PyTorch memory, this process has 22.62 GiB memory in use. Of the allocated memory 21.05 GiB is allocated by PyTorch, and 343.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model tf_efficientnet_b5.ra_in1k created, param count: 30389784
Running train benchmark on tf_efficientnet_b5.ra_in1k for 40 steps w/ input size (3, 456, 456) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 762.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 410.06 MiB is free. Including non-PyTorch memory, this process has 23.24 GiB memory in use. Of the allocated memory 21.57 GiB is allocated by PyTorch, and 450.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model tf_efficientnet_b5.ra_in1k created, param count: 30389784
Running train benchmark on tf_efficientnet_b5.ra_in1k for 40 steps w/ input size (3, 456, 456) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 572.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 88.06 MiB is free. Including non-PyTorch memory, this process has 23.55 GiB memory in use. Of the allocated memory 21.96 GiB is allocated by PyTorch, and 365.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model tf_efficientnet_b5.ra_in1k created, param count: 30389784
Running train benchmark on tf_efficientnet_b5.ra_in1k for 40 steps w/ input size (3, 456, 456) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 154.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 56.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 22.09 GiB is allocated by PyTorch, and 265.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model tf_efficientnet_b5.ra_in1k created, param count: 30389784
Running train benchmark on tf_efficientnet_b5.ra_in1k for 40 steps w/ input size (3, 456, 456) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 4.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 22.18 GiB is allocated by PyTorch, and 233.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model tf_efficientnet_b5.ra_in1k created, param count: 30389784
Running train benchmark on tf_efficientnet_b5.ra_in1k for 40 steps w/ input size (3, 456, 456) and batch size 32.
Train [8/40]. 140.41 samples/sec. 227.906 ms/step.
Train [16/40]. 140.41 samples/sec. 227.903 ms/step.
Train [24/40]. 140.42 samples/sec. 227.894 ms/step.
Train [32/40]. 140.41 samples/sec. 227.897 ms/step.
Train [40/40]. 140.42 samples/sec. 227.894 ms/step.
Train benchmark of tf_efficientnet_b5.ra_in1k done. 139.31 samples/sec, 227.89 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tf_efficientnet_b6.aa_in1k created, param count: 43040704
Running inference benchmark on tf_efficientnet_b6.aa_in1k for 40 steps w/ input size (3, 528, 528) and batch size 256.
ERROR: "Expected canUse32BitIndexMath(input) && canUse32BitIndexMath(output) to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model tf_efficientnet_b6.aa_in1k created, param count: 43040704
Running inference benchmark on tf_efficientnet_b6.aa_in1k for 40 steps w/ input size (3, 528, 528) and batch size 192.
ERROR: "Expected canUse32BitIndexMath(input) && canUse32BitIndexMath(output) to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model tf_efficientnet_b6.aa_in1k created, param count: 43040704
Running inference benchmark on tf_efficientnet_b6.aa_in1k for 40 steps w/ input size (3, 528, 528) and batch size 128.
Infer [8/40]. 324.45 samples/sec. 394.512 ms/step.
Infer [16/40]. 324.46 samples/sec. 394.500 ms/step.
Infer [24/40]. 324.46 samples/sec. 394.498 ms/step.
Infer [32/40]. 324.46 samples/sec. 394.504 ms/step.
Infer [40/40]. 324.46 samples/sec. 394.507 ms/step.
Inference benchmark of tf_efficientnet_b6.aa_in1k done. 324.42 samples/sec, 394.51 ms/step
Model tf_efficientnet_b6.aa_in1k created, param count: 43040704
Running train benchmark on tf_efficientnet_b6.aa_in1k for 40 steps w/ input size (3, 528, 528) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 1.06 GiB. GPU 0 has a total capacty of 23.65 GiB of which 698.06 MiB is free. Including non-PyTorch memory, this process has 22.96 GiB memory in use. Of the allocated memory 21.38 GiB is allocated by PyTorch, and 358.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model tf_efficientnet_b6.aa_in1k created, param count: 43040704
Running train benchmark on tf_efficientnet_b6.aa_in1k for 40 steps w/ input size (3, 528, 528) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 818.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 310.06 MiB is free. Including non-PyTorch memory, this process has 23.34 GiB memory in use. Of the allocated memory 21.64 GiB is allocated by PyTorch, and 477.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model tf_efficientnet_b6.aa_in1k created, param count: 43040704
Running train benchmark on tf_efficientnet_b6.aa_in1k for 40 steps w/ input size (3, 528, 528) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 3.19 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.19 GiB is free. Including non-PyTorch memory, this process has 22.45 GiB memory in use. Of the allocated memory 20.84 GiB is allocated by PyTorch, and 388.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model tf_efficientnet_b6.aa_in1k created, param count: 43040704
Running train benchmark on tf_efficientnet_b6.aa_in1k for 40 steps w/ input size (3, 528, 528) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 766.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 118.06 MiB is free. Including non-PyTorch memory, this process has 23.53 GiB memory in use. Of the allocated memory 21.45 GiB is allocated by PyTorch, and 860.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model tf_efficientnet_b6.aa_in1k created, param count: 43040704
Running train benchmark on tf_efficientnet_b6.aa_in1k for 40 steps w/ input size (3, 528, 528) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 512.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 48.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 22.15 GiB is allocated by PyTorch, and 220.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model tf_efficientnet_b6.aa_in1k created, param count: 43040704
Running train benchmark on tf_efficientnet_b6.aa_in1k for 40 steps w/ input size (3, 528, 528) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 384.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 298.06 MiB is free. Including non-PyTorch memory, this process has 23.35 GiB memory in use. Of the allocated memory 21.75 GiB is allocated by PyTorch, and 377.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model tf_efficientnet_b6.aa_in1k created, param count: 43040704
Running train benchmark on tf_efficientnet_b6.aa_in1k for 40 steps w/ input size (3, 528, 528) and batch size 32.
ERROR: "CUDA out of memory. Tried to allocate 58.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 32.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 22.24 GiB is allocated by PyTorch, and 139.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model tf_efficientnet_b6.aa_in1k created, param count: 43040704
Running train benchmark on tf_efficientnet_b6.aa_in1k for 40 steps w/ input size (3, 528, 528) and batch size 24.
ERROR: "CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 16.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 268.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model tf_efficientnet_b6.aa_in1k created, param count: 43040704
Running train benchmark on tf_efficientnet_b6.aa_in1k for 40 steps w/ input size (3, 528, 528) and batch size 16.
Train [8/40]. 88.29 samples/sec. 181.228 ms/step.
Train [16/40]. 88.29 samples/sec. 181.218 ms/step.
Train [24/40]. 88.30 samples/sec. 181.198 ms/step.
Train [32/40]. 88.30 samples/sec. 181.207 ms/step.
Train [40/40]. 88.31 samples/sec. 181.185 ms/step.
Train benchmark of tf_efficientnet_b6.aa_in1k done. 87.39 samples/sec, 181.19 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tf_efficientnet_b6.ap_in1k created, param count: 43040704
Running inference benchmark on tf_efficientnet_b6.ap_in1k for 40 steps w/ input size (3, 528, 528) and batch size 256.
ERROR: "Expected canUse32BitIndexMath(input) && canUse32BitIndexMath(output) to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model tf_efficientnet_b6.ap_in1k created, param count: 43040704
Running inference benchmark on tf_efficientnet_b6.ap_in1k for 40 steps w/ input size (3, 528, 528) and batch size 192.
ERROR: "Expected canUse32BitIndexMath(input) && canUse32BitIndexMath(output) to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model tf_efficientnet_b6.ap_in1k created, param count: 43040704
Running inference benchmark on tf_efficientnet_b6.ap_in1k for 40 steps w/ input size (3, 528, 528) and batch size 128.
Infer [8/40]. 324.43 samples/sec. 394.533 ms/step.
Infer [16/40]. 324.43 samples/sec. 394.536 ms/step.
Infer [24/40]. 324.43 samples/sec. 394.538 ms/step.
Infer [32/40]. 324.42 samples/sec. 394.546 ms/step.
Infer [40/40]. 324.42 samples/sec. 394.549 ms/step.
Inference benchmark of tf_efficientnet_b6.ap_in1k done. 324.38 samples/sec, 394.55 ms/step
Model tf_efficientnet_b6.ap_in1k created, param count: 43040704
Running train benchmark on tf_efficientnet_b6.ap_in1k for 40 steps w/ input size (3, 528, 528) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 1.06 GiB. GPU 0 has a total capacty of 23.65 GiB of which 698.06 MiB is free. Including non-PyTorch memory, this process has 22.96 GiB memory in use. Of the allocated memory 21.38 GiB is allocated by PyTorch, and 358.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model tf_efficientnet_b6.ap_in1k created, param count: 43040704
Running train benchmark on tf_efficientnet_b6.ap_in1k for 40 steps w/ input size (3, 528, 528) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 818.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 310.06 MiB is free. Including non-PyTorch memory, this process has 23.34 GiB memory in use. Of the allocated memory 21.64 GiB is allocated by PyTorch, and 477.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model tf_efficientnet_b6.ap_in1k created, param count: 43040704
Running train benchmark on tf_efficientnet_b6.ap_in1k for 40 steps w/ input size (3, 528, 528) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 3.19 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.19 GiB is free. Including non-PyTorch memory, this process has 22.45 GiB memory in use. Of the allocated memory 20.84 GiB is allocated by PyTorch, and 388.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model tf_efficientnet_b6.ap_in1k created, param count: 43040704
Running train benchmark on tf_efficientnet_b6.ap_in1k for 40 steps w/ input size (3, 528, 528) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 766.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 118.06 MiB is free. Including non-PyTorch memory, this process has 23.53 GiB memory in use. Of the allocated memory 21.45 GiB is allocated by PyTorch, and 860.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model tf_efficientnet_b6.ap_in1k created, param count: 43040704
Running train benchmark on tf_efficientnet_b6.ap_in1k for 40 steps w/ input size (3, 528, 528) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 512.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 330.06 MiB is free. Including non-PyTorch memory, this process has 23.32 GiB memory in use. Of the allocated memory 21.65 GiB is allocated by PyTorch, and 450.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model tf_efficientnet_b6.ap_in1k created, param count: 43040704
Running train benchmark on tf_efficientnet_b6.ap_in1k for 40 steps w/ input size (3, 528, 528) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 384.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 348.06 MiB is free. Including non-PyTorch memory, this process has 23.30 GiB memory in use. Of the allocated memory 21.75 GiB is allocated by PyTorch, and 328.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model tf_efficientnet_b6.ap_in1k created, param count: 43040704
Running train benchmark on tf_efficientnet_b6.ap_in1k for 40 steps w/ input size (3, 528, 528) and batch size 32.
ERROR: "CUDA out of memory. Tried to allocate 58.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 10.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.24 GiB is allocated by PyTorch, and 163.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model tf_efficientnet_b6.ap_in1k created, param count: 43040704
Running train benchmark on tf_efficientnet_b6.ap_in1k for 40 steps w/ input size (3, 528, 528) and batch size 24.
ERROR: "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 44.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 21.99 GiB is allocated by PyTorch, and 381.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model tf_efficientnet_b6.ap_in1k created, param count: 43040704
Running train benchmark on tf_efficientnet_b6.ap_in1k for 40 steps w/ input size (3, 528, 528) and batch size 16.
Train [8/40]. 88.45 samples/sec. 180.885 ms/step.
Train [16/40]. 88.46 samples/sec. 180.877 ms/step.
Train [24/40]. 88.45 samples/sec. 180.899 ms/step.
Train [32/40]. 88.43 samples/sec. 180.942 ms/step.
Train [40/40]. 88.42 samples/sec. 180.963 ms/step.
Train benchmark of tf_efficientnet_b6.ap_in1k done. 87.50 samples/sec, 180.96 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tf_efficientnet_b6.ns_jft_in1k created, param count: 43040704
Running inference benchmark on tf_efficientnet_b6.ns_jft_in1k for 40 steps w/ input size (3, 528, 528) and batch size 256.
ERROR: "Expected canUse32BitIndexMath(input) && canUse32BitIndexMath(output) to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model tf_efficientnet_b6.ns_jft_in1k created, param count: 43040704
Running inference benchmark on tf_efficientnet_b6.ns_jft_in1k for 40 steps w/ input size (3, 528, 528) and batch size 192.
ERROR: "Expected canUse32BitIndexMath(input) && canUse32BitIndexMath(output) to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model tf_efficientnet_b6.ns_jft_in1k created, param count: 43040704
Running inference benchmark on tf_efficientnet_b6.ns_jft_in1k for 40 steps w/ input size (3, 528, 528) and batch size 128.
Infer [8/40]. 324.44 samples/sec. 394.524 ms/step.
Infer [16/40]. 324.44 samples/sec. 394.527 ms/step.
Infer [24/40]. 324.44 samples/sec. 394.527 ms/step.
Infer [32/40]. 324.44 samples/sec. 394.531 ms/step.
Infer [40/40]. 324.43 samples/sec. 394.539 ms/step.
Inference benchmark of tf_efficientnet_b6.ns_jft_in1k done. 324.39 samples/sec, 394.54 ms/step
Model tf_efficientnet_b6.ns_jft_in1k created, param count: 43040704
Running train benchmark on tf_efficientnet_b6.ns_jft_in1k for 40 steps w/ input size (3, 528, 528) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 1.06 GiB. GPU 0 has a total capacty of 23.65 GiB of which 698.06 MiB is free. Including non-PyTorch memory, this process has 22.96 GiB memory in use. Of the allocated memory 21.38 GiB is allocated by PyTorch, and 358.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model tf_efficientnet_b6.ns_jft_in1k created, param count: 43040704
Running train benchmark on tf_efficientnet_b6.ns_jft_in1k for 40 steps w/ input size (3, 528, 528) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 818.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 310.06 MiB is free. Including non-PyTorch memory, this process has 23.34 GiB memory in use. Of the allocated memory 21.64 GiB is allocated by PyTorch, and 477.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model tf_efficientnet_b6.ns_jft_in1k created, param count: 43040704
Running train benchmark on tf_efficientnet_b6.ns_jft_in1k for 40 steps w/ input size (3, 528, 528) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 3.19 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.19 GiB is free. Including non-PyTorch memory, this process has 22.45 GiB memory in use. Of the allocated memory 20.84 GiB is allocated by PyTorch, and 388.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model tf_efficientnet_b6.ns_jft_in1k created, param count: 43040704
Running train benchmark on tf_efficientnet_b6.ns_jft_in1k for 40 steps w/ input size (3, 528, 528) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 766.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 118.06 MiB is free. Including non-PyTorch memory, this process has 23.53 GiB memory in use. Of the allocated memory 21.45 GiB is allocated by PyTorch, and 860.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model tf_efficientnet_b6.ns_jft_in1k created, param count: 43040704
Running train benchmark on tf_efficientnet_b6.ns_jft_in1k for 40 steps w/ input size (3, 528, 528) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 512.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 330.06 MiB is free. Including non-PyTorch memory, this process has 23.32 GiB memory in use. Of the allocated memory 21.65 GiB is allocated by PyTorch, and 450.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model tf_efficientnet_b6.ns_jft_in1k created, param count: 43040704
Running train benchmark on tf_efficientnet_b6.ns_jft_in1k for 40 steps w/ input size (3, 528, 528) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 384.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 348.06 MiB is free. Including non-PyTorch memory, this process has 23.30 GiB memory in use. Of the allocated memory 21.75 GiB is allocated by PyTorch, and 328.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model tf_efficientnet_b6.ns_jft_in1k created, param count: 43040704
Running train benchmark on tf_efficientnet_b6.ns_jft_in1k for 40 steps w/ input size (3, 528, 528) and batch size 32.
ERROR: "CUDA out of memory. Tried to allocate 58.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 10.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.24 GiB is allocated by PyTorch, and 163.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model tf_efficientnet_b6.ns_jft_in1k created, param count: 43040704
Running train benchmark on tf_efficientnet_b6.ns_jft_in1k for 40 steps w/ input size (3, 528, 528) and batch size 24.
ERROR: "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 44.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 21.99 GiB is allocated by PyTorch, and 381.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model tf_efficientnet_b6.ns_jft_in1k created, param count: 43040704
Running train benchmark on tf_efficientnet_b6.ns_jft_in1k for 40 steps w/ input size (3, 528, 528) and batch size 16.
Train [8/40]. 89.51 samples/sec. 178.743 ms/step.
Train [16/40]. 89.51 samples/sec. 178.753 ms/step.
Train [24/40]. 89.51 samples/sec. 178.760 ms/step.
Train [32/40]. 89.51 samples/sec. 178.751 ms/step.
Train [40/40]. 89.51 samples/sec. 178.752 ms/step.
Train benchmark of tf_efficientnet_b6.ns_jft_in1k done. 88.61 samples/sec, 178.75 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tf_efficientnet_b7.aa_in1k created, param count: 66347960
Running inference benchmark on tf_efficientnet_b7.aa_in1k for 40 steps w/ input size (3, 600, 600) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 2.06 GiB. GPU 0 has a total capacty of 23.65 GiB of which 918.06 MiB is free. Including non-PyTorch memory, this process has 22.74 GiB memory in use. Of the allocated memory 21.31 GiB is allocated by PyTorch, and 204.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model tf_efficientnet_b7.aa_in1k created, param count: 66347960
Running inference benchmark on tf_efficientnet_b7.aa_in1k for 40 steps w/ input size (3, 600, 600) and batch size 192.
ERROR: "Expected canUse32BitIndexMath(input) && canUse32BitIndexMath(output) to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model tf_efficientnet_b7.aa_in1k created, param count: 66347960
Running inference benchmark on tf_efficientnet_b7.aa_in1k for 40 steps w/ input size (3, 600, 600) and batch size 128.
ERROR: "Expected canUse32BitIndexMath(input) && canUse32BitIndexMath(output) to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model tf_efficientnet_b7.aa_in1k created, param count: 66347960
Running inference benchmark on tf_efficientnet_b7.aa_in1k for 40 steps w/ input size (3, 600, 600) and batch size 96.
Infer [8/40]. 186.76 samples/sec. 514.020 ms/step.
Infer [16/40]. 186.76 samples/sec. 514.026 ms/step.
Infer [24/40]. 186.76 samples/sec. 514.035 ms/step.
Infer [32/40]. 186.75 samples/sec. 514.046 ms/step.
Infer [40/40]. 186.75 samples/sec. 514.048 ms/step.
Inference benchmark of tf_efficientnet_b7.aa_in1k done. 186.74 samples/sec, 514.05 ms/step
Model tf_efficientnet_b7.aa_in1k created, param count: 66347960
Running train benchmark on tf_efficientnet_b7.aa_in1k for 40 steps w/ input size (3, 600, 600) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 1.38 GiB. GPU 0 has a total capacty of 23.65 GiB of which 436.06 MiB is free. Including non-PyTorch memory, this process has 23.21 GiB memory in use. Of the allocated memory 21.77 GiB is allocated by PyTorch, and 213.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model tf_efficientnet_b7.aa_in1k created, param count: 66347960
Running train benchmark on tf_efficientnet_b7.aa_in1k for 40 steps w/ input size (3, 600, 600) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 1.03 GiB. GPU 0 has a total capacty of 23.65 GiB of which 566.06 MiB is free. Including non-PyTorch memory, this process has 23.09 GiB memory in use. Of the allocated memory 21.52 GiB is allocated by PyTorch, and 345.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model tf_efficientnet_b7.aa_in1k created, param count: 66347960
Running train benchmark on tf_efficientnet_b7.aa_in1k for 40 steps w/ input size (3, 600, 600) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 704.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 246.06 MiB is free. Including non-PyTorch memory, this process has 23.40 GiB memory in use. Of the allocated memory 21.96 GiB is allocated by PyTorch, and 214.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model tf_efficientnet_b7.aa_in1k created, param count: 66347960
Running train benchmark on tf_efficientnet_b7.aa_in1k for 40 steps w/ input size (3, 600, 600) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 3.09 GiB. GPU 0 has a total capacty of 23.65 GiB of which 950.06 MiB is free. Including non-PyTorch memory, this process has 22.71 GiB memory in use. Of the allocated memory 21.14 GiB is allocated by PyTorch, and 349.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model tf_efficientnet_b7.aa_in1k created, param count: 66347960
Running train benchmark on tf_efficientnet_b7.aa_in1k for 40 steps w/ input size (3, 600, 600) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 792.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 678.06 MiB is free. Including non-PyTorch memory, this process has 22.98 GiB memory in use. Of the allocated memory 21.37 GiB is allocated by PyTorch, and 380.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model tf_efficientnet_b7.aa_in1k created, param count: 66347960
Running train benchmark on tf_efficientnet_b7.aa_in1k for 40 steps w/ input size (3, 600, 600) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 594.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 78.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 22.06 GiB is allocated by PyTorch, and 280.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model tf_efficientnet_b7.aa_in1k created, param count: 66347960
Running train benchmark on tf_efficientnet_b7.aa_in1k for 40 steps w/ input size (3, 600, 600) and batch size 32.
ERROR: "CUDA out of memory. Tried to allocate 396.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 236.06 MiB is free. Including non-PyTorch memory, this process has 23.41 GiB memory in use. Of the allocated memory 22.10 GiB is allocated by PyTorch, and 79.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model tf_efficientnet_b7.aa_in1k created, param count: 66347960
Running train benchmark on tf_efficientnet_b7.aa_in1k for 40 steps w/ input size (3, 600, 600) and batch size 24.
ERROR: "CUDA out of memory. Tried to allocate 124.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 122.06 MiB is free. Including non-PyTorch memory, this process has 23.52 GiB memory in use. Of the allocated memory 21.89 GiB is allocated by PyTorch, and 405.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model tf_efficientnet_b7.aa_in1k created, param count: 66347960
Running train benchmark on tf_efficientnet_b7.aa_in1k for 40 steps w/ input size (3, 600, 600) and batch size 16.
ERROR: "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 16.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.21 GiB is allocated by PyTorch, and 183.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 12 for retry.
Model tf_efficientnet_b7.aa_in1k created, param count: 66347960
Running train benchmark on tf_efficientnet_b7.aa_in1k for 40 steps w/ input size (3, 600, 600) and batch size 12.
ERROR: "CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 4.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 21.62 GiB is allocated by PyTorch, and 801.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 8 for retry.
Model tf_efficientnet_b7.aa_in1k created, param count: 66347960
Running train benchmark on tf_efficientnet_b7.aa_in1k for 40 steps w/ input size (3, 600, 600) and batch size 8.
Train [8/40]. 44.83 samples/sec. 178.453 ms/step.
Train [16/40]. 44.85 samples/sec. 178.389 ms/step.
Train [24/40]. 44.85 samples/sec. 178.392 ms/step.
Train [32/40]. 44.84 samples/sec. 178.406 ms/step.
Train [40/40]. 44.84 samples/sec. 178.414 ms/step.
Train benchmark of tf_efficientnet_b7.aa_in1k done. 44.31 samples/sec, 178.41 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tf_efficientnet_b7.ap_in1k created, param count: 66347960
Running inference benchmark on tf_efficientnet_b7.ap_in1k for 40 steps w/ input size (3, 600, 600) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 8.24 GiB. GPU 0 has a total capacty of 23.65 GiB of which 7.82 GiB is free. Including non-PyTorch memory, this process has 15.82 GiB memory in use. Of the allocated memory 13.02 GiB is allocated by PyTorch, and 1.57 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model tf_efficientnet_b7.ap_in1k created, param count: 66347960
Running inference benchmark on tf_efficientnet_b7.ap_in1k for 40 steps w/ input size (3, 600, 600) and batch size 192.
ERROR: "Expected canUse32BitIndexMath(input) && canUse32BitIndexMath(output) to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model tf_efficientnet_b7.ap_in1k created, param count: 66347960
Running inference benchmark on tf_efficientnet_b7.ap_in1k for 40 steps w/ input size (3, 600, 600) and batch size 128.
ERROR: "Expected canUse32BitIndexMath(input) && canUse32BitIndexMath(output) to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model tf_efficientnet_b7.ap_in1k created, param count: 66347960
Running inference benchmark on tf_efficientnet_b7.ap_in1k for 40 steps w/ input size (3, 600, 600) and batch size 96.
Infer [8/40]. 186.83 samples/sec. 513.831 ms/step.
Infer [16/40]. 186.82 samples/sec. 513.852 ms/step.
Infer [24/40]. 186.82 samples/sec. 513.875 ms/step.
Infer [32/40]. 186.81 samples/sec. 513.884 ms/step.
Infer [40/40]. 186.81 samples/sec. 513.890 ms/step.
Inference benchmark of tf_efficientnet_b7.ap_in1k done. 186.79 samples/sec, 513.89 ms/step
Model tf_efficientnet_b7.ap_in1k created, param count: 66347960
Running train benchmark on tf_efficientnet_b7.ap_in1k for 40 steps w/ input size (3, 600, 600) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 1.38 GiB. GPU 0 has a total capacty of 23.65 GiB of which 436.06 MiB is free. Including non-PyTorch memory, this process has 23.21 GiB memory in use. Of the allocated memory 21.77 GiB is allocated by PyTorch, and 213.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model tf_efficientnet_b7.ap_in1k created, param count: 66347960
Running train benchmark on tf_efficientnet_b7.ap_in1k for 40 steps w/ input size (3, 600, 600) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 1.03 GiB. GPU 0 has a total capacty of 23.65 GiB of which 566.06 MiB is free. Including non-PyTorch memory, this process has 23.09 GiB memory in use. Of the allocated memory 21.52 GiB is allocated by PyTorch, and 345.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model tf_efficientnet_b7.ap_in1k created, param count: 66347960
Running train benchmark on tf_efficientnet_b7.ap_in1k for 40 steps w/ input size (3, 600, 600) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 704.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 246.06 MiB is free. Including non-PyTorch memory, this process has 23.40 GiB memory in use. Of the allocated memory 21.96 GiB is allocated by PyTorch, and 214.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model tf_efficientnet_b7.ap_in1k created, param count: 66347960
Running train benchmark on tf_efficientnet_b7.ap_in1k for 40 steps w/ input size (3, 600, 600) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 3.09 GiB. GPU 0 has a total capacty of 23.65 GiB of which 950.06 MiB is free. Including non-PyTorch memory, this process has 22.71 GiB memory in use. Of the allocated memory 21.14 GiB is allocated by PyTorch, and 349.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model tf_efficientnet_b7.ap_in1k created, param count: 66347960
Running train benchmark on tf_efficientnet_b7.ap_in1k for 40 steps w/ input size (3, 600, 600) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 792.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 326.06 MiB is free. Including non-PyTorch memory, this process has 23.32 GiB memory in use. Of the allocated memory 21.37 GiB is allocated by PyTorch, and 732.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model tf_efficientnet_b7.ap_in1k created, param count: 66347960
Running train benchmark on tf_efficientnet_b7.ap_in1k for 40 steps w/ input size (3, 600, 600) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 594.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 408.06 MiB is free. Including non-PyTorch memory, this process has 23.24 GiB memory in use. Of the allocated memory 21.48 GiB is allocated by PyTorch, and 544.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model tf_efficientnet_b7.ap_in1k created, param count: 66347960
Running train benchmark on tf_efficientnet_b7.ap_in1k for 40 steps w/ input size (3, 600, 600) and batch size 32.
ERROR: "CUDA out of memory. Tried to allocate 396.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 130.06 MiB is free. Including non-PyTorch memory, this process has 23.51 GiB memory in use. Of the allocated memory 22.10 GiB is allocated by PyTorch, and 185.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model tf_efficientnet_b7.ap_in1k created, param count: 66347960
Running train benchmark on tf_efficientnet_b7.ap_in1k for 40 steps w/ input size (3, 600, 600) and batch size 24.
ERROR: "CUDA out of memory. Tried to allocate 124.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 116.06 MiB is free. Including non-PyTorch memory, this process has 23.53 GiB memory in use. Of the allocated memory 22.01 GiB is allocated by PyTorch, and 288.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model tf_efficientnet_b7.ap_in1k created, param count: 66347960
Running train benchmark on tf_efficientnet_b7.ap_in1k for 40 steps w/ input size (3, 600, 600) and batch size 16.
ERROR: "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 16.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.21 GiB is allocated by PyTorch, and 183.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 12 for retry.
Model tf_efficientnet_b7.ap_in1k created, param count: 66347960
Running train benchmark on tf_efficientnet_b7.ap_in1k for 40 steps w/ input size (3, 600, 600) and batch size 12.
ERROR: "CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 2.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 21.72 GiB is allocated by PyTorch, and 706.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 8 for retry.
Model tf_efficientnet_b7.ap_in1k created, param count: 66347960
Running train benchmark on tf_efficientnet_b7.ap_in1k for 40 steps w/ input size (3, 600, 600) and batch size 8.
Train [8/40]. 44.11 samples/sec. 181.361 ms/step.
Train [16/40]. 44.10 samples/sec. 181.388 ms/step.
Train [24/40]. 44.10 samples/sec. 181.399 ms/step.
Train [32/40]. 44.09 samples/sec. 181.427 ms/step.
Train [40/40]. 44.10 samples/sec. 181.425 ms/step.
Train benchmark of tf_efficientnet_b7.ap_in1k done. 43.56 samples/sec, 181.43 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tf_efficientnet_b7.ns_jft_in1k created, param count: 66347960
Running inference benchmark on tf_efficientnet_b7.ns_jft_in1k for 40 steps w/ input size (3, 600, 600) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 8.24 GiB. GPU 0 has a total capacty of 23.65 GiB of which 7.82 GiB is free. Including non-PyTorch memory, this process has 15.82 GiB memory in use. Of the allocated memory 13.02 GiB is allocated by PyTorch, and 1.57 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model tf_efficientnet_b7.ns_jft_in1k created, param count: 66347960
Running inference benchmark on tf_efficientnet_b7.ns_jft_in1k for 40 steps w/ input size (3, 600, 600) and batch size 192.
ERROR: "Expected canUse32BitIndexMath(input) && canUse32BitIndexMath(output) to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model tf_efficientnet_b7.ns_jft_in1k created, param count: 66347960
Running inference benchmark on tf_efficientnet_b7.ns_jft_in1k for 40 steps w/ input size (3, 600, 600) and batch size 128.
ERROR: "Expected canUse32BitIndexMath(input) && canUse32BitIndexMath(output) to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model tf_efficientnet_b7.ns_jft_in1k created, param count: 66347960
Running inference benchmark on tf_efficientnet_b7.ns_jft_in1k for 40 steps w/ input size (3, 600, 600) and batch size 96.
Infer [8/40]. 186.81 samples/sec. 513.883 ms/step.
Infer [16/40]. 186.81 samples/sec. 513.898 ms/step.
Infer [24/40]. 186.80 samples/sec. 513.913 ms/step.
Infer [32/40]. 186.80 samples/sec. 513.917 ms/step.
Infer [40/40]. 186.80 samples/sec. 513.921 ms/step.
Inference benchmark of tf_efficientnet_b7.ns_jft_in1k done. 186.78 samples/sec, 513.92 ms/step
Model tf_efficientnet_b7.ns_jft_in1k created, param count: 66347960
Running train benchmark on tf_efficientnet_b7.ns_jft_in1k for 40 steps w/ input size (3, 600, 600) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 1.38 GiB. GPU 0 has a total capacty of 23.65 GiB of which 436.06 MiB is free. Including non-PyTorch memory, this process has 23.21 GiB memory in use. Of the allocated memory 21.77 GiB is allocated by PyTorch, and 213.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model tf_efficientnet_b7.ns_jft_in1k created, param count: 66347960
Running train benchmark on tf_efficientnet_b7.ns_jft_in1k for 40 steps w/ input size (3, 600, 600) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 1.03 GiB. GPU 0 has a total capacty of 23.65 GiB of which 566.06 MiB is free. Including non-PyTorch memory, this process has 23.09 GiB memory in use. Of the allocated memory 21.52 GiB is allocated by PyTorch, and 345.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model tf_efficientnet_b7.ns_jft_in1k created, param count: 66347960
Running train benchmark on tf_efficientnet_b7.ns_jft_in1k for 40 steps w/ input size (3, 600, 600) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 704.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 246.06 MiB is free. Including non-PyTorch memory, this process has 23.40 GiB memory in use. Of the allocated memory 21.96 GiB is allocated by PyTorch, and 214.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model tf_efficientnet_b7.ns_jft_in1k created, param count: 66347960
Running train benchmark on tf_efficientnet_b7.ns_jft_in1k for 40 steps w/ input size (3, 600, 600) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 3.09 GiB. GPU 0 has a total capacty of 23.65 GiB of which 950.06 MiB is free. Including non-PyTorch memory, this process has 22.71 GiB memory in use. Of the allocated memory 21.14 GiB is allocated by PyTorch, and 349.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model tf_efficientnet_b7.ns_jft_in1k created, param count: 66347960
Running train benchmark on tf_efficientnet_b7.ns_jft_in1k for 40 steps w/ input size (3, 600, 600) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 792.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 326.06 MiB is free. Including non-PyTorch memory, this process has 23.32 GiB memory in use. Of the allocated memory 21.37 GiB is allocated by PyTorch, and 732.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model tf_efficientnet_b7.ns_jft_in1k created, param count: 66347960
Running train benchmark on tf_efficientnet_b7.ns_jft_in1k for 40 steps w/ input size (3, 600, 600) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 594.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 408.06 MiB is free. Including non-PyTorch memory, this process has 23.24 GiB memory in use. Of the allocated memory 21.48 GiB is allocated by PyTorch, and 544.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model tf_efficientnet_b7.ns_jft_in1k created, param count: 66347960
Running train benchmark on tf_efficientnet_b7.ns_jft_in1k for 40 steps w/ input size (3, 600, 600) and batch size 32.
ERROR: "CUDA out of memory. Tried to allocate 396.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 130.06 MiB is free. Including non-PyTorch memory, this process has 23.51 GiB memory in use. Of the allocated memory 22.10 GiB is allocated by PyTorch, and 185.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model tf_efficientnet_b7.ns_jft_in1k created, param count: 66347960
Running train benchmark on tf_efficientnet_b7.ns_jft_in1k for 40 steps w/ input size (3, 600, 600) and batch size 24.
ERROR: "CUDA out of memory. Tried to allocate 124.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 116.06 MiB is free. Including non-PyTorch memory, this process has 23.53 GiB memory in use. Of the allocated memory 22.01 GiB is allocated by PyTorch, and 288.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model tf_efficientnet_b7.ns_jft_in1k created, param count: 66347960
Running train benchmark on tf_efficientnet_b7.ns_jft_in1k for 40 steps w/ input size (3, 600, 600) and batch size 16.
ERROR: "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 16.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.21 GiB is allocated by PyTorch, and 183.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 12 for retry.
Model tf_efficientnet_b7.ns_jft_in1k created, param count: 66347960
Running train benchmark on tf_efficientnet_b7.ns_jft_in1k for 40 steps w/ input size (3, 600, 600) and batch size 12.
ERROR: "CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 2.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 21.72 GiB is allocated by PyTorch, and 706.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 8 for retry.
Model tf_efficientnet_b7.ns_jft_in1k created, param count: 66347960
Running train benchmark on tf_efficientnet_b7.ns_jft_in1k for 40 steps w/ input size (3, 600, 600) and batch size 8.
Train [8/40]. 44.80 samples/sec. 178.561 ms/step.
Train [16/40]. 44.81 samples/sec. 178.551 ms/step.
Train [24/40]. 44.67 samples/sec. 179.075 ms/step.
Train [32/40]. 44.72 samples/sec. 178.896 ms/step.
Train [40/40]. 44.74 samples/sec. 178.807 ms/step.
Train benchmark of tf_efficientnet_b7.ns_jft_in1k done. 44.20 samples/sec, 178.81 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tf_efficientnet_b7.ra_in1k created, param count: 66347960
Running inference benchmark on tf_efficientnet_b7.ra_in1k for 40 steps w/ input size (3, 600, 600) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 8.24 GiB. GPU 0 has a total capacty of 23.65 GiB of which 7.82 GiB is free. Including non-PyTorch memory, this process has 15.82 GiB memory in use. Of the allocated memory 13.02 GiB is allocated by PyTorch, and 1.57 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model tf_efficientnet_b7.ra_in1k created, param count: 66347960
Running inference benchmark on tf_efficientnet_b7.ra_in1k for 40 steps w/ input size (3, 600, 600) and batch size 192.
ERROR: "Expected canUse32BitIndexMath(input) && canUse32BitIndexMath(output) to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model tf_efficientnet_b7.ra_in1k created, param count: 66347960
Running inference benchmark on tf_efficientnet_b7.ra_in1k for 40 steps w/ input size (3, 600, 600) and batch size 128.
ERROR: "Expected canUse32BitIndexMath(input) && canUse32BitIndexMath(output) to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model tf_efficientnet_b7.ra_in1k created, param count: 66347960
Running inference benchmark on tf_efficientnet_b7.ra_in1k for 40 steps w/ input size (3, 600, 600) and batch size 96.
Infer [8/40]. 186.84 samples/sec. 513.807 ms/step.
Infer [16/40]. 186.83 samples/sec. 513.840 ms/step.
Infer [24/40]. 186.82 samples/sec. 513.852 ms/step.
Infer [32/40]. 186.82 samples/sec. 513.875 ms/step.
Infer [40/40]. 186.81 samples/sec. 513.882 ms/step.
Inference benchmark of tf_efficientnet_b7.ra_in1k done. 186.80 samples/sec, 513.88 ms/step
Model tf_efficientnet_b7.ra_in1k created, param count: 66347960
Running train benchmark on tf_efficientnet_b7.ra_in1k for 40 steps w/ input size (3, 600, 600) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 1.38 GiB. GPU 0 has a total capacty of 23.65 GiB of which 436.06 MiB is free. Including non-PyTorch memory, this process has 23.21 GiB memory in use. Of the allocated memory 21.77 GiB is allocated by PyTorch, and 213.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model tf_efficientnet_b7.ra_in1k created, param count: 66347960
Running train benchmark on tf_efficientnet_b7.ra_in1k for 40 steps w/ input size (3, 600, 600) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 1.03 GiB. GPU 0 has a total capacty of 23.65 GiB of which 566.06 MiB is free. Including non-PyTorch memory, this process has 23.09 GiB memory in use. Of the allocated memory 21.52 GiB is allocated by PyTorch, and 345.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model tf_efficientnet_b7.ra_in1k created, param count: 66347960
Running train benchmark on tf_efficientnet_b7.ra_in1k for 40 steps w/ input size (3, 600, 600) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 704.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 246.06 MiB is free. Including non-PyTorch memory, this process has 23.40 GiB memory in use. Of the allocated memory 21.96 GiB is allocated by PyTorch, and 214.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model tf_efficientnet_b7.ra_in1k created, param count: 66347960
Running train benchmark on tf_efficientnet_b7.ra_in1k for 40 steps w/ input size (3, 600, 600) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 3.09 GiB. GPU 0 has a total capacty of 23.65 GiB of which 950.06 MiB is free. Including non-PyTorch memory, this process has 22.71 GiB memory in use. Of the allocated memory 21.14 GiB is allocated by PyTorch, and 349.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model tf_efficientnet_b7.ra_in1k created, param count: 66347960
Running train benchmark on tf_efficientnet_b7.ra_in1k for 40 steps w/ input size (3, 600, 600) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 792.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 326.06 MiB is free. Including non-PyTorch memory, this process has 23.32 GiB memory in use. Of the allocated memory 21.37 GiB is allocated by PyTorch, and 732.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model tf_efficientnet_b7.ra_in1k created, param count: 66347960
Running train benchmark on tf_efficientnet_b7.ra_in1k for 40 steps w/ input size (3, 600, 600) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 594.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 408.06 MiB is free. Including non-PyTorch memory, this process has 23.24 GiB memory in use. Of the allocated memory 21.48 GiB is allocated by PyTorch, and 544.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model tf_efficientnet_b7.ra_in1k created, param count: 66347960
Running train benchmark on tf_efficientnet_b7.ra_in1k for 40 steps w/ input size (3, 600, 600) and batch size 32.
ERROR: "CUDA out of memory. Tried to allocate 396.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 130.06 MiB is free. Including non-PyTorch memory, this process has 23.51 GiB memory in use. Of the allocated memory 22.10 GiB is allocated by PyTorch, and 185.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model tf_efficientnet_b7.ra_in1k created, param count: 66347960
Running train benchmark on tf_efficientnet_b7.ra_in1k for 40 steps w/ input size (3, 600, 600) and batch size 24.
ERROR: "CUDA out of memory. Tried to allocate 124.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 116.06 MiB is free. Including non-PyTorch memory, this process has 23.53 GiB memory in use. Of the allocated memory 22.01 GiB is allocated by PyTorch, and 288.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model tf_efficientnet_b7.ra_in1k created, param count: 66347960
Running train benchmark on tf_efficientnet_b7.ra_in1k for 40 steps w/ input size (3, 600, 600) and batch size 16.
ERROR: "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 16.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.21 GiB is allocated by PyTorch, and 183.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 12 for retry.
Model tf_efficientnet_b7.ra_in1k created, param count: 66347960
Running train benchmark on tf_efficientnet_b7.ra_in1k for 40 steps w/ input size (3, 600, 600) and batch size 12.
ERROR: "CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 2.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 21.72 GiB is allocated by PyTorch, and 706.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 8 for retry.
Model tf_efficientnet_b7.ra_in1k created, param count: 66347960
Running train benchmark on tf_efficientnet_b7.ra_in1k for 40 steps w/ input size (3, 600, 600) and batch size 8.
Train [8/40]. 43.26 samples/sec. 184.927 ms/step.
Train [16/40]. 43.30 samples/sec. 184.752 ms/step.
Train [24/40]. 43.32 samples/sec. 184.654 ms/step.
Train [32/40]. 43.33 samples/sec. 184.631 ms/step.
Train [40/40]. 43.34 samples/sec. 184.583 ms/step.
Train benchmark of tf_efficientnet_b7.ra_in1k done. 42.81 samples/sec, 184.58 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tf_efficientnet_b8.ap_in1k created, param count: 87413142
Running inference benchmark on tf_efficientnet_b8.ap_in1k for 40 steps w/ input size (3, 672, 672) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 10.34 GiB. GPU 0 has a total capacty of 23.65 GiB of which 5.61 GiB is free. Including non-PyTorch memory, this process has 18.04 GiB memory in use. Of the allocated memory 16.76 GiB is allocated by PyTorch, and 40.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model tf_efficientnet_b8.ap_in1k created, param count: 87413142
Running inference benchmark on tf_efficientnet_b8.ap_in1k for 40 steps w/ input size (3, 672, 672) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 1.94 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.68 GiB is free. Including non-PyTorch memory, this process has 21.96 GiB memory in use. Of the allocated memory 20.42 GiB is allocated by PyTorch, and 316.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model tf_efficientnet_b8.ap_in1k created, param count: 87413142
Running inference benchmark on tf_efficientnet_b8.ap_in1k for 40 steps w/ input size (3, 672, 672) and batch size 128.
ERROR: "Expected canUse32BitIndexMath(input) && canUse32BitIndexMath(output) to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model tf_efficientnet_b8.ap_in1k created, param count: 87413142
Running inference benchmark on tf_efficientnet_b8.ap_in1k for 40 steps w/ input size (3, 672, 672) and batch size 96.
Infer [8/40]. 122.69 samples/sec. 782.468 ms/step.
Infer [16/40]. 122.69 samples/sec. 782.476 ms/step.
Infer [24/40]. 122.69 samples/sec. 782.485 ms/step.
Infer [32/40]. 122.68 samples/sec. 782.499 ms/step.
Infer [40/40]. 122.68 samples/sec. 782.512 ms/step.
Inference benchmark of tf_efficientnet_b8.ap_in1k done. 122.67 samples/sec, 782.51 ms/step
Model tf_efficientnet_b8.ap_in1k created, param count: 87413142
Running train benchmark on tf_efficientnet_b8.ap_in1k for 40 steps w/ input size (3, 672, 672) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 3.88 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.51 GiB is free. Including non-PyTorch memory, this process has 22.13 GiB memory in use. Of the allocated memory 20.86 GiB is allocated by PyTorch, and 40.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model tf_efficientnet_b8.ap_in1k created, param count: 87413142
Running train benchmark on tf_efficientnet_b8.ap_in1k for 40 steps w/ input size (3, 672, 672) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 1.29 GiB. GPU 0 has a total capacty of 23.65 GiB of which 606.06 MiB is free. Including non-PyTorch memory, this process has 23.05 GiB memory in use. Of the allocated memory 21.50 GiB is allocated by PyTorch, and 324.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model tf_efficientnet_b8.ap_in1k created, param count: 87413142
Running train benchmark on tf_efficientnet_b8.ap_in1k for 40 steps w/ input size (3, 672, 672) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 882.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 698.06 MiB is free. Including non-PyTorch memory, this process has 22.96 GiB memory in use. Of the allocated memory 21.29 GiB is allocated by PyTorch, and 451.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model tf_efficientnet_b8.ap_in1k created, param count: 87413142
Running train benchmark on tf_efficientnet_b8.ap_in1k for 40 steps w/ input size (3, 672, 672) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 662.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 296.06 MiB is free. Including non-PyTorch memory, this process has 23.35 GiB memory in use. Of the allocated memory 21.83 GiB is allocated by PyTorch, and 295.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model tf_efficientnet_b8.ap_in1k created, param count: 87413142
Running train benchmark on tf_efficientnet_b8.ap_in1k for 40 steps w/ input size (3, 672, 672) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 2.58 GiB. GPU 0 has a total capacty of 23.65 GiB of which 888.06 MiB is free. Including non-PyTorch memory, this process has 22.77 GiB memory in use. Of the allocated memory 21.09 GiB is allocated by PyTorch, and 462.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model tf_efficientnet_b8.ap_in1k created, param count: 87413142
Running train benchmark on tf_efficientnet_b8.ap_in1k for 40 steps w/ input size (3, 672, 672) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 870.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 246.06 MiB is free. Including non-PyTorch memory, this process has 23.40 GiB memory in use. Of the allocated memory 21.71 GiB is allocated by PyTorch, and 466.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model tf_efficientnet_b8.ap_in1k created, param count: 87413142
Running train benchmark on tf_efficientnet_b8.ap_in1k for 40 steps w/ input size (3, 672, 672) and batch size 32.
ERROR: "CUDA out of memory. Tried to allocate 580.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 460.06 MiB is free. Including non-PyTorch memory, this process has 23.19 GiB memory in use. Of the allocated memory 21.70 GiB is allocated by PyTorch, and 266.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model tf_efficientnet_b8.ap_in1k created, param count: 87413142
Running train benchmark on tf_efficientnet_b8.ap_in1k for 40 steps w/ input size (3, 672, 672) and batch size 24.
ERROR: "CUDA out of memory. Tried to allocate 436.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 422.06 MiB is free. Including non-PyTorch memory, this process has 23.23 GiB memory in use. Of the allocated memory 21.56 GiB is allocated by PyTorch, and 448.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model tf_efficientnet_b8.ap_in1k created, param count: 87413142
Running train benchmark on tf_efficientnet_b8.ap_in1k for 40 steps w/ input size (3, 672, 672) and batch size 16.
ERROR: "CUDA out of memory. Tried to allocate 290.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 40.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 21.90 GiB is allocated by PyTorch, and 481.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 12 for retry.
Model tf_efficientnet_b8.ap_in1k created, param count: 87413142
Running train benchmark on tf_efficientnet_b8.ap_in1k for 40 steps w/ input size (3, 672, 672) and batch size 12.
ERROR: "CUDA out of memory. Tried to allocate 44.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 34.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 22.17 GiB is allocated by PyTorch, and 211.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 8 for retry.
Model tf_efficientnet_b8.ap_in1k created, param count: 87413142
Running train benchmark on tf_efficientnet_b8.ap_in1k for 40 steps w/ input size (3, 672, 672) and batch size 8.
ERROR: "CUDA out of memory. Tried to allocate 30.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 14.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.00 GiB is allocated by PyTorch, and 404.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 6 for retry.
Model tf_efficientnet_b8.ap_in1k created, param count: 87413142
Running train benchmark on tf_efficientnet_b8.ap_in1k for 40 steps w/ input size (3, 672, 672) and batch size 6.
Train [8/40]. 30.34 samples/sec. 197.785 ms/step.
Train [16/40]. 30.33 samples/sec. 197.794 ms/step.
Train [24/40]. 30.33 samples/sec. 197.801 ms/step.
Train [32/40]. 30.34 samples/sec. 197.777 ms/step.
Train [40/40]. 30.34 samples/sec. 197.787 ms/step.
Train benchmark of tf_efficientnet_b8.ap_in1k done. 29.97 samples/sec, 197.79 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tf_efficientnet_b8.ra_in1k created, param count: 87413142
Running inference benchmark on tf_efficientnet_b8.ra_in1k for 40 steps w/ input size (3, 672, 672) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 10.34 GiB. GPU 0 has a total capacty of 23.65 GiB of which 3.45 GiB is free. Including non-PyTorch memory, this process has 20.19 GiB memory in use. Of the allocated memory 16.76 GiB is allocated by PyTorch, and 2.19 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model tf_efficientnet_b8.ra_in1k created, param count: 87413142
Running inference benchmark on tf_efficientnet_b8.ra_in1k for 40 steps w/ input size (3, 672, 672) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 1.94 GiB. GPU 0 has a total capacty of 23.65 GiB of which 70.06 MiB is free. Including non-PyTorch memory, this process has 23.57 GiB memory in use. Of the allocated memory 20.41 GiB is allocated by PyTorch, and 1.93 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model tf_efficientnet_b8.ra_in1k created, param count: 87413142
Running inference benchmark on tf_efficientnet_b8.ra_in1k for 40 steps w/ input size (3, 672, 672) and batch size 128.
ERROR: "Expected canUse32BitIndexMath(input) && canUse32BitIndexMath(output) to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model tf_efficientnet_b8.ra_in1k created, param count: 87413142
Running inference benchmark on tf_efficientnet_b8.ra_in1k for 40 steps w/ input size (3, 672, 672) and batch size 96.
Infer [8/40]. 122.71 samples/sec. 782.309 ms/step.
Infer [16/40]. 122.70 samples/sec. 782.370 ms/step.
Infer [24/40]. 122.70 samples/sec. 782.392 ms/step.
Infer [32/40]. 122.70 samples/sec. 782.413 ms/step.
Infer [40/40]. 122.69 samples/sec. 782.436 ms/step.
Inference benchmark of tf_efficientnet_b8.ra_in1k done. 122.69 samples/sec, 782.44 ms/step
Model tf_efficientnet_b8.ra_in1k created, param count: 87413142
Running train benchmark on tf_efficientnet_b8.ra_in1k for 40 steps w/ input size (3, 672, 672) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 3.88 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.51 GiB is free. Including non-PyTorch memory, this process has 22.13 GiB memory in use. Of the allocated memory 20.86 GiB is allocated by PyTorch, and 40.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model tf_efficientnet_b8.ra_in1k created, param count: 87413142
Running train benchmark on tf_efficientnet_b8.ra_in1k for 40 steps w/ input size (3, 672, 672) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 1.29 GiB. GPU 0 has a total capacty of 23.65 GiB of which 606.06 MiB is free. Including non-PyTorch memory, this process has 23.05 GiB memory in use. Of the allocated memory 21.50 GiB is allocated by PyTorch, and 324.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model tf_efficientnet_b8.ra_in1k created, param count: 87413142
Running train benchmark on tf_efficientnet_b8.ra_in1k for 40 steps w/ input size (3, 672, 672) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 882.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 698.06 MiB is free. Including non-PyTorch memory, this process has 22.96 GiB memory in use. Of the allocated memory 21.29 GiB is allocated by PyTorch, and 451.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model tf_efficientnet_b8.ra_in1k created, param count: 87413142
Running train benchmark on tf_efficientnet_b8.ra_in1k for 40 steps w/ input size (3, 672, 672) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 662.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 296.06 MiB is free. Including non-PyTorch memory, this process has 23.35 GiB memory in use. Of the allocated memory 21.83 GiB is allocated by PyTorch, and 295.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model tf_efficientnet_b8.ra_in1k created, param count: 87413142
Running train benchmark on tf_efficientnet_b8.ra_in1k for 40 steps w/ input size (3, 672, 672) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 2.58 GiB. GPU 0 has a total capacty of 23.65 GiB of which 888.06 MiB is free. Including non-PyTorch memory, this process has 22.77 GiB memory in use. Of the allocated memory 21.09 GiB is allocated by PyTorch, and 462.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model tf_efficientnet_b8.ra_in1k created, param count: 87413142
Running train benchmark on tf_efficientnet_b8.ra_in1k for 40 steps w/ input size (3, 672, 672) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 870.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 784.06 MiB is free. Including non-PyTorch memory, this process has 22.88 GiB memory in use. Of the allocated memory 20.86 GiB is allocated by PyTorch, and 796.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model tf_efficientnet_b8.ra_in1k created, param count: 87413142
Running train benchmark on tf_efficientnet_b8.ra_in1k for 40 steps w/ input size (3, 672, 672) and batch size 32.
ERROR: "CUDA out of memory. Tried to allocate 580.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 464.06 MiB is free. Including non-PyTorch memory, this process has 23.19 GiB memory in use. Of the allocated memory 21.70 GiB is allocated by PyTorch, and 262.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model tf_efficientnet_b8.ra_in1k created, param count: 87413142
Running train benchmark on tf_efficientnet_b8.ra_in1k for 40 steps w/ input size (3, 672, 672) and batch size 24.
ERROR: "CUDA out of memory. Tried to allocate 436.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 22.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 21.98 GiB is allocated by PyTorch, and 413.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model tf_efficientnet_b8.ra_in1k created, param count: 87413142
Running train benchmark on tf_efficientnet_b8.ra_in1k for 40 steps w/ input size (3, 672, 672) and batch size 16.
ERROR: "CUDA out of memory. Tried to allocate 290.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 252.06 MiB is free. Including non-PyTorch memory, this process has 23.39 GiB memory in use. Of the allocated memory 21.62 GiB is allocated by PyTorch, and 559.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 12 for retry.
Model tf_efficientnet_b8.ra_in1k created, param count: 87413142
Running train benchmark on tf_efficientnet_b8.ra_in1k for 40 steps w/ input size (3, 672, 672) and batch size 12.
ERROR: "CUDA out of memory. Tried to allocate 44.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 36.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 22.17 GiB is allocated by PyTorch, and 211.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 8 for retry.
Model tf_efficientnet_b8.ra_in1k created, param count: 87413142
Running train benchmark on tf_efficientnet_b8.ra_in1k for 40 steps w/ input size (3, 672, 672) and batch size 8.
ERROR: "CUDA out of memory. Tried to allocate 30.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 14.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.00 GiB is allocated by PyTorch, and 405.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 6 for retry.
Model tf_efficientnet_b8.ra_in1k created, param count: 87413142
Running train benchmark on tf_efficientnet_b8.ra_in1k for 40 steps w/ input size (3, 672, 672) and batch size 6.
Train [8/40]. 29.24 samples/sec. 205.195 ms/step.
Train [16/40]. 29.38 samples/sec. 204.220 ms/step.
Train [24/40]. 29.42 samples/sec. 203.947 ms/step.
Train [32/40]. 29.44 samples/sec. 203.777 ms/step.
Train [40/40]. 29.46 samples/sec. 203.678 ms/step.
Train benchmark of tf_efficientnet_b8.ra_in1k done. 29.10 samples/sec, 203.68 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tf_efficientnet_cc_b0_4e.in1k created, param count: 13314116
Running inference benchmark on tf_efficientnet_cc_b0_4e.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 4655.68 samples/sec. 54.987 ms/step.
Infer [16/40]. 4656.69 samples/sec. 54.975 ms/step.
Infer [24/40]. 4657.57 samples/sec. 54.964 ms/step.
Infer [32/40]. 4656.51 samples/sec. 54.977 ms/step.
Infer [40/40]. 4656.64 samples/sec. 54.975 ms/step.
Inference benchmark of tf_efficientnet_cc_b0_4e.in1k done. 4654.64 samples/sec, 54.98 ms/step
Model tf_efficientnet_cc_b0_4e.in1k created, param count: 13314116
Running train benchmark on tf_efficientnet_cc_b0_4e.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1152.68 samples/sec. 222.091 ms/step.
Train [16/40]. 1152.49 samples/sec. 222.128 ms/step.
Train [24/40]. 1152.36 samples/sec. 222.152 ms/step.
Train [32/40]. 1152.43 samples/sec. 222.139 ms/step.
Train [40/40]. 1152.31 samples/sec. 222.162 ms/step.
Train benchmark of tf_efficientnet_cc_b0_4e.in1k done. 1146.90 samples/sec, 222.16 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tf_efficientnet_cc_b0_8e.in1k created, param count: 24013284
Running inference benchmark on tf_efficientnet_cc_b0_8e.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 4805.39 samples/sec. 53.273 ms/step.
Infer [16/40]. 4803.51 samples/sec. 53.294 ms/step.
Infer [24/40]. 4795.63 samples/sec. 53.382 ms/step.
Infer [32/40]. 4798.32 samples/sec. 53.352 ms/step.
Infer [40/40]. 4798.38 samples/sec. 53.351 ms/step.
Inference benchmark of tf_efficientnet_cc_b0_8e.in1k done. 4796.32 samples/sec, 53.35 ms/step
Model tf_efficientnet_cc_b0_8e.in1k created, param count: 24013284
Running train benchmark on tf_efficientnet_cc_b0_8e.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1165.33 samples/sec. 219.679 ms/step.
Train [16/40]. 1165.23 samples/sec. 219.700 ms/step.
Train [24/40]. 1165.26 samples/sec. 219.693 ms/step.
Train [32/40]. 1165.18 samples/sec. 219.708 ms/step.
Train [40/40]. 1165.08 samples/sec. 219.727 ms/step.
Train benchmark of tf_efficientnet_cc_b0_8e.in1k done. 1159.65 samples/sec, 219.73 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tf_efficientnet_cc_b1_8e.in1k created, param count: 39715968
Running inference benchmark on tf_efficientnet_cc_b1_8e.in1k for 40 steps w/ input size (3, 240, 240) and batch size 256.
Infer [8/40]. 3450.95 samples/sec. 74.182 ms/step.
Infer [16/40]. 3447.90 samples/sec. 74.248 ms/step.
Infer [24/40]. 3449.39 samples/sec. 74.216 ms/step.
Infer [32/40]. 3448.49 samples/sec. 74.235 ms/step.
Infer [40/40]. 3449.80 samples/sec. 74.207 ms/step.
Inference benchmark of tf_efficientnet_cc_b1_8e.in1k done. 3448.73 samples/sec, 74.21 ms/step
Model tf_efficientnet_cc_b1_8e.in1k created, param count: 39715968
Running train benchmark on tf_efficientnet_cc_b1_8e.in1k for 40 steps w/ input size (3, 240, 240) and batch size 256.
Train [8/40]. 737.33 samples/sec. 347.198 ms/step.
Train [16/40]. 737.46 samples/sec. 347.135 ms/step.
Train [24/40]. 737.43 samples/sec. 347.153 ms/step.
Train [32/40]. 737.41 samples/sec. 347.160 ms/step.
Train [40/40]. 737.40 samples/sec. 347.168 ms/step.
Train benchmark of tf_efficientnet_cc_b1_8e.in1k done. 734.36 samples/sec, 347.17 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tf_efficientnet_el.in1k created, param count: 10589712
Running inference benchmark on tf_efficientnet_el.in1k for 40 steps w/ input size (3, 300, 300) and batch size 256.
Infer [8/40]. 1765.45 samples/sec. 145.006 ms/step.
Infer [16/40]. 1765.47 samples/sec. 145.004 ms/step.
Infer [24/40]. 1765.46 samples/sec. 145.005 ms/step.
Infer [32/40]. 1765.45 samples/sec. 145.006 ms/step.
Infer [40/40]. 1765.45 samples/sec. 145.006 ms/step.
Inference benchmark of tf_efficientnet_el.in1k done. 1765.05 samples/sec, 145.01 ms/step
Model tf_efficientnet_el.in1k created, param count: 10589712
Running train benchmark on tf_efficientnet_el.in1k for 40 steps w/ input size (3, 300, 300) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 158.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 60.06 MiB is free. Including non-PyTorch memory, this process has 23.58 GiB memory in use. Of the allocated memory 22.23 GiB is allocated by PyTorch, and 121.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model tf_efficientnet_el.in1k created, param count: 10589712
Running train benchmark on tf_efficientnet_el.in1k for 40 steps w/ input size (3, 300, 300) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 68.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 14.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.19 GiB is allocated by PyTorch, and 204.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model tf_efficientnet_el.in1k created, param count: 10589712
Running train benchmark on tf_efficientnet_el.in1k for 40 steps w/ input size (3, 300, 300) and batch size 128.
Train [8/40]. 483.65 samples/sec. 264.656 ms/step.
Train [16/40]. 483.63 samples/sec. 264.664 ms/step.
Train [24/40]. 483.66 samples/sec. 264.650 ms/step.
Train [32/40]. 483.64 samples/sec. 264.657 ms/step.
Train [40/40]. 483.62 samples/sec. 264.668 ms/step.
Train benchmark of tf_efficientnet_el.in1k done. 481.48 samples/sec, 264.67 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tf_efficientnet_em.in1k created, param count: 6899496
Running inference benchmark on tf_efficientnet_em.in1k for 40 steps w/ input size (3, 240, 240) and batch size 256.
Infer [8/40]. 3972.87 samples/sec. 64.437 ms/step.
Infer [16/40]. 3971.75 samples/sec. 64.455 ms/step.
Infer [24/40]. 3971.32 samples/sec. 64.462 ms/step.
Infer [32/40]. 3971.17 samples/sec. 64.465 ms/step.
Infer [40/40]. 3971.10 samples/sec. 64.466 ms/step.
Inference benchmark of tf_efficientnet_em.in1k done. 3969.51 samples/sec, 64.47 ms/step
Model tf_efficientnet_em.in1k created, param count: 6899496
Running train benchmark on tf_efficientnet_em.in1k for 40 steps w/ input size (3, 240, 240) and batch size 256.
Train [8/40]. 1028.95 samples/sec. 248.796 ms/step.
Train [16/40]. 1028.95 samples/sec. 248.798 ms/step.
Train [24/40]. 1028.95 samples/sec. 248.797 ms/step.
Train [32/40]. 1028.95 samples/sec. 248.797 ms/step.
Train [40/40]. 1028.94 samples/sec. 248.799 ms/step.
Train benchmark of tf_efficientnet_em.in1k done. 1024.39 samples/sec, 248.80 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tf_efficientnet_es.in1k created, param count: 5438392
Running inference benchmark on tf_efficientnet_es.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 6491.46 samples/sec. 39.436 ms/step.
Infer [16/40]. 6490.19 samples/sec. 39.444 ms/step.
Infer [24/40]. 6489.64 samples/sec. 39.447 ms/step.
Infer [32/40]. 6489.55 samples/sec. 39.448 ms/step.
Infer [40/40]. 6489.58 samples/sec. 39.448 ms/step.
Inference benchmark of tf_efficientnet_es.in1k done. 6485.31 samples/sec, 39.45 ms/step
Model tf_efficientnet_es.in1k created, param count: 5438392
Running train benchmark on tf_efficientnet_es.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1711.75 samples/sec. 149.554 ms/step.
Train [16/40]. 1711.79 samples/sec. 149.551 ms/step.
Train [24/40]. 1711.84 samples/sec. 149.547 ms/step.
Train [32/40]. 1711.81 samples/sec. 149.549 ms/step.
Train [40/40]. 1711.83 samples/sec. 149.547 ms/step.
Train benchmark of tf_efficientnet_es.in1k done. 1703.54 samples/sec, 149.55 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tf_efficientnet_l2.ns_jft_in1k created, param count: 480309308
Running inference benchmark on tf_efficientnet_l2.ns_jft_in1k for 40 steps w/ input size (3, 800, 800) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 10.38 GiB. GPU 0 has a total capacty of 23.65 GiB of which 10.19 GiB is free. Including non-PyTorch memory, this process has 13.45 GiB memory in use. Of the allocated memory 12.21 GiB is allocated by PyTorch, and 10.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model tf_efficientnet_l2.ns_jft_in1k created, param count: 480309308
Running inference benchmark on tf_efficientnet_l2.ns_jft_in1k for 40 steps w/ input size (3, 800, 800) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 23.65 GiB of which 5.06 GiB is free. Including non-PyTorch memory, this process has 18.58 GiB memory in use. Of the allocated memory 17.18 GiB is allocated by PyTorch, and 174.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model tf_efficientnet_l2.ns_jft_in1k created, param count: 480309308
Running inference benchmark on tf_efficientnet_l2.ns_jft_in1k for 40 steps w/ input size (3, 800, 800) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 16.48 GiB. GPU 0 has a total capacty of 23.65 GiB of which 10.18 GiB is free. Including non-PyTorch memory, this process has 13.46 GiB memory in use. Of the allocated memory 9.32 GiB is allocated by PyTorch, and 2.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model tf_efficientnet_l2.ns_jft_in1k created, param count: 480309308
Running inference benchmark on tf_efficientnet_l2.ns_jft_in1k for 40 steps w/ input size (3, 800, 800) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 12.36 GiB. GPU 0 has a total capacty of 23.65 GiB of which 592.06 MiB is free. Including non-PyTorch memory, this process has 23.06 GiB memory in use. Of the allocated memory 19.58 GiB is allocated by PyTorch, and 2.25 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model tf_efficientnet_l2.ns_jft_in1k created, param count: 480309308
Running inference benchmark on tf_efficientnet_l2.ns_jft_in1k for 40 steps w/ input size (3, 800, 800) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 8.24 GiB. GPU 0 has a total capacty of 23.65 GiB of which 7.14 GiB is free. Including non-PyTorch memory, this process has 16.50 GiB memory in use. Of the allocated memory 13.36 GiB is allocated by PyTorch, and 1.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model tf_efficientnet_l2.ns_jft_in1k created, param count: 480309308
Running inference benchmark on tf_efficientnet_l2.ns_jft_in1k for 40 steps w/ input size (3, 800, 800) and batch size 48.
Infer [8/40]. 30.51 samples/sec. 1573.376 ms/step.
Infer [16/40]. 30.51 samples/sec. 1573.450 ms/step.
Infer [24/40]. 30.50 samples/sec. 1573.559 ms/step.
Infer [32/40]. 30.50 samples/sec. 1573.646 ms/step.
Infer [40/40]. 30.50 samples/sec. 1573.711 ms/step.
Inference benchmark of tf_efficientnet_l2.ns_jft_in1k done. 30.50 samples/sec, 1573.71 ms/step
Model tf_efficientnet_l2.ns_jft_in1k created, param count: 480309308
Running train benchmark on tf_efficientnet_l2.ns_jft_in1k for 40 steps w/ input size (3, 800, 800) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 10.38 GiB. GPU 0 has a total capacty of 23.65 GiB of which 9.27 GiB is free. Including non-PyTorch memory, this process has 14.37 GiB memory in use. Of the allocated memory 13.12 GiB is allocated by PyTorch, and 10.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model tf_efficientnet_l2.ns_jft_in1k created, param count: 480309308
Running train benchmark on tf_efficientnet_l2.ns_jft_in1k for 40 steps w/ input size (3, 800, 800) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 23.65 GiB of which 4.14 GiB is free. Including non-PyTorch memory, this process has 19.50 GiB memory in use. Of the allocated memory 17.87 GiB is allocated by PyTorch, and 407.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model tf_efficientnet_l2.ns_jft_in1k created, param count: 480309308
Running train benchmark on tf_efficientnet_l2.ns_jft_in1k for 40 steps w/ input size (3, 800, 800) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 5.19 GiB. GPU 0 has a total capacty of 23.65 GiB of which 4.53 GiB is free. Including non-PyTorch memory, this process has 19.11 GiB memory in use. Of the allocated memory 17.41 GiB is allocated by PyTorch, and 480.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model tf_efficientnet_l2.ns_jft_in1k created, param count: 480309308
Running train benchmark on tf_efficientnet_l2.ns_jft_in1k for 40 steps w/ input size (3, 800, 800) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 3.89 GiB. GPU 0 has a total capacty of 23.65 GiB of which 938.06 MiB is free. Including non-PyTorch memory, this process has 22.72 GiB memory in use. Of the allocated memory 21.07 GiB is allocated by PyTorch, and 434.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model tf_efficientnet_l2.ns_jft_in1k created, param count: 480309308
Running train benchmark on tf_efficientnet_l2.ns_jft_in1k for 40 steps w/ input size (3, 800, 800) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 1.38 GiB. GPU 0 has a total capacty of 23.65 GiB of which 812.06 MiB is free. Including non-PyTorch memory, this process has 22.85 GiB memory in use. Of the allocated memory 20.92 GiB is allocated by PyTorch, and 716.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model tf_efficientnet_l2.ns_jft_in1k created, param count: 480309308
Running train benchmark on tf_efficientnet_l2.ns_jft_in1k for 40 steps w/ input size (3, 800, 800) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 1.03 GiB. GPU 0 has a total capacty of 23.65 GiB of which 784.06 MiB is free. Including non-PyTorch memory, this process has 22.88 GiB memory in use. Of the allocated memory 21.07 GiB is allocated by PyTorch, and 587.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model tf_efficientnet_l2.ns_jft_in1k created, param count: 480309308
Running train benchmark on tf_efficientnet_l2.ns_jft_in1k for 40 steps w/ input size (3, 800, 800) and batch size 32.
ERROR: "CUDA out of memory. Tried to allocate 704.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 492.06 MiB is free. Including non-PyTorch memory, this process has 23.16 GiB memory in use. Of the allocated memory 21.23 GiB is allocated by PyTorch, and 711.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model tf_efficientnet_l2.ns_jft_in1k created, param count: 480309308
Running train benchmark on tf_efficientnet_l2.ns_jft_in1k for 40 steps w/ input size (3, 800, 800) and batch size 24.
ERROR: "CUDA out of memory. Tried to allocate 528.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 488.06 MiB is free. Including non-PyTorch memory, this process has 23.16 GiB memory in use. Of the allocated memory 21.32 GiB is allocated by PyTorch, and 629.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model tf_efficientnet_l2.ns_jft_in1k created, param count: 480309308
Running train benchmark on tf_efficientnet_l2.ns_jft_in1k for 40 steps w/ input size (3, 800, 800) and batch size 16.
ERROR: "CUDA out of memory. Tried to allocate 2.06 GiB. GPU 0 has a total capacty of 23.65 GiB of which 934.06 MiB is free. Including non-PyTorch memory, this process has 22.73 GiB memory in use. Of the allocated memory 20.70 GiB is allocated by PyTorch, and 813.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 12 for retry.
Model tf_efficientnet_l2.ns_jft_in1k created, param count: 480309308
Running train benchmark on tf_efficientnet_l2.ns_jft_in1k for 40 steps w/ input size (3, 800, 800) and batch size 12.
ERROR: "CUDA out of memory. Tried to allocate 572.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 474.06 MiB is free. Including non-PyTorch memory, this process has 23.18 GiB memory in use. Of the allocated memory 21.28 GiB is allocated by PyTorch, and 683.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 8 for retry.
Model tf_efficientnet_l2.ns_jft_in1k created, param count: 480309308
Running train benchmark on tf_efficientnet_l2.ns_jft_in1k for 40 steps w/ input size (3, 800, 800) and batch size 8.
ERROR: "CUDA out of memory. Tried to allocate 382.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 164.06 MiB is free. Including non-PyTorch memory, this process has 23.48 GiB memory in use. Of the allocated memory 21.56 GiB is allocated by PyTorch, and 706.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 6 for retry.
Model tf_efficientnet_l2.ns_jft_in1k created, param count: 480309308
Running train benchmark on tf_efficientnet_l2.ns_jft_in1k for 40 steps w/ input size (3, 800, 800) and batch size 6.
ERROR: "CUDA out of memory. Tried to allocate 286.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 252.06 MiB is free. Including non-PyTorch memory, this process has 23.39 GiB memory in use. Of the allocated memory 21.63 GiB is allocated by PyTorch, and 541.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 4 for retry.
Model tf_efficientnet_l2.ns_jft_in1k created, param count: 480309308
Running train benchmark on tf_efficientnet_l2.ns_jft_in1k for 40 steps w/ input size (3, 800, 800) and batch size 4.
ERROR: "CUDA out of memory. Tried to allocate 82.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 52.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 21.70 GiB is allocated by PyTorch, and 675.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 3 for retry.
Model tf_efficientnet_l2.ns_jft_in1k created, param count: 480309308
Running train benchmark on tf_efficientnet_l2.ns_jft_in1k for 40 steps w/ input size (3, 800, 800) and batch size 3.
ERROR: "CUDA out of memory. Tried to allocate 30.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 20.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 21.67 GiB is allocated by PyTorch, and 738.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 2 for retry.
Model tf_efficientnet_l2.ns_jft_in1k created, param count: 480309308
Running train benchmark on tf_efficientnet_l2.ns_jft_in1k for 40 steps w/ input size (3, 800, 800) and batch size 2.
ERROR: "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 6.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.03 GiB is allocated by PyTorch, and 378.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 1 for retry.
Model tf_efficientnet_l2.ns_jft_in1k created, param count: 480309308
Running train benchmark on tf_efficientnet_l2.ns_jft_in1k for 40 steps w/ input size (3, 800, 800) and batch size 1.
Train [8/40]. 4.09 samples/sec. 244.733 ms/step.
Train [16/40]. 4.15 samples/sec. 241.127 ms/step.
Train [24/40]. 4.17 samples/sec. 239.959 ms/step.
Train [32/40]. 4.18 samples/sec. 239.403 ms/step.
Train [40/40]. 4.18 samples/sec. 239.030 ms/step.
Train benchmark of tf_efficientnet_l2.ns_jft_in1k done. 4.12 samples/sec, 239.03 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tf_efficientnet_l2.ns_jft_in1k_475 created, param count: 480309308
Running inference benchmark on tf_efficientnet_l2.ns_jft_in1k_475 for 40 steps w/ input size (3, 475, 475) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 11.67 GiB. GPU 0 has a total capacty of 23.65 GiB of which 2.14 GiB is free. Including non-PyTorch memory, this process has 21.50 GiB memory in use. Of the allocated memory 18.52 GiB is allocated by PyTorch, and 1.74 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model tf_efficientnet_l2.ns_jft_in1k_475 created, param count: 480309308
Running inference benchmark on tf_efficientnet_l2.ns_jft_in1k_475 for 40 steps w/ input size (3, 475, 475) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 8.75 GiB. GPU 0 has a total capacty of 23.65 GiB of which 6.04 GiB is free. Including non-PyTorch memory, this process has 17.60 GiB memory in use. Of the allocated memory 14.14 GiB is allocated by PyTorch, and 2.23 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model tf_efficientnet_l2.ns_jft_in1k_475 created, param count: 480309308
Running inference benchmark on tf_efficientnet_l2.ns_jft_in1k_475 for 40 steps w/ input size (3, 475, 475) and batch size 128.
Infer [8/40]. 83.55 samples/sec. 1532.042 ms/step.
Infer [16/40]. 83.53 samples/sec. 1532.321 ms/step.
Infer [24/40]. 83.53 samples/sec. 1532.381 ms/step.
Infer [32/40]. 83.53 samples/sec. 1532.421 ms/step.
Infer [40/40]. 83.53 samples/sec. 1532.456 ms/step.
Inference benchmark of tf_efficientnet_l2.ns_jft_in1k_475 done. 83.52 samples/sec, 1532.46 ms/step
Model tf_efficientnet_l2.ns_jft_in1k_475 created, param count: 480309308
Running train benchmark on tf_efficientnet_l2.ns_jft_in1k_475 for 40 steps w/ input size (3, 475, 475) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 3.67 GiB. GPU 0 has a total capacty of 23.65 GiB of which 2.46 GiB is free. Including non-PyTorch memory, this process has 21.18 GiB memory in use. Of the allocated memory 19.93 GiB is allocated by PyTorch, and 11.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model tf_efficientnet_l2.ns_jft_in1k_475 created, param count: 480309308
Running train benchmark on tf_efficientnet_l2.ns_jft_in1k_475 for 40 steps w/ input size (3, 475, 475) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 1.46 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.01 GiB is free. Including non-PyTorch memory, this process has 22.63 GiB memory in use. Of the allocated memory 20.71 GiB is allocated by PyTorch, and 709.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model tf_efficientnet_l2.ns_jft_in1k_475 created, param count: 480309308
Running train benchmark on tf_efficientnet_l2.ns_jft_in1k_475 for 40 steps w/ input size (3, 475, 475) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 996.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 130.06 MiB is free. Including non-PyTorch memory, this process has 23.51 GiB memory in use. Of the allocated memory 21.88 GiB is allocated by PyTorch, and 412.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model tf_efficientnet_l2.ns_jft_in1k_475 created, param count: 480309308
Running train benchmark on tf_efficientnet_l2.ns_jft_in1k_475 for 40 steps w/ input size (3, 475, 475) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 748.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 578.06 MiB is free. Including non-PyTorch memory, this process has 23.08 GiB memory in use. Of the allocated memory 21.02 GiB is allocated by PyTorch, and 840.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model tf_efficientnet_l2.ns_jft_in1k_475 created, param count: 480309308
Running train benchmark on tf_efficientnet_l2.ns_jft_in1k_475 for 40 steps w/ input size (3, 475, 475) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 498.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 310.06 MiB is free. Including non-PyTorch memory, this process has 23.34 GiB memory in use. Of the allocated memory 21.61 GiB is allocated by PyTorch, and 506.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model tf_efficientnet_l2.ns_jft_in1k_475 created, param count: 480309308
Running train benchmark on tf_efficientnet_l2.ns_jft_in1k_475 for 40 steps w/ input size (3, 475, 475) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 2.19 GiB. GPU 0 has a total capacty of 23.65 GiB of which 2.17 GiB is free. Including non-PyTorch memory, this process has 21.47 GiB memory in use. Of the allocated memory 19.74 GiB is allocated by PyTorch, and 501.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model tf_efficientnet_l2.ns_jft_in1k_475 created, param count: 480309308
Running train benchmark on tf_efficientnet_l2.ns_jft_in1k_475 for 40 steps w/ input size (3, 475, 475) and batch size 32.
ERROR: "CUDA out of memory. Tried to allocate 540.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 328.06 MiB is free. Including non-PyTorch memory, this process has 23.32 GiB memory in use. Of the allocated memory 21.18 GiB is allocated by PyTorch, and 927.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model tf_efficientnet_l2.ns_jft_in1k_475 created, param count: 480309308
Running train benchmark on tf_efficientnet_l2.ns_jft_in1k_475 for 40 steps w/ input size (3, 475, 475) and batch size 24.
ERROR: "CUDA out of memory. Tried to allocate 406.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 78.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 21.91 GiB is allocated by PyTorch, and 428.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model tf_efficientnet_l2.ns_jft_in1k_475 created, param count: 480309308
Running train benchmark on tf_efficientnet_l2.ns_jft_in1k_475 for 40 steps w/ input size (3, 475, 475) and batch size 16.
ERROR: "CUDA out of memory. Tried to allocate 270.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 264.06 MiB is free. Including non-PyTorch memory, this process has 23.38 GiB memory in use. Of the allocated memory 21.34 GiB is allocated by PyTorch, and 831.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 12 for retry.
Model tf_efficientnet_l2.ns_jft_in1k_475 created, param count: 480309308
Running train benchmark on tf_efficientnet_l2.ns_jft_in1k_475 for 40 steps w/ input size (3, 475, 475) and batch size 12.
ERROR: "CUDA out of memory. Tried to allocate 204.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 22.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.04 GiB is allocated by PyTorch, and 350.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 8 for retry.
Model tf_efficientnet_l2.ns_jft_in1k_475 created, param count: 480309308
Running train benchmark on tf_efficientnet_l2.ns_jft_in1k_475 for 40 steps w/ input size (3, 475, 475) and batch size 8.
ERROR: "CUDA out of memory. Tried to allocate 30.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 8.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 21.72 GiB is allocated by PyTorch, and 700.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 6 for retry.
Model tf_efficientnet_l2.ns_jft_in1k_475 created, param count: 480309308
Running train benchmark on tf_efficientnet_l2.ns_jft_in1k_475 for 40 steps w/ input size (3, 475, 475) and batch size 6.
ERROR: "CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 6.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.09 GiB is allocated by PyTorch, and 321.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 4 for retry.
Model tf_efficientnet_l2.ns_jft_in1k_475 created, param count: 480309308
Running train benchmark on tf_efficientnet_l2.ns_jft_in1k_475 for 40 steps w/ input size (3, 475, 475) and batch size 4.
Train [8/40]. 16.14 samples/sec. 247.792 ms/step.
Train [16/40]. 16.15 samples/sec. 247.745 ms/step.
Train [24/40]. 16.15 samples/sec. 247.707 ms/step.
Train [32/40]. 16.15 samples/sec. 247.723 ms/step.
Train [40/40]. 16.15 samples/sec. 247.735 ms/step.
Train benchmark of tf_efficientnet_l2.ns_jft_in1k_475 done. 15.92 samples/sec, 247.74 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tf_efficientnet_lite0.in1k created, param count: 4652008
Running inference benchmark on tf_efficientnet_lite0.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 9482.06 samples/sec. 26.998 ms/step.
Infer [16/40]. 9480.89 samples/sec. 27.002 ms/step.
Infer [24/40]. 9480.61 samples/sec. 27.002 ms/step.
Infer [32/40]. 9480.61 samples/sec. 27.002 ms/step.
Infer [40/40]. 9480.69 samples/sec. 27.002 ms/step.
Inference benchmark of tf_efficientnet_lite0.in1k done. 9472.13 samples/sec, 27.00 ms/step
Model tf_efficientnet_lite0.in1k created, param count: 4652008
Running train benchmark on tf_efficientnet_lite0.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 2125.64 samples/sec. 120.434 ms/step.
Train [16/40]. 2125.54 samples/sec. 120.440 ms/step.
Train [24/40]. 2125.62 samples/sec. 120.435 ms/step.
Train [32/40]. 2125.45 samples/sec. 120.445 ms/step.
Train [40/40]. 2125.52 samples/sec. 120.441 ms/step.
Train benchmark of tf_efficientnet_lite0.in1k done. 2113.64 samples/sec, 120.44 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tf_efficientnet_lite1.in1k created, param count: 5416680
Running inference benchmark on tf_efficientnet_lite1.in1k for 40 steps w/ input size (3, 240, 240) and batch size 256.
Infer [8/40]. 6615.42 samples/sec. 38.697 ms/step.
Infer [16/40]. 6615.39 samples/sec. 38.698 ms/step.
Infer [24/40]. 6615.37 samples/sec. 38.698 ms/step.
Infer [32/40]. 6615.25 samples/sec. 38.698 ms/step.
Infer [40/40]. 6615.28 samples/sec. 38.698 ms/step.
Inference benchmark of tf_efficientnet_lite1.in1k done. 6611.07 samples/sec, 38.70 ms/step
Model tf_efficientnet_lite1.in1k created, param count: 5416680
Running train benchmark on tf_efficientnet_lite1.in1k for 40 steps w/ input size (3, 240, 240) and batch size 256.
Train [8/40]. 1478.12 samples/sec. 173.193 ms/step.
Train [16/40]. 1477.52 samples/sec. 173.263 ms/step.
Train [24/40]. 1476.98 samples/sec. 173.327 ms/step.
Train [32/40]. 1476.72 samples/sec. 173.358 ms/step.
Train [40/40]. 1476.52 samples/sec. 173.381 ms/step.
Train benchmark of tf_efficientnet_lite1.in1k done. 1468.60 samples/sec, 173.38 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tf_efficientnet_lite2.in1k created, param count: 6092072
Running inference benchmark on tf_efficientnet_lite2.in1k for 40 steps w/ input size (3, 260, 260) and batch size 256.
Infer [8/40]. 4957.94 samples/sec. 51.634 ms/step.
Infer [16/40]. 4957.61 samples/sec. 51.638 ms/step.
Infer [24/40]. 4957.55 samples/sec. 51.638 ms/step.
Infer [32/40]. 4957.48 samples/sec. 51.639 ms/step.
Infer [40/40]. 4957.51 samples/sec. 51.639 ms/step.
Inference benchmark of tf_efficientnet_lite2.in1k done. 4955.15 samples/sec, 51.64 ms/step
Model tf_efficientnet_lite2.in1k created, param count: 6092072
Running train benchmark on tf_efficientnet_lite2.in1k for 40 steps w/ input size (3, 260, 260) and batch size 256.
Train [8/40]. 1114.63 samples/sec. 229.673 ms/step.
Train [16/40]. 1114.76 samples/sec. 229.646 ms/step.
Train [24/40]. 1114.73 samples/sec. 229.651 ms/step.
Train [32/40]. 1114.76 samples/sec. 229.646 ms/step.
Train [40/40]. 1114.79 samples/sec. 229.639 ms/step.
Train benchmark of tf_efficientnet_lite2.in1k done. 1109.58 samples/sec, 229.64 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tf_efficientnet_lite3.in1k created, param count: 8197096
Running inference benchmark on tf_efficientnet_lite3.in1k for 40 steps w/ input size (3, 300, 300) and batch size 256.
Infer [8/40]. 2897.60 samples/sec. 88.349 ms/step.
Infer [16/40]. 2897.52 samples/sec. 88.351 ms/step.
Infer [24/40]. 2897.53 samples/sec. 88.351 ms/step.
Infer [32/40]. 2897.56 samples/sec. 88.350 ms/step.
Infer [40/40]. 2897.58 samples/sec. 88.350 ms/step.
Inference benchmark of tf_efficientnet_lite3.in1k done. 2896.76 samples/sec, 88.35 ms/step
Model tf_efficientnet_lite3.in1k created, param count: 8197096
Running train benchmark on tf_efficientnet_lite3.in1k for 40 steps w/ input size (3, 300, 300) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 102.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 8.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.22 GiB is allocated by PyTorch, and 183.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model tf_efficientnet_lite3.in1k created, param count: 8197096
Running train benchmark on tf_efficientnet_lite3.in1k for 40 steps w/ input size (3, 300, 300) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 52.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 36.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 22.17 GiB is allocated by PyTorch, and 206.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model tf_efficientnet_lite3.in1k created, param count: 8197096
Running train benchmark on tf_efficientnet_lite3.in1k for 40 steps w/ input size (3, 300, 300) and batch size 128.
Train [8/40]. 727.58 samples/sec. 175.926 ms/step.
Train [16/40]. 727.67 samples/sec. 175.905 ms/step.
Train [24/40]. 727.45 samples/sec. 175.958 ms/step.
Train [32/40]. 727.22 samples/sec. 176.012 ms/step.
Train [40/40]. 727.09 samples/sec. 176.044 ms/step.
Train benchmark of tf_efficientnet_lite3.in1k done. 723.05 samples/sec, 176.04 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tf_efficientnet_lite4.in1k created, param count: 13006568
Running inference benchmark on tf_efficientnet_lite4.in1k for 40 steps w/ input size (3, 380, 380) and batch size 256.
Infer [8/40]. 1433.73 samples/sec. 178.555 ms/step.
Infer [16/40]. 1433.72 samples/sec. 178.557 ms/step.
Infer [24/40]. 1433.62 samples/sec. 178.570 ms/step.
Infer [32/40]. 1433.52 samples/sec. 178.581 ms/step.
Infer [40/40]. 1433.50 samples/sec. 178.583 ms/step.
Inference benchmark of tf_efficientnet_lite4.in1k done. 1433.23 samples/sec, 178.58 ms/step
Model tf_efficientnet_lite4.in1k created, param count: 13006568
Running train benchmark on tf_efficientnet_lite4.in1k for 40 steps w/ input size (3, 380, 380) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 848.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 240.06 MiB is free. Including non-PyTorch memory, this process has 23.41 GiB memory in use. Of the allocated memory 21.90 GiB is allocated by PyTorch, and 282.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model tf_efficientnet_lite4.in1k created, param count: 13006568
Running train benchmark on tf_efficientnet_lite4.in1k for 40 steps w/ input size (3, 380, 380) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 106.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 82.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 22.22 GiB is allocated by PyTorch, and 107.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model tf_efficientnet_lite4.in1k created, param count: 13006568
Running train benchmark on tf_efficientnet_lite4.in1k for 40 steps w/ input size (3, 380, 380) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 86.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 22.15 GiB is allocated by PyTorch, and 173.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model tf_efficientnet_lite4.in1k created, param count: 13006568
Running train benchmark on tf_efficientnet_lite4.in1k for 40 steps w/ input size (3, 380, 380) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 44.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 34.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 22.19 GiB is allocated by PyTorch, and 192.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model tf_efficientnet_lite4.in1k created, param count: 13006568
Running train benchmark on tf_efficientnet_lite4.in1k for 40 steps w/ input size (3, 380, 380) and batch size 64.
Train [8/40]. 370.12 samples/sec. 172.916 ms/step.
Train [16/40]. 369.95 samples/sec. 172.994 ms/step.
Train [24/40]. 369.87 samples/sec. 173.035 ms/step.
Train [32/40]. 369.83 samples/sec. 173.051 ms/step.
Train [40/40]. 369.81 samples/sec. 173.062 ms/step.
Train benchmark of tf_efficientnet_lite4.in1k done. 367.34 samples/sec, 173.06 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tf_efficientnetv2_b0.in1k created, param count: 7139704
Running inference benchmark on tf_efficientnetv2_b0.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 11691.13 samples/sec. 21.897 ms/step.
Infer [16/40]. 11690.87 samples/sec. 21.897 ms/step.
Infer [24/40]. 11691.12 samples/sec. 21.897 ms/step.
Infer [32/40]. 11686.57 samples/sec. 21.905 ms/step.
Infer [40/40]. 11683.69 samples/sec. 21.911 ms/step.
Inference benchmark of tf_efficientnetv2_b0.in1k done. 11670.85 samples/sec, 21.91 ms/step
Model tf_efficientnetv2_b0.in1k created, param count: 7139704
Running train benchmark on tf_efficientnetv2_b0.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 2747.50 samples/sec. 93.176 ms/step.
Train [16/40]. 2745.16 samples/sec. 93.255 ms/step.
Train [24/40]. 2744.20 samples/sec. 93.288 ms/step.
Train [32/40]. 2743.72 samples/sec. 93.304 ms/step.
Train [40/40]. 2743.43 samples/sec. 93.314 ms/step.
Train benchmark of tf_efficientnetv2_b0.in1k done. 2716.99 samples/sec, 93.31 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tf_efficientnetv2_b1.in1k created, param count: 8141052
Running inference benchmark on tf_efficientnetv2_b1.in1k for 40 steps w/ input size (3, 240, 240) and batch size 256.
Infer [8/40]. 7682.86 samples/sec. 33.321 ms/step.
Infer [16/40]. 7682.20 samples/sec. 33.324 ms/step.
Infer [24/40]. 7678.09 samples/sec. 33.342 ms/step.
Infer [32/40]. 7676.46 samples/sec. 33.349 ms/step.
Infer [40/40]. 7675.47 samples/sec. 33.353 ms/step.
Inference benchmark of tf_efficientnetv2_b1.in1k done. 7669.88 samples/sec, 33.35 ms/step
Model tf_efficientnetv2_b1.in1k created, param count: 8141052
Running train benchmark on tf_efficientnetv2_b1.in1k for 40 steps w/ input size (3, 240, 240) and batch size 256.
Train [8/40]. 1735.79 samples/sec. 147.483 ms/step.
Train [16/40]. 1735.92 samples/sec. 147.472 ms/step.
Train [24/40]. 1735.93 samples/sec. 147.472 ms/step.
Train [32/40]. 1735.88 samples/sec. 147.476 ms/step.
Train [40/40]. 1735.91 samples/sec. 147.473 ms/step.
Train benchmark of tf_efficientnetv2_b1.in1k done. 1722.75 samples/sec, 147.47 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tf_efficientnetv2_b2.in1k created, param count: 10096086
Running inference benchmark on tf_efficientnetv2_b2.in1k for 40 steps w/ input size (3, 260, 260) and batch size 256.
Infer [8/40]. 5549.04 samples/sec. 46.134 ms/step.
Infer [16/40]. 5549.30 samples/sec. 46.132 ms/step.
Infer [24/40]. 5549.33 samples/sec. 46.132 ms/step.
Infer [32/40]. 5549.04 samples/sec. 46.134 ms/step.
Infer [40/40]. 5548.97 samples/sec. 46.135 ms/step.
Inference benchmark of tf_efficientnetv2_b2.in1k done. 5545.98 samples/sec, 46.13 ms/step
Model tf_efficientnetv2_b2.in1k created, param count: 10096086
Running train benchmark on tf_efficientnetv2_b2.in1k for 40 steps w/ input size (3, 260, 260) and batch size 256.
Train [8/40]. 1257.17 samples/sec. 203.633 ms/step.
Train [16/40]. 1257.15 samples/sec. 203.636 ms/step.
Train [24/40]. 1257.10 samples/sec. 203.644 ms/step.
Train [32/40]. 1257.09 samples/sec. 203.645 ms/step.
Train [40/40]. 1257.10 samples/sec. 203.643 ms/step.
Train benchmark of tf_efficientnetv2_b2.in1k done. 1249.02 samples/sec, 203.64 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tf_efficientnetv2_b3.in1k created, param count: 14358406
Running inference benchmark on tf_efficientnetv2_b3.in1k for 40 steps w/ input size (3, 300, 300) and batch size 256.
Infer [8/40]. 3239.22 samples/sec. 79.031 ms/step.
Infer [16/40]. 3239.19 samples/sec. 79.032 ms/step.
Infer [24/40]. 3239.14 samples/sec. 79.033 ms/step.
Infer [32/40]. 3239.09 samples/sec. 79.035 ms/step.
Infer [40/40]. 3238.69 samples/sec. 79.044 ms/step.
Inference benchmark of tf_efficientnetv2_b3.in1k done. 3237.54 samples/sec, 79.04 ms/step
Model tf_efficientnetv2_b3.in1k created, param count: 14358406
Running train benchmark on tf_efficientnetv2_b3.in1k for 40 steps w/ input size (3, 300, 300) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 68.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 16.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 898.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model tf_efficientnetv2_b3.in1k created, param count: 14358406
Running train benchmark on tf_efficientnetv2_b3.in1k for 40 steps w/ input size (3, 300, 300) and batch size 192.
Train [8/40]. 798.14 samples/sec. 240.559 ms/step.
Train [16/40]. 798.05 samples/sec. 240.587 ms/step.
Train [24/40]. 798.07 samples/sec. 240.580 ms/step.
Train [32/40]. 798.06 samples/sec. 240.582 ms/step.
Train [40/40]. 798.06 samples/sec. 240.584 ms/step.
Train benchmark of tf_efficientnetv2_b3.in1k done. 793.24 samples/sec, 240.58 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tf_efficientnetv2_b3.in21k_ft_in1k created, param count: 14358406
Running inference benchmark on tf_efficientnetv2_b3.in21k_ft_in1k for 40 steps w/ input size (3, 300, 300) and batch size 256.
Infer [8/40]. 3240.51 samples/sec. 79.000 ms/step.
Infer [16/40]. 3240.49 samples/sec. 79.000 ms/step.
Infer [24/40]. 3240.52 samples/sec. 79.000 ms/step.
Infer [32/40]. 3240.50 samples/sec. 79.000 ms/step.
Infer [40/40]. 3240.45 samples/sec. 79.001 ms/step.
Inference benchmark of tf_efficientnetv2_b3.in21k_ft_in1k done. 3239.39 samples/sec, 79.00 ms/step
Model tf_efficientnetv2_b3.in21k_ft_in1k created, param count: 14358406
Running train benchmark on tf_efficientnetv2_b3.in21k_ft_in1k for 40 steps w/ input size (3, 300, 300) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 68.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 16.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 898.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model tf_efficientnetv2_b3.in21k_ft_in1k created, param count: 14358406
Running train benchmark on tf_efficientnetv2_b3.in21k_ft_in1k for 40 steps w/ input size (3, 300, 300) and batch size 192.
Train [8/40]. 797.80 samples/sec. 240.663 ms/step.
Train [16/40]. 797.80 samples/sec. 240.661 ms/step.
Train [24/40]. 797.81 samples/sec. 240.658 ms/step.
Train [32/40]. 797.82 samples/sec. 240.657 ms/step.
Train [40/40]. 797.82 samples/sec. 240.656 ms/step.
Train benchmark of tf_efficientnetv2_b3.in21k_ft_in1k done. 792.96 samples/sec, 240.66 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tf_efficientnetv2_l.in1k created, param count: 118515272
Running inference benchmark on tf_efficientnetv2_l.in1k for 40 steps w/ input size (3, 480, 480) and batch size 256.
Infer [8/40]. 318.00 samples/sec. 805.031 ms/step.
Infer [16/40]. 317.99 samples/sec. 805.052 ms/step.
Infer [24/40]. 317.99 samples/sec. 805.057 ms/step.
Infer [32/40]. 317.99 samples/sec. 805.067 ms/step.
Infer [40/40]. 317.98 samples/sec. 805.072 ms/step.
Inference benchmark of tf_efficientnetv2_l.in1k done. 317.97 samples/sec, 805.07 ms/step
Model tf_efficientnetv2_l.in1k created, param count: 118515272
Running train benchmark on tf_efficientnetv2_l.in1k for 40 steps w/ input size (3, 480, 480) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 1.76 GiB. GPU 0 has a total capacty of 23.65 GiB of which 78.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 22.00 GiB is allocated by PyTorch, and 338.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model tf_efficientnetv2_l.in1k created, param count: 118515272
Running train benchmark on tf_efficientnetv2_l.in1k for 40 steps w/ input size (3, 480, 480) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 1.32 GiB. GPU 0 has a total capacty of 23.65 GiB of which 480.06 MiB is free. Including non-PyTorch memory, this process has 23.17 GiB memory in use. Of the allocated memory 21.18 GiB is allocated by PyTorch, and 774.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model tf_efficientnetv2_l.in1k created, param count: 118515272
Running train benchmark on tf_efficientnetv2_l.in1k for 40 steps w/ input size (3, 480, 480) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 900.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 626.06 MiB is free. Including non-PyTorch memory, this process has 23.03 GiB memory in use. Of the allocated memory 21.68 GiB is allocated by PyTorch, and 123.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model tf_efficientnetv2_l.in1k created, param count: 118515272
Running train benchmark on tf_efficientnetv2_l.in1k for 40 steps w/ input size (3, 480, 480) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 254.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 124.06 MiB is free. Including non-PyTorch memory, this process has 23.52 GiB memory in use. Of the allocated memory 21.89 GiB is allocated by PyTorch, and 402.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model tf_efficientnetv2_l.in1k created, param count: 118515272
Running train benchmark on tf_efficientnetv2_l.in1k for 40 steps w/ input size (3, 480, 480) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 86.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 10.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.19 GiB is allocated by PyTorch, and 214.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model tf_efficientnetv2_l.in1k created, param count: 118515272
Running train benchmark on tf_efficientnetv2_l.in1k for 40 steps w/ input size (3, 480, 480) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 104.06 MiB is free. Including non-PyTorch memory, this process has 23.54 GiB memory in use. Of the allocated memory 22.04 GiB is allocated by PyTorch, and 277.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model tf_efficientnetv2_l.in1k created, param count: 118515272
Running train benchmark on tf_efficientnetv2_l.in1k for 40 steps w/ input size (3, 480, 480) and batch size 32.
ERROR: "CUDA out of memory. Tried to allocate 74.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 46.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 21.59 GiB is allocated by PyTorch, and 790.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model tf_efficientnetv2_l.in1k created, param count: 118515272
Running train benchmark on tf_efficientnetv2_l.in1k for 40 steps w/ input size (3, 480, 480) and batch size 24.
ERROR: "CUDA out of memory. Tried to allocate 40.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 10.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.20 GiB is allocated by PyTorch, and 207.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model tf_efficientnetv2_l.in1k created, param count: 118515272
Running train benchmark on tf_efficientnetv2_l.in1k for 40 steps w/ input size (3, 480, 480) and batch size 16.
Train [8/40]. 77.97 samples/sec. 205.206 ms/step.
Train [16/40]. 77.99 samples/sec. 205.164 ms/step.
Train [24/40]. 77.98 samples/sec. 205.173 ms/step.
Train [32/40]. 77.96 samples/sec. 205.231 ms/step.
Train [40/40]. 77.95 samples/sec. 205.269 ms/step.
Train benchmark of tf_efficientnetv2_l.in1k done. 76.95 samples/sec, 205.27 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tf_efficientnetv2_l.in21k_ft_in1k created, param count: 118515272
Running inference benchmark on tf_efficientnetv2_l.in21k_ft_in1k for 40 steps w/ input size (3, 480, 480) and batch size 256.
Infer [8/40]. 277.35 samples/sec. 923.026 ms/step.
Infer [16/40]. 277.17 samples/sec. 923.623 ms/step.
Infer [24/40]. 277.40 samples/sec. 922.864 ms/step.
Infer [32/40]. 277.23 samples/sec. 923.433 ms/step.
Infer [40/40]. 277.22 samples/sec. 923.446 ms/step.
Inference benchmark of tf_efficientnetv2_l.in21k_ft_in1k done. 277.21 samples/sec, 923.45 ms/step
Model tf_efficientnetv2_l.in21k_ft_in1k created, param count: 118515272
Running train benchmark on tf_efficientnetv2_l.in21k_ft_in1k for 40 steps w/ input size (3, 480, 480) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 1.76 GiB. GPU 0 has a total capacty of 23.65 GiB of which 78.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 22.00 GiB is allocated by PyTorch, and 338.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model tf_efficientnetv2_l.in21k_ft_in1k created, param count: 118515272
Running train benchmark on tf_efficientnetv2_l.in21k_ft_in1k for 40 steps w/ input size (3, 480, 480) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 1.32 GiB. GPU 0 has a total capacty of 23.65 GiB of which 480.06 MiB is free. Including non-PyTorch memory, this process has 23.17 GiB memory in use. Of the allocated memory 21.18 GiB is allocated by PyTorch, and 774.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model tf_efficientnetv2_l.in21k_ft_in1k created, param count: 118515272
Running train benchmark on tf_efficientnetv2_l.in21k_ft_in1k for 40 steps w/ input size (3, 480, 480) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 900.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 626.06 MiB is free. Including non-PyTorch memory, this process has 23.03 GiB memory in use. Of the allocated memory 21.68 GiB is allocated by PyTorch, and 123.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model tf_efficientnetv2_l.in21k_ft_in1k created, param count: 118515272
Running train benchmark on tf_efficientnetv2_l.in21k_ft_in1k for 40 steps w/ input size (3, 480, 480) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 254.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 124.06 MiB is free. Including non-PyTorch memory, this process has 23.52 GiB memory in use. Of the allocated memory 21.89 GiB is allocated by PyTorch, and 402.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model tf_efficientnetv2_l.in21k_ft_in1k created, param count: 118515272
Running train benchmark on tf_efficientnetv2_l.in21k_ft_in1k for 40 steps w/ input size (3, 480, 480) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 86.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 82.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 22.11 GiB is allocated by PyTorch, and 220.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model tf_efficientnetv2_l.in21k_ft_in1k created, param count: 118515272
Running train benchmark on tf_efficientnetv2_l.in21k_ft_in1k for 40 steps w/ input size (3, 480, 480) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 82.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 22.04 GiB is allocated by PyTorch, and 299.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model tf_efficientnetv2_l.in21k_ft_in1k created, param count: 118515272
Running train benchmark on tf_efficientnetv2_l.in21k_ft_in1k for 40 steps w/ input size (3, 480, 480) and batch size 32.
ERROR: "CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 2.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 22.10 GiB is allocated by PyTorch, and 313.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model tf_efficientnetv2_l.in21k_ft_in1k created, param count: 118515272
Running train benchmark on tf_efficientnetv2_l.in21k_ft_in1k for 40 steps w/ input size (3, 480, 480) and batch size 24.
ERROR: "CUDA out of memory. Tried to allocate 40.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 6.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.19 GiB is allocated by PyTorch, and 213.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model tf_efficientnetv2_l.in21k_ft_in1k created, param count: 118515272
Running train benchmark on tf_efficientnetv2_l.in21k_ft_in1k for 40 steps w/ input size (3, 480, 480) and batch size 16.
Train [8/40]. 78.48 samples/sec. 203.875 ms/step.
Train [16/40]. 78.49 samples/sec. 203.852 ms/step.
Train [24/40]. 78.49 samples/sec. 203.840 ms/step.
Train [32/40]. 78.49 samples/sec. 203.852 ms/step.
Train [40/40]. 78.49 samples/sec. 203.841 ms/step.
Train benchmark of tf_efficientnetv2_l.in21k_ft_in1k done. 77.48 samples/sec, 203.84 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tf_efficientnetv2_m.in1k created, param count: 54139356
Running inference benchmark on tf_efficientnetv2_m.in1k for 40 steps w/ input size (3, 480, 480) and batch size 256.
Infer [8/40]. 565.21 samples/sec. 452.927 ms/step.
Infer [16/40]. 565.20 samples/sec. 452.937 ms/step.
Infer [24/40]. 565.19 samples/sec. 452.941 ms/step.
Infer [32/40]. 565.19 samples/sec. 452.947 ms/step.
Infer [40/40]. 565.18 samples/sec. 452.950 ms/step.
Inference benchmark of tf_efficientnetv2_m.in1k done. 565.13 samples/sec, 452.95 ms/step
Model tf_efficientnetv2_m.in1k created, param count: 54139356
Running train benchmark on tf_efficientnetv2_m.in1k for 40 steps w/ input size (3, 480, 480) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 1.32 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.09 GiB is free. Including non-PyTorch memory, this process has 22.55 GiB memory in use. Of the allocated memory 21.23 GiB is allocated by PyTorch, and 88.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model tf_efficientnetv2_m.in1k created, param count: 54139356
Running train benchmark on tf_efficientnetv2_m.in1k for 40 steps w/ input size (3, 480, 480) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 1014.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 504.06 MiB is free. Including non-PyTorch memory, this process has 23.15 GiB memory in use. Of the allocated memory 21.39 GiB is allocated by PyTorch, and 542.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model tf_efficientnetv2_m.in1k created, param count: 54139356
Running train benchmark on tf_efficientnetv2_m.in1k for 40 steps w/ input size (3, 480, 480) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 142.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 86.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 22.21 GiB is allocated by PyTorch, and 112.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model tf_efficientnetv2_m.in1k created, param count: 54139356
Running train benchmark on tf_efficientnetv2_m.in1k for 40 steps w/ input size (3, 480, 480) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 176.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 94.06 MiB is free. Including non-PyTorch memory, this process has 23.55 GiB memory in use. Of the allocated memory 21.62 GiB is allocated by PyTorch, and 713.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model tf_efficientnetv2_m.in1k created, param count: 54139356
Running train benchmark on tf_efficientnetv2_m.in1k for 40 steps w/ input size (3, 480, 480) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 118.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 70.06 MiB is free. Including non-PyTorch memory, this process has 23.57 GiB memory in use. Of the allocated memory 21.71 GiB is allocated by PyTorch, and 648.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model tf_efficientnetv2_m.in1k created, param count: 54139356
Running train benchmark on tf_efficientnetv2_m.in1k for 40 steps w/ input size (3, 480, 480) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 2.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 21.99 GiB is allocated by PyTorch, and 425.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model tf_efficientnetv2_m.in1k created, param count: 54139356
Running train benchmark on tf_efficientnetv2_m.in1k for 40 steps w/ input size (3, 480, 480) and batch size 32.
Train [8/40]. 159.19 samples/sec. 201.018 ms/step.
Train [16/40]. 159.19 samples/sec. 201.019 ms/step.
Train [24/40]. 159.18 samples/sec. 201.028 ms/step.
Train [32/40]. 159.18 samples/sec. 201.027 ms/step.
Train [40/40]. 159.18 samples/sec. 201.027 ms/step.
Train benchmark of tf_efficientnetv2_m.in1k done. 157.60 samples/sec, 201.03 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tf_efficientnetv2_m.in21k_ft_in1k created, param count: 54139356
Running inference benchmark on tf_efficientnetv2_m.in21k_ft_in1k for 40 steps w/ input size (3, 480, 480) and batch size 256.
Infer [8/40]. 565.14 samples/sec. 452.988 ms/step.
Infer [16/40]. 565.14 samples/sec. 452.982 ms/step.
Infer [24/40]. 565.14 samples/sec. 452.982 ms/step.
Infer [32/40]. 565.14 samples/sec. 452.987 ms/step.
Infer [40/40]. 565.14 samples/sec. 452.989 ms/step.
Inference benchmark of tf_efficientnetv2_m.in21k_ft_in1k done. 565.08 samples/sec, 452.99 ms/step
Model tf_efficientnetv2_m.in21k_ft_in1k created, param count: 54139356
Running train benchmark on tf_efficientnetv2_m.in21k_ft_in1k for 40 steps w/ input size (3, 480, 480) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 1.32 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.09 GiB is free. Including non-PyTorch memory, this process has 22.55 GiB memory in use. Of the allocated memory 21.23 GiB is allocated by PyTorch, and 88.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model tf_efficientnetv2_m.in21k_ft_in1k created, param count: 54139356
Running train benchmark on tf_efficientnetv2_m.in21k_ft_in1k for 40 steps w/ input size (3, 480, 480) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 1014.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 504.06 MiB is free. Including non-PyTorch memory, this process has 23.15 GiB memory in use. Of the allocated memory 21.39 GiB is allocated by PyTorch, and 542.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model tf_efficientnetv2_m.in21k_ft_in1k created, param count: 54139356
Running train benchmark on tf_efficientnetv2_m.in21k_ft_in1k for 40 steps w/ input size (3, 480, 480) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 142.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 58.06 MiB is free. Including non-PyTorch memory, this process has 23.58 GiB memory in use. Of the allocated memory 22.21 GiB is allocated by PyTorch, and 140.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model tf_efficientnetv2_m.in21k_ft_in1k created, param count: 54139356
Running train benchmark on tf_efficientnetv2_m.in21k_ft_in1k for 40 steps w/ input size (3, 480, 480) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 176.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 94.06 MiB is free. Including non-PyTorch memory, this process has 23.55 GiB memory in use. Of the allocated memory 21.62 GiB is allocated by PyTorch, and 713.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model tf_efficientnetv2_m.in21k_ft_in1k created, param count: 54139356
Running train benchmark on tf_efficientnetv2_m.in21k_ft_in1k for 40 steps w/ input size (3, 480, 480) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 118.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 20.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 21.83 GiB is allocated by PyTorch, and 571.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model tf_efficientnetv2_m.in21k_ft_in1k created, param count: 54139356
Running train benchmark on tf_efficientnetv2_m.in21k_ft_in1k for 40 steps w/ input size (3, 480, 480) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 32.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 21.99 GiB is allocated by PyTorch, and 393.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model tf_efficientnetv2_m.in21k_ft_in1k created, param count: 54139356
Running train benchmark on tf_efficientnetv2_m.in21k_ft_in1k for 40 steps w/ input size (3, 480, 480) and batch size 32.
Train [8/40]. 159.27 samples/sec. 200.921 ms/step.
Train [16/40]. 159.27 samples/sec. 200.920 ms/step.
Train [24/40]. 159.27 samples/sec. 200.918 ms/step.
Train [32/40]. 159.27 samples/sec. 200.922 ms/step.
Train [40/40]. 159.27 samples/sec. 200.921 ms/step.
Train benchmark of tf_efficientnetv2_m.in21k_ft_in1k done. 157.65 samples/sec, 200.92 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tf_efficientnetv2_s.in1k created, param count: 21458488
Running inference benchmark on tf_efficientnetv2_s.in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 1459.62 samples/sec. 175.389 ms/step.
Infer [16/40]. 1459.56 samples/sec. 175.395 ms/step.
Infer [24/40]. 1459.50 samples/sec. 175.403 ms/step.
Infer [32/40]. 1459.47 samples/sec. 175.406 ms/step.
Infer [40/40]. 1459.44 samples/sec. 175.410 ms/step.
Inference benchmark of tf_efficientnetv2_s.in1k done. 1459.15 samples/sec, 175.41 ms/step
Model tf_efficientnetv2_s.in1k created, param count: 21458488
Running train benchmark on tf_efficientnetv2_s.in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 144.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 74.06 MiB is free. Including non-PyTorch memory, this process has 23.57 GiB memory in use. Of the allocated memory 22.16 GiB is allocated by PyTorch, and 182.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model tf_efficientnetv2_s.in1k created, param count: 21458488
Running train benchmark on tf_efficientnetv2_s.in1k for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 204.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 56.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 22.22 GiB is allocated by PyTorch, and 135.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model tf_efficientnetv2_s.in1k created, param count: 21458488
Running train benchmark on tf_efficientnetv2_s.in1k for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 54.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 26.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.12 GiB is allocated by PyTorch, and 265.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model tf_efficientnetv2_s.in1k created, param count: 21458488
Running train benchmark on tf_efficientnetv2_s.in1k for 40 steps w/ input size (3, 384, 384) and batch size 96.
Train [8/40]. 375.70 samples/sec. 255.523 ms/step.
Train [16/40]. 375.71 samples/sec. 255.518 ms/step.
Train [24/40]. 375.72 samples/sec. 255.511 ms/step.
Train [32/40]. 375.72 samples/sec. 255.507 ms/step.
Train [40/40]. 375.72 samples/sec. 255.511 ms/step.
Train benchmark of tf_efficientnetv2_s.in1k done. 373.13 samples/sec, 255.51 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tf_efficientnetv2_s.in21k_ft_in1k created, param count: 21458488
Running inference benchmark on tf_efficientnetv2_s.in21k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 1459.66 samples/sec. 175.384 ms/step.
Infer [16/40]. 1459.58 samples/sec. 175.393 ms/step.
Infer [24/40]. 1459.55 samples/sec. 175.397 ms/step.
Infer [32/40]. 1459.55 samples/sec. 175.396 ms/step.
Infer [40/40]. 1459.56 samples/sec. 175.395 ms/step.
Inference benchmark of tf_efficientnetv2_s.in21k_ft_in1k done. 1459.28 samples/sec, 175.40 ms/step
Model tf_efficientnetv2_s.in21k_ft_in1k created, param count: 21458488
Running train benchmark on tf_efficientnetv2_s.in21k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 144.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 74.06 MiB is free. Including non-PyTorch memory, this process has 23.57 GiB memory in use. Of the allocated memory 22.16 GiB is allocated by PyTorch, and 182.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model tf_efficientnetv2_s.in21k_ft_in1k created, param count: 21458488
Running train benchmark on tf_efficientnetv2_s.in21k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 204.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 164.06 MiB is free. Including non-PyTorch memory, this process has 23.48 GiB memory in use. Of the allocated memory 22.02 GiB is allocated by PyTorch, and 228.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model tf_efficientnetv2_s.in21k_ft_in1k created, param count: 21458488
Running train benchmark on tf_efficientnetv2_s.in21k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 54.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 10.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.12 GiB is allocated by PyTorch, and 281.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model tf_efficientnetv2_s.in21k_ft_in1k created, param count: 21458488
Running train benchmark on tf_efficientnetv2_s.in21k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 96.
Train [8/40]. 375.58 samples/sec. 255.606 ms/step.
Train [16/40]. 375.61 samples/sec. 255.583 ms/step.
Train [24/40]. 375.60 samples/sec. 255.589 ms/step.
Train [32/40]. 375.60 samples/sec. 255.590 ms/step.
Train [40/40]. 375.60 samples/sec. 255.590 ms/step.
Train benchmark of tf_efficientnetv2_s.in21k_ft_in1k done. 372.95 samples/sec, 255.59 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tf_efficientnetv2_xl.in21k_ft_in1k created, param count: 208119808
Running inference benchmark on tf_efficientnetv2_xl.in21k_ft_in1k for 40 steps w/ input size (3, 512, 512) and batch size 256.
Infer [8/40]. 211.54 samples/sec. 1210.168 ms/step.
Infer [16/40]. 211.54 samples/sec. 1210.197 ms/step.
Infer [24/40]. 211.53 samples/sec. 1210.217 ms/step.
Infer [32/40]. 211.53 samples/sec. 1210.238 ms/step.
Infer [40/40]. 211.52 samples/sec. 1210.263 ms/step.
Inference benchmark of tf_efficientnetv2_xl.in21k_ft_in1k done. 211.52 samples/sec, 1210.26 ms/step
Model tf_efficientnetv2_xl.in21k_ft_in1k created, param count: 208119808
Running train benchmark on tf_efficientnetv2_xl.in21k_ft_in1k for 40 steps w/ input size (3, 512, 512) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.06 GiB is free. Including non-PyTorch memory, this process has 22.58 GiB memory in use. Of the allocated memory 21.17 GiB is allocated by PyTorch, and 185.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model tf_efficientnetv2_xl.in21k_ft_in1k created, param count: 208119808
Running train benchmark on tf_efficientnetv2_xl.in21k_ft_in1k for 40 steps w/ input size (3, 512, 512) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 1.50 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.01 GiB is free. Including non-PyTorch memory, this process has 22.63 GiB memory in use. Of the allocated memory 21.23 GiB is allocated by PyTorch, and 174.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model tf_efficientnetv2_xl.in21k_ft_in1k created, param count: 208119808
Running train benchmark on tf_efficientnetv2_xl.in21k_ft_in1k for 40 steps w/ input size (3, 512, 512) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 606.06 MiB is free. Including non-PyTorch memory, this process has 23.05 GiB memory in use. Of the allocated memory 21.29 GiB is allocated by PyTorch, and 541.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model tf_efficientnetv2_xl.in21k_ft_in1k created, param count: 208119808
Running train benchmark on tf_efficientnetv2_xl.in21k_ft_in1k for 40 steps w/ input size (3, 512, 512) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 768.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 700.06 MiB is free. Including non-PyTorch memory, this process has 22.96 GiB memory in use. Of the allocated memory 21.32 GiB is allocated by PyTorch, and 416.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model tf_efficientnetv2_xl.in21k_ft_in1k created, param count: 208119808
Running train benchmark on tf_efficientnetv2_xl.in21k_ft_in1k for 40 steps w/ input size (3, 512, 512) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 70.06 MiB is free. Including non-PyTorch memory, this process has 23.57 GiB memory in use. Of the allocated memory 21.85 GiB is allocated by PyTorch, and 501.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model tf_efficientnetv2_xl.in21k_ft_in1k created, param count: 208119808
Running train benchmark on tf_efficientnetv2_xl.in21k_ft_in1k for 40 steps w/ input size (3, 512, 512) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 72.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 18.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 21.99 GiB is allocated by PyTorch, and 409.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model tf_efficientnetv2_xl.in21k_ft_in1k created, param count: 208119808
Running train benchmark on tf_efficientnetv2_xl.in21k_ft_in1k for 40 steps w/ input size (3, 512, 512) and batch size 32.
ERROR: "CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 68.06 MiB is free. Including non-PyTorch memory, this process has 23.57 GiB memory in use. Of the allocated memory 21.86 GiB is allocated by PyTorch, and 488.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model tf_efficientnetv2_xl.in21k_ft_in1k created, param count: 208119808
Running train benchmark on tf_efficientnetv2_xl.in21k_ft_in1k for 40 steps w/ input size (3, 512, 512) and batch size 24.
ERROR: "CUDA out of memory. Tried to allocate 72.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 66.06 MiB is free. Including non-PyTorch memory, this process has 23.58 GiB memory in use. Of the allocated memory 20.90 GiB is allocated by PyTorch, and 1.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model tf_efficientnetv2_xl.in21k_ft_in1k created, param count: 208119808
Running train benchmark on tf_efficientnetv2_xl.in21k_ft_in1k for 40 steps w/ input size (3, 512, 512) and batch size 16.
ERROR: "CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 16.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.09 GiB is allocated by PyTorch, and 305.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 12 for retry.
Model tf_efficientnetv2_xl.in21k_ft_in1k created, param count: 208119808
Running train benchmark on tf_efficientnetv2_xl.in21k_ft_in1k for 40 steps w/ input size (3, 512, 512) and batch size 12.
Train [8/40]. 48.32 samples/sec. 248.328 ms/step.
Train [16/40]. 48.30 samples/sec. 248.427 ms/step.
Train [24/40]. 48.31 samples/sec. 248.392 ms/step.
Train [32/40]. 48.32 samples/sec. 248.358 ms/step.
Train [40/40]. 48.32 samples/sec. 248.362 ms/step.
Train benchmark of tf_efficientnetv2_xl.in21k_ft_in1k done. 47.65 samples/sec, 248.36 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tf_mixnet_l.in1k created, param count: 7329252
Running inference benchmark on tf_mixnet_l.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3791.79 samples/sec. 67.514 ms/step.
Infer [16/40]. 3790.65 samples/sec. 67.535 ms/step.
Infer [24/40]. 3790.29 samples/sec. 67.541 ms/step.
Infer [32/40]. 3790.11 samples/sec. 67.544 ms/step.
Infer [40/40]. 3789.94 samples/sec. 67.547 ms/step.
Inference benchmark of tf_mixnet_l.in1k done. 3788.52 samples/sec, 67.55 ms/step
Model tf_mixnet_l.in1k created, param count: 7329252
Running train benchmark on tf_mixnet_l.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 921.91 samples/sec. 277.685 ms/step.
Train [16/40]. 921.92 samples/sec. 277.682 ms/step.
Train [24/40]. 921.85 samples/sec. 277.702 ms/step.
Train [32/40]. 921.84 samples/sec. 277.707 ms/step.
Train [40/40]. 921.83 samples/sec. 277.709 ms/step.
Train benchmark of tf_mixnet_l.in1k done. 916.78 samples/sec, 277.71 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tf_mixnet_m.in1k created, param count: 5014382
Running inference benchmark on tf_mixnet_m.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 5113.35 samples/sec. 50.065 ms/step.
Infer [16/40]. 5113.27 samples/sec. 50.066 ms/step.
Infer [24/40]. 5113.15 samples/sec. 50.067 ms/step.
Infer [32/40]. 5113.15 samples/sec. 50.067 ms/step.
Infer [40/40]. 5113.16 samples/sec. 50.067 ms/step.
Inference benchmark of tf_mixnet_m.in1k done. 5110.71 samples/sec, 50.07 ms/step
Model tf_mixnet_m.in1k created, param count: 5014382
Running train benchmark on tf_mixnet_m.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1190.32 samples/sec. 215.068 ms/step.
Train [16/40]. 1190.34 samples/sec. 215.064 ms/step.
Train [24/40]. 1190.33 samples/sec. 215.067 ms/step.
Train [32/40]. 1190.33 samples/sec. 215.067 ms/step.
Train [40/40]. 1190.30 samples/sec. 215.072 ms/step.
Train benchmark of tf_mixnet_m.in1k done. 1182.44 samples/sec, 215.07 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tf_mixnet_s.in1k created, param count: 4134606
Running inference benchmark on tf_mixnet_s.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 7205.01 samples/sec. 35.531 ms/step.
Infer [16/40]. 7202.11 samples/sec. 35.545 ms/step.
Infer [24/40]. 7201.72 samples/sec. 35.547 ms/step.
Infer [32/40]. 7201.14 samples/sec. 35.550 ms/step.
Infer [40/40]. 7200.87 samples/sec. 35.551 ms/step.
Inference benchmark of tf_mixnet_s.in1k done. 7196.12 samples/sec, 35.55 ms/step
Model tf_mixnet_s.in1k created, param count: 4134606
Running train benchmark on tf_mixnet_s.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1613.35 samples/sec. 158.676 ms/step.
Train [16/40]. 1613.34 samples/sec. 158.677 ms/step.
Train [24/40]. 1613.32 samples/sec. 158.679 ms/step.
Train [32/40]. 1613.31 samples/sec. 158.680 ms/step.
Train [40/40]. 1613.31 samples/sec. 158.680 ms/step.
Train benchmark of tf_mixnet_s.in1k done. 1602.12 samples/sec, 158.68 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tf_mobilenetv3_large_075.in1k created, param count: 3993528
Running inference benchmark on tf_mobilenetv3_large_075.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 15413.15 samples/sec. 16.609 ms/step.
Infer [16/40]. 15413.94 samples/sec. 16.608 ms/step.
Infer [24/40]. 15412.13 samples/sec. 16.610 ms/step.
Infer [32/40]. 15410.72 samples/sec. 16.612 ms/step.
Infer [40/40]. 15409.61 samples/sec. 16.613 ms/step.
Inference benchmark of tf_mobilenetv3_large_075.in1k done. 15388.09 samples/sec, 16.61 ms/step
Model tf_mobilenetv3_large_075.in1k created, param count: 3993528
Running train benchmark on tf_mobilenetv3_large_075.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 3002.44 samples/sec. 85.264 ms/step.
Train [16/40]. 3001.95 samples/sec. 85.278 ms/step.
Train [24/40]. 3001.83 samples/sec. 85.281 ms/step.
Train [32/40]. 3001.65 samples/sec. 85.286 ms/step.
Train [40/40]. 3001.64 samples/sec. 85.287 ms/step.
Train benchmark of tf_mobilenetv3_large_075.in1k done. 2977.31 samples/sec, 85.29 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tf_mobilenetv3_large_100.in1k created, param count: 5483032
Running inference benchmark on tf_mobilenetv3_large_100.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 13846.26 samples/sec. 18.489 ms/step.
Infer [16/40]. 13845.23 samples/sec. 18.490 ms/step.
Infer [24/40]. 13844.51 samples/sec. 18.491 ms/step.
Infer [32/40]. 13843.65 samples/sec. 18.492 ms/step.
Infer [40/40]. 13843.72 samples/sec. 18.492 ms/step.
Inference benchmark of tf_mobilenetv3_large_100.in1k done. 13826.32 samples/sec, 18.49 ms/step
Model tf_mobilenetv3_large_100.in1k created, param count: 5483032
Running train benchmark on tf_mobilenetv3_large_100.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 2784.67 samples/sec. 91.932 ms/step.
Train [16/40]. 2784.46 samples/sec. 91.939 ms/step.
Train [24/40]. 2784.59 samples/sec. 91.935 ms/step.
Train [32/40]. 2784.62 samples/sec. 91.933 ms/step.
Train [40/40]. 2784.70 samples/sec. 91.931 ms/step.
Train benchmark of tf_mobilenetv3_large_100.in1k done. 2762.95 samples/sec, 91.93 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tf_mobilenetv3_large_minimal_100.in1k created, param count: 3924288
Running inference benchmark on tf_mobilenetv3_large_minimal_100.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 14837.22 samples/sec. 17.254 ms/step.
Infer [16/40]. 14834.84 samples/sec. 17.257 ms/step.
Infer [24/40]. 14833.81 samples/sec. 17.258 ms/step.
Infer [32/40]. 14832.44 samples/sec. 17.259 ms/step.
Infer [40/40]. 14831.07 samples/sec. 17.261 ms/step.
Inference benchmark of tf_mobilenetv3_large_minimal_100.in1k done. 14811.59 samples/sec, 17.26 ms/step
Model tf_mobilenetv3_large_minimal_100.in1k created, param count: 3924288
Running train benchmark on tf_mobilenetv3_large_minimal_100.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 3077.55 samples/sec. 83.183 ms/step.
Train [16/40]. 3077.47 samples/sec. 83.185 ms/step.
Train [24/40]. 3077.50 samples/sec. 83.184 ms/step.
Train [32/40]. 3077.49 samples/sec. 83.185 ms/step.
Train [40/40]. 3077.49 samples/sec. 83.185 ms/step.
Train benchmark of tf_mobilenetv3_large_minimal_100.in1k done. 3055.81 samples/sec, 83.19 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tf_mobilenetv3_small_075.in1k created, param count: 2041872
Running inference benchmark on tf_mobilenetv3_small_075.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 25635.90 samples/sec. 9.986 ms/step.
Infer [16/40]. 25624.25 samples/sec. 9.991 ms/step.
Infer [24/40]. 25611.30 samples/sec. 9.996 ms/step.
Infer [32/40]. 25606.15 samples/sec. 9.998 ms/step.
Infer [40/40]. 25599.31 samples/sec. 10.000 ms/step.
Inference benchmark of tf_mobilenetv3_small_075.in1k done. 25547.45 samples/sec, 10.00 ms/step
Model tf_mobilenetv3_small_075.in1k created, param count: 2041872
Running train benchmark on tf_mobilenetv3_small_075.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 6738.65 samples/sec. 37.990 ms/step.
Train [16/40]. 6736.26 samples/sec. 38.003 ms/step.
Train [24/40]. 6737.83 samples/sec. 37.994 ms/step.
Train [32/40]. 6737.28 samples/sec. 37.998 ms/step.
Train [40/40]. 6737.38 samples/sec. 37.997 ms/step.
Train benchmark of tf_mobilenetv3_small_075.in1k done. 6637.63 samples/sec, 38.00 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tf_mobilenetv3_small_100.in1k created, param count: 2542856
Running inference benchmark on tf_mobilenetv3_small_100.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 25468.57 samples/sec. 10.052 ms/step.
Infer [16/40]. 25475.74 samples/sec. 10.049 ms/step.
Infer [24/40]. 25452.98 samples/sec. 10.058 ms/step.
Infer [32/40]. 25456.26 samples/sec. 10.056 ms/step.
Infer [40/40]. 25454.05 samples/sec. 10.057 ms/step.
Inference benchmark of tf_mobilenetv3_small_100.in1k done. 25403.25 samples/sec, 10.06 ms/step
Model tf_mobilenetv3_small_100.in1k created, param count: 2542856
Running train benchmark on tf_mobilenetv3_small_100.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 6707.48 samples/sec. 38.166 ms/step.
Train [16/40]. 6709.23 samples/sec. 38.156 ms/step.
Train [24/40]. 6710.13 samples/sec. 38.151 ms/step.
Train [32/40]. 6710.37 samples/sec. 38.150 ms/step.
Train [40/40]. 6711.27 samples/sec. 38.145 ms/step.
Train benchmark of tf_mobilenetv3_small_100.in1k done. 6612.39 samples/sec, 38.15 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tf_mobilenetv3_small_minimal_100.in1k created, param count: 2044736
Running inference benchmark on tf_mobilenetv3_small_minimal_100.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 34610.03 samples/sec. 7.397 ms/step.
Infer [16/40]. 34604.13 samples/sec. 7.398 ms/step.
Infer [24/40]. 34592.15 samples/sec. 7.401 ms/step.
Infer [32/40]. 34597.90 samples/sec. 7.399 ms/step.
Infer [40/40]. 34598.11 samples/sec. 7.399 ms/step.
Inference benchmark of tf_mobilenetv3_small_minimal_100.in1k done. 34513.01 samples/sec, 7.40 ms/step
Model tf_mobilenetv3_small_minimal_100.in1k created, param count: 2044736
Running train benchmark on tf_mobilenetv3_small_minimal_100.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 8606.97 samples/sec. 29.743 ms/step.
Train [16/40]. 8602.16 samples/sec. 29.760 ms/step.
Train [24/40]. 8603.72 samples/sec. 29.755 ms/step.
Train [32/40]. 8603.65 samples/sec. 29.755 ms/step.
Train [40/40]. 8603.58 samples/sec. 29.755 ms/step.
Train benchmark of tf_mobilenetv3_small_minimal_100.in1k done. 8473.83 samples/sec, 29.75 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tinynet_a.in1k created, param count: 6187972
Running inference benchmark on tinynet_a.in1k for 40 steps w/ input size (3, 192, 192) and batch size 256.
Infer [8/40]. 11462.39 samples/sec. 22.334 ms/step.
Infer [16/40]. 11462.12 samples/sec. 22.334 ms/step.
Infer [24/40]. 11462.48 samples/sec. 22.334 ms/step.
Infer [32/40]. 11462.25 samples/sec. 22.334 ms/step.
Infer [40/40]. 11462.46 samples/sec. 22.334 ms/step.
Inference benchmark of tinynet_a.in1k done. 11450.25 samples/sec, 22.33 ms/step
Model tinynet_a.in1k created, param count: 6187972
Running train benchmark on tinynet_a.in1k for 40 steps w/ input size (3, 192, 192) and batch size 256.
Train [8/40]. 2380.22 samples/sec. 107.553 ms/step.
Train [16/40]. 2380.22 samples/sec. 107.553 ms/step.
Train [24/40]. 2378.80 samples/sec. 107.617 ms/step.
Train [32/40]. 2378.00 samples/sec. 107.653 ms/step.
Train [40/40]. 2377.55 samples/sec. 107.674 ms/step.
Train benchmark of tinynet_a.in1k done. 2357.33 samples/sec, 107.67 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tinynet_b.in1k created, param count: 3730562
Running inference benchmark on tinynet_b.in1k for 40 steps w/ input size (3, 188, 188) and batch size 256.
Infer [8/40]. 14301.51 samples/sec. 17.900 ms/step.
Infer [16/40]. 14302.10 samples/sec. 17.899 ms/step.
Infer [24/40]. 14301.36 samples/sec. 17.900 ms/step.
Infer [32/40]. 14301.64 samples/sec. 17.900 ms/step.
Infer [40/40]. 14301.71 samples/sec. 17.900 ms/step.
Inference benchmark of tinynet_b.in1k done. 14284.07 samples/sec, 17.90 ms/step
Model tinynet_b.in1k created, param count: 3730562
Running train benchmark on tinynet_b.in1k for 40 steps w/ input size (3, 188, 188) and batch size 256.
Train [8/40]. 2808.64 samples/sec. 91.147 ms/step.
Train [16/40]. 2808.90 samples/sec. 91.139 ms/step.
Train [24/40]. 2809.10 samples/sec. 91.132 ms/step.
Train [32/40]. 2808.95 samples/sec. 91.137 ms/step.
Train [40/40]. 2808.98 samples/sec. 91.136 ms/step.
Train benchmark of tinynet_b.in1k done. 2784.96 samples/sec, 91.14 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tinynet_c.in1k created, param count: 2457234
Running inference benchmark on tinynet_c.in1k for 40 steps w/ input size (3, 184, 184) and batch size 256.
Infer [8/40]. 18560.21 samples/sec. 13.793 ms/step.
Infer [16/40]. 18538.03 samples/sec. 13.809 ms/step.
Infer [24/40]. 18531.84 samples/sec. 13.814 ms/step.
Infer [32/40]. 18539.73 samples/sec. 13.808 ms/step.
Infer [40/40]. 18544.85 samples/sec. 13.804 ms/step.
Inference benchmark of tinynet_c.in1k done. 18515.57 samples/sec, 13.80 ms/step
Model tinynet_c.in1k created, param count: 2457234
Running train benchmark on tinynet_c.in1k for 40 steps w/ input size (3, 184, 184) and batch size 256.
Train [8/40]. 3948.58 samples/sec. 64.833 ms/step.
Train [16/40]. 3950.23 samples/sec. 64.806 ms/step.
Train [24/40]. 3949.34 samples/sec. 64.821 ms/step.
Train [32/40]. 3950.26 samples/sec. 64.806 ms/step.
Train [40/40]. 3949.67 samples/sec. 64.815 ms/step.
Train benchmark of tinynet_c.in1k done. 3905.93 samples/sec, 64.81 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tinynet_d.in1k created, param count: 2338446
Running inference benchmark on tinynet_d.in1k for 40 steps w/ input size (3, 152, 152) and batch size 256.
Infer [8/40]. 25233.92 samples/sec. 10.145 ms/step.
Infer [16/40]. 25233.74 samples/sec. 10.145 ms/step.
Infer [24/40]. 25228.53 samples/sec. 10.147 ms/step.
Infer [32/40]. 25229.54 samples/sec. 10.147 ms/step.
Infer [40/40]. 25229.86 samples/sec. 10.147 ms/step.
Inference benchmark of tinynet_d.in1k done. 25178.21 samples/sec, 10.15 ms/step
Model tinynet_d.in1k created, param count: 2338446
Running train benchmark on tinynet_d.in1k for 40 steps w/ input size (3, 152, 152) and batch size 256.
Train [8/40]. 6304.96 samples/sec. 40.603 ms/step.
Train [16/40]. 6305.08 samples/sec. 40.602 ms/step.
Train [24/40]. 6302.96 samples/sec. 40.616 ms/step.
Train [32/40]. 6302.20 samples/sec. 40.621 ms/step.
Train [40/40]. 6302.88 samples/sec. 40.616 ms/step.
Train benchmark of tinynet_d.in1k done. 6215.24 samples/sec, 40.62 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tinynet_e.in1k created, param count: 2042972
Running inference benchmark on tinynet_e.in1k for 40 steps w/ input size (3, 106, 106) and batch size 256.
Infer [8/40]. 27642.85 samples/sec. 9.261 ms/step.
Infer [16/40]. 27622.26 samples/sec. 9.268 ms/step.
Infer [24/40]. 27626.11 samples/sec. 9.267 ms/step.
Infer [32/40]. 27629.77 samples/sec. 9.265 ms/step.
Infer [40/40]. 27628.03 samples/sec. 9.266 ms/step.
Inference benchmark of tinynet_e.in1k done. 27568.19 samples/sec, 9.27 ms/step
Model tinynet_e.in1k created, param count: 2042972
Running train benchmark on tinynet_e.in1k for 40 steps w/ input size (3, 106, 106) and batch size 256.
Train [8/40]. 8663.79 samples/sec. 29.548 ms/step.
Train [16/40]. 8663.60 samples/sec. 29.549 ms/step.
Train [24/40]. 8662.83 samples/sec. 29.552 ms/step.
Train [32/40]. 8662.73 samples/sec. 29.552 ms/step.
Train [40/40]. 8660.51 samples/sec. 29.559 ms/step.
Train benchmark of tinynet_e.in1k done. 8505.76 samples/sec, 29.56 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tnt_s_patch16_224 created, param count: 23755336
Running inference benchmark on tnt_s_patch16_224 for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 2448.14 samples/sec. 104.569 ms/step.
Infer [16/40]. 2448.08 samples/sec. 104.572 ms/step.
Infer [24/40]. 2448.04 samples/sec. 104.573 ms/step.
Infer [32/40]. 2446.87 samples/sec. 104.624 ms/step.
Infer [40/40]. 2446.08 samples/sec. 104.657 ms/step.
Inference benchmark of tnt_s_patch16_224 done. 2445.45 samples/sec, 104.66 ms/step
Model tnt_s_patch16_224 created, param count: 23755336
Running train benchmark on tnt_s_patch16_224 for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 727.55 samples/sec. 351.866 ms/step.
Train [16/40]. 727.53 samples/sec. 351.873 ms/step.
Train [24/40]. 727.54 samples/sec. 351.871 ms/step.
Train [32/40]. 727.54 samples/sec. 351.870 ms/step.
Train [40/40]. 727.54 samples/sec. 351.870 ms/step.
Train benchmark of tnt_s_patch16_224 done. 723.62 samples/sec, 351.87 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tresnet_l.miil_in1k created, param count: 55989256
Running inference benchmark on tresnet_l.miil_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 2601.28 samples/sec. 98.413 ms/step.
Infer [16/40]. 2601.25 samples/sec. 98.414 ms/step.
Infer [24/40]. 2601.28 samples/sec. 98.413 ms/step.
Infer [32/40]. 2601.30 samples/sec. 98.412 ms/step.
Infer [40/40]. 2601.32 samples/sec. 98.411 ms/step.
Inference benchmark of tresnet_l.miil_in1k done. 2600.60 samples/sec, 98.41 ms/step
Model tresnet_l.miil_in1k created, param count: 55989256
Running train benchmark on tresnet_l.miil_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 751.81 samples/sec. 340.510 ms/step.
Train [16/40]. 751.81 samples/sec. 340.511 ms/step.
Train [24/40]. 751.81 samples/sec. 340.510 ms/step.
Train [32/40]. 751.80 samples/sec. 340.516 ms/step.
Train [40/40]. 751.81 samples/sec. 340.510 ms/step.
Train benchmark of tresnet_l.miil_in1k done. 748.30 samples/sec, 340.51 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tresnet_l.miil_in1k_448 created, param count: 55989256
Running inference benchmark on tresnet_l.miil_in1k_448 for 40 steps w/ input size (3, 448, 448) and batch size 256.
Infer [8/40]. 640.62 samples/sec. 399.610 ms/step.
Infer [16/40]. 640.52 samples/sec. 399.672 ms/step.
Infer [24/40]. 640.48 samples/sec. 399.697 ms/step.
Infer [32/40]. 640.47 samples/sec. 399.704 ms/step.
Infer [40/40]. 640.46 samples/sec. 399.714 ms/step.
Inference benchmark of tresnet_l.miil_in1k_448 done. 640.38 samples/sec, 399.71 ms/step
Model tresnet_l.miil_in1k_448 created, param count: 55989256
Running train benchmark on tresnet_l.miil_in1k_448 for 40 steps w/ input size (3, 448, 448) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 466.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 112.06 MiB is free. Including non-PyTorch memory, this process has 23.53 GiB memory in use. Of the allocated memory 21.59 GiB is allocated by PyTorch, and 720.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model tresnet_l.miil_in1k_448 created, param count: 55989256
Running train benchmark on tresnet_l.miil_in1k_448 for 40 steps w/ input size (3, 448, 448) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 350.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 210.06 MiB is free. Including non-PyTorch memory, this process has 23.44 GiB memory in use. Of the allocated memory 21.55 GiB is allocated by PyTorch, and 664.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model tresnet_l.miil_in1k_448 created, param count: 55989256
Running train benchmark on tresnet_l.miil_in1k_448 for 40 steps w/ input size (3, 448, 448) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 234.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 210.06 MiB is free. Including non-PyTorch memory, this process has 23.44 GiB memory in use. Of the allocated memory 21.56 GiB is allocated by PyTorch, and 662.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model tresnet_l.miil_in1k_448 created, param count: 55989256
Running train benchmark on tresnet_l.miil_in1k_448 for 40 steps w/ input size (3, 448, 448) and batch size 96.
Train [8/40]. 189.02 samples/sec. 507.891 ms/step.
Train [16/40]. 189.01 samples/sec. 507.897 ms/step.
Train [24/40]. 189.01 samples/sec. 507.904 ms/step.
Train [32/40]. 189.01 samples/sec. 507.906 ms/step.
Train [40/40]. 189.00 samples/sec. 507.942 ms/step.
Train benchmark of tresnet_l.miil_in1k_448 done. 188.36 samples/sec, 507.94 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tresnet_m.miil_in1k created, param count: 31389032
Running inference benchmark on tresnet_m.miil_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 2893.98 samples/sec. 88.460 ms/step.
Infer [16/40]. 2896.30 samples/sec. 88.389 ms/step.
Infer [24/40]. 2891.90 samples/sec. 88.523 ms/step.
Infer [32/40]. 2896.17 samples/sec. 88.393 ms/step.
Infer [40/40]. 2898.73 samples/sec. 88.314 ms/step.
Inference benchmark of tresnet_m.miil_in1k done. 2897.80 samples/sec, 88.31 ms/step
Model tresnet_m.miil_in1k created, param count: 31389032
Running train benchmark on tresnet_m.miil_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1174.72 samples/sec. 217.924 ms/step.
Train [16/40]. 1174.74 samples/sec. 217.921 ms/step.
Train [24/40]. 1174.77 samples/sec. 217.915 ms/step.
Train [32/40]. 1174.75 samples/sec. 217.919 ms/step.
Train [40/40]. 1174.63 samples/sec. 217.941 ms/step.
Train benchmark of tresnet_m.miil_in1k done. 1168.50 samples/sec, 217.94 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tresnet_m.miil_in1k_448 created, param count: 31389032
Running inference benchmark on tresnet_m.miil_in1k_448 for 40 steps w/ input size (3, 448, 448) and batch size 256.
Infer [8/40]. 1048.75 samples/sec. 244.101 ms/step.
Infer [16/40]. 1048.11 samples/sec. 244.250 ms/step.
Infer [24/40]. 1049.16 samples/sec. 244.004 ms/step.
Infer [32/40]. 1047.76 samples/sec. 244.331 ms/step.
Infer [40/40]. 1047.80 samples/sec. 244.321 ms/step.
Inference benchmark of tresnet_m.miil_in1k_448 done. 1047.63 samples/sec, 244.32 ms/step
Model tresnet_m.miil_in1k_448 created, param count: 31389032
Running train benchmark on tresnet_m.miil_in1k_448 for 40 steps w/ input size (3, 448, 448) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 392.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 40.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 22.19 GiB is allocated by PyTorch, and 179.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model tresnet_m.miil_in1k_448 created, param count: 31389032
Running train benchmark on tresnet_m.miil_in1k_448 for 40 steps w/ input size (3, 448, 448) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 148.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 22.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.27 GiB is allocated by PyTorch, and 121.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model tresnet_m.miil_in1k_448 created, param count: 31389032
Running train benchmark on tresnet_m.miil_in1k_448 for 40 steps w/ input size (3, 448, 448) and batch size 128.
Train [8/40]. 359.89 samples/sec. 355.659 ms/step.
Train [16/40]. 359.89 samples/sec. 355.665 ms/step.
Train [24/40]. 359.88 samples/sec. 355.671 ms/step.
Train [32/40]. 359.88 samples/sec. 355.673 ms/step.
Train [40/40]. 359.88 samples/sec. 355.673 ms/step.
Train benchmark of tresnet_m.miil_in1k_448 done. 358.58 samples/sec, 355.67 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tresnet_m.miil_in21k_ft_in1k created, param count: 31389032
Running inference benchmark on tresnet_m.miil_in21k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 2798.25 samples/sec. 91.486 ms/step.
Infer [16/40]. 2816.32 samples/sec. 90.899 ms/step.
Infer [24/40]. 2824.11 samples/sec. 90.648 ms/step.
Infer [32/40]. 2812.01 samples/sec. 91.038 ms/step.
Infer [40/40]. 2815.29 samples/sec. 90.932 ms/step.
Inference benchmark of tresnet_m.miil_in21k_ft_in1k done. 2814.47 samples/sec, 90.93 ms/step
Model tresnet_m.miil_in21k_ft_in1k created, param count: 31389032
Running train benchmark on tresnet_m.miil_in21k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1143.99 samples/sec. 223.779 ms/step.
Train [16/40]. 1144.76 samples/sec. 223.628 ms/step.
Train [24/40]. 1145.98 samples/sec. 223.390 ms/step.
Train [32/40]. 1145.92 samples/sec. 223.401 ms/step.
Train [40/40]. 1145.53 samples/sec. 223.477 ms/step.
Train benchmark of tresnet_m.miil_in21k_ft_in1k done. 1139.95 samples/sec, 223.48 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tresnet_v2_l.miil_in21k_ft_in1k created, param count: 46174824
Running inference benchmark on tresnet_v2_l.miil_in21k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1696.81 samples/sec. 150.872 ms/step.
Infer [16/40]. 1693.79 samples/sec. 151.140 ms/step.
Infer [24/40]. 1694.07 samples/sec. 151.115 ms/step.
Infer [32/40]. 1692.16 samples/sec. 151.286 ms/step.
Infer [40/40]. 1692.64 samples/sec. 151.243 ms/step.
Inference benchmark of tresnet_v2_l.miil_in21k_ft_in1k done. 1692.28 samples/sec, 151.24 ms/step
Model tresnet_v2_l.miil_in21k_ft_in1k created, param count: 46174824
Running train benchmark on tresnet_v2_l.miil_in21k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 680.68 samples/sec. 376.093 ms/step.
Train [16/40]. 680.77 samples/sec. 376.046 ms/step.
Train [24/40]. 680.17 samples/sec. 376.377 ms/step.
Train [32/40]. 680.28 samples/sec. 376.315 ms/step.
Train [40/40]. 680.36 samples/sec. 376.273 ms/step.
Train benchmark of tresnet_v2_l.miil_in21k_ft_in1k done. 677.26 samples/sec, 376.27 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tresnet_xl.miil_in1k created, param count: 78436244
Running inference benchmark on tresnet_xl.miil_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1957.96 samples/sec. 130.749 ms/step.
Infer [16/40]. 1957.69 samples/sec. 130.767 ms/step.
Infer [24/40]. 1957.51 samples/sec. 130.779 ms/step.
Infer [32/40]. 1957.47 samples/sec. 130.781 ms/step.
Infer [40/40]. 1957.46 samples/sec. 130.781 ms/step.
Inference benchmark of tresnet_xl.miil_in1k done. 1957.01 samples/sec, 130.78 ms/step
Model tresnet_xl.miil_in1k created, param count: 78436244
Running train benchmark on tresnet_xl.miil_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 569.95 samples/sec. 449.161 ms/step.
Train [16/40]. 569.92 samples/sec. 449.185 ms/step.
Train [24/40]. 569.90 samples/sec. 449.198 ms/step.
Train [32/40]. 569.91 samples/sec. 449.196 ms/step.
Train [40/40]. 569.91 samples/sec. 449.196 ms/step.
Train benchmark of tresnet_xl.miil_in1k done. 567.56 samples/sec, 449.20 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model tresnet_xl.miil_in1k_448 created, param count: 78436244
Running inference benchmark on tresnet_xl.miil_in1k_448 for 40 steps w/ input size (3, 448, 448) and batch size 256.
Infer [8/40]. 476.81 samples/sec. 536.907 ms/step.
Infer [16/40]. 476.79 samples/sec. 536.920 ms/step.
Infer [24/40]. 476.78 samples/sec. 536.934 ms/step.
Infer [32/40]. 476.78 samples/sec. 536.938 ms/step.
Infer [40/40]. 476.78 samples/sec. 536.933 ms/step.
Inference benchmark of tresnet_xl.miil_in1k_448 done. 476.74 samples/sec, 536.93 ms/step
Model tresnet_xl.miil_in1k_448 created, param count: 78436244
Running train benchmark on tresnet_xl.miil_in1k_448 for 40 steps w/ input size (3, 448, 448) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 510.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 470.06 MiB is free. Including non-PyTorch memory, this process has 23.18 GiB memory in use. Of the allocated memory 21.07 GiB is allocated by PyTorch, and 902.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model tresnet_xl.miil_in1k_448 created, param count: 78436244
Running train benchmark on tresnet_xl.miil_in1k_448 for 40 steps w/ input size (3, 448, 448) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 382.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 328.06 MiB is free. Including non-PyTorch memory, this process has 23.32 GiB memory in use. Of the allocated memory 21.47 GiB is allocated by PyTorch, and 634.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model tresnet_xl.miil_in1k_448 created, param count: 78436244
Running train benchmark on tresnet_xl.miil_in1k_448 for 40 steps w/ input size (3, 448, 448) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 204.06 MiB is free. Including non-PyTorch memory, this process has 23.44 GiB memory in use. Of the allocated memory 21.63 GiB is allocated by PyTorch, and 591.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model tresnet_xl.miil_in1k_448 created, param count: 78436244
Running train benchmark on tresnet_xl.miil_in1k_448 for 40 steps w/ input size (3, 448, 448) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 82.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 21.33 GiB is allocated by PyTorch, and 1017.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model tresnet_xl.miil_in1k_448 created, param count: 78436244
Running train benchmark on tresnet_xl.miil_in1k_448 for 40 steps w/ input size (3, 448, 448) and batch size 64.
Train [8/40]. 146.80 samples/sec. 435.979 ms/step.
Train [16/40]. 146.80 samples/sec. 435.968 ms/step.
Train [24/40]. 146.80 samples/sec. 435.981 ms/step.
Train [32/40]. 146.80 samples/sec. 435.973 ms/step.
Train [40/40]. 146.80 samples/sec. 435.975 ms/step.
Train benchmark of tresnet_xl.miil_in1k_448 done. 146.18 samples/sec, 435.98 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model twins_pcpvt_base.in1k created, param count: 43828456
Running inference benchmark on twins_pcpvt_base.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3681.01 samples/sec. 69.546 ms/step.
Infer [16/40]. 3680.91 samples/sec. 69.548 ms/step.
Infer [24/40]. 3680.91 samples/sec. 69.548 ms/step.
Infer [32/40]. 3680.81 samples/sec. 69.550 ms/step.
Infer [40/40]. 3680.89 samples/sec. 69.548 ms/step.
Inference benchmark of twins_pcpvt_base.in1k done. 3679.49 samples/sec, 69.55 ms/step
Model twins_pcpvt_base.in1k created, param count: 43828456
Running train benchmark on twins_pcpvt_base.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 990.05 samples/sec. 258.572 ms/step.
Train [16/40]. 990.10 samples/sec. 258.559 ms/step.
Train [24/40]. 990.10 samples/sec. 258.559 ms/step.
Train [32/40]. 990.11 samples/sec. 258.558 ms/step.
Train [40/40]. 990.10 samples/sec. 258.560 ms/step.
Train benchmark of twins_pcpvt_base.in1k done. 981.80 samples/sec, 258.56 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model twins_pcpvt_large.in1k created, param count: 60989672
Running inference benchmark on twins_pcpvt_large.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 2666.32 samples/sec. 96.013 ms/step.
Infer [16/40]. 2666.19 samples/sec. 96.017 ms/step.
Infer [24/40]. 2666.07 samples/sec. 96.022 ms/step.
Infer [32/40]. 2666.08 samples/sec. 96.021 ms/step.
Infer [40/40]. 2666.07 samples/sec. 96.021 ms/step.
Inference benchmark of twins_pcpvt_large.in1k done. 2665.29 samples/sec, 96.02 ms/step
Model twins_pcpvt_large.in1k created, param count: 60989672
Running train benchmark on twins_pcpvt_large.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 2.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 22.18 GiB is allocated by PyTorch, and 230.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model twins_pcpvt_large.in1k created, param count: 60989672
Running train benchmark on twins_pcpvt_large.in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
Train [8/40]. 712.01 samples/sec. 269.659 ms/step.
Train [16/40]. 712.05 samples/sec. 269.642 ms/step.
Train [24/40]. 712.04 samples/sec. 269.648 ms/step.
Train [32/40]. 712.06 samples/sec. 269.642 ms/step.
Train [40/40]. 712.07 samples/sec. 269.637 ms/step.
Train benchmark of twins_pcpvt_large.in1k done. 703.87 samples/sec, 269.64 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model twins_pcpvt_small.in1k created, param count: 24106216
Running inference benchmark on twins_pcpvt_small.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 5065.68 samples/sec. 50.536 ms/step.
Infer [16/40]. 5065.70 samples/sec. 50.536 ms/step.
Infer [24/40]. 5065.69 samples/sec. 50.536 ms/step.
Infer [32/40]. 5065.25 samples/sec. 50.540 ms/step.
Infer [40/40]. 5064.41 samples/sec. 50.549 ms/step.
Inference benchmark of twins_pcpvt_small.in1k done. 5061.81 samples/sec, 50.55 ms/step
Model twins_pcpvt_small.in1k created, param count: 24106216
Running train benchmark on twins_pcpvt_small.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1375.23 samples/sec. 186.151 ms/step.
Train [16/40]. 1375.16 samples/sec. 186.161 ms/step.
Train [24/40]. 1375.12 samples/sec. 186.165 ms/step.
Train [32/40]. 1375.09 samples/sec. 186.170 ms/step.
Train [40/40]. 1375.10 samples/sec. 186.168 ms/step.
Train benchmark of twins_pcpvt_small.in1k done. 1363.98 samples/sec, 186.17 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model twins_svt_base.in1k created, param count: 56070952
Running inference benchmark on twins_svt_base.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 2885.66 samples/sec. 88.715 ms/step.
Infer [16/40]. 2885.27 samples/sec. 88.727 ms/step.
Infer [24/40]. 2885.13 samples/sec. 88.731 ms/step.
Infer [32/40]. 2885.02 samples/sec. 88.734 ms/step.
Infer [40/40]. 2884.90 samples/sec. 88.738 ms/step.
Inference benchmark of twins_svt_base.in1k done. 2884.00 samples/sec, 88.74 ms/step
Model twins_svt_base.in1k created, param count: 56070952
Running train benchmark on twins_svt_base.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 872.05 samples/sec. 293.562 ms/step.
Train [16/40]. 872.01 samples/sec. 293.574 ms/step.
Train [24/40]. 871.98 samples/sec. 293.586 ms/step.
Train [32/40]. 871.97 samples/sec. 293.588 ms/step.
Train [40/40]. 871.97 samples/sec. 293.590 ms/step.
Train benchmark of twins_svt_base.in1k done. 865.86 samples/sec, 293.59 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model twins_svt_large.in1k created, param count: 99271400
Running inference benchmark on twins_svt_large.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1976.32 samples/sec. 129.534 ms/step.
Infer [16/40]. 1976.48 samples/sec. 129.523 ms/step.
Infer [24/40]. 1976.51 samples/sec. 129.521 ms/step.
Infer [32/40]. 1976.43 samples/sec. 129.527 ms/step.
Infer [40/40]. 1976.45 samples/sec. 129.525 ms/step.
Inference benchmark of twins_svt_large.in1k done. 1975.98 samples/sec, 129.53 ms/step
Model twins_svt_large.in1k created, param count: 99271400
Running train benchmark on twins_svt_large.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 10.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.33 GiB is allocated by PyTorch, and 65.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model twins_svt_large.in1k created, param count: 99271400
Running train benchmark on twins_svt_large.in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
Train [8/40]. 627.02 samples/sec. 306.213 ms/step.
Train [16/40]. 626.93 samples/sec. 306.252 ms/step.
Train [24/40]. 626.92 samples/sec. 306.257 ms/step.
Train [32/40]. 626.93 samples/sec. 306.256 ms/step.
Train [40/40]. 626.92 samples/sec. 306.261 ms/step.
Train benchmark of twins_svt_large.in1k done. 622.79 samples/sec, 306.26 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model twins_svt_small.in1k created, param count: 24060776
Running inference benchmark on twins_svt_small.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 5851.12 samples/sec. 43.752 ms/step.
Infer [16/40]. 5850.49 samples/sec. 43.757 ms/step.
Infer [24/40]. 5850.40 samples/sec. 43.758 ms/step.
Infer [32/40]. 5850.29 samples/sec. 43.759 ms/step.
Infer [40/40]. 5850.20 samples/sec. 43.759 ms/step.
Inference benchmark of twins_svt_small.in1k done. 5846.84 samples/sec, 43.76 ms/step
Model twins_svt_small.in1k created, param count: 24060776
Running train benchmark on twins_svt_small.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1699.64 samples/sec. 150.620 ms/step.
Train [16/40]. 1699.72 samples/sec. 150.613 ms/step.
Train [24/40]. 1699.77 samples/sec. 150.608 ms/step.
Train [32/40]. 1699.75 samples/sec. 150.611 ms/step.
Train [40/40]. 1699.71 samples/sec. 150.614 ms/step.
Train benchmark of twins_svt_small.in1k done. 1683.73 samples/sec, 150.61 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model vgg11.tv_in1k created, param count: 132863336
Running inference benchmark on vgg11.tv_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 4481.42 samples/sec. 57.125 ms/step.
Infer [16/40]. 4481.31 samples/sec. 57.126 ms/step.
Infer [24/40]. 4481.39 samples/sec. 57.125 ms/step.
Infer [32/40]. 4481.43 samples/sec. 57.125 ms/step.
Infer [40/40]. 4481.35 samples/sec. 57.126 ms/step.
Inference benchmark of vgg11.tv_in1k done. 4479.30 samples/sec, 57.13 ms/step
Model vgg11.tv_in1k created, param count: 132863336
Running train benchmark on vgg11.tv_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1406.86 samples/sec. 181.966 ms/step.
Train [16/40]. 1407.02 samples/sec. 181.945 ms/step.
Train [24/40]. 1407.11 samples/sec. 181.934 ms/step.
Train [32/40]. 1407.08 samples/sec. 181.937 ms/step.
Train [40/40]. 1407.04 samples/sec. 181.942 ms/step.
Train benchmark of vgg11.tv_in1k done. 1404.25 samples/sec, 181.94 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model vgg11_bn.tv_in1k created, param count: 132868840
Running inference benchmark on vgg11_bn.tv_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3909.01 samples/sec. 65.490 ms/step.
Infer [16/40]. 3908.66 samples/sec. 65.496 ms/step.
Infer [24/40]. 3908.08 samples/sec. 65.505 ms/step.
Infer [32/40]. 3907.47 samples/sec. 65.516 ms/step.
Infer [40/40]. 3907.09 samples/sec. 65.522 ms/step.
Inference benchmark of vgg11_bn.tv_in1k done. 3905.38 samples/sec, 65.52 ms/step
Model vgg11_bn.tv_in1k created, param count: 132868840
Running train benchmark on vgg11_bn.tv_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1098.36 samples/sec. 233.075 ms/step.
Train [16/40]. 1098.50 samples/sec. 233.046 ms/step.
Train [24/40]. 1098.54 samples/sec. 233.037 ms/step.
Train [32/40]. 1098.53 samples/sec. 233.039 ms/step.
Train [40/40]. 1098.50 samples/sec. 233.044 ms/step.
Train benchmark of vgg11_bn.tv_in1k done. 1096.31 samples/sec, 233.04 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model vgg13.tv_in1k created, param count: 133047848
Running inference benchmark on vgg13.tv_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 2240.80 samples/sec. 114.245 ms/step.
Infer [16/40]. 2254.87 samples/sec. 113.532 ms/step.
Infer [24/40]. 2252.67 samples/sec. 113.643 ms/step.
Infer [32/40]. 2258.30 samples/sec. 113.360 ms/step.
Infer [40/40]. 2264.12 samples/sec. 113.068 ms/step.
Inference benchmark of vgg13.tv_in1k done. 2263.53 samples/sec, 113.07 ms/step
Model vgg13.tv_in1k created, param count: 133047848
Running train benchmark on vgg13.tv_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 845.99 samples/sec. 302.603 ms/step.
Train [16/40]. 847.45 samples/sec. 302.083 ms/step.
Train [24/40]. 848.00 samples/sec. 301.888 ms/step.
Train [32/40]. 848.35 samples/sec. 301.762 ms/step.
Train [40/40]. 848.77 samples/sec. 301.615 ms/step.
Train benchmark of vgg13.tv_in1k done. 847.48 samples/sec, 301.62 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model vgg13_bn.tv_in1k created, param count: 133053736
Running inference benchmark on vgg13_bn.tv_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 2004.66 samples/sec. 127.703 ms/step.
Infer [16/40]. 2008.28 samples/sec. 127.472 ms/step.
Infer [24/40]. 2007.01 samples/sec. 127.553 ms/step.
Infer [32/40]. 2005.85 samples/sec. 127.627 ms/step.
Infer [40/40]. 2005.30 samples/sec. 127.661 ms/step.
Inference benchmark of vgg13_bn.tv_in1k done. 2004.81 samples/sec, 127.66 ms/step
Model vgg13_bn.tv_in1k created, param count: 133053736
Running train benchmark on vgg13_bn.tv_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 655.57 samples/sec. 390.501 ms/step.
Train [16/40]. 655.44 samples/sec. 390.578 ms/step.
Train [24/40]. 655.78 samples/sec. 390.378 ms/step.
Train [32/40]. 655.78 samples/sec. 390.374 ms/step.
Train [40/40]. 655.32 samples/sec. 390.650 ms/step.
Train benchmark of vgg13_bn.tv_in1k done. 654.39 samples/sec, 390.65 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model vgg16.tv_in1k created, param count: 138357544
Running inference benchmark on vgg16.tv_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1934.03 samples/sec. 132.366 ms/step.
Infer [16/40]. 1936.05 samples/sec. 132.228 ms/step.
Infer [24/40]. 1935.11 samples/sec. 132.292 ms/step.
Infer [32/40]. 1940.19 samples/sec. 131.946 ms/step.
Infer [40/40]. 1939.55 samples/sec. 131.989 ms/step.
Inference benchmark of vgg16.tv_in1k done. 1939.08 samples/sec, 131.99 ms/step
Model vgg16.tv_in1k created, param count: 138357544
Running train benchmark on vgg16.tv_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 719.56 samples/sec. 355.774 ms/step.
Train [16/40]. 720.15 samples/sec. 355.484 ms/step.
Train [24/40]. 719.80 samples/sec. 355.655 ms/step.
Train [32/40]. 719.67 samples/sec. 355.718 ms/step.
Train [40/40]. 718.95 samples/sec. 356.075 ms/step.
Train benchmark of vgg16.tv_in1k done. 717.94 samples/sec, 356.07 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model vgg16_bn.tv_in1k created, param count: 138365992
Running inference benchmark on vgg16_bn.tv_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1734.41 samples/sec. 147.600 ms/step.
Infer [16/40]. 1736.02 samples/sec. 147.464 ms/step.
Infer [24/40]. 1736.15 samples/sec. 147.452 ms/step.
Infer [32/40]. 1735.61 samples/sec. 147.499 ms/step.
Infer [40/40]. 1734.62 samples/sec. 147.583 ms/step.
Inference benchmark of vgg16_bn.tv_in1k done. 1734.22 samples/sec, 147.58 ms/step
Model vgg16_bn.tv_in1k created, param count: 138365992
Running train benchmark on vgg16_bn.tv_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 568.18 samples/sec. 450.558 ms/step.
Train [16/40]. 567.87 samples/sec. 450.810 ms/step.
Train [24/40]. 567.69 samples/sec. 450.949 ms/step.
Train [32/40]. 567.60 samples/sec. 451.020 ms/step.
Train [40/40]. 567.49 samples/sec. 451.108 ms/step.
Train benchmark of vgg16_bn.tv_in1k done. 566.74 samples/sec, 451.11 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model vgg19.tv_in1k created, param count: 143667240
Running inference benchmark on vgg19.tv_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1702.22 samples/sec. 150.392 ms/step.
Infer [16/40]. 1702.63 samples/sec. 150.355 ms/step.
Infer [24/40]. 1702.07 samples/sec. 150.405 ms/step.
Infer [32/40]. 1701.43 samples/sec. 150.462 ms/step.
Infer [40/40]. 1701.25 samples/sec. 150.478 ms/step.
Inference benchmark of vgg19.tv_in1k done. 1700.86 samples/sec, 150.48 ms/step
Model vgg19.tv_in1k created, param count: 143667240
Running train benchmark on vgg19.tv_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 624.18 samples/sec. 410.138 ms/step.
Train [16/40]. 624.13 samples/sec. 410.172 ms/step.
Train [24/40]. 624.26 samples/sec. 410.088 ms/step.
Train [32/40]. 623.97 samples/sec. 410.274 ms/step.
Train [40/40]. 624.05 samples/sec. 410.224 ms/step.
Train benchmark of vgg19.tv_in1k done. 623.25 samples/sec, 410.22 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model vgg19_bn.tv_in1k created, param count: 143678248
Running inference benchmark on vgg19_bn.tv_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 1533.85 samples/sec. 166.900 ms/step.
Infer [16/40]. 1535.31 samples/sec. 166.742 ms/step.
Infer [24/40]. 1538.44 samples/sec. 166.402 ms/step.
Infer [32/40]. 1537.74 samples/sec. 166.478 ms/step.
Infer [40/40]. 1536.75 samples/sec. 166.586 ms/step.
Inference benchmark of vgg19_bn.tv_in1k done. 1536.44 samples/sec, 166.59 ms/step
Model vgg19_bn.tv_in1k created, param count: 143678248
Running train benchmark on vgg19_bn.tv_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 500.17 samples/sec. 511.824 ms/step.
Train [16/40]. 500.07 samples/sec. 511.933 ms/step.
Train [24/40]. 499.97 samples/sec. 512.028 ms/step.
Train [32/40]. 499.58 samples/sec. 512.427 ms/step.
Train [40/40]. 499.59 samples/sec. 512.425 ms/step.
Train benchmark of vgg19_bn.tv_in1k done. 498.96 samples/sec, 512.42 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model visformer_small.in1k created, param count: 40219592
Running inference benchmark on visformer_small.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 5037.55 samples/sec. 50.818 ms/step.
Infer [16/40]. 5036.92 samples/sec. 50.825 ms/step.
Infer [24/40]. 5036.48 samples/sec. 50.829 ms/step.
Infer [32/40]. 5036.48 samples/sec. 50.829 ms/step.
Infer [40/40]. 5036.30 samples/sec. 50.831 ms/step.
Inference benchmark of visformer_small.in1k done. 5033.68 samples/sec, 50.83 ms/step
Model visformer_small.in1k created, param count: 40219592
Running train benchmark on visformer_small.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 1599.57 samples/sec. 160.043 ms/step.
Train [16/40]. 1599.66 samples/sec. 160.034 ms/step.
Train [24/40]. 1599.72 samples/sec. 160.028 ms/step.
Train [32/40]. 1599.77 samples/sec. 160.024 ms/step.
Train [40/40]. 1599.71 samples/sec. 160.029 ms/step.
Train benchmark of visformer_small.in1k done. 1592.29 samples/sec, 160.03 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model visformer_tiny.in1k created, param count: 10321368
Running inference benchmark on visformer_tiny.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 12219.26 samples/sec. 20.951 ms/step.
Infer [16/40]. 12217.15 samples/sec. 20.954 ms/step.
Infer [24/40]. 12216.23 samples/sec. 20.956 ms/step.
Infer [32/40]. 12214.57 samples/sec. 20.959 ms/step.
Infer [40/40]. 12213.78 samples/sec. 20.960 ms/step.
Inference benchmark of visformer_tiny.in1k done. 12199.86 samples/sec, 20.96 ms/step
Model visformer_tiny.in1k created, param count: 10321368
Running train benchmark on visformer_tiny.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 3292.81 samples/sec. 77.745 ms/step.
Train [16/40]. 3292.85 samples/sec. 77.744 ms/step.
Train [24/40]. 3292.99 samples/sec. 77.741 ms/step.
Train [32/40]. 3292.92 samples/sec. 77.743 ms/step.
Train [40/40]. 3292.86 samples/sec. 77.744 ms/step.
Train benchmark of visformer_tiny.in1k done. 3268.10 samples/sec, 77.74 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model vit_base_patch8_224.augreg2_in21k_ft_in1k created, param count: 86576872
Running inference benchmark on vit_base_patch8_224.augreg2_in21k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 705.70 samples/sec. 362.760 ms/step.
Infer [16/40]. 705.71 samples/sec. 362.753 ms/step.
Infer [24/40]. 705.39 samples/sec. 362.920 ms/step.
Infer [32/40]. 705.03 samples/sec. 363.107 ms/step.
Infer [40/40]. 704.75 samples/sec. 363.251 ms/step.
Inference benchmark of vit_base_patch8_224.augreg2_in21k_ft_in1k done. 704.66 samples/sec, 363.25 ms/step
Model vit_base_patch8_224.augreg2_in21k_ft_in1k created, param count: 86576872
Running train benchmark on vit_base_patch8_224.augreg2_in21k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 1.15 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.12 GiB is free. Including non-PyTorch memory, this process has 22.53 GiB memory in use. Of the allocated memory 21.02 GiB is allocated by PyTorch, and 282.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model vit_base_patch8_224.augreg2_in21k_ft_in1k created, param count: 86576872
Running train benchmark on vit_base_patch8_224.augreg2_in21k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 664.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 618.06 MiB is free. Including non-PyTorch memory, this process has 23.04 GiB memory in use. Of the allocated memory 21.43 GiB is allocated by PyTorch, and 387.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model vit_base_patch8_224.augreg2_in21k_ft_in1k created, param count: 86576872
Running train benchmark on vit_base_patch8_224.augreg2_in21k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 590.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 98.06 MiB is free. Including non-PyTorch memory, this process has 23.54 GiB memory in use. Of the allocated memory 22.16 GiB is allocated by PyTorch, and 151.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model vit_base_patch8_224.augreg2_in21k_ft_in1k created, param count: 86576872
Running train benchmark on vit_base_patch8_224.augreg2_in21k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 96.
Train [8/40]. 218.39 samples/sec. 439.585 ms/step.
Train [16/40]. 218.35 samples/sec. 439.653 ms/step.
Train [24/40]. 218.36 samples/sec. 439.638 ms/step.
Train [32/40]. 218.36 samples/sec. 439.633 ms/step.
Train [40/40]. 218.35 samples/sec. 439.657 ms/step.
Train benchmark of vit_base_patch8_224.augreg2_in21k_ft_in1k done. 217.84 samples/sec, 439.66 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model vit_base_patch8_224.augreg_in21k_ft_in1k created, param count: 86576872
Running inference benchmark on vit_base_patch8_224.augreg_in21k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 703.97 samples/sec. 363.652 ms/step.
Infer [16/40]. 704.21 samples/sec. 363.528 ms/step.
Infer [24/40]. 704.18 samples/sec. 363.546 ms/step.
Infer [32/40]. 704.11 samples/sec. 363.581 ms/step.
Infer [40/40]. 704.05 samples/sec. 363.610 ms/step.
Inference benchmark of vit_base_patch8_224.augreg_in21k_ft_in1k done. 703.96 samples/sec, 363.61 ms/step
Model vit_base_patch8_224.augreg_in21k_ft_in1k created, param count: 86576872
Running train benchmark on vit_base_patch8_224.augreg_in21k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 1.15 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.12 GiB is free. Including non-PyTorch memory, this process has 22.53 GiB memory in use. Of the allocated memory 21.02 GiB is allocated by PyTorch, and 282.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model vit_base_patch8_224.augreg_in21k_ft_in1k created, param count: 86576872
Running train benchmark on vit_base_patch8_224.augreg_in21k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 664.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 618.06 MiB is free. Including non-PyTorch memory, this process has 23.04 GiB memory in use. Of the allocated memory 21.43 GiB is allocated by PyTorch, and 387.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model vit_base_patch8_224.augreg_in21k_ft_in1k created, param count: 86576872
Running train benchmark on vit_base_patch8_224.augreg_in21k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 590.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 98.06 MiB is free. Including non-PyTorch memory, this process has 23.54 GiB memory in use. Of the allocated memory 22.16 GiB is allocated by PyTorch, and 151.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model vit_base_patch8_224.augreg_in21k_ft_in1k created, param count: 86576872
Running train benchmark on vit_base_patch8_224.augreg_in21k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 96.
Train [8/40]. 218.28 samples/sec. 439.799 ms/step.
Train [16/40]. 218.26 samples/sec. 439.834 ms/step.
Train [24/40]. 218.24 samples/sec. 439.882 ms/step.
Train [32/40]. 218.24 samples/sec. 439.886 ms/step.
Train [40/40]. 218.23 samples/sec. 439.906 ms/step.
Train benchmark of vit_base_patch8_224.augreg_in21k_ft_in1k done. 217.71 samples/sec, 439.91 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model vit_base_patch8_224.dino created, param count: 85807872
Running inference benchmark on vit_base_patch8_224.dino for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 704.11 samples/sec. 363.577 ms/step.
Infer [16/40]. 704.05 samples/sec. 363.610 ms/step.
Infer [24/40]. 703.98 samples/sec. 363.646 ms/step.
Infer [32/40]. 703.90 samples/sec. 363.686 ms/step.
Infer [40/40]. 703.72 samples/sec. 363.782 ms/step.
Inference benchmark of vit_base_patch8_224.dino done. 703.62 samples/sec, 363.78 ms/step
Model vit_base_patch8_224.dino created, param count: 85807872
Running train benchmark on vit_base_patch8_224.dino for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 1.15 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.12 GiB is free. Including non-PyTorch memory, this process has 22.53 GiB memory in use. Of the allocated memory 21.02 GiB is allocated by PyTorch, and 283.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model vit_base_patch8_224.dino created, param count: 85807872
Running train benchmark on vit_base_patch8_224.dino for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 664.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 618.06 MiB is free. Including non-PyTorch memory, this process has 23.04 GiB memory in use. Of the allocated memory 21.43 GiB is allocated by PyTorch, and 388.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model vit_base_patch8_224.dino created, param count: 85807872
Running train benchmark on vit_base_patch8_224.dino for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 590.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 98.06 MiB is free. Including non-PyTorch memory, this process has 23.54 GiB memory in use. Of the allocated memory 22.16 GiB is allocated by PyTorch, and 152.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model vit_base_patch8_224.dino created, param count: 85807872
Running train benchmark on vit_base_patch8_224.dino for 40 steps w/ input size (3, 224, 224) and batch size 96.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model vit_base_patch8_224.dino created, param count: 85807872
Running train benchmark on vit_base_patch8_224.dino for 40 steps w/ input size (3, 224, 224) and batch size 64.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model vit_base_patch8_224.dino created, param count: 85807872
Running train benchmark on vit_base_patch8_224.dino for 40 steps w/ input size (3, 224, 224) and batch size 48.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model vit_base_patch8_224.dino created, param count: 85807872
Running train benchmark on vit_base_patch8_224.dino for 40 steps w/ input size (3, 224, 224) and batch size 32.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model vit_base_patch8_224.dino created, param count: 85807872
Running train benchmark on vit_base_patch8_224.dino for 40 steps w/ input size (3, 224, 224) and batch size 24.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model vit_base_patch8_224.dino created, param count: 85807872
Running train benchmark on vit_base_patch8_224.dino for 40 steps w/ input size (3, 224, 224) and batch size 16.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 12 for retry.
Model vit_base_patch8_224.dino created, param count: 85807872
Running train benchmark on vit_base_patch8_224.dino for 40 steps w/ input size (3, 224, 224) and batch size 12.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 8 for retry.
Model vit_base_patch8_224.dino created, param count: 85807872
Running train benchmark on vit_base_patch8_224.dino for 40 steps w/ input size (3, 224, 224) and batch size 8.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 6 for retry.
Model vit_base_patch8_224.dino created, param count: 85807872
Running train benchmark on vit_base_patch8_224.dino for 40 steps w/ input size (3, 224, 224) and batch size 6.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 4 for retry.
Model vit_base_patch8_224.dino created, param count: 85807872
Running train benchmark on vit_base_patch8_224.dino for 40 steps w/ input size (3, 224, 224) and batch size 4.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 3 for retry.
Model vit_base_patch8_224.dino created, param count: 85807872
Running train benchmark on vit_base_patch8_224.dino for 40 steps w/ input size (3, 224, 224) and batch size 3.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 2 for retry.
Model vit_base_patch8_224.dino created, param count: 85807872
Running train benchmark on vit_base_patch8_224.dino for 40 steps w/ input size (3, 224, 224) and batch size 2.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 1 for retry.
Model vit_base_patch8_224.dino created, param count: 85807872
Running train benchmark on vit_base_patch8_224.dino for 40 steps w/ input size (3, 224, 224) and batch size 1.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 0 for retry.
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model vit_base_patch14_dinov2.lvd142m created, param count: 86579712
Running inference benchmark on vit_base_patch14_dinov2.lvd142m for 40 steps w/ input size (3, 518, 518) and batch size 256.
Infer [8/40]. 336.17 samples/sec. 761.529 ms/step.
Infer [16/40]. 335.95 samples/sec. 762.028 ms/step.
Infer [24/40]. 335.82 samples/sec. 762.318 ms/step.
Infer [32/40]. 335.78 samples/sec. 762.395 ms/step.
Infer [40/40]. 335.74 samples/sec. 762.504 ms/step.
Inference benchmark of vit_base_patch14_dinov2.lvd142m done. 335.71 samples/sec, 762.50 ms/step
Model vit_base_patch14_dinov2.lvd142m created, param count: 86579712
Running train benchmark on vit_base_patch14_dinov2.lvd142m for 40 steps w/ input size (3, 518, 518) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 514.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 8.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.21 GiB is allocated by PyTorch, and 198.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model vit_base_patch14_dinov2.lvd142m created, param count: 86579712
Running train benchmark on vit_base_patch14_dinov2.lvd142m for 40 steps w/ input size (3, 518, 518) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 1.13 GiB. GPU 0 has a total capacty of 23.65 GiB of which 510.06 MiB is free. Including non-PyTorch memory, this process has 23.14 GiB memory in use. Of the allocated memory 21.61 GiB is allocated by PyTorch, and 309.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model vit_base_patch14_dinov2.lvd142m created, param count: 86579712
Running train benchmark on vit_base_patch14_dinov2.lvd142m for 40 steps w/ input size (3, 518, 518) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 1.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 464.06 MiB is free. Including non-PyTorch memory, this process has 23.19 GiB memory in use. Of the allocated memory 21.75 GiB is allocated by PyTorch, and 209.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model vit_base_patch14_dinov2.lvd142m created, param count: 86579712
Running train benchmark on vit_base_patch14_dinov2.lvd142m for 40 steps w/ input size (3, 518, 518) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 194.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 54.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 22.20 GiB is allocated by PyTorch, and 158.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model vit_base_patch14_dinov2.lvd142m created, param count: 86579712
Running train benchmark on vit_base_patch14_dinov2.lvd142m for 40 steps w/ input size (3, 518, 518) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 514.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 402.06 MiB is free. Including non-PyTorch memory, this process has 23.25 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 233.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model vit_base_patch14_dinov2.lvd142m created, param count: 86579712
Running train benchmark on vit_base_patch14_dinov2.lvd142m for 40 steps w/ input size (3, 518, 518) and batch size 48.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model vit_base_patch14_dinov2.lvd142m created, param count: 86579712
Running train benchmark on vit_base_patch14_dinov2.lvd142m for 40 steps w/ input size (3, 518, 518) and batch size 32.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model vit_base_patch14_dinov2.lvd142m created, param count: 86579712
Running train benchmark on vit_base_patch14_dinov2.lvd142m for 40 steps w/ input size (3, 518, 518) and batch size 24.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model vit_base_patch14_dinov2.lvd142m created, param count: 86579712
Running train benchmark on vit_base_patch14_dinov2.lvd142m for 40 steps w/ input size (3, 518, 518) and batch size 16.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 12 for retry.
Model vit_base_patch14_dinov2.lvd142m created, param count: 86579712
Running train benchmark on vit_base_patch14_dinov2.lvd142m for 40 steps w/ input size (3, 518, 518) and batch size 12.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 8 for retry.
Model vit_base_patch14_dinov2.lvd142m created, param count: 86579712
Running train benchmark on vit_base_patch14_dinov2.lvd142m for 40 steps w/ input size (3, 518, 518) and batch size 8.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 6 for retry.
Model vit_base_patch14_dinov2.lvd142m created, param count: 86579712
Running train benchmark on vit_base_patch14_dinov2.lvd142m for 40 steps w/ input size (3, 518, 518) and batch size 6.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 4 for retry.
Model vit_base_patch14_dinov2.lvd142m created, param count: 86579712
Running train benchmark on vit_base_patch14_dinov2.lvd142m for 40 steps w/ input size (3, 518, 518) and batch size 4.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 3 for retry.
Model vit_base_patch14_dinov2.lvd142m created, param count: 86579712
Running train benchmark on vit_base_patch14_dinov2.lvd142m for 40 steps w/ input size (3, 518, 518) and batch size 3.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 2 for retry.
Model vit_base_patch14_dinov2.lvd142m created, param count: 86579712
Running train benchmark on vit_base_patch14_dinov2.lvd142m for 40 steps w/ input size (3, 518, 518) and batch size 2.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 1 for retry.
Model vit_base_patch14_dinov2.lvd142m created, param count: 86579712
Running train benchmark on vit_base_patch14_dinov2.lvd142m for 40 steps w/ input size (3, 518, 518) and batch size 1.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 0 for retry.
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model vit_base_patch16_224.augreg2_in21k_ft_in1k created, param count: 86567656
Running inference benchmark on vit_base_patch16_224.augreg2_in21k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3241.38 samples/sec. 78.979 ms/step.
Infer [16/40]. 3243.36 samples/sec. 78.930 ms/step.
Infer [24/40]. 3242.78 samples/sec. 78.945 ms/step.
Infer [32/40]. 3242.73 samples/sec. 78.946 ms/step.
Infer [40/40]. 3243.00 samples/sec. 78.939 ms/step.
Inference benchmark of vit_base_patch16_224.augreg2_in21k_ft_in1k done. 3241.93 samples/sec, 78.94 ms/step
Model vit_base_patch16_224.augreg2_in21k_ft_in1k created, param count: 86567656
Running train benchmark on vit_base_patch16_224.augreg2_in21k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 978.16 samples/sec. 261.715 ms/step.
Train [16/40]. 978.12 samples/sec. 261.727 ms/step.
Train [24/40]. 978.11 samples/sec. 261.729 ms/step.
Train [32/40]. 978.11 samples/sec. 261.730 ms/step.
Train [40/40]. 978.11 samples/sec. 261.728 ms/step.
Train benchmark of vit_base_patch16_224.augreg2_in21k_ft_in1k done. 974.59 samples/sec, 261.73 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model vit_base_patch16_224.augreg_in1k created, param count: 86567656
Running inference benchmark on vit_base_patch16_224.augreg_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3240.02 samples/sec. 79.012 ms/step.
Infer [16/40]. 3238.80 samples/sec. 79.042 ms/step.
Infer [24/40]. 3237.74 samples/sec. 79.067 ms/step.
Infer [32/40]. 3237.04 samples/sec. 79.085 ms/step.
Infer [40/40]. 3236.84 samples/sec. 79.089 ms/step.
Inference benchmark of vit_base_patch16_224.augreg_in1k done. 3235.71 samples/sec, 79.09 ms/step
Model vit_base_patch16_224.augreg_in1k created, param count: 86567656
Running train benchmark on vit_base_patch16_224.augreg_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 978.11 samples/sec. 261.730 ms/step.
Train [16/40]. 978.14 samples/sec. 261.723 ms/step.
Train [24/40]. 978.13 samples/sec. 261.724 ms/step.
Train [32/40]. 978.13 samples/sec. 261.724 ms/step.
Train [40/40]. 978.13 samples/sec. 261.723 ms/step.
Train benchmark of vit_base_patch16_224.augreg_in1k done. 974.68 samples/sec, 261.72 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model vit_base_patch16_224.augreg_in21k_ft_in1k created, param count: 86567656
Running inference benchmark on vit_base_patch16_224.augreg_in21k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3241.72 samples/sec. 78.971 ms/step.
Infer [16/40]. 3242.53 samples/sec. 78.951 ms/step.
Infer [24/40]. 3240.19 samples/sec. 79.008 ms/step.
Infer [32/40]. 3239.03 samples/sec. 79.036 ms/step.
Infer [40/40]. 3238.63 samples/sec. 79.046 ms/step.
Inference benchmark of vit_base_patch16_224.augreg_in21k_ft_in1k done. 3237.56 samples/sec, 79.05 ms/step
Model vit_base_patch16_224.augreg_in21k_ft_in1k created, param count: 86567656
Running train benchmark on vit_base_patch16_224.augreg_in21k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 978.14 samples/sec. 261.720 ms/step.
Train [16/40]. 978.15 samples/sec. 261.718 ms/step.
Train [24/40]. 978.13 samples/sec. 261.725 ms/step.
Train [32/40]. 978.11 samples/sec. 261.729 ms/step.
Train [40/40]. 978.12 samples/sec. 261.727 ms/step.
Train benchmark of vit_base_patch16_224.augreg_in21k_ft_in1k done. 974.69 samples/sec, 261.73 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model vit_base_patch16_224.dino created, param count: 85798656
Running inference benchmark on vit_base_patch16_224.dino for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3238.77 samples/sec. 79.042 ms/step.
Infer [16/40]. 3237.66 samples/sec. 79.070 ms/step.
Infer [24/40]. 3236.75 samples/sec. 79.092 ms/step.
Infer [32/40]. 3236.55 samples/sec. 79.097 ms/step.
Infer [40/40]. 3236.22 samples/sec. 79.105 ms/step.
Inference benchmark of vit_base_patch16_224.dino done. 3235.06 samples/sec, 79.11 ms/step
Model vit_base_patch16_224.dino created, param count: 85798656
Running train benchmark on vit_base_patch16_224.dino for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model vit_base_patch16_224.dino created, param count: 85798656
Running train benchmark on vit_base_patch16_224.dino for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model vit_base_patch16_224.dino created, param count: 85798656
Running train benchmark on vit_base_patch16_224.dino for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model vit_base_patch16_224.dino created, param count: 85798656
Running train benchmark on vit_base_patch16_224.dino for 40 steps w/ input size (3, 224, 224) and batch size 96.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model vit_base_patch16_224.dino created, param count: 85798656
Running train benchmark on vit_base_patch16_224.dino for 40 steps w/ input size (3, 224, 224) and batch size 64.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model vit_base_patch16_224.dino created, param count: 85798656
Running train benchmark on vit_base_patch16_224.dino for 40 steps w/ input size (3, 224, 224) and batch size 48.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model vit_base_patch16_224.dino created, param count: 85798656
Running train benchmark on vit_base_patch16_224.dino for 40 steps w/ input size (3, 224, 224) and batch size 32.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model vit_base_patch16_224.dino created, param count: 85798656
Running train benchmark on vit_base_patch16_224.dino for 40 steps w/ input size (3, 224, 224) and batch size 24.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model vit_base_patch16_224.dino created, param count: 85798656
Running train benchmark on vit_base_patch16_224.dino for 40 steps w/ input size (3, 224, 224) and batch size 16.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 12 for retry.
Model vit_base_patch16_224.dino created, param count: 85798656
Running train benchmark on vit_base_patch16_224.dino for 40 steps w/ input size (3, 224, 224) and batch size 12.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 8 for retry.
Model vit_base_patch16_224.dino created, param count: 85798656
Running train benchmark on vit_base_patch16_224.dino for 40 steps w/ input size (3, 224, 224) and batch size 8.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 6 for retry.
Model vit_base_patch16_224.dino created, param count: 85798656
Running train benchmark on vit_base_patch16_224.dino for 40 steps w/ input size (3, 224, 224) and batch size 6.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 4 for retry.
Model vit_base_patch16_224.dino created, param count: 85798656
Running train benchmark on vit_base_patch16_224.dino for 40 steps w/ input size (3, 224, 224) and batch size 4.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 3 for retry.
Model vit_base_patch16_224.dino created, param count: 85798656
Running train benchmark on vit_base_patch16_224.dino for 40 steps w/ input size (3, 224, 224) and batch size 3.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 2 for retry.
Model vit_base_patch16_224.dino created, param count: 85798656
Running train benchmark on vit_base_patch16_224.dino for 40 steps w/ input size (3, 224, 224) and batch size 2.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 1 for retry.
Model vit_base_patch16_224.dino created, param count: 85798656
Running train benchmark on vit_base_patch16_224.dino for 40 steps w/ input size (3, 224, 224) and batch size 1.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 0 for retry.
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model vit_base_patch16_224.mae created, param count: 85798656
Running inference benchmark on vit_base_patch16_224.mae for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3244.91 samples/sec. 78.893 ms/step.
Infer [16/40]. 3244.70 samples/sec. 78.898 ms/step.
Infer [24/40]. 3244.66 samples/sec. 78.899 ms/step.
Infer [32/40]. 3244.82 samples/sec. 78.895 ms/step.
Infer [40/40]. 3244.66 samples/sec. 78.899 ms/step.
Inference benchmark of vit_base_patch16_224.mae done. 3243.45 samples/sec, 78.90 ms/step
Model vit_base_patch16_224.mae created, param count: 85798656
Running train benchmark on vit_base_patch16_224.mae for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model vit_base_patch16_224.mae created, param count: 85798656
Running train benchmark on vit_base_patch16_224.mae for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model vit_base_patch16_224.mae created, param count: 85798656
Running train benchmark on vit_base_patch16_224.mae for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model vit_base_patch16_224.mae created, param count: 85798656
Running train benchmark on vit_base_patch16_224.mae for 40 steps w/ input size (3, 224, 224) and batch size 96.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model vit_base_patch16_224.mae created, param count: 85798656
Running train benchmark on vit_base_patch16_224.mae for 40 steps w/ input size (3, 224, 224) and batch size 64.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model vit_base_patch16_224.mae created, param count: 85798656
Running train benchmark on vit_base_patch16_224.mae for 40 steps w/ input size (3, 224, 224) and batch size 48.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model vit_base_patch16_224.mae created, param count: 85798656
Running train benchmark on vit_base_patch16_224.mae for 40 steps w/ input size (3, 224, 224) and batch size 32.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model vit_base_patch16_224.mae created, param count: 85798656
Running train benchmark on vit_base_patch16_224.mae for 40 steps w/ input size (3, 224, 224) and batch size 24.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model vit_base_patch16_224.mae created, param count: 85798656
Running train benchmark on vit_base_patch16_224.mae for 40 steps w/ input size (3, 224, 224) and batch size 16.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 12 for retry.
Model vit_base_patch16_224.mae created, param count: 85798656
Running train benchmark on vit_base_patch16_224.mae for 40 steps w/ input size (3, 224, 224) and batch size 12.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 8 for retry.
Model vit_base_patch16_224.mae created, param count: 85798656
Running train benchmark on vit_base_patch16_224.mae for 40 steps w/ input size (3, 224, 224) and batch size 8.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 6 for retry.
Model vit_base_patch16_224.mae created, param count: 85798656
Running train benchmark on vit_base_patch16_224.mae for 40 steps w/ input size (3, 224, 224) and batch size 6.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 4 for retry.
Model vit_base_patch16_224.mae created, param count: 85798656
Running train benchmark on vit_base_patch16_224.mae for 40 steps w/ input size (3, 224, 224) and batch size 4.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 3 for retry.
Model vit_base_patch16_224.mae created, param count: 85798656
Running train benchmark on vit_base_patch16_224.mae for 40 steps w/ input size (3, 224, 224) and batch size 3.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 2 for retry.
Model vit_base_patch16_224.mae created, param count: 85798656
Running train benchmark on vit_base_patch16_224.mae for 40 steps w/ input size (3, 224, 224) and batch size 2.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 1 for retry.
Model vit_base_patch16_224.mae created, param count: 85798656
Running train benchmark on vit_base_patch16_224.mae for 40 steps w/ input size (3, 224, 224) and batch size 1.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 0 for retry.
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model vit_base_patch16_224.orig_in21k_ft_in1k created, param count: 86567656
Running inference benchmark on vit_base_patch16_224.orig_in21k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3249.29 samples/sec. 78.786 ms/step.
Infer [16/40]. 3247.74 samples/sec. 78.824 ms/step.
Infer [24/40]. 3247.15 samples/sec. 78.838 ms/step.
Infer [32/40]. 3246.92 samples/sec. 78.844 ms/step.
Infer [40/40]. 3246.54 samples/sec. 78.853 ms/step.
Inference benchmark of vit_base_patch16_224.orig_in21k_ft_in1k done. 3245.48 samples/sec, 78.85 ms/step
Model vit_base_patch16_224.orig_in21k_ft_in1k created, param count: 86567656
Running train benchmark on vit_base_patch16_224.orig_in21k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 978.41 samples/sec. 261.648 ms/step.
Train [16/40]. 978.52 samples/sec. 261.619 ms/step.
Train [24/40]. 978.57 samples/sec. 261.607 ms/step.
Train [32/40]. 978.58 samples/sec. 261.604 ms/step.
Train [40/40]. 978.58 samples/sec. 261.603 ms/step.
Train benchmark of vit_base_patch16_224.orig_in21k_ft_in1k done. 975.14 samples/sec, 261.60 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model vit_base_patch16_224.sam_in1k created, param count: 86567656
Running inference benchmark on vit_base_patch16_224.sam_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3243.50 samples/sec. 78.927 ms/step.
Infer [16/40]. 3243.65 samples/sec. 78.923 ms/step.
Infer [24/40]. 3243.30 samples/sec. 78.932 ms/step.
Infer [32/40]. 3242.91 samples/sec. 78.941 ms/step.
Infer [40/40]. 3242.48 samples/sec. 78.952 ms/step.
Inference benchmark of vit_base_patch16_224.sam_in1k done. 3241.40 samples/sec, 78.95 ms/step
Model vit_base_patch16_224.sam_in1k created, param count: 86567656
Running train benchmark on vit_base_patch16_224.sam_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 978.10 samples/sec. 261.731 ms/step.
Train [16/40]. 978.09 samples/sec. 261.734 ms/step.
Train [24/40]. 978.10 samples/sec. 261.732 ms/step.
Train [32/40]. 978.08 samples/sec. 261.736 ms/step.
Train [40/40]. 978.09 samples/sec. 261.735 ms/step.
Train benchmark of vit_base_patch16_224.sam_in1k done. 974.55 samples/sec, 261.74 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model vit_base_patch16_224_miil.in21k_ft_in1k created, param count: 86540008
Running inference benchmark on vit_base_patch16_224_miil.in21k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3241.78 samples/sec. 78.969 ms/step.
Infer [16/40]. 3243.27 samples/sec. 78.933 ms/step.
Infer [24/40]. 3240.97 samples/sec. 78.989 ms/step.
Infer [32/40]. 3240.20 samples/sec. 79.008 ms/step.
Infer [40/40]. 3239.52 samples/sec. 79.024 ms/step.
Inference benchmark of vit_base_patch16_224_miil.in21k_ft_in1k done. 3238.42 samples/sec, 79.02 ms/step
Model vit_base_patch16_224_miil.in21k_ft_in1k created, param count: 86540008
Running train benchmark on vit_base_patch16_224_miil.in21k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 988.88 samples/sec. 258.879 ms/step.
Train [16/40]. 988.90 samples/sec. 258.875 ms/step.
Train [24/40]. 988.90 samples/sec. 258.873 ms/step.
Train [32/40]. 988.90 samples/sec. 258.872 ms/step.
Train [40/40]. 988.89 samples/sec. 258.876 ms/step.
Train benchmark of vit_base_patch16_224_miil.in21k_ft_in1k done. 985.38 samples/sec, 258.88 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model vit_base_patch16_384.augreg_in1k created, param count: 86859496
Running inference benchmark on vit_base_patch16_384.augreg_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 1011.57 samples/sec. 253.072 ms/step.
Infer [16/40]. 1011.53 samples/sec. 253.081 ms/step.
Infer [24/40]. 1011.19 samples/sec. 253.168 ms/step.
Infer [32/40]. 1011.12 samples/sec. 253.184 ms/step.
Infer [40/40]. 1010.98 samples/sec. 253.220 ms/step.
Inference benchmark of vit_base_patch16_384.augreg_in1k done. 1010.80 samples/sec, 253.22 ms/step
Model vit_base_patch16_384.augreg_in1k created, param count: 86859496
Running train benchmark on vit_base_patch16_384.augreg_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 218.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 94.06 MiB is free. Including non-PyTorch memory, this process has 23.55 GiB memory in use. Of the allocated memory 22.23 GiB is allocated by PyTorch, and 89.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model vit_base_patch16_384.augreg_in1k created, param count: 86859496
Running train benchmark on vit_base_patch16_384.augreg_in1k for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 650.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 84.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 21.96 GiB is allocated by PyTorch, and 370.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model vit_base_patch16_384.augreg_in1k created, param count: 86859496
Running train benchmark on vit_base_patch16_384.augreg_in1k for 40 steps w/ input size (3, 384, 384) and batch size 128.
Train [8/40]. 306.51 samples/sec. 417.607 ms/step.
Train [16/40]. 306.55 samples/sec. 417.556 ms/step.
Train [24/40]. 306.51 samples/sec. 417.602 ms/step.
Train [32/40]. 306.50 samples/sec. 417.625 ms/step.
Train [40/40]. 306.47 samples/sec. 417.656 ms/step.
Train benchmark of vit_base_patch16_384.augreg_in1k done. 305.75 samples/sec, 417.66 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model vit_base_patch16_384.augreg_in21k_ft_in1k created, param count: 86859496
Running inference benchmark on vit_base_patch16_384.augreg_in21k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 1011.06 samples/sec. 253.200 ms/step.
Infer [16/40]. 1010.94 samples/sec. 253.230 ms/step.
Infer [24/40]. 1010.80 samples/sec. 253.264 ms/step.
Infer [32/40]. 1010.62 samples/sec. 253.310 ms/step.
Infer [40/40]. 1010.48 samples/sec. 253.346 ms/step.
Inference benchmark of vit_base_patch16_384.augreg_in21k_ft_in1k done. 1010.30 samples/sec, 253.35 ms/step
Model vit_base_patch16_384.augreg_in21k_ft_in1k created, param count: 86859496
Running train benchmark on vit_base_patch16_384.augreg_in21k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 218.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 94.06 MiB is free. Including non-PyTorch memory, this process has 23.55 GiB memory in use. Of the allocated memory 22.23 GiB is allocated by PyTorch, and 89.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model vit_base_patch16_384.augreg_in21k_ft_in1k created, param count: 86859496
Running train benchmark on vit_base_patch16_384.augreg_in21k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 650.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 84.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 21.96 GiB is allocated by PyTorch, and 370.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model vit_base_patch16_384.augreg_in21k_ft_in1k created, param count: 86859496
Running train benchmark on vit_base_patch16_384.augreg_in21k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 128.
Train [8/40]. 306.48 samples/sec. 417.648 ms/step.
Train [16/40]. 306.23 samples/sec. 417.988 ms/step.
Train [24/40]. 306.11 samples/sec. 418.146 ms/step.
Train [32/40]. 306.05 samples/sec. 418.228 ms/step.
Train [40/40]. 306.04 samples/sec. 418.251 ms/step.
Train benchmark of vit_base_patch16_384.augreg_in21k_ft_in1k done. 305.30 samples/sec, 418.25 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model vit_base_patch16_384.orig_in21k_ft_in1k created, param count: 86859496
Running inference benchmark on vit_base_patch16_384.orig_in21k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 1010.26 samples/sec. 253.400 ms/step.
Infer [16/40]. 1010.14 samples/sec. 253.431 ms/step.
Infer [24/40]. 1010.14 samples/sec. 253.431 ms/step.
Infer [32/40]. 1010.21 samples/sec. 253.412 ms/step.
Infer [40/40]. 1010.16 samples/sec. 253.426 ms/step.
Inference benchmark of vit_base_patch16_384.orig_in21k_ft_in1k done. 1009.99 samples/sec, 253.43 ms/step
Model vit_base_patch16_384.orig_in21k_ft_in1k created, param count: 86859496
Running train benchmark on vit_base_patch16_384.orig_in21k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 218.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 94.06 MiB is free. Including non-PyTorch memory, this process has 23.55 GiB memory in use. Of the allocated memory 22.23 GiB is allocated by PyTorch, and 89.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model vit_base_patch16_384.orig_in21k_ft_in1k created, param count: 86859496
Running train benchmark on vit_base_patch16_384.orig_in21k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 650.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 84.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 21.96 GiB is allocated by PyTorch, and 370.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model vit_base_patch16_384.orig_in21k_ft_in1k created, param count: 86859496
Running train benchmark on vit_base_patch16_384.orig_in21k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 128.
Train [8/40]. 306.53 samples/sec. 417.584 ms/step.
Train [16/40]. 306.24 samples/sec. 417.972 ms/step.
Train [24/40]. 306.14 samples/sec. 418.116 ms/step.
Train [32/40]. 306.12 samples/sec. 418.142 ms/step.
Train [40/40]. 306.08 samples/sec. 418.197 ms/step.
Train benchmark of vit_base_patch16_384.orig_in21k_ft_in1k done. 305.36 samples/sec, 418.20 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model vit_base_patch16_clip_224.datacompxl created, param count: 86193152
Running inference benchmark on vit_base_patch16_clip_224.datacompxl for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3228.74 samples/sec. 79.288 ms/step.
Infer [16/40]. 3230.77 samples/sec. 79.238 ms/step.
Infer [24/40]. 3231.95 samples/sec. 79.209 ms/step.
Infer [32/40]. 3231.54 samples/sec. 79.219 ms/step.
Infer [40/40]. 3231.88 samples/sec. 79.211 ms/step.
Inference benchmark of vit_base_patch16_clip_224.datacompxl done. 3230.78 samples/sec, 79.21 ms/step
Model vit_base_patch16_clip_224.datacompxl created, param count: 86193152
Running train benchmark on vit_base_patch16_clip_224.datacompxl for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 976.66 samples/sec. 262.118 ms/step.
Train [16/40]. 976.66 samples/sec. 262.117 ms/step.
Train [24/40]. 976.66 samples/sec. 262.119 ms/step.
Train [32/40]. 976.64 samples/sec. 262.124 ms/step.
Train [40/40]. 976.60 samples/sec. 262.133 ms/step.
Train benchmark of vit_base_patch16_clip_224.datacompxl done. 973.12 samples/sec, 262.13 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model vit_base_patch16_clip_224.laion2b created, param count: 86193152
Running inference benchmark on vit_base_patch16_clip_224.laion2b for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3233.04 samples/sec. 79.183 ms/step.
Infer [16/40]. 3234.44 samples/sec. 79.148 ms/step.
Infer [24/40]. 3233.53 samples/sec. 79.170 ms/step.
Infer [32/40]. 3232.66 samples/sec. 79.192 ms/step.
Infer [40/40]. 3233.10 samples/sec. 79.181 ms/step.
Inference benchmark of vit_base_patch16_clip_224.laion2b done. 3232.05 samples/sec, 79.18 ms/step
Model vit_base_patch16_clip_224.laion2b created, param count: 86193152
Running train benchmark on vit_base_patch16_clip_224.laion2b for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 976.61 samples/sec. 262.132 ms/step.
Train [16/40]. 976.63 samples/sec. 262.127 ms/step.
Train [24/40]. 976.61 samples/sec. 262.133 ms/step.
Train [32/40]. 976.60 samples/sec. 262.133 ms/step.
Train [40/40]. 976.59 samples/sec. 262.136 ms/step.
Train benchmark of vit_base_patch16_clip_224.laion2b done. 973.13 samples/sec, 262.14 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model vit_base_patch16_clip_224.laion2b_ft_in1k created, param count: 86568424
Running inference benchmark on vit_base_patch16_clip_224.laion2b_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3237.73 samples/sec. 79.068 ms/step.
Infer [16/40]. 3239.77 samples/sec. 79.018 ms/step.
Infer [24/40]. 3236.99 samples/sec. 79.086 ms/step.
Infer [32/40]. 3236.39 samples/sec. 79.101 ms/step.
Infer [40/40]. 3236.23 samples/sec. 79.104 ms/step.
Inference benchmark of vit_base_patch16_clip_224.laion2b_ft_in1k done. 3235.14 samples/sec, 79.10 ms/step
Model vit_base_patch16_clip_224.laion2b_ft_in1k created, param count: 86568424
Running train benchmark on vit_base_patch16_clip_224.laion2b_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 976.54 samples/sec. 262.151 ms/step.
Train [16/40]. 976.53 samples/sec. 262.152 ms/step.
Train [24/40]. 976.54 samples/sec. 262.150 ms/step.
Train [32/40]. 976.55 samples/sec. 262.146 ms/step.
Train [40/40]. 976.56 samples/sec. 262.146 ms/step.
Train benchmark of vit_base_patch16_clip_224.laion2b_ft_in1k done. 973.07 samples/sec, 262.15 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model vit_base_patch16_clip_224.laion2b_ft_in12k created, param count: 94889773
Running inference benchmark on vit_base_patch16_clip_224.laion2b_ft_in12k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3229.71 samples/sec. 79.264 ms/step.
Infer [16/40]. 3230.46 samples/sec. 79.246 ms/step.
Infer [24/40]. 3230.15 samples/sec. 79.253 ms/step.
Infer [32/40]. 3229.73 samples/sec. 79.264 ms/step.
Infer [40/40]. 3230.26 samples/sec. 79.251 ms/step.
Inference benchmark of vit_base_patch16_clip_224.laion2b_ft_in12k done. 3229.14 samples/sec, 79.25 ms/step
Model vit_base_patch16_clip_224.laion2b_ft_in12k created, param count: 94889773
Running train benchmark on vit_base_patch16_clip_224.laion2b_ft_in12k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 975.58 samples/sec. 262.407 ms/step.
Train [16/40]. 975.37 samples/sec. 262.465 ms/step.
Train [24/40]. 975.29 samples/sec. 262.486 ms/step.
Train [32/40]. 975.26 samples/sec. 262.495 ms/step.
Train [40/40]. 975.22 samples/sec. 262.505 ms/step.
Train benchmark of vit_base_patch16_clip_224.laion2b_ft_in12k done. 971.66 samples/sec, 262.50 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model vit_base_patch16_clip_224.laion2b_ft_in12k_in1k created, param count: 86568424
Running inference benchmark on vit_base_patch16_clip_224.laion2b_ft_in12k_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3236.97 samples/sec. 79.086 ms/step.
Infer [16/40]. 3239.79 samples/sec. 79.018 ms/step.
Infer [24/40]. 3238.62 samples/sec. 79.046 ms/step.
Infer [32/40]. 3238.41 samples/sec. 79.051 ms/step.
Infer [40/40]. 3238.43 samples/sec. 79.051 ms/step.
Inference benchmark of vit_base_patch16_clip_224.laion2b_ft_in12k_in1k done. 3237.38 samples/sec, 79.05 ms/step
Model vit_base_patch16_clip_224.laion2b_ft_in12k_in1k created, param count: 86568424
Running train benchmark on vit_base_patch16_clip_224.laion2b_ft_in12k_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 976.99 samples/sec. 262.030 ms/step.
Train [16/40]. 977.01 samples/sec. 262.023 ms/step.
Train [24/40]. 976.87 samples/sec. 262.061 ms/step.
Train [32/40]. 976.76 samples/sec. 262.091 ms/step.
Train [40/40]. 976.69 samples/sec. 262.108 ms/step.
Train benchmark of vit_base_patch16_clip_224.laion2b_ft_in12k_in1k done. 973.25 samples/sec, 262.11 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model vit_base_patch16_clip_224.openai created, param count: 86193152
Running inference benchmark on vit_base_patch16_clip_224.openai for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3237.68 samples/sec. 79.069 ms/step.
Infer [16/40]. 3237.28 samples/sec. 79.079 ms/step.
Infer [24/40]. 3236.25 samples/sec. 79.104 ms/step.
Infer [32/40]. 3235.97 samples/sec. 79.111 ms/step.
Infer [40/40]. 3235.87 samples/sec. 79.113 ms/step.
Inference benchmark of vit_base_patch16_clip_224.openai done. 3234.77 samples/sec, 79.11 ms/step
Model vit_base_patch16_clip_224.openai created, param count: 86193152
Running train benchmark on vit_base_patch16_clip_224.openai for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 977.01 samples/sec. 262.025 ms/step.
Train [16/40]. 976.80 samples/sec. 262.081 ms/step.
Train [24/40]. 976.73 samples/sec. 262.099 ms/step.
Train [32/40]. 976.71 samples/sec. 262.106 ms/step.
Train [40/40]. 976.68 samples/sec. 262.112 ms/step.
Train benchmark of vit_base_patch16_clip_224.openai done. 973.22 samples/sec, 262.11 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model vit_base_patch16_clip_224.openai_ft_in1k created, param count: 86568424
Running inference benchmark on vit_base_patch16_clip_224.openai_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3234.07 samples/sec. 79.157 ms/step.
Infer [16/40]. 3235.86 samples/sec. 79.113 ms/step.
Infer [24/40]. 3234.24 samples/sec. 79.153 ms/step.
Infer [32/40]. 3233.82 samples/sec. 79.163 ms/step.
Infer [40/40]. 3233.93 samples/sec. 79.161 ms/step.
Inference benchmark of vit_base_patch16_clip_224.openai_ft_in1k done. 3232.86 samples/sec, 79.16 ms/step
Model vit_base_patch16_clip_224.openai_ft_in1k created, param count: 86568424
Running train benchmark on vit_base_patch16_clip_224.openai_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 976.49 samples/sec. 262.163 ms/step.
Train [16/40]. 976.45 samples/sec. 262.173 ms/step.
Train [24/40]. 976.47 samples/sec. 262.169 ms/step.
Train [32/40]. 976.48 samples/sec. 262.166 ms/step.
Train [40/40]. 976.49 samples/sec. 262.164 ms/step.
Train benchmark of vit_base_patch16_clip_224.openai_ft_in1k done. 972.99 samples/sec, 262.16 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model vit_base_patch16_clip_224.openai_ft_in12k created, param count: 94889773
Running inference benchmark on vit_base_patch16_clip_224.openai_ft_in12k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3232.17 samples/sec. 79.204 ms/step.
Infer [16/40]. 3232.20 samples/sec. 79.203 ms/step.
Infer [24/40]. 3231.82 samples/sec. 79.212 ms/step.
Infer [32/40]. 3231.53 samples/sec. 79.220 ms/step.
Infer [40/40]. 3231.74 samples/sec. 79.214 ms/step.
Inference benchmark of vit_base_patch16_clip_224.openai_ft_in12k done. 3230.62 samples/sec, 79.21 ms/step
Model vit_base_patch16_clip_224.openai_ft_in12k created, param count: 94889773
Running train benchmark on vit_base_patch16_clip_224.openai_ft_in12k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 975.34 samples/sec. 262.471 ms/step.
Train [16/40]. 975.23 samples/sec. 262.502 ms/step.
Train [24/40]. 975.21 samples/sec. 262.509 ms/step.
Train [32/40]. 975.19 samples/sec. 262.513 ms/step.
Train [40/40]. 975.19 samples/sec. 262.514 ms/step.
Train benchmark of vit_base_patch16_clip_224.openai_ft_in12k done. 971.68 samples/sec, 262.51 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model vit_base_patch16_clip_224.openai_ft_in12k_in1k created, param count: 86568424
Running inference benchmark on vit_base_patch16_clip_224.openai_ft_in12k_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 3237.98 samples/sec. 79.062 ms/step.
Infer [16/40]. 3237.25 samples/sec. 79.079 ms/step.
Infer [24/40]. 3236.24 samples/sec. 79.104 ms/step.
Infer [32/40]. 3236.01 samples/sec. 79.110 ms/step.
Infer [40/40]. 3235.84 samples/sec. 79.114 ms/step.
Inference benchmark of vit_base_patch16_clip_224.openai_ft_in12k_in1k done. 3234.80 samples/sec, 79.11 ms/step
Model vit_base_patch16_clip_224.openai_ft_in12k_in1k created, param count: 86568424
Running train benchmark on vit_base_patch16_clip_224.openai_ft_in12k_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 976.98 samples/sec. 262.031 ms/step.
Train [16/40]. 976.92 samples/sec. 262.048 ms/step.
Train [24/40]. 976.80 samples/sec. 262.081 ms/step.
Train [32/40]. 976.73 samples/sec. 262.100 ms/step.
Train [40/40]. 976.69 samples/sec. 262.111 ms/step.
Train benchmark of vit_base_patch16_clip_224.openai_ft_in12k_in1k done. 973.13 samples/sec, 262.11 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model vit_base_patch16_clip_384.laion2b_ft_in1k created, param count: 86860264
Running inference benchmark on vit_base_patch16_clip_384.laion2b_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 1010.81 samples/sec. 253.262 ms/step.
Infer [16/40]. 1010.47 samples/sec. 253.346 ms/step.
Infer [24/40]. 1010.49 samples/sec. 253.343 ms/step.
Infer [32/40]. 1010.47 samples/sec. 253.348 ms/step.
Infer [40/40]. 1010.27 samples/sec. 253.397 ms/step.
Inference benchmark of vit_base_patch16_clip_384.laion2b_ft_in1k done. 1010.10 samples/sec, 253.40 ms/step
Model vit_base_patch16_clip_384.laion2b_ft_in1k created, param count: 86860264
Running train benchmark on vit_base_patch16_clip_384.laion2b_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 218.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 94.06 MiB is free. Including non-PyTorch memory, this process has 23.55 GiB memory in use. Of the allocated memory 22.23 GiB is allocated by PyTorch, and 88.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model vit_base_patch16_clip_384.laion2b_ft_in1k created, param count: 86860264
Running train benchmark on vit_base_patch16_clip_384.laion2b_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 164.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 84.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 21.96 GiB is allocated by PyTorch, and 370.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model vit_base_patch16_clip_384.laion2b_ft_in1k created, param count: 86860264
Running train benchmark on vit_base_patch16_clip_384.laion2b_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 128.
Train [8/40]. 306.24 samples/sec. 417.967 ms/step.
Train [16/40]. 306.16 samples/sec. 418.082 ms/step.
Train [24/40]. 306.16 samples/sec. 418.086 ms/step.
Train [32/40]. 306.16 samples/sec. 418.078 ms/step.
Train [40/40]. 306.15 samples/sec. 418.100 ms/step.
Train benchmark of vit_base_patch16_clip_384.laion2b_ft_in1k done. 305.42 samples/sec, 418.10 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model vit_base_patch16_clip_384.laion2b_ft_in12k_in1k created, param count: 86860264
Running inference benchmark on vit_base_patch16_clip_384.laion2b_ft_in12k_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 1010.41 samples/sec. 253.363 ms/step.
Infer [16/40]. 1010.13 samples/sec. 253.432 ms/step.
Infer [24/40]. 1010.09 samples/sec. 253.443 ms/step.
Infer [32/40]. 1009.96 samples/sec. 253.476 ms/step.
Infer [40/40]. 1010.01 samples/sec. 253.462 ms/step.
Inference benchmark of vit_base_patch16_clip_384.laion2b_ft_in12k_in1k done. 1009.85 samples/sec, 253.46 ms/step
Model vit_base_patch16_clip_384.laion2b_ft_in12k_in1k created, param count: 86860264
Running train benchmark on vit_base_patch16_clip_384.laion2b_ft_in12k_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 218.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 94.06 MiB is free. Including non-PyTorch memory, this process has 23.55 GiB memory in use. Of the allocated memory 22.23 GiB is allocated by PyTorch, and 88.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model vit_base_patch16_clip_384.laion2b_ft_in12k_in1k created, param count: 86860264
Running train benchmark on vit_base_patch16_clip_384.laion2b_ft_in12k_in1k for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 164.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 84.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 21.96 GiB is allocated by PyTorch, and 370.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model vit_base_patch16_clip_384.laion2b_ft_in12k_in1k created, param count: 86860264
Running train benchmark on vit_base_patch16_clip_384.laion2b_ft_in12k_in1k for 40 steps w/ input size (3, 384, 384) and batch size 128.
Train [8/40]. 306.20 samples/sec. 418.031 ms/step.
Train [16/40]. 305.97 samples/sec. 418.346 ms/step.
Train [24/40]. 305.87 samples/sec. 418.473 ms/step.
Train [32/40]. 305.82 samples/sec. 418.552 ms/step.
Train [40/40]. 305.77 samples/sec. 418.616 ms/step.
Train benchmark of vit_base_patch16_clip_384.laion2b_ft_in12k_in1k done. 305.04 samples/sec, 418.62 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model vit_base_patch16_clip_384.openai_ft_in1k created, param count: 86860264
Running inference benchmark on vit_base_patch16_clip_384.openai_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 1009.60 samples/sec. 253.565 ms/step.
Infer [16/40]. 1009.55 samples/sec. 253.578 ms/step.
Infer [24/40]. 1009.42 samples/sec. 253.612 ms/step.
Infer [32/40]. 1009.19 samples/sec. 253.668 ms/step.
Infer [40/40]. 1009.10 samples/sec. 253.691 ms/step.
Inference benchmark of vit_base_patch16_clip_384.openai_ft_in1k done. 1008.94 samples/sec, 253.69 ms/step
Model vit_base_patch16_clip_384.openai_ft_in1k created, param count: 86860264
Running train benchmark on vit_base_patch16_clip_384.openai_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 218.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 94.06 MiB is free. Including non-PyTorch memory, this process has 23.55 GiB memory in use. Of the allocated memory 22.23 GiB is allocated by PyTorch, and 88.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model vit_base_patch16_clip_384.openai_ft_in1k created, param count: 86860264
Running train benchmark on vit_base_patch16_clip_384.openai_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 164.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 84.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 21.96 GiB is allocated by PyTorch, and 370.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model vit_base_patch16_clip_384.openai_ft_in1k created, param count: 86860264
Running train benchmark on vit_base_patch16_clip_384.openai_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 128.
Train [8/40]. 305.64 samples/sec. 418.799 ms/step.
Train [16/40]. 305.63 samples/sec. 418.804 ms/step.
Train [24/40]. 305.61 samples/sec. 418.829 ms/step.
Train [32/40]. 305.60 samples/sec. 418.844 ms/step.
Train [40/40]. 305.60 samples/sec. 418.853 ms/step.
Train benchmark of vit_base_patch16_clip_384.openai_ft_in1k done. 304.86 samples/sec, 418.85 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model vit_base_patch16_clip_384.openai_ft_in12k_in1k created, param count: 86860264
Running inference benchmark on vit_base_patch16_clip_384.openai_ft_in12k_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 1009.54 samples/sec. 253.582 ms/step.
Infer [16/40]. 1009.42 samples/sec. 253.612 ms/step.
Infer [24/40]. 1009.25 samples/sec. 253.654 ms/step.
Infer [32/40]. 1008.92 samples/sec. 253.737 ms/step.
Infer [40/40]. 1008.74 samples/sec. 253.782 ms/step.
Inference benchmark of vit_base_patch16_clip_384.openai_ft_in12k_in1k done. 1008.56 samples/sec, 253.78 ms/step
Model vit_base_patch16_clip_384.openai_ft_in12k_in1k created, param count: 86860264
Running train benchmark on vit_base_patch16_clip_384.openai_ft_in12k_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 218.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 94.06 MiB is free. Including non-PyTorch memory, this process has 23.55 GiB memory in use. Of the allocated memory 22.23 GiB is allocated by PyTorch, and 88.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model vit_base_patch16_clip_384.openai_ft_in12k_in1k created, param count: 86860264
Running train benchmark on vit_base_patch16_clip_384.openai_ft_in12k_in1k for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 164.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 84.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 21.96 GiB is allocated by PyTorch, and 370.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model vit_base_patch16_clip_384.openai_ft_in12k_in1k created, param count: 86860264
Running train benchmark on vit_base_patch16_clip_384.openai_ft_in12k_in1k for 40 steps w/ input size (3, 384, 384) and batch size 128.
Train [8/40]. 306.08 samples/sec. 418.187 ms/step.
Train [16/40]. 306.11 samples/sec. 418.147 ms/step.
Train [24/40]. 306.12 samples/sec. 418.133 ms/step.
Train [32/40]. 306.09 samples/sec. 418.181 ms/step.
Train [40/40]. 306.07 samples/sec. 418.201 ms/step.
Train benchmark of vit_base_patch16_clip_384.openai_ft_in12k_in1k done. 305.34 samples/sec, 418.20 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model vit_base_patch16_rpn_224.sw_in1k created, param count: 86538472
Running inference benchmark on vit_base_patch16_rpn_224.sw_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 2489.72 samples/sec. 102.823 ms/step.
Infer [16/40]. 2489.57 samples/sec. 102.829 ms/step.
Infer [24/40]. 2489.38 samples/sec. 102.837 ms/step.
Infer [32/40]. 2489.04 samples/sec. 102.851 ms/step.
Infer [40/40]. 2488.88 samples/sec. 102.857 ms/step.
Inference benchmark of vit_base_patch16_rpn_224.sw_in1k done. 2488.22 samples/sec, 102.86 ms/step
Model vit_base_patch16_rpn_224.sw_in1k created, param count: 86538472
Running train benchmark on vit_base_patch16_rpn_224.sw_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 902.93 samples/sec. 283.523 ms/step.
Train [16/40]. 902.95 samples/sec. 283.517 ms/step.
Train [24/40]. 902.94 samples/sec. 283.517 ms/step.
Train [32/40]. 902.93 samples/sec. 283.521 ms/step.
Train [40/40]. 902.92 samples/sec. 283.525 ms/step.
Train benchmark of vit_base_patch16_rpn_224.sw_in1k done. 900.00 samples/sec, 283.52 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model vit_base_patch32_224.augreg_in1k created, param count: 88224232
Running inference benchmark on vit_base_patch32_224.augreg_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 11994.81 samples/sec. 21.343 ms/step.
Infer [16/40]. 11993.32 samples/sec. 21.345 ms/step.
Infer [24/40]. 11993.45 samples/sec. 21.345 ms/step.
Infer [32/40]. 11993.04 samples/sec. 21.346 ms/step.
Infer [40/40]. 11993.26 samples/sec. 21.345 ms/step.
Inference benchmark of vit_base_patch32_224.augreg_in1k done. 11979.88 samples/sec, 21.34 ms/step
Model vit_base_patch32_224.augreg_in1k created, param count: 88224232
Running train benchmark on vit_base_patch32_224.augreg_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 3850.00 samples/sec. 66.493 ms/step.
Train [16/40]. 3849.99 samples/sec. 66.494 ms/step.
Train [24/40]. 3849.94 samples/sec. 66.494 ms/step.
Train [32/40]. 3849.84 samples/sec. 66.496 ms/step.
Train [40/40]. 3849.80 samples/sec. 66.497 ms/step.
Train benchmark of vit_base_patch32_224.augreg_in1k done. 3813.93 samples/sec, 66.50 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model vit_base_patch32_224.augreg_in21k_ft_in1k created, param count: 88224232
Running inference benchmark on vit_base_patch32_224.augreg_in21k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 12006.17 samples/sec. 21.322 ms/step.
Infer [16/40]. 12000.08 samples/sec. 21.333 ms/step.
Infer [24/40]. 11997.58 samples/sec. 21.338 ms/step.
Infer [32/40]. 11995.78 samples/sec. 21.341 ms/step.
Infer [40/40]. 11994.82 samples/sec. 21.343 ms/step.
Inference benchmark of vit_base_patch32_224.augreg_in21k_ft_in1k done. 11981.35 samples/sec, 21.34 ms/step
Model vit_base_patch32_224.augreg_in21k_ft_in1k created, param count: 88224232
Running train benchmark on vit_base_patch32_224.augreg_in21k_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 3846.94 samples/sec. 66.546 ms/step.
Train [16/40]. 3847.03 samples/sec. 66.545 ms/step.
Train [24/40]. 3846.99 samples/sec. 66.545 ms/step.
Train [32/40]. 3846.99 samples/sec. 66.545 ms/step.
Train [40/40]. 3847.03 samples/sec. 66.545 ms/step.
Train benchmark of vit_base_patch32_224.augreg_in21k_ft_in1k done. 3810.82 samples/sec, 66.55 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model vit_base_patch32_224.sam_in1k created, param count: 88224232
Running inference benchmark on vit_base_patch32_224.sam_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 12012.13 samples/sec. 21.312 ms/step.
Infer [16/40]. 12008.22 samples/sec. 21.319 ms/step.
Infer [24/40]. 12005.29 samples/sec. 21.324 ms/step.
Infer [32/40]. 12003.15 samples/sec. 21.328 ms/step.
Infer [40/40]. 12001.95 samples/sec. 21.330 ms/step.
Inference benchmark of vit_base_patch32_224.sam_in1k done. 11988.69 samples/sec, 21.33 ms/step
Model vit_base_patch32_224.sam_in1k created, param count: 88224232
Running train benchmark on vit_base_patch32_224.sam_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 3848.25 samples/sec. 66.524 ms/step.
Train [16/40]. 3848.23 samples/sec. 66.524 ms/step.
Train [24/40]. 3848.39 samples/sec. 66.521 ms/step.
Train [32/40]. 3848.34 samples/sec. 66.522 ms/step.
Train [40/40]. 3848.07 samples/sec. 66.527 ms/step.
Train benchmark of vit_base_patch32_224.sam_in1k done. 3812.23 samples/sec, 66.53 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model vit_base_patch32_384.augreg_in1k created, param count: 88297192
Running inference benchmark on vit_base_patch32_384.augreg_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 4104.90 samples/sec. 62.365 ms/step.
Infer [16/40]. 4108.28 samples/sec. 62.313 ms/step.
Infer [24/40]. 4108.61 samples/sec. 62.308 ms/step.
Infer [32/40]. 4108.90 samples/sec. 62.304 ms/step.
Infer [40/40]. 4109.63 samples/sec. 62.293 ms/step.
Inference benchmark of vit_base_patch32_384.augreg_in1k done. 4107.98 samples/sec, 62.29 ms/step
Model vit_base_patch32_384.augreg_in1k created, param count: 88297192
Running train benchmark on vit_base_patch32_384.augreg_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
Train [8/40]. 1294.79 samples/sec. 197.716 ms/step.
Train [16/40]. 1294.56 samples/sec. 197.751 ms/step.
Train [24/40]. 1294.60 samples/sec. 197.745 ms/step.
Train [32/40]. 1294.45 samples/sec. 197.767 ms/step.
Train [40/40]. 1294.45 samples/sec. 197.767 ms/step.
Train benchmark of vit_base_patch32_384.augreg_in1k done. 1288.89 samples/sec, 197.77 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model vit_base_patch32_384.augreg_in21k_ft_in1k created, param count: 88297192
Running inference benchmark on vit_base_patch32_384.augreg_in21k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 4117.47 samples/sec. 62.174 ms/step.
Infer [16/40]. 4113.52 samples/sec. 62.234 ms/step.
Infer [24/40]. 4114.44 samples/sec. 62.220 ms/step.
Infer [32/40]. 4112.55 samples/sec. 62.249 ms/step.
Infer [40/40]. 4112.57 samples/sec. 62.248 ms/step.
Inference benchmark of vit_base_patch32_384.augreg_in21k_ft_in1k done. 4110.90 samples/sec, 62.25 ms/step
Model vit_base_patch32_384.augreg_in21k_ft_in1k created, param count: 88297192
Running train benchmark on vit_base_patch32_384.augreg_in21k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
Train [8/40]. 1295.00 samples/sec. 197.683 ms/step.
Train [16/40]. 1294.85 samples/sec. 197.706 ms/step.
Train [24/40]. 1294.90 samples/sec. 197.699 ms/step.
Train [32/40]. 1294.89 samples/sec. 197.700 ms/step.
Train [40/40]. 1294.94 samples/sec. 197.693 ms/step.
Train benchmark of vit_base_patch32_384.augreg_in21k_ft_in1k done. 1289.28 samples/sec, 197.69 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model vit_base_patch32_clip_224.laion2b created, param count: 87849728
Running inference benchmark on vit_base_patch32_clip_224.laion2b for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 12003.34 samples/sec. 21.327 ms/step.
Infer [16/40]. 12002.34 samples/sec. 21.329 ms/step.
Infer [24/40]. 12001.02 samples/sec. 21.332 ms/step.
Infer [32/40]. 12000.09 samples/sec. 21.333 ms/step.
Infer [40/40]. 11999.43 samples/sec. 21.334 ms/step.
Inference benchmark of vit_base_patch32_clip_224.laion2b done. 11986.38 samples/sec, 21.33 ms/step
Model vit_base_patch32_clip_224.laion2b created, param count: 87849728
Running train benchmark on vit_base_patch32_clip_224.laion2b for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 3847.83 samples/sec. 66.531 ms/step.
Train [16/40]. 3847.79 samples/sec. 66.532 ms/step.
Train [24/40]. 3847.76 samples/sec. 66.532 ms/step.
Train [32/40]. 3847.73 samples/sec. 66.533 ms/step.
Train [40/40]. 3847.71 samples/sec. 66.533 ms/step.
Train benchmark of vit_base_patch32_clip_224.laion2b done. 3812.24 samples/sec, 66.53 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model vit_base_patch32_clip_224.laion2b_ft_in1k created, param count: 88225000
Running inference benchmark on vit_base_patch32_clip_224.laion2b_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 12017.72 samples/sec. 21.302 ms/step.
Infer [16/40]. 12011.87 samples/sec. 21.312 ms/step.
Infer [24/40]. 12008.74 samples/sec. 21.318 ms/step.
Infer [32/40]. 12007.38 samples/sec. 21.320 ms/step.
Infer [40/40]. 12006.41 samples/sec. 21.322 ms/step.
Inference benchmark of vit_base_patch32_clip_224.laion2b_ft_in1k done. 11993.49 samples/sec, 21.32 ms/step
Model vit_base_patch32_clip_224.laion2b_ft_in1k created, param count: 88225000
Running train benchmark on vit_base_patch32_clip_224.laion2b_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 3846.83 samples/sec. 66.548 ms/step.
Train [16/40]. 3846.95 samples/sec. 66.546 ms/step.
Train [24/40]. 3846.98 samples/sec. 66.546 ms/step.
Train [32/40]. 3846.94 samples/sec. 66.546 ms/step.
Train [40/40]. 3846.96 samples/sec. 66.546 ms/step.
Train benchmark of vit_base_patch32_clip_224.laion2b_ft_in1k done. 3811.27 samples/sec, 66.55 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model vit_base_patch32_clip_224.laion2b_ft_in12k_in1k created, param count: 88225000
Running inference benchmark on vit_base_patch32_clip_224.laion2b_ft_in12k_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 12041.63 samples/sec. 21.260 ms/step.
Infer [16/40]. 12023.99 samples/sec. 21.291 ms/step.
Infer [24/40]. 12018.06 samples/sec. 21.301 ms/step.
Infer [32/40]. 12015.12 samples/sec. 21.306 ms/step.
Infer [40/40]. 12011.99 samples/sec. 21.312 ms/step.
Inference benchmark of vit_base_patch32_clip_224.laion2b_ft_in12k_in1k done. 11999.22 samples/sec, 21.31 ms/step
Model vit_base_patch32_clip_224.laion2b_ft_in12k_in1k created, param count: 88225000
Running train benchmark on vit_base_patch32_clip_224.laion2b_ft_in12k_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 3847.71 samples/sec. 66.533 ms/step.
Train [16/40]. 3847.74 samples/sec. 66.533 ms/step.
Train [24/40]. 3847.76 samples/sec. 66.532 ms/step.
Train [32/40]. 3847.73 samples/sec. 66.533 ms/step.
Train [40/40]. 3847.65 samples/sec. 66.534 ms/step.
Train benchmark of vit_base_patch32_clip_224.laion2b_ft_in12k_in1k done. 3813.26 samples/sec, 66.53 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model vit_base_patch32_clip_224.openai created, param count: 87849728
Running inference benchmark on vit_base_patch32_clip_224.openai for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 11998.73 samples/sec. 21.336 ms/step.
Infer [16/40]. 11998.50 samples/sec. 21.336 ms/step.
Infer [24/40]. 11997.30 samples/sec. 21.338 ms/step.
Infer [32/40]. 11997.08 samples/sec. 21.339 ms/step.
Infer [40/40]. 11996.58 samples/sec. 21.339 ms/step.
Inference benchmark of vit_base_patch32_clip_224.openai done. 11982.93 samples/sec, 21.34 ms/step
Model vit_base_patch32_clip_224.openai created, param count: 87849728
Running train benchmark on vit_base_patch32_clip_224.openai for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 3851.68 samples/sec. 66.464 ms/step.
Train [16/40]. 3851.73 samples/sec. 66.464 ms/step.
Train [24/40]. 3851.57 samples/sec. 66.466 ms/step.
Train [32/40]. 3851.61 samples/sec. 66.466 ms/step.
Train [40/40]. 3851.61 samples/sec. 66.466 ms/step.
Train benchmark of vit_base_patch32_clip_224.openai done. 3815.56 samples/sec, 66.47 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model vit_base_patch32_clip_224.openai_ft_in1k created, param count: 88225000
Running inference benchmark on vit_base_patch32_clip_224.openai_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 12023.08 samples/sec. 21.292 ms/step.
Infer [16/40]. 12014.23 samples/sec. 21.308 ms/step.
Infer [24/40]. 12010.28 samples/sec. 21.315 ms/step.
Infer [32/40]. 12008.54 samples/sec. 21.318 ms/step.
Infer [40/40]. 12007.35 samples/sec. 21.320 ms/step.
Inference benchmark of vit_base_patch32_clip_224.openai_ft_in1k done. 11994.06 samples/sec, 21.32 ms/step
Model vit_base_patch32_clip_224.openai_ft_in1k created, param count: 88225000
Running train benchmark on vit_base_patch32_clip_224.openai_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Train [8/40]. 3849.38 samples/sec. 66.504 ms/step.
Train [16/40]. 3849.21 samples/sec. 66.507 ms/step.
Train [24/40]. 3849.02 samples/sec. 66.510 ms/step.
Train [32/40]. 3848.96 samples/sec. 66.512 ms/step.
Train [40/40]. 3848.95 samples/sec. 66.512 ms/step.
Train benchmark of vit_base_patch32_clip_224.openai_ft_in1k done. 3814.27 samples/sec, 66.51 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model vit_base_patch32_clip_384.laion2b_ft_in12k_in1k created, param count: 88297960
Running inference benchmark on vit_base_patch32_clip_384.laion2b_ft_in12k_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 4119.89 samples/sec. 62.138 ms/step.
Infer [16/40]. 4116.64 samples/sec. 62.187 ms/step.
Infer [24/40]. 4117.36 samples/sec. 62.176 ms/step.
Infer [32/40]. 4115.99 samples/sec. 62.197 ms/step.
Infer [40/40]. 4115.70 samples/sec. 62.201 ms/step.
Inference benchmark of vit_base_patch32_clip_384.laion2b_ft_in12k_in1k done. 4114.09 samples/sec, 62.20 ms/step
Model vit_base_patch32_clip_384.laion2b_ft_in12k_in1k created, param count: 88297960
Running train benchmark on vit_base_patch32_clip_384.laion2b_ft_in12k_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
Train [8/40]. 1293.10 samples/sec. 197.975 ms/step.
Train [16/40]. 1293.10 samples/sec. 197.974 ms/step.
Train [24/40]. 1293.02 samples/sec. 197.987 ms/step.
Train [32/40]. 1293.05 samples/sec. 197.982 ms/step.
Train [40/40]. 1292.99 samples/sec. 197.991 ms/step.
Train benchmark of vit_base_patch32_clip_384.laion2b_ft_in12k_in1k done. 1287.55 samples/sec, 197.99 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model vit_base_patch32_clip_384.openai_ft_in12k_in1k created, param count: 88297960
Running inference benchmark on vit_base_patch32_clip_384.openai_ft_in12k_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 4114.60 samples/sec. 62.217 ms/step.
Infer [16/40]. 4111.57 samples/sec. 62.263 ms/step.
Infer [24/40]. 4112.47 samples/sec. 62.250 ms/step.
Infer [32/40]. 4111.43 samples/sec. 62.265 ms/step.
Infer [40/40]. 4111.86 samples/sec. 62.259 ms/step.
Inference benchmark of vit_base_patch32_clip_384.openai_ft_in12k_in1k done. 4110.23 samples/sec, 62.26 ms/step
Model vit_base_patch32_clip_384.openai_ft_in12k_in1k created, param count: 88297960
Running train benchmark on vit_base_patch32_clip_384.openai_ft_in12k_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
Train [8/40]. 1292.98 samples/sec. 197.993 ms/step.
Train [16/40]. 1292.74 samples/sec. 198.029 ms/step.
Train [24/40]. 1292.66 samples/sec. 198.042 ms/step.
Train [32/40]. 1292.65 samples/sec. 198.044 ms/step.
Train [40/40]. 1292.58 samples/sec. 198.053 ms/step.
Train benchmark of vit_base_patch32_clip_384.openai_ft_in12k_in1k done. 1286.98 samples/sec, 198.05 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model vit_base_patch32_clip_448.laion2b_ft_in12k_in1k created, param count: 88337896
Running inference benchmark on vit_base_patch32_clip_448.laion2b_ft_in12k_in1k for 40 steps w/ input size (3, 448, 448) and batch size 256.
Infer [8/40]. 3031.90 samples/sec. 84.436 ms/step.
Infer [16/40]. 3033.31 samples/sec. 84.396 ms/step.
Infer [24/40]. 3033.97 samples/sec. 84.378 ms/step.
Infer [32/40]. 3033.35 samples/sec. 84.395 ms/step.
Infer [40/40]. 3033.48 samples/sec. 84.392 ms/step.
Inference benchmark of vit_base_patch32_clip_448.laion2b_ft_in12k_in1k done. 3032.50 samples/sec, 84.39 ms/step
Model vit_base_patch32_clip_448.laion2b_ft_in12k_in1k created, param count: 88337896
Running train benchmark on vit_base_patch32_clip_448.laion2b_ft_in12k_in1k for 40 steps w/ input size (3, 448, 448) and batch size 256.
Train [8/40]. 940.90 samples/sec. 272.081 ms/step.
Train [16/40]. 940.87 samples/sec. 272.089 ms/step.
Train [24/40]. 940.80 samples/sec. 272.109 ms/step.
Train [32/40]. 940.67 samples/sec. 272.145 ms/step.
Train [40/40]. 940.63 samples/sec. 272.159 ms/step.
Train benchmark of vit_base_patch32_clip_448.laion2b_ft_in12k_in1k done. 937.24 samples/sec, 272.16 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model vit_base_r50_s16_384.orig_in21k_ft_in1k created, param count: 98950952
Running inference benchmark on vit_base_r50_s16_384.orig_in21k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
Infer [8/40]. 563.54 samples/sec. 454.271 ms/step.
Infer [16/40]. 563.51 samples/sec. 454.294 ms/step.
Infer [24/40]. 563.49 samples/sec. 454.311 ms/step.
Infer [32/40]. 563.50 samples/sec. 454.301 ms/step.
Infer [40/40]. 563.50 samples/sec. 454.307 ms/step.
Inference benchmark of vit_base_r50_s16_384.orig_in21k_ft_in1k done. 563.44 samples/sec, 454.31 ms/step
Model vit_base_r50_s16_384.orig_in21k_ft_in1k created, param count: 98950952
Running train benchmark on vit_base_r50_s16_384.orig_in21k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 576.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 16.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.03 GiB is allocated by PyTorch, and 371.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model vit_base_r50_s16_384.orig_in21k_ft_in1k created, param count: 98950952
Running train benchmark on vit_base_r50_s16_384.orig_in21k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 216.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 54.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 22.18 GiB is allocated by PyTorch, and 178.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model vit_base_r50_s16_384.orig_in21k_ft_in1k created, param count: 98950952
Running train benchmark on vit_base_r50_s16_384.orig_in21k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 434.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 84.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 22.21 GiB is allocated by PyTorch, and 115.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model vit_base_r50_s16_384.orig_in21k_ft_in1k created, param count: 98950952
Running train benchmark on vit_base_r50_s16_384.orig_in21k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 326.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 98.06 MiB is free. Including non-PyTorch memory, this process has 23.54 GiB memory in use. Of the allocated memory 22.15 GiB is allocated by PyTorch, and 161.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model vit_base_r50_s16_384.orig_in21k_ft_in1k created, param count: 98950952
Running train benchmark on vit_base_r50_s16_384.orig_in21k_ft_in1k for 40 steps w/ input size (3, 384, 384) and batch size 64.
Train [8/40]. 182.84 samples/sec. 350.024 ms/step.
Train [16/40]. 182.81 samples/sec. 350.081 ms/step.
Train [24/40]. 182.79 samples/sec. 350.123 ms/step.
Train [32/40]. 182.79 samples/sec. 350.137 ms/step.
Train [40/40]. 182.78 samples/sec. 350.145 ms/step.
Train benchmark of vit_base_r50_s16_384.orig_in21k_ft_in1k done. 181.94 samples/sec, 350.14 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model vit_giant_patch14_clip_224.laion2b created, param count: 1012646656
Running inference benchmark on vit_giant_patch14_clip_224.laion2b for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 237.84 samples/sec. 1076.357 ms/step.
Infer [16/40]. 238.20 samples/sec. 1074.706 ms/step.
Infer [24/40]. 238.17 samples/sec. 1074.861 ms/step.
Infer [32/40]. 238.15 samples/sec. 1074.953 ms/step.
Infer [40/40]. 238.16 samples/sec. 1074.893 ms/step.
Inference benchmark of vit_giant_patch14_clip_224.laion2b done. 238.15 samples/sec, 1074.89 ms/step
Model vit_giant_patch14_clip_224.laion2b created, param count: 1012646656
Running train benchmark on vit_giant_patch14_clip_224.laion2b for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 772.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 464.06 MiB is free. Including non-PyTorch memory, this process has 23.19 GiB memory in use. Of the allocated memory 21.66 GiB is allocated by PyTorch, and 305.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model vit_giant_patch14_clip_224.laion2b created, param count: 1012646656
Running train benchmark on vit_giant_patch14_clip_224.laion2b for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 100.06 MiB is free. Including non-PyTorch memory, this process has 23.54 GiB memory in use. Of the allocated memory 21.75 GiB is allocated by PyTorch, and 573.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model vit_giant_patch14_clip_224.laion2b created, param count: 1012646656
Running train benchmark on vit_giant_patch14_clip_224.laion2b for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 386.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 248.06 MiB is free. Including non-PyTorch memory, this process has 23.40 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 245.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model vit_giant_patch14_clip_224.laion2b created, param count: 1012646656
Running train benchmark on vit_giant_patch14_clip_224.laion2b for 40 steps w/ input size (3, 224, 224) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 68.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 30.06 MiB is free. Including non-PyTorch memory, this process has 23.61 GiB memory in use. Of the allocated memory 21.88 GiB is allocated by PyTorch, and 510.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model vit_giant_patch14_clip_224.laion2b created, param count: 1012646656
Running train benchmark on vit_giant_patch14_clip_224.laion2b for 40 steps w/ input size (3, 224, 224) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 194.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 82.06 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 21.84 GiB is allocated by PyTorch, and 500.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model vit_giant_patch14_clip_224.laion2b created, param count: 1012646656
Running train benchmark on vit_giant_patch14_clip_224.laion2b for 40 steps w/ input size (3, 224, 224) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 146.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 8.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.05 GiB is allocated by PyTorch, and 356.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model vit_giant_patch14_clip_224.laion2b created, param count: 1012646656
Running train benchmark on vit_giant_patch14_clip_224.laion2b for 40 steps w/ input size (3, 224, 224) and batch size 32.
Train [8/40]. 69.63 samples/sec. 459.593 ms/step.
Train [16/40]. 69.63 samples/sec. 459.576 ms/step.
Train [24/40]. 69.63 samples/sec. 459.577 ms/step.
Train [32/40]. 69.63 samples/sec. 459.580 ms/step.
Train [40/40]. 69.63 samples/sec. 459.596 ms/step.
Train benchmark of vit_giant_patch14_clip_224.laion2b done. 69.24 samples/sec, 459.60 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model vit_giant_patch14_dinov2.lvd142m created, param count: 1136479232
Running inference benchmark on vit_giant_patch14_dinov2.lvd142m for 40 steps w/ input size (3, 518, 518) and batch size 256.
Infer [8/40]. 33.83 samples/sec. 7566.862 ms/step.
Infer [16/40]. 33.83 samples/sec. 7567.393 ms/step.
Infer [24/40]. 33.82 samples/sec. 7569.711 ms/step.
Infer [32/40]. 33.81 samples/sec. 7571.195 ms/step.
Infer [40/40]. 33.81 samples/sec. 7571.852 ms/step.
Inference benchmark of vit_giant_patch14_dinov2.lvd142m done. 33.81 samples/sec, 7571.85 ms/step
Model vit_giant_patch14_dinov2.lvd142m created, param count: 1136479232
Running train benchmark on vit_giant_patch14_dinov2.lvd142m for 40 steps w/ input size (3, 518, 518) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 1.00 GiB. GPU 0 has a total capacty of 23.65 GiB of which 4.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 22.31 GiB is allocated by PyTorch, and 100.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model vit_giant_patch14_dinov2.lvd142m created, param count: 1136479232
Running train benchmark on vit_giant_patch14_dinov2.lvd142m for 40 steps w/ input size (3, 518, 518) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 2.26 GiB. GPU 0 has a total capacty of 23.65 GiB of which 1.84 GiB is free. Including non-PyTorch memory, this process has 21.80 GiB memory in use. Of the allocated memory 19.53 GiB is allocated by PyTorch, and 1.03 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model vit_giant_patch14_dinov2.lvd142m created, param count: 1136479232
Running train benchmark on vit_giant_patch14_dinov2.lvd142m for 40 steps w/ input size (3, 518, 518) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 1.34 GiB. GPU 0 has a total capacty of 23.65 GiB of which 978.06 MiB is free. Including non-PyTorch memory, this process has 22.69 GiB memory in use. Of the allocated memory 21.28 GiB is allocated by PyTorch, and 181.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model vit_giant_patch14_dinov2.lvd142m created, param count: 1136479232
Running train benchmark on vit_giant_patch14_dinov2.lvd142m for 40 steps w/ input size (3, 518, 518) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 2.01 GiB. GPU 0 has a total capacty of 23.65 GiB of which 438.06 MiB is free. Including non-PyTorch memory, this process has 23.21 GiB memory in use. Of the allocated memory 21.29 GiB is allocated by PyTorch, and 703.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model vit_giant_patch14_dinov2.lvd142m created, param count: 1136479232
Running train benchmark on vit_giant_patch14_dinov2.lvd142m for 40 steps w/ input size (3, 518, 518) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 686.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 54.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 22.10 GiB is allocated by PyTorch, and 259.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model vit_giant_patch14_dinov2.lvd142m created, param count: 1136479232
Running train benchmark on vit_giant_patch14_dinov2.lvd142m for 40 steps w/ input size (3, 518, 518) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 194.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 150.06 MiB is free. Including non-PyTorch memory, this process has 23.49 GiB memory in use. Of the allocated memory 21.52 GiB is allocated by PyTorch, and 764.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model vit_giant_patch14_dinov2.lvd142m created, param count: 1136479232
Running train benchmark on vit_giant_patch14_dinov2.lvd142m for 40 steps w/ input size (3, 518, 518) and batch size 32.
ERROR: "CUDA out of memory. Tried to allocate 344.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 4.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 22.18 GiB is allocated by PyTorch, and 225.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model vit_giant_patch14_dinov2.lvd142m created, param count: 1136479232
Running train benchmark on vit_giant_patch14_dinov2.lvd142m for 40 steps w/ input size (3, 518, 518) and batch size 24.
ERROR: "CUDA out of memory. Tried to allocate 290.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 70.06 MiB is free. Including non-PyTorch memory, this process has 23.57 GiB memory in use. Of the allocated memory 21.86 GiB is allocated by PyTorch, and 493.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model vit_giant_patch14_dinov2.lvd142m created, param count: 1136479232
Running train benchmark on vit_giant_patch14_dinov2.lvd142m for 40 steps w/ input size (3, 518, 518) and batch size 16.
ERROR: "CUDA out of memory. Tried to allocate 66.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 8.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 22.04 GiB is allocated by PyTorch, and 364.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 12 for retry.
Model vit_giant_patch14_dinov2.lvd142m created, param count: 1136479232
Running train benchmark on vit_giant_patch14_dinov2.lvd142m for 40 steps w/ input size (3, 518, 518) and batch size 12.
ERROR: "CUDA out of memory. Tried to allocate 146.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 122.06 MiB is free. Including non-PyTorch memory, this process has 23.52 GiB memory in use. Of the allocated memory 21.74 GiB is allocated by PyTorch, and 562.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 8 for retry.
Model vit_giant_patch14_dinov2.lvd142m created, param count: 1136479232
Running train benchmark on vit_giant_patch14_dinov2.lvd142m for 40 steps w/ input size (3, 518, 518) and batch size 8.
ERROR: "CUDA out of memory. Tried to allocate 66.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 48.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 21.88 GiB is allocated by PyTorch, and 489.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 6 for retry.
Model vit_giant_patch14_dinov2.lvd142m created, param count: 1136479232
Running train benchmark on vit_giant_patch14_dinov2.lvd142m for 40 steps w/ input size (3, 518, 518) and batch size 6.
ERROR: "CUDA out of memory. Tried to allocate 66.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 2.06 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 21.55 GiB is allocated by PyTorch, and 877.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 4 for retry.
Model vit_giant_patch14_dinov2.lvd142m created, param count: 1136479232
Running train benchmark on vit_giant_patch14_dinov2.lvd142m for 40 steps w/ input size (3, 518, 518) and batch size 4.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 3 for retry.
Model vit_giant_patch14_dinov2.lvd142m created, param count: 1136479232
Running train benchmark on vit_giant_patch14_dinov2.lvd142m for 40 steps w/ input size (3, 518, 518) and batch size 3.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 2 for retry.
Model vit_giant_patch14_dinov2.lvd142m created, param count: 1136479232
Running train benchmark on vit_giant_patch14_dinov2.lvd142m for 40 steps w/ input size (3, 518, 518) and batch size 2.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 1 for retry.
Model vit_giant_patch14_dinov2.lvd142m created, param count: 1136479232
Running train benchmark on vit_giant_patch14_dinov2.lvd142m for 40 steps w/ input size (3, 518, 518) and batch size 1.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 0 for retry.
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model vit_gigantic_patch14_clip_224.laion2b created, param count: 1844908544
Running inference benchmark on vit_gigantic_patch14_clip_224.laion2b for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 138.20 samples/sec. 1852.422 ms/step.
Infer [16/40]. 138.17 samples/sec. 1852.774 ms/step.
Infer [24/40]. 138.13 samples/sec. 1853.267 ms/step.
Infer [32/40]. 138.11 samples/sec. 1853.625 ms/step.
Infer [40/40]. 138.08 samples/sec. 1854.032 ms/step.
Inference benchmark of vit_gigantic_patch14_clip_224.laion2b done. 138.07 samples/sec, 1854.03 ms/step
Model vit_gigantic_patch14_clip_224.laion2b created, param count: 1844908544
Running train benchmark on vit_gigantic_patch14_clip_224.laion2b for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 210.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 164.06 MiB is free. Including non-PyTorch memory, this process has 23.48 GiB memory in use. Of the allocated memory 21.96 GiB is allocated by PyTorch, and 296.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model vit_gigantic_patch14_clip_224.laion2b created, param count: 1844908544
Running train benchmark on vit_gigantic_patch14_clip_224.laion2b for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 772.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 636.06 MiB is free. Including non-PyTorch memory, this process has 23.02 GiB memory in use. Of the allocated memory 21.31 GiB is allocated by PyTorch, and 485.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model vit_gigantic_patch14_clip_224.laion2b created, param count: 1844908544
Running train benchmark on vit_gigantic_patch14_clip_224.laion2b for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 314.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 216.06 MiB is free. Including non-PyTorch memory, this process has 23.43 GiB memory in use. Of the allocated memory 22.03 GiB is allocated by PyTorch, and 169.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model vit_gigantic_patch14_clip_224.laion2b created, param count: 1844908544
Running train benchmark on vit_gigantic_patch14_clip_224.laion2b for 40 steps w/ input size (3, 224, 224) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 386.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 16.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 21.96 GiB is allocated by PyTorch, and 445.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model vit_gigantic_patch14_clip_224.laion2b created, param count: 1844908544
Running train benchmark on vit_gigantic_patch14_clip_224.laion2b for 40 steps w/ input size (3, 224, 224) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 54.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 40.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 22.05 GiB is allocated by PyTorch, and 327.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model vit_gigantic_patch14_clip_224.laion2b created, param count: 1844908544
Running train benchmark on vit_gigantic_patch14_clip_224.laion2b for 40 steps w/ input size (3, 224, 224) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 118.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 20.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.16 GiB is allocated by PyTorch, and 233.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model vit_gigantic_patch14_clip_224.laion2b created, param count: 1844908544
Running train benchmark on vit_gigantic_patch14_clip_224.laion2b for 40 steps w/ input size (3, 224, 224) and batch size 32.
ERROR: "CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 6.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 21.74 GiB is allocated by PyTorch, and 676.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model vit_gigantic_patch14_clip_224.laion2b created, param count: 1844908544
Running train benchmark on vit_gigantic_patch14_clip_224.laion2b for 40 steps w/ input size (3, 224, 224) and batch size 24.
ERROR: "CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 58.06 MiB is free. Including non-PyTorch memory, this process has 23.58 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 432.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model vit_gigantic_patch14_clip_224.laion2b created, param count: 1844908544
Running train benchmark on vit_gigantic_patch14_clip_224.laion2b for 40 steps w/ input size (3, 224, 224) and batch size 16.
Train [8/40]. 36.40 samples/sec. 439.545 ms/step.
Train [16/40]. 36.40 samples/sec. 439.547 ms/step.
Train [24/40]. 36.40 samples/sec. 439.529 ms/step.
Train [32/40]. 36.40 samples/sec. 439.515 ms/step.
Train [40/40]. 36.40 samples/sec. 439.512 ms/step.
Train benchmark of vit_gigantic_patch14_clip_224.laion2b done. 36.17 samples/sec, 439.51 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model vit_gigantic_patch16_224_ijepa.in22k created, param count: 1842975360
Running inference benchmark on vit_gigantic_patch16_224_ijepa.in22k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 184.95 samples/sec. 1384.182 ms/step.
Infer [16/40]. 184.89 samples/sec. 1384.587 ms/step.
Infer [24/40]. 184.80 samples/sec. 1385.252 ms/step.
Infer [32/40]. 184.71 samples/sec. 1385.966 ms/step.
Infer [40/40]. 184.63 samples/sec. 1386.567 ms/step.
Inference benchmark of vit_gigantic_patch16_224_ijepa.in22k done. 184.62 samples/sec, 1386.57 ms/step
Model vit_gigantic_patch16_224_ijepa.in22k created, param count: 1842975360
Running train benchmark on vit_gigantic_patch16_224_ijepa.in22k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 788.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 752.06 MiB is free. Including non-PyTorch memory, this process has 22.91 GiB memory in use. Of the allocated memory 21.55 GiB is allocated by PyTorch, and 128.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model vit_gigantic_patch16_224_ijepa.in22k created, param count: 1842975360
Running train benchmark on vit_gigantic_patch16_224_ijepa.in22k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 592.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 218.06 MiB is free. Including non-PyTorch memory, this process has 23.43 GiB memory in use. Of the allocated memory 21.81 GiB is allocated by PyTorch, and 397.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model vit_gigantic_patch16_224_ijepa.in22k created, param count: 1842975360
Running train benchmark on vit_gigantic_patch16_224_ijepa.in22k for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 82.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 56.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 22.12 GiB is allocated by PyTorch, and 236.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model vit_gigantic_patch16_224_ijepa.in22k created, param count: 1842975360
Running train benchmark on vit_gigantic_patch16_224_ijepa.in22k for 40 steps w/ input size (3, 224, 224) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 296.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 198.06 MiB is free. Including non-PyTorch memory, this process has 23.45 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 434.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model vit_gigantic_patch16_224_ijepa.in22k created, param count: 1842975360
Running train benchmark on vit_gigantic_patch16_224_ijepa.in22k for 40 steps w/ input size (3, 224, 224) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 24.06 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 21.96 GiB is allocated by PyTorch, and 438.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model vit_gigantic_patch16_224_ijepa.in22k created, param count: 1842975360
Running train benchmark on vit_gigantic_patch16_224_ijepa.in22k for 40 steps w/ input size (3, 224, 224) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 92.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 6.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 21.88 GiB is allocated by PyTorch, and 535.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model vit_gigantic_patch16_224_ijepa.in22k created, param count: 1842975360
Running train benchmark on vit_gigantic_patch16_224_ijepa.in22k for 40 steps w/ input size (3, 224, 224) and batch size 32.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model vit_gigantic_patch16_224_ijepa.in22k created, param count: 1842975360
Running train benchmark on vit_gigantic_patch16_224_ijepa.in22k for 40 steps w/ input size (3, 224, 224) and batch size 24.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model vit_gigantic_patch16_224_ijepa.in22k created, param count: 1842975360
Running train benchmark on vit_gigantic_patch16_224_ijepa.in22k for 40 steps w/ input size (3, 224, 224) and batch size 16.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 12 for retry.
Model vit_gigantic_patch16_224_ijepa.in22k created, param count: 1842975360
Running train benchmark on vit_gigantic_patch16_224_ijepa.in22k for 40 steps w/ input size (3, 224, 224) and batch size 12.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 8 for retry.
Model vit_gigantic_patch16_224_ijepa.in22k created, param count: 1842975360
Running train benchmark on vit_gigantic_patch16_224_ijepa.in22k for 40 steps w/ input size (3, 224, 224) and batch size 8.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 6 for retry.
Model vit_gigantic_patch16_224_ijepa.in22k created, param count: 1842975360
Running train benchmark on vit_gigantic_patch16_224_ijepa.in22k for 40 steps w/ input size (3, 224, 224) and batch size 6.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 4 for retry.
Model vit_gigantic_patch16_224_ijepa.in22k created, param count: 1842975360
Running train benchmark on vit_gigantic_patch16_224_ijepa.in22k for 40 steps w/ input size (3, 224, 224) and batch size 4.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 3 for retry.
Model vit_gigantic_patch16_224_ijepa.in22k created, param count: 1842975360
Running train benchmark on vit_gigantic_patch16_224_ijepa.in22k for 40 steps w/ input size (3, 224, 224) and batch size 3.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 2 for retry.
Model vit_gigantic_patch16_224_ijepa.in22k created, param count: 1842975360
Running train benchmark on vit_gigantic_patch16_224_ijepa.in22k for 40 steps w/ input size (3, 224, 224) and batch size 2.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 1 for retry.
Model vit_gigantic_patch16_224_ijepa.in22k created, param count: 1842975360
Running train benchmark on vit_gigantic_patch16_224_ijepa.in22k for 40 steps w/ input size (3, 224, 224) and batch size 1.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 0 for retry.
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model vit_huge_patch14_224.mae created, param count: 630764800
Running inference benchmark on vit_huge_patch14_224.mae for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 377.98 samples/sec. 677.281 ms/step.
Infer [16/40]. 377.91 samples/sec. 677.416 ms/step.
Infer [24/40]. 377.87 samples/sec. 677.483 ms/step.
Infer [32/40]. 377.76 samples/sec. 677.677 ms/step.
Infer [40/40]. 377.72 samples/sec. 677.742 ms/step.
Inference benchmark of vit_huge_patch14_224.mae done. 377.69 samples/sec, 677.74 ms/step
Model vit_huge_patch14_224.mae created, param count: 630764800
Running train benchmark on vit_huge_patch14_224.mae for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 482.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 464.06 MiB is free. Including non-PyTorch memory, this process has 23.19 GiB memory in use. Of the allocated memory 21.70 GiB is allocated by PyTorch, and 261.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model vit_huge_patch14_224.mae created, param count: 630764800
Running train benchmark on vit_huge_patch14_224.mae for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 482.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 458.06 MiB is free. Including non-PyTorch memory, this process has 23.19 GiB memory in use. Of the allocated memory 21.53 GiB is allocated by PyTorch, and 437.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model vit_huge_patch14_224.mae created, param count: 630764800
Running train benchmark on vit_huge_patch14_224.mae for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 322.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 184.06 MiB is free. Including non-PyTorch memory, this process has 23.46 GiB memory in use. Of the allocated memory 22.00 GiB is allocated by PyTorch, and 230.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model vit_huge_patch14_224.mae created, param count: 630764800
Running train benchmark on vit_huge_patch14_224.mae for 40 steps w/ input size (3, 224, 224) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 242.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 134.06 MiB is free. Including non-PyTorch memory, this process has 23.51 GiB memory in use. Of the allocated memory 21.73 GiB is allocated by PyTorch, and 555.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model vit_huge_patch14_224.mae created, param count: 630764800
Running train benchmark on vit_huge_patch14_224.mae for 40 steps w/ input size (3, 224, 224) and batch size 64.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model vit_huge_patch14_224.mae created, param count: 630764800
Running train benchmark on vit_huge_patch14_224.mae for 40 steps w/ input size (3, 224, 224) and batch size 48.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model vit_huge_patch14_224.mae created, param count: 630764800
Running train benchmark on vit_huge_patch14_224.mae for 40 steps w/ input size (3, 224, 224) and batch size 32.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model vit_huge_patch14_224.mae created, param count: 630764800
Running train benchmark on vit_huge_patch14_224.mae for 40 steps w/ input size (3, 224, 224) and batch size 24.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model vit_huge_patch14_224.mae created, param count: 630764800
Running train benchmark on vit_huge_patch14_224.mae for 40 steps w/ input size (3, 224, 224) and batch size 16.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 12 for retry.
Model vit_huge_patch14_224.mae created, param count: 630764800
Running train benchmark on vit_huge_patch14_224.mae for 40 steps w/ input size (3, 224, 224) and batch size 12.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 8 for retry.
Model vit_huge_patch14_224.mae created, param count: 630764800
Running train benchmark on vit_huge_patch14_224.mae for 40 steps w/ input size (3, 224, 224) and batch size 8.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 6 for retry.
Model vit_huge_patch14_224.mae created, param count: 630764800
Running train benchmark on vit_huge_patch14_224.mae for 40 steps w/ input size (3, 224, 224) and batch size 6.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 4 for retry.
Model vit_huge_patch14_224.mae created, param count: 630764800
Running train benchmark on vit_huge_patch14_224.mae for 40 steps w/ input size (3, 224, 224) and batch size 4.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 3 for retry.
Model vit_huge_patch14_224.mae created, param count: 630764800
Running train benchmark on vit_huge_patch14_224.mae for 40 steps w/ input size (3, 224, 224) and batch size 3.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 2 for retry.
Model vit_huge_patch14_224.mae created, param count: 630764800
Running train benchmark on vit_huge_patch14_224.mae for 40 steps w/ input size (3, 224, 224) and batch size 2.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 1 for retry.
Model vit_huge_patch14_224.mae created, param count: 630764800
Running train benchmark on vit_huge_patch14_224.mae for 40 steps w/ input size (3, 224, 224) and batch size 1.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 0 for retry.
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model vit_huge_patch14_224_ijepa.in1k created, param count: 630762240
Running inference benchmark on vit_huge_patch14_224_ijepa.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 340.21 samples/sec. 752.478 ms/step.
Infer [16/40]. 340.05 samples/sec. 752.828 ms/step.
Infer [24/40]. 340.02 samples/sec. 752.906 ms/step.
Infer [32/40]. 339.99 samples/sec. 752.959 ms/step.
Infer [40/40]. 339.97 samples/sec. 753.009 ms/step.
Inference benchmark of vit_huge_patch14_224_ijepa.in1k done. 339.95 samples/sec, 753.01 ms/step
Model vit_huge_patch14_224_ijepa.in1k created, param count: 630762240
Running train benchmark on vit_huge_patch14_224_ijepa.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 160.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 108.06 MiB is free. Including non-PyTorch memory, this process has 23.54 GiB memory in use. Of the allocated memory 22.24 GiB is allocated by PyTorch, and 59.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model vit_huge_patch14_224_ijepa.in1k created, param count: 630762240
Running train benchmark on vit_huge_patch14_224_ijepa.in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 120.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 40.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 22.15 GiB is allocated by PyTorch, and 227.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model vit_huge_patch14_224_ijepa.in1k created, param count: 630762240
Running train benchmark on vit_huge_patch14_224_ijepa.in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 320.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 146.06 MiB is free. Including non-PyTorch memory, this process has 23.50 GiB memory in use. Of the allocated memory 22.21 GiB is allocated by PyTorch, and 58.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model vit_huge_patch14_224_ijepa.in1k created, param count: 630762240
Running train benchmark on vit_huge_patch14_224_ijepa.in1k for 40 steps w/ input size (3, 224, 224) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 166.06 MiB is free. Including non-PyTorch memory, this process has 23.48 GiB memory in use. Of the allocated memory 22.00 GiB is allocated by PyTorch, and 251.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model vit_huge_patch14_224_ijepa.in1k created, param count: 630762240
Running train benchmark on vit_huge_patch14_224_ijepa.in1k for 40 steps w/ input size (3, 224, 224) and batch size 64.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model vit_huge_patch14_224_ijepa.in1k created, param count: 630762240
Running train benchmark on vit_huge_patch14_224_ijepa.in1k for 40 steps w/ input size (3, 224, 224) and batch size 48.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model vit_huge_patch14_224_ijepa.in1k created, param count: 630762240
Running train benchmark on vit_huge_patch14_224_ijepa.in1k for 40 steps w/ input size (3, 224, 224) and batch size 32.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model vit_huge_patch14_224_ijepa.in1k created, param count: 630762240
Running train benchmark on vit_huge_patch14_224_ijepa.in1k for 40 steps w/ input size (3, 224, 224) and batch size 24.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model vit_huge_patch14_224_ijepa.in1k created, param count: 630762240
Running train benchmark on vit_huge_patch14_224_ijepa.in1k for 40 steps w/ input size (3, 224, 224) and batch size 16.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 12 for retry.
Model vit_huge_patch14_224_ijepa.in1k created, param count: 630762240
Running train benchmark on vit_huge_patch14_224_ijepa.in1k for 40 steps w/ input size (3, 224, 224) and batch size 12.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 8 for retry.
Model vit_huge_patch14_224_ijepa.in1k created, param count: 630762240
Running train benchmark on vit_huge_patch14_224_ijepa.in1k for 40 steps w/ input size (3, 224, 224) and batch size 8.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 6 for retry.
Model vit_huge_patch14_224_ijepa.in1k created, param count: 630762240
Running train benchmark on vit_huge_patch14_224_ijepa.in1k for 40 steps w/ input size (3, 224, 224) and batch size 6.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 4 for retry.
Model vit_huge_patch14_224_ijepa.in1k created, param count: 630762240
Running train benchmark on vit_huge_patch14_224_ijepa.in1k for 40 steps w/ input size (3, 224, 224) and batch size 4.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 3 for retry.
Model vit_huge_patch14_224_ijepa.in1k created, param count: 630762240
Running train benchmark on vit_huge_patch14_224_ijepa.in1k for 40 steps w/ input size (3, 224, 224) and batch size 3.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 2 for retry.
Model vit_huge_patch14_224_ijepa.in1k created, param count: 630762240
Running train benchmark on vit_huge_patch14_224_ijepa.in1k for 40 steps w/ input size (3, 224, 224) and batch size 2.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 1 for retry.
Model vit_huge_patch14_224_ijepa.in1k created, param count: 630762240
Running train benchmark on vit_huge_patch14_224_ijepa.in1k for 40 steps w/ input size (3, 224, 224) and batch size 1.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 0 for retry.
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model vit_huge_patch14_224_ijepa.in22k created, param count: 630762240
Running inference benchmark on vit_huge_patch14_224_ijepa.in22k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 340.03 samples/sec. 752.883 ms/step.
Infer [16/40]. 340.02 samples/sec. 752.894 ms/step.
Infer [24/40]. 340.02 samples/sec. 752.905 ms/step.
Infer [32/40]. 339.96 samples/sec. 753.022 ms/step.
Infer [40/40]. 339.93 samples/sec. 753.098 ms/step.
Inference benchmark of vit_huge_patch14_224_ijepa.in22k done. 339.91 samples/sec, 753.10 ms/step
Model vit_huge_patch14_224_ijepa.in22k created, param count: 630762240
Running train benchmark on vit_huge_patch14_224_ijepa.in22k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 160.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 108.06 MiB is free. Including non-PyTorch memory, this process has 23.54 GiB memory in use. Of the allocated memory 22.24 GiB is allocated by PyTorch, and 59.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model vit_huge_patch14_224_ijepa.in22k created, param count: 630762240
Running train benchmark on vit_huge_patch14_224_ijepa.in22k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 120.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 40.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 22.15 GiB is allocated by PyTorch, and 227.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model vit_huge_patch14_224_ijepa.in22k created, param count: 630762240
Running train benchmark on vit_huge_patch14_224_ijepa.in22k for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 320.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 146.06 MiB is free. Including non-PyTorch memory, this process has 23.50 GiB memory in use. Of the allocated memory 22.21 GiB is allocated by PyTorch, and 58.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model vit_huge_patch14_224_ijepa.in22k created, param count: 630762240
Running train benchmark on vit_huge_patch14_224_ijepa.in22k for 40 steps w/ input size (3, 224, 224) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 166.06 MiB is free. Including non-PyTorch memory, this process has 23.48 GiB memory in use. Of the allocated memory 22.00 GiB is allocated by PyTorch, and 251.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model vit_huge_patch14_224_ijepa.in22k created, param count: 630762240
Running train benchmark on vit_huge_patch14_224_ijepa.in22k for 40 steps w/ input size (3, 224, 224) and batch size 64.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model vit_huge_patch14_224_ijepa.in22k created, param count: 630762240
Running train benchmark on vit_huge_patch14_224_ijepa.in22k for 40 steps w/ input size (3, 224, 224) and batch size 48.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model vit_huge_patch14_224_ijepa.in22k created, param count: 630762240
Running train benchmark on vit_huge_patch14_224_ijepa.in22k for 40 steps w/ input size (3, 224, 224) and batch size 32.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model vit_huge_patch14_224_ijepa.in22k created, param count: 630762240
Running train benchmark on vit_huge_patch14_224_ijepa.in22k for 40 steps w/ input size (3, 224, 224) and batch size 24.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 16 for retry.
Model vit_huge_patch14_224_ijepa.in22k created, param count: 630762240
Running train benchmark on vit_huge_patch14_224_ijepa.in22k for 40 steps w/ input size (3, 224, 224) and batch size 16.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 12 for retry.
Model vit_huge_patch14_224_ijepa.in22k created, param count: 630762240
Running train benchmark on vit_huge_patch14_224_ijepa.in22k for 40 steps w/ input size (3, 224, 224) and batch size 12.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 8 for retry.
Model vit_huge_patch14_224_ijepa.in22k created, param count: 630762240
Running train benchmark on vit_huge_patch14_224_ijepa.in22k for 40 steps w/ input size (3, 224, 224) and batch size 8.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 6 for retry.
Model vit_huge_patch14_224_ijepa.in22k created, param count: 630762240
Running train benchmark on vit_huge_patch14_224_ijepa.in22k for 40 steps w/ input size (3, 224, 224) and batch size 6.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 4 for retry.
Model vit_huge_patch14_224_ijepa.in22k created, param count: 630762240
Running train benchmark on vit_huge_patch14_224_ijepa.in22k for 40 steps w/ input size (3, 224, 224) and batch size 4.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 3 for retry.
Model vit_huge_patch14_224_ijepa.in22k created, param count: 630762240
Running train benchmark on vit_huge_patch14_224_ijepa.in22k for 40 steps w/ input size (3, 224, 224) and batch size 3.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 2 for retry.
Model vit_huge_patch14_224_ijepa.in22k created, param count: 630762240
Running train benchmark on vit_huge_patch14_224_ijepa.in22k for 40 steps w/ input size (3, 224, 224) and batch size 2.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 1 for retry.
Model vit_huge_patch14_224_ijepa.in22k created, param count: 630762240
Running train benchmark on vit_huge_patch14_224_ijepa.in22k for 40 steps w/ input size (3, 224, 224) and batch size 1.
ERROR: "random_ expects 'from' to be less than 'to', but got from=0 >= to=0" while running benchmark.
WARNING: Reducing batch size to 0 for retry.
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model vit_huge_patch14_clip_224.laion2b created, param count: 632077824
Running inference benchmark on vit_huge_patch14_clip_224.laion2b for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 377.18 samples/sec. 678.712 ms/step.
Infer [16/40]. 377.19 samples/sec. 678.703 ms/step.
Infer [24/40]. 377.17 samples/sec. 678.745 ms/step.
Infer [32/40]. 377.14 samples/sec. 678.787 ms/step.
Infer [40/40]. 377.11 samples/sec. 678.849 ms/step.
Inference benchmark of vit_huge_patch14_clip_224.laion2b done. 377.08 samples/sec, 678.85 ms/step
Model vit_huge_patch14_clip_224.laion2b created, param count: 632077824
Running train benchmark on vit_huge_patch14_clip_224.laion2b for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 482.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 302.06 MiB is free. Including non-PyTorch memory, this process has 23.35 GiB memory in use. Of the allocated memory 21.86 GiB is allocated by PyTorch, and 259.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model vit_huge_patch14_clip_224.laion2b created, param count: 632077824
Running train benchmark on vit_huge_patch14_clip_224.laion2b for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 482.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 316.06 MiB is free. Including non-PyTorch memory, this process has 23.33 GiB memory in use. Of the allocated memory 21.65 GiB is allocated by PyTorch, and 455.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model vit_huge_patch14_clip_224.laion2b created, param count: 632077824
Running train benchmark on vit_huge_patch14_clip_224.laion2b for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 322.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 102.06 MiB is free. Including non-PyTorch memory, this process has 23.54 GiB memory in use. Of the allocated memory 22.09 GiB is allocated by PyTorch, and 228.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model vit_huge_patch14_clip_224.laion2b created, param count: 632077824
Running train benchmark on vit_huge_patch14_clip_224.laion2b for 40 steps w/ input size (3, 224, 224) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 242.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 44.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 21.80 GiB is allocated by PyTorch, and 583.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model vit_huge_patch14_clip_224.laion2b created, param count: 632077824
Running train benchmark on vit_huge_patch14_clip_224.laion2b for 40 steps w/ input size (3, 224, 224) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 162.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 156.06 MiB is free. Including non-PyTorch memory, this process has 23.49 GiB memory in use. Of the allocated memory 21.14 GiB is allocated by PyTorch, and 1.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model vit_huge_patch14_clip_224.laion2b created, param count: 632077824
Running train benchmark on vit_huge_patch14_clip_224.laion2b for 40 steps w/ input size (3, 224, 224) and batch size 48.
Train [8/40]. 108.35 samples/sec. 443.004 ms/step.
Train [16/40]. 108.35 samples/sec. 443.013 ms/step.
Train [24/40]. 108.35 samples/sec. 443.023 ms/step.
Train [32/40]. 108.33 samples/sec. 443.080 ms/step.
Train [40/40]. 108.32 samples/sec. 443.133 ms/step.
Train benchmark of vit_huge_patch14_clip_224.laion2b done. 107.75 samples/sec, 443.13 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model vit_huge_patch14_clip_224.laion2b_ft_in1k created, param count: 632047080
Running inference benchmark on vit_huge_patch14_clip_224.laion2b_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 378.08 samples/sec. 677.106 ms/step.
Infer [16/40]. 378.14 samples/sec. 676.993 ms/step.
Infer [24/40]. 378.04 samples/sec. 677.178 ms/step.
Infer [32/40]. 377.96 samples/sec. 677.327 ms/step.
Infer [40/40]. 377.88 samples/sec. 677.467 ms/step.
Inference benchmark of vit_huge_patch14_clip_224.laion2b_ft_in1k done. 377.85 samples/sec, 677.47 ms/step
Model vit_huge_patch14_clip_224.laion2b_ft_in1k created, param count: 632047080
Running train benchmark on vit_huge_patch14_clip_224.laion2b_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 482.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 302.06 MiB is free. Including non-PyTorch memory, this process has 23.35 GiB memory in use. Of the allocated memory 21.86 GiB is allocated by PyTorch, and 259.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model vit_huge_patch14_clip_224.laion2b_ft_in1k created, param count: 632047080
Running train benchmark on vit_huge_patch14_clip_224.laion2b_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 482.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 316.06 MiB is free. Including non-PyTorch memory, this process has 23.33 GiB memory in use. Of the allocated memory 21.65 GiB is allocated by PyTorch, and 455.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model vit_huge_patch14_clip_224.laion2b_ft_in1k created, param count: 632047080
Running train benchmark on vit_huge_patch14_clip_224.laion2b_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 322.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 102.06 MiB is free. Including non-PyTorch memory, this process has 23.54 GiB memory in use. Of the allocated memory 22.09 GiB is allocated by PyTorch, and 228.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model vit_huge_patch14_clip_224.laion2b_ft_in1k created, param count: 632047080
Running train benchmark on vit_huge_patch14_clip_224.laion2b_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 242.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 44.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 21.80 GiB is allocated by PyTorch, and 583.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model vit_huge_patch14_clip_224.laion2b_ft_in1k created, param count: 632047080
Running train benchmark on vit_huge_patch14_clip_224.laion2b_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 162.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 52.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 21.14 GiB is allocated by PyTorch, and 1.21 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model vit_huge_patch14_clip_224.laion2b_ft_in1k created, param count: 632047080
Running train benchmark on vit_huge_patch14_clip_224.laion2b_ft_in1k for 40 steps w/ input size (3, 224, 224) and batch size 48.
Train [8/40]. 108.34 samples/sec. 443.047 ms/step.
Train [16/40]. 108.33 samples/sec. 443.071 ms/step.
Train [24/40]. 108.33 samples/sec. 443.076 ms/step.
Train [32/40]. 108.33 samples/sec. 443.082 ms/step.
Train [40/40]. 108.33 samples/sec. 443.082 ms/step.
Train benchmark of vit_huge_patch14_clip_224.laion2b_ft_in1k done. 107.78 samples/sec, 443.08 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model vit_huge_patch14_clip_224.laion2b_ft_in12k created, param count: 645908781
Running inference benchmark on vit_huge_patch14_clip_224.laion2b_ft_in12k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 377.52 samples/sec. 678.117 ms/step.
Infer [16/40]. 377.42 samples/sec. 678.282 ms/step.
Infer [24/40]. 377.37 samples/sec. 678.383 ms/step.
Infer [32/40]. 377.27 samples/sec. 678.567 ms/step.
Infer [40/40]. 377.18 samples/sec. 678.727 ms/step.
Inference benchmark of vit_huge_patch14_clip_224.laion2b_ft_in12k done. 377.15 samples/sec, 678.73 ms/step
Model vit_huge_patch14_clip_224.laion2b_ft_in12k created, param count: 645908781
Running train benchmark on vit_huge_patch14_clip_224.laion2b_ft_in12k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 482.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 272.06 MiB is free. Including non-PyTorch memory, this process has 23.38 GiB memory in use. Of the allocated memory 21.89 GiB is allocated by PyTorch, and 262.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model vit_huge_patch14_clip_224.laion2b_ft_in12k created, param count: 645908781
Running train benchmark on vit_huge_patch14_clip_224.laion2b_ft_in12k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 482.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 310.06 MiB is free. Including non-PyTorch memory, this process has 23.34 GiB memory in use. Of the allocated memory 21.68 GiB is allocated by PyTorch, and 434.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model vit_huge_patch14_clip_224.laion2b_ft_in12k created, param count: 645908781
Running train benchmark on vit_huge_patch14_clip_224.laion2b_ft_in12k for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 322.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 72.06 MiB is free. Including non-PyTorch memory, this process has 23.57 GiB memory in use. Of the allocated memory 22.11 GiB is allocated by PyTorch, and 232.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model vit_huge_patch14_clip_224.laion2b_ft_in12k created, param count: 645908781
Running train benchmark on vit_huge_patch14_clip_224.laion2b_ft_in12k for 40 steps w/ input size (3, 224, 224) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 242.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 12.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 21.82 GiB is allocated by PyTorch, and 588.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model vit_huge_patch14_clip_224.laion2b_ft_in12k created, param count: 645908781
Running train benchmark on vit_huge_patch14_clip_224.laion2b_ft_in12k for 40 steps w/ input size (3, 224, 224) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 162.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 14.06 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 21.19 GiB is allocated by PyTorch, and 1.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model vit_huge_patch14_clip_224.laion2b_ft_in12k created, param count: 645908781
Running train benchmark on vit_huge_patch14_clip_224.laion2b_ft_in12k for 40 steps w/ input size (3, 224, 224) and batch size 48.
Train [8/40]. 108.22 samples/sec. 443.546 ms/step.
Train [16/40]. 108.22 samples/sec. 443.549 ms/step.
Train [24/40]. 108.22 samples/sec. 443.549 ms/step.
Train [32/40]. 108.22 samples/sec. 443.548 ms/step.
Train [40/40]. 108.21 samples/sec. 443.586 ms/step.
Train benchmark of vit_huge_patch14_clip_224.laion2b_ft_in12k done. 107.65 samples/sec, 443.59 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model vit_huge_patch14_clip_224.laion2b_ft_in12k_in1k created, param count: 632047080
Running inference benchmark on vit_huge_patch14_clip_224.laion2b_ft_in12k_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Infer [8/40]. 377.52 samples/sec. 678.117 ms/step.
Infer [16/40]. 377.36 samples/sec. 678.404 ms/step.
Infer [24/40]. 377.29 samples/sec. 678.521 ms/step.
Infer [32/40]. 377.17 samples/sec. 678.730 ms/step.
Infer [40/40]. 377.07 samples/sec. 678.924 ms/step.
Inference benchmark of vit_huge_patch14_clip_224.laion2b_ft_in12k_in1k done. 377.04 samples/sec, 678.92 ms/step
Model vit_huge_patch14_clip_224.laion2b_ft_in12k_in1k created, param count: 632047080
Running train benchmark on vit_huge_patch14_clip_224.laion2b_ft_in12k_in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 482.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 302.06 MiB is free. Including non-PyTorch memory, this process has 23.35 GiB memory in use. Of the allocated memory 21.86 GiB is allocated by PyTorch, and 259.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model vit_huge_patch14_clip_224.laion2b_ft_in12k_in1k created, param count: 632047080
Running train benchmark on vit_huge_patch14_clip_224.laion2b_ft_in12k_in1k for 40 steps w/ input size (3, 224, 224) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 482.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 316.06 MiB is free. Including non-PyTorch memory, this process has 23.33 GiB memory in use. Of the allocated memory 21.65 GiB is allocated by PyTorch, and 455.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model vit_huge_patch14_clip_224.laion2b_ft_in12k_in1k created, param count: 632047080
Running train benchmark on vit_huge_patch14_clip_224.laion2b_ft_in12k_in1k for 40 steps w/ input size (3, 224, 224) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 322.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 102.06 MiB is free. Including non-PyTorch memory, this process has 23.54 GiB memory in use. Of the allocated memory 22.09 GiB is allocated by PyTorch, and 228.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model vit_huge_patch14_clip_224.laion2b_ft_in12k_in1k created, param count: 632047080
Running train benchmark on vit_huge_patch14_clip_224.laion2b_ft_in12k_in1k for 40 steps w/ input size (3, 224, 224) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 242.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 44.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 21.80 GiB is allocated by PyTorch, and 583.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model vit_huge_patch14_clip_224.laion2b_ft_in12k_in1k created, param count: 632047080
Running train benchmark on vit_huge_patch14_clip_224.laion2b_ft_in12k_in1k for 40 steps w/ input size (3, 224, 224) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 162.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 52.06 MiB is free. Including non-PyTorch memory, this process has 23.59 GiB memory in use. Of the allocated memory 21.14 GiB is allocated by PyTorch, and 1.21 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model vit_huge_patch14_clip_224.laion2b_ft_in12k_in1k created, param count: 632047080
Running train benchmark on vit_huge_patch14_clip_224.laion2b_ft_in12k_in1k for 40 steps w/ input size (3, 224, 224) and batch size 48.
Train [8/40]. 108.32 samples/sec. 443.120 ms/step.
Train [16/40]. 108.33 samples/sec. 443.107 ms/step.
Train [24/40]. 108.33 samples/sec. 443.107 ms/step.
Train [32/40]. 108.33 samples/sec. 443.101 ms/step.
Train [40/40]. 108.33 samples/sec. 443.103 ms/step.
Train benchmark of vit_huge_patch14_clip_224.laion2b_ft_in12k_in1k done. 107.77 samples/sec, 443.10 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model vit_huge_patch14_clip_336.laion2b_ft_in12k_in1k created, param count: 632456680
Running inference benchmark on vit_huge_patch14_clip_336.laion2b_ft_in12k_in1k for 40 steps w/ input size (3, 336, 336) and batch size 256.
Infer [8/40]. 146.99 samples/sec. 1741.606 ms/step.
Infer [16/40]. 146.88 samples/sec. 1742.921 ms/step.
Infer [24/40]. 146.81 samples/sec. 1743.707 ms/step.
Infer [32/40]. 146.75 samples/sec. 1744.444 ms/step.
Infer [40/40]. 146.71 samples/sec. 1744.900 ms/step.
Inference benchmark of vit_huge_patch14_clip_336.laion2b_ft_in12k_in1k done. 146.71 samples/sec, 1744.90 ms/step
Model vit_huge_patch14_clip_336.laion2b_ft_in12k_in1k created, param count: 632456680
Running train benchmark on vit_huge_patch14_clip_336.laion2b_ft_in12k_in1k for 40 steps w/ input size (3, 336, 336) and batch size 256.
ERROR: "CUDA out of memory. Tried to allocate 1.41 GiB. GPU 0 has a total capacty of 23.65 GiB of which 540.06 MiB is free. Including non-PyTorch memory, this process has 23.11 GiB memory in use. Of the allocated memory 21.48 GiB is allocated by PyTorch, and 412.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 192 for retry.
Model vit_huge_patch14_clip_336.laion2b_ft_in12k_in1k created, param count: 632456680
Running train benchmark on vit_huge_patch14_clip_336.laion2b_ft_in12k_in1k for 40 steps w/ input size (3, 336, 336) and batch size 192.
ERROR: "CUDA out of memory. Tried to allocate 1.06 GiB. GPU 0 has a total capacty of 23.65 GiB of which 224.06 MiB is free. Including non-PyTorch memory, this process has 23.42 GiB memory in use. Of the allocated memory 21.70 GiB is allocated by PyTorch, and 500.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 128 for retry.
Model vit_huge_patch14_clip_336.laion2b_ft_in12k_in1k created, param count: 632456680
Running train benchmark on vit_huge_patch14_clip_336.laion2b_ft_in12k_in1k for 40 steps w/ input size (3, 336, 336) and batch size 128.
ERROR: "CUDA out of memory. Tried to allocate 182.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 164.06 MiB is free. Including non-PyTorch memory, this process has 23.48 GiB memory in use. Of the allocated memory 22.11 GiB is allocated by PyTorch, and 145.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 96 for retry.
Model vit_huge_patch14_clip_336.laion2b_ft_in12k_in1k created, param count: 632456680
Running train benchmark on vit_huge_patch14_clip_336.laion2b_ft_in12k_in1k for 40 steps w/ input size (3, 336, 336) and batch size 96.
ERROR: "CUDA out of memory. Tried to allocate 542.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 356.06 MiB is free. Including non-PyTorch memory, this process has 23.29 GiB memory in use. Of the allocated memory 21.54 GiB is allocated by PyTorch, and 528.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 64 for retry.
Model vit_huge_patch14_clip_336.laion2b_ft_in12k_in1k created, param count: 632456680
Running train benchmark on vit_huge_patch14_clip_336.laion2b_ft_in12k_in1k for 40 steps w/ input size (3, 336, 336) and batch size 64.
ERROR: "CUDA out of memory. Tried to allocate 362.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 310.06 MiB is free. Including non-PyTorch memory, this process has 23.34 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 318.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 48 for retry.
Model vit_huge_patch14_clip_336.laion2b_ft_in12k_in1k created, param count: 632456680
Running train benchmark on vit_huge_patch14_clip_336.laion2b_ft_in12k_in1k for 40 steps w/ input size (3, 336, 336) and batch size 48.
ERROR: "CUDA out of memory. Tried to allocate 272.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 118.06 MiB is free. Including non-PyTorch memory, this process has 23.53 GiB memory in use. Of the allocated memory 21.97 GiB is allocated by PyTorch, and 328.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 32 for retry.
Model vit_huge_patch14_clip_336.laion2b_ft_in12k_in1k created, param count: 632456680
Running train benchmark on vit_huge_patch14_clip_336.laion2b_ft_in12k_in1k for 40 steps w/ input size (3, 336, 336) and batch size 32.
ERROR: "CUDA out of memory. Tried to allocate 46.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 44.06 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 22.20 GiB is allocated by PyTorch, and 165.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" while running benchmark.
WARNING: Reducing batch size to 24 for retry.
Model vit_huge_patch14_clip_336.laion2b_ft_in12k_in1k created, param count: 632456680
Running train benchmark on vit_huge_patch14_clip_336.laion2b_ft_in12k_in1k for 40 steps w/ input size (3, 336, 336) and batch size 24.
Train [8/40]. 45.88 samples/sec. 523.105 ms/step.
Train [16/40]. 45.88 samples/sec. 523.094 ms/step.
Train [24/40]. 45.88 samples/sec. 523.089 ms/step.
Train [32/40]. 45.88 samples/sec. 523.081 ms/step.
Train [40/40]. 45.88 samples/sec. 523.080 ms/step.
Train benchmark of vit_huge_patch14_clip_336.laion2b_ft_in12k_in1k done. 45.68 samples/sec, 523.08 ms/sample
Benchmarking in float16 precision. NCHW layout. torchscript disabled
Model vit_huge_patch16_448_ijepa.in1k created, param count: 631668480
Running inference benchmark on vit_huge_patch16_448_ijepa.in1k for 40 steps w/ input size (3, 224, 224) and batch size 256.
Traceback (most recent call last):
  File "/home/nenkoru/pytorch-image-models/benchmark.py", line 696, in <module>
    main()
  File "/home/nenkoru/pytorch-image-models/benchmark.py", line 656, in main
    r = benchmark(args)
  File "/home/nenkoru/pytorch-image-models/benchmark.py", line 605, in benchmark
    run_results = _try_run(
  File "/home/nenkoru/pytorch-image-models/benchmark.py", line 552, in _try_run
    results = bench.run()
  File "/home/nenkoru/pytorch-image-models/benchmark.py", line 325, in run
    _step()
  File "/home/nenkoru/pytorch-image-models/benchmark.py", line 313, in _step
    output = self.model(self.example_inputs)
  File "/home/nenkoru/miniconda3/envs/starcoder/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1505, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/nenkoru/miniconda3/envs/starcoder/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1514, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/nenkoru/pytorch-image-models/timm/models/vision_transformer.py", line 632, in forward
    x = self.forward_features(x)
  File "/home/nenkoru/pytorch-image-models/timm/models/vision_transformer.py", line 613, in forward_features
    x = self.patch_embed(x)
  File "/home/nenkoru/miniconda3/envs/starcoder/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1505, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/nenkoru/miniconda3/envs/starcoder/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1514, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/nenkoru/pytorch-image-models/timm/layers/patch_embed.py", line 69, in forward
    _assert(H == self.img_size[0], f"Input height ({H}) doesn't match model ({self.img_size[0]}).")
  File "/home/nenkoru/miniconda3/envs/starcoder/lib/python3.10/site-packages/torch/__init__.py", line 1380, in _assert
    assert condition, message
AssertionError: Input height (224) doesn't match model (448).
